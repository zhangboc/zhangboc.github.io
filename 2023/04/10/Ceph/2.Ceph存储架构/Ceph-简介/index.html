<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Ceph简介 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="article">
<meta property="og:title" content="Ceph简介">
<meta property="og:url" content="https://github.com/zhangboc/zhangboc.github.io.git/2023/04/10/Ceph/2.Ceph%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84/Ceph-%E7%AE%80%E4%BB%8B/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://github.com/images/Ceph%E7%AE%80%E4%BB%8B/1.jpg">
<meta property="article:published_time" content="2023-04-09T16:00:00.000Z">
<meta property="article:modified_time" content="2023-04-11T09:39:24.608Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Ceph简介">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/images/Ceph%E7%AE%80%E4%BB%8B/1.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://github.com/zhangboc/zhangboc.github.io.git"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Ceph/2.Ceph存储架构/Ceph-简介" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/04/10/Ceph/2.Ceph%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84/Ceph-%E7%AE%80%E4%BB%8B/" class="article-date">
  <time class="dt-published" datetime="2023-04-09T16:00:00.000Z" itemprop="datePublished">2023-04-10</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Ceph/">Ceph</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Ceph简介
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1><span id></span></h1><span id="more"></span>

<!-- toc -->

<ul>
<li><a href="#ceph-%E7%AE%80%E4%BB%8B">CEPH 简介</a><ul>
<li><a href="#%E5%BB%BA%E8%AE%AE">建议</a></li>
<li><a href="#%E7%A1%AC%E4%BB%B6%E6%8E%A8%E8%8D%90">硬件推荐</a><ul>
<li><a href="#cpu">CPU</a></li>
<li><a href="#%E5%86%85%E5%AD%98">内存</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8">数据存储</a></li>
<li><a href="#%E7%BD%91%E7%BB%9C">网络</a></li>
<li><a href="#%E6%95%85%E9%9A%9C%E5%9F%9F">故障域</a></li>
</ul>
</li>
<li><a href="#%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%BB%BA%E8%AE%AE">操作系统建议</a><ul>
<li><a href="#ceph-%E4%BE%9D%E8%B5%96%E9%A1%B9">CEPH 依赖项</a></li>
<li><a href="#%E5%B9%B3%E5%8F%B0">平台</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->

<h1><span id="ceph-简介">CEPH 简介</span></h1><p>无论您是想为云平台提供Ceph 对象存储和&#x2F;或 Ceph 块设备服务、部署Ceph 文件系统还是将 Ceph 用于其他目的，所有 Ceph 存储集群部署都从设置每个 Ceph 节点、您的网络和 Ceph存储集群。一个 Ceph 存储集群至少需要一个 Ceph Monitor、Ceph Manager 和 Ceph OSD（Object Storage Daemon）。运行 Ceph 文件系统客户端时也需要 Ceph 元数据服务器。<br><img src="/images/Ceph%E7%AE%80%E4%BB%8B/1.jpg"></p>
<ul>
<li>Monitors：Ceph Monitor ( ceph-mon) 维护集群状态图，包括监视器图、管理器图、OSD 图、MDS 图和 CRUSH 图。这些映射是 Ceph 守护进程相互协调所需的关键集群状态。监视器还负责管理守护进程和客户端之间的身份验证。通常至少需要三个监视器才能实现冗余和高可用性。</li>
<li>管理器：Ceph 管理器守护进程 ( ceph-mgr) 负责跟踪运行时指标和 Ceph 集群的当前状态，包括存储利用率、当前性能指标和系统负载。Ceph Manager 守护进程还托管基于 python 的模块来管理和公开 Ceph 集群信息，包括基于 Web 的Ceph Dashboard和 REST API。高可用性通常至少需要两个管理器。</li>
<li>Ceph OSDs：一个对象存储守护进程（Ceph OSD， ceph-osd）存储数据，处理数据复制，恢复，重新平衡，并通过检查其他 Ceph OSD 守护进程的心跳向 Ceph Monitors 和 Managers 提供一些监控信息。通常至少需要三个 Ceph OSD 才能实现冗余和高可用性。</li>
<li>MDS：Ceph 元数据服务器（MDS ceph-mds）代表Ceph 文件系统存储元数据（即 Ceph 块设备和 Ceph 对象存储不使用 MDS）。Ceph 元数据服务器允许 POSIX 文件系统用户执行基本命令（如 ls、find等），而不会给 Ceph 存储集群带来巨大负担。</li>
</ul>
<p>Ceph 将数据作为对象存储在逻辑存储池中。使用 CRUSH算法，Ceph 计算出哪个归置组 (PG) 应该包含该对象，以及哪个 OSD 应该存储该归置组。CRUSH 算法使 Ceph 存储集群能够动态扩展、重新平衡和恢复。</p>
<h2><span id="建议">建议</span></h2><p>要开始在生产中使用 Ceph，您应该查看我们的硬件建议和操作系统建议。</p>
<ul>
<li>硬件推荐<ul>
<li>中央处理器</li>
<li>内存</li>
<li>记忆</li>
<li>数据存储</li>
<li>网络</li>
<li>故障域</li>
<li>最低硬件建议</li>
</ul>
</li>
<li>操作系统建议<ul>
<li>Ceph 依赖项</li>
<li>平台</li>
</ul>
</li>
</ul>
<h2><span id="硬件推荐">硬件推荐</span></h2><p>Ceph 被设计为在商用硬件上运行，这使得构建和维护 PB 级数据集群在经济上是可行的。在规划集群硬件时，您需要平衡许多考虑因素，包括故障域和潜在的性能问题。硬件规划应该包括在许多主机上分布 Ceph 守护进程和其他使用 Ceph 的进程。通常，我们建议在为特定类型的守护进程配置的主机上运行特定类型的 Ceph 守护进程。我们建议将其他主机用于利用您的数据集群的进程（例如，OpenStack、CloudStack 等）。<br>也可以查看<a target="_blank" rel="noopener" href="https://ceph.com/community/blog/">Ceph 博客。</a></p>
<h3><span id="cpu">CPU</span></h3><p>CephFS 元数据服务器 (MDS) 是 CPU 密集型的。因此，CephFS 元数据服务器 (MDS) 应具有四核（或更好）CPU 和高时钟频率 (GHz)。OSD 节点需要足够的处理能力来运行 RADOS 服务、使用 CRUSH 计算数据放置、复制数据以及维护它们自己的集群映射副本。</p>
<blockquote>
<p><strong>一个 Ceph 集群的要求与另一个集群的要求不同，但这里有一些通用准则。</strong></p>
</blockquote>
<p>在 Ceph 的早期版本中，我们会根据每个 OSD 的核心数来提出硬件建议，但这个每个 OSD 的核心数指标不再像每个 IOP 的周期数和每个 OSD 的 IOP 数一样有用。例如，对于 NVMe 驱动器，Ceph 可以轻松地在真实集群上使用五个或六个内核，并在单个 OSD 上隔离使用多达大约十四个内核。因此，每个 OSD 的核心不再像以前那样紧迫。选择硬件时，选择每个内核的 IOP。</p>
<p>监控节点和管理器节点对 CPU 的要求不高，只需要适度的处理器。如果您的主机除了运行 Ceph 守护进程外还将运行 CPU 密集型进程，请确保您有足够的处理能力来运行 CPU 密集型进程和 Ceph 守护进程。（OpenStack Nova 是 CPU 密集型进程的一个例子。）我们建议您在单独的主机上（即，在不是您的监视器和管理器节点的主机上）运行非 Ceph CPU 密集型进程，以避免占用资源争论。</p>
<h3><span id="内存">内存</span></h3><p>通常，RAM 越大越好。适度集群的监视器&#x2F;管理器节点可能使用 64GB 就可以了；对于拥有数百个 OSD 的更大集群，128GB 是一个合理的目标。BlueStore OSD 的内存目标默认为 4GB。考虑到操作系统和管理任务（如监控和指标）的谨慎余量以及恢复期间增加的消耗：建议为每个 BlueStore OSD 预配 ~8GB。</p>
<h4><span id="监视器和管理器ceph-mon-和-ceph-mgr">监视器和管理器（CEPH-MON 和 CEPH-MGR）</span></h4><p>监视器和管理器守护进程的内存使用量通常随集群的大小而变化。请注意，在启动时以及拓扑更改和恢复期间，这些守护程序将需要比稳态操作期间更多的 RAM，因此请计划使用高峰期。对于非常小的集群，32 GB 就足够了。对于多达 300 个 OSD 的集群，需要 64GB</p>
<h4><span id="元数据服务器-ceph-mds">元数据服务器 (CEPH-MDS)</span></h4><p>元数据守护程序内存利用率取决于其缓存配置为消耗的内存量。对于大多数系统，我们建议至少使用 1 GB。 看mds_cache_memory。</p>
<h4><span id="记忆">记忆</span></h4><p>Bluestore 使用自己的内存来缓存数据，而不是依赖操作系统的页面缓存。在 Bluestore 中，您可以通过更改配置选项来调整 OSD 尝试消耗的内存量osd_memory_target 。</p>
<ul>
<li>通常不建议设置osd_memory_target低于 2GB（Ceph 可能无法将内存消耗保持在 2GB 以下，这可能会导致性能极慢）。</li>
<li>将内存目标设置在 2GB 和 4GB 之间通常可行，但可能会导致性能下降，因为元数据可能会在 IO 期间从磁盘读取，除非活动数据集相对较小。</li>
<li>4GB 是当前的默认osd_memory_target大小。此默认值是为典型用例选择的，旨在平衡内存需求和 OSD 性能。</li>
<li>osd_memory_target当有很多（小）对象或处理大（256GB&#x2F;OSD 或更多）数据集时，设置高于 4GB 可以提高性能。</li>
</ul>
<blockquote>
<p>OSD 内存自动调整是“尽力而为”。虽然 OSD 可以取消映射内存以允许内核回收它，但不能保证内核会在特定时间范围内实际回收释放的内存。这尤其适用于旧版本的 Ceph，其中透明大页面可以防止内核回收从碎片大页面中释放的内存。现代版本的 Ceph 在应用程序级别禁用透明大页面以避免这种情况，尽管这仍然不能保证内核会立即回收未映射的内存。OSD 有时仍可能超出其内存目标。我们建议在您的系统上预算大约 20% 的额外内存，以防止 OSD 在临时峰值期间或由于内核回收已释放页面的任何延迟而出现 OOM。</p>
</blockquote>
<p>使用旧版 FileStore 后端时，页面缓存用于缓存数据，因此通常不需要调整。<br>使用旧版 FileStore 后端时，OSD 内存消耗与系统中每个守护进程的 PG 数量有关。</p>
<h3><span id="数据存储">数据存储</span></h3><p>仔细规划您的数据存储配置。在规划数据存储时，需要考虑显着的成本和性能权衡。同时进行的操作系统操作以及多个守护进程对单个驱动器的读写操作的同时请求会大大降低性能。</p>
<h4><span id="硬盘驱动器">硬盘驱动器</span></h4><p>OSD 应该有足够的硬盘驱动器空间来存储对象数据。我们建议最小硬盘驱动器大小为 1 TB。考虑更大磁盘的每 GB 成本优势。我们建议将硬盘驱动器的价格除以千兆字节数，得出每千兆字节的成本，因为更大的驱动器可能会对每千兆字节的成本产生重大影响。例如，价格为 75.00 美元的 1 TB 硬盘的成本为每 GB 0.07 美元（即 75 美元 &#x2F; 1024 &#x3D; 0.0732）。相比之下，价格为 150.00 美元的 3 TB 硬盘的成本为每 GB 0.05 美元（即 150 美元 &#x2F; 3072 &#x3D; 0.0488）。在前面的示例中，使用 1 TB 的磁盘通常会使每 GB 的成本增加 40%——使您的集群的成本效率大大降低。</p>
<blockquote>
<p>在单个 SAS &#x2F; SATA 驱动器上运行多个 OSD 不是一个好主意。但是，NVMe 驱动器可以通过拆分为两个或更多 OSD 来提高性能。<br>在单个驱动器上运行 OSD 和监视器或元数据服务器也不是一个好主意。</p>
</blockquote>
<p>存储驱动器受寻道时间、访问时间、读写时间以及总吞吐量的限制。这些物理限制会影响整体系统性能，尤其是在恢复期间。我们建议为操作系统和软件使用专用（最好是镜像）驱动器，并为主机上运行的每个 Ceph OSD 守护进程使用一个驱动器（上面的模数 NVMe）。许多不是由硬件故障引起的“慢 OSD”问题是由于在同一驱动器上运行操作系统和多个 OSD 而引起的。由于在小型集群上解决性能问题的成本可能超过额外磁盘驱动器的成本，因此您可以通过避免让 OSD 存储驱动器负担过重的诱惑来优化您的集群设计规划。</p>
<blockquote>
<p>您可以在每个 SAS &#x2F; SATA 驱动器上运行多个 Ceph OSD 守护进程，但这可能会导致资源争用并降低整体吞吐量。</p>
</blockquote>
<h4><span id="态硬盘">态硬盘</span></h4><p>性能改进的一个机会是使用固态驱动器 (SSD) 来减少随机访问时间和读取延迟，同时加快吞吐量。与硬盘驱动器相比，SSD 每 GB 的成本通常是硬盘驱动器的 10 倍以上，但 SSD 的访问时间通常至少比硬盘驱动器快 100 倍。<br>SSD 没有移动机械部件，因此它们不一定受到与硬盘驱动器相同类型的限制。SSD 确实有很大的局限性。在评估 SSD 时，重要的是要考虑顺序读写的性能。</p>
<blockquote>
<p>我们建议探索使用 SSD 来提高性能。但是，在对 SSD 进行重大投资之前，我们强烈建议查看 SSD 的性能指标并在测试配置中测试 SSD 以衡量性能。</p>
</blockquote>
<p>相对便宜的 SSD 可能会吸引您的经济意识。谨慎使用。选择用于 Ceph 的 SSD 时，可接受的 IOPS 是不够的。</p>
<p>SSD 在历史上一直是对象存储的成本高昂，但新兴的 QLC 驱动器正在缩小差距。通过将 WAL+DB 卸载到 SSD，HDD OSD 可能会看到显着的性能提升。</p>
<p>Ceph 加速 CephFS 文件系统性能的一种方法是将 CephFS 元数据的存储与 CephFS 文件内容的存储分开。Ceph 为 CephFS 元数据提供了一个默认metadata池。您永远不必为 CephFS 元数据创建一个池，但您可以为您的 CephFS 元数据池创建一个仅指向主机的 SSD 存储介质的 CRUSH 映射层次结构。有关详细信息，请参阅 <a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rados/operations/crush-map-edits/#crush-map-device-class">CRUSH 设备类</a>。</p>
<h4><span id="控制器">控制器</span></h4><p>磁盘控制器 (HBA) 会对写入吞吐量产生重大影响。仔细考虑您的选择以确保它们不会造成性能瓶颈。值得注意的是，RAID 模式 (IR) HBA 可能比更简单的“JBOD”(IT) 模式 HBA 表现出更高的延迟，并且 RAID SoC、写缓存和电池备份会显着增加硬件和维护成本。某些 RAID HBA 可以配置有 IT 模式“个性”。</p>
<p><a target="_blank" rel="noopener" href="https://ceph.com/community/blog/">Ceph 博客</a>通常是有关 Ceph 性能问题的极佳信息来源。有关更多详细信息，请参阅<a target="_blank" rel="noopener" href="https://ceph.com/community/ceph-performance-part-1-disk-controller-write-throughput/">Ceph 写入吞吐量 1</a>和<a target="_blank" rel="noopener" href="https://ceph.com/community/ceph-performance-part-2-write-throughput-without-ssd-journals/">Ceph 写入吞吐量 2</a>。</p>
<h4><span id="对标">对标</span></h4><p>BlueStore 在 O_DIRECT 中打开块设备，并频繁使用 fsync 以确保数据安全地持久化到介质中。您可以使用 评估驱动器的低级写入性能fio。例如，4kB 随机写性能测量如下：</p>
<pre><code>#fio --name=/dev/sdX --ioengine=libaio --direct=1 --fsync=1 --readwrite=randwrite --blocksize=4k --runtime=300
</code></pre>
<h4><span id="写缓存">写缓存</span></h4><p>企业 SSD 和 HDD 通常包括断电保护功能，使用多级缓存来加速直接或同步写入。这些设备可以在两种缓存模式之间切换——易失性缓存通过 fsync 刷新到持久介质，或同步写入的非易失性缓存。</p>
<p>通过“启用”或“禁用”写入（易失性）缓存来选择这两种模式。当启用易失性缓存时，Linux 使用“回写”模式的设备，禁用时，它使用“直写”。</p>
<p>默认配置（通常启用缓存）可能不是最佳配置，并且 OSD 性能可能会通过禁用写缓存在增加 IOPS 和减少 commit_latency 方面得到显着提高。</p>
<p>因此，鼓励用户fio如前所述对他们的设备进行基准测试，并为他们的设备保留最佳缓存配置。<br>hdparm可以使用、sdparm或 smartctl读取中的值来查询缓存配置&#x2F;sys&#x2F;class&#x2F;scsi_disk&#x2F;*&#x2F;cache_type，例如：</p>
<pre><code>#hdparm -W /dev/sda

/dev/sda:
 write-caching =  1 (on)

# sdparm --get WCE /dev/sda
    /dev/sda: ATA       TOSHIBA MG07ACA1  0101
WCE           1  [cha: y]
# smartctl -g wcache /dev/sda
smartctl 7.1 2020-04-05 r5049 [x86_64-linux-4.18.0-305.19.1.el8_4.x86_64] (local build)
Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.org

Write cache is:   Enabled

# cat /sys/class/scsi_disk/0\:0\:0\:0/cache_type
write back
</code></pre>
<p>可以使用相同的工具禁用写缓存：</p>
<pre><code>#hdparm -W0 /dev/sda

/dev/sda:
 setting drive write-caching to 0 (off)
 write-caching =  0 (off)

# sdparm --clear WCE /dev/sda
    /dev/sda: ATA       TOSHIBA MG07ACA1  0101
# smartctl -s wcache,off /dev/sda
smartctl 7.1 2020-04-05 r5049 [x86_64-linux-4.18.0-305.19.1.el8_4.x86_64] (local build)
Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.org

=== START OF ENABLE/DISABLE COMMANDS SECTION ===
Write cache disabled
</code></pre>
<p>hdparm通常，使用、sdparm或禁用缓存smartctl 会导致 cache_type 自动更改为“write through”。如果不是这样，你可以尝试直接如下设置。（用户应注意，设置 cache_type 也会正确保留设备的缓存模式，直到下一次重启）：</p>
<pre><code>#echo &quot;write through&quot; &gt; /sys/class/scsi_disk/0\:0\:0\:0/cache_type

# hdparm -W /dev/sda

/dev/sda:
 write-caching =  0 (off)
</code></pre>
<p>这个 udev 规则（在 CentOS 8 上测试过）会将所有 SATA&#x2F;SAS 设备 cache_types 设置为“write through”：</p>
<pre><code>#cat /etc/udev/rules.d/99-ceph-write-through.rules
ACTION==&quot;add&quot;, SUBSYSTEM==&quot;scsi_disk&quot;, ATTR&#123;cache_type&#125;:=&quot;write through&quot;
</code></pre>
<p>这个 udev 规则（在 CentOS 7 上测试过）会将所有 SATA&#x2F;SAS 设备 cache_types 设置为“write through”：</p>
<pre><code>#cat /etc/udev/rules.d/99-ceph-write-through-el7.rules
ACTION==&quot;add&quot;, SUBSYSTEM==&quot;scsi_disk&quot;, RUN+=&quot;/bin/sh -c &#39;echo write through &gt; /sys/class/scsi_disk/$kernel/cache_type&#39;&quot;
</code></pre>
<p>该sdparm实用程序可用于一次查看&#x2F;更改多个设备上的易失性写入缓存：</p>
<pre><code>#sdparm --get WCE /dev/sd*
    /dev/sda: ATA       TOSHIBA MG07ACA1  0101
WCE           0  [cha: y]
    /dev/sdb: ATA       TOSHIBA MG07ACA1  0101
WCE           0  [cha: y]
# sdparm --clear WCE /dev/sd*
    /dev/sda: ATA       TOSHIBA MG07ACA1  0101
    /dev/sdb: ATA       TOSHIBA MG07ACA1  0101
</code></pre>
<h4><span id="其他注意事项">其他注意事项</span></h4><p>您通常会在每个主机上运行多个 OSD，但您应该确保 OSD 驱动器的总吞吐量不超过满足客户端读取或写入数据需求所需的网络带宽。您还应该考虑集群在每个主机上存储的总数据的百分比。如果特定主机上的百分比很大并且该主机发生故障，则可能导致诸如超过 之类的问题，这会导致 Ceph 停止操作以作为防止数据丢失的安全预防措施。full ratio</p>
<p>当您在每个主机上运行多个 OSD 时，您还需要确保内核是最新的。请参阅<a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/start/os-recommendations">OS Recommendations</a>了解有关注意事项glibc并 syncfs(2)确保您的硬件在每个主机运行多个 OSD 时按预期运行。</p>
<h3><span id="网络">网络</span></h3><p>在您的机架中提供至少 10 Gb&#x2F;s 的网络。</p>
<h4><span id="速度">速度</span></h4><p>在 1 Gb&#x2F;s 网络上复制 1 TB 数据需要三个小时，在 1 Gb&#x2F;s 网络上复制 10 TB 数据需要三十个小时。但是在 10 Gb&#x2F;s 网络上复制 1 TB 数据只需要 20 分钟，而在 10 Gb&#x2F;s 网络上复制 10 TB 数据只需要一个小时。</p>
<h4><span id="成本">成本</span></h4><p>Ceph 集群越大，OSD 故障就越常见。degraded归置组 (PG) 从一种状态恢复到另一种状态的速度越快越好。值得注意的是，快速恢复将可能导致数据暂时不可用甚至丢失的多重重叠故障的可能性降至最低。当然，在配置您的网络时，您必须在价格与性能之间取得平衡。active + clean</p>
<p>一些部署工具使用 VLAN 来使硬件和网络布线更易于管理。使用 802.1q 协议的 VLAN 需要支持 VLAN 的 NIC 和交换机。该硬件的额外费用可能会被网络设置和维护方面节省的运营成本所抵消。当使用 VLAN 处理集群和计算堆栈（例如 OpenStack、CloudStack 等）之间的 VM 流量时，使用 10 Gb&#x2F;s 以太网或更好的以太网具有额外的价值；截至 2022 年，40 Gb&#x2F;s 或25&#x2F;50&#x2F;100 Gb&#x2F;s 网络在生产集群中很常见。</p>
<blockquote>
<p>架顶式 (TOR) 交换机还需要快速和冗余的上行链路来旋转主干交换机&#x2F;路由器，通常至少为 40 Gb&#x2F;s。</p>
</blockquote>
<h4><span id="底板管理控制器-bmc">底板管理控制器 (BMC)</span></h4><p>您的服务器机箱应该有底板管理控制器 (BMC)。众所周知的例子是 iDRAC (Dell)、CIMC (Cisco UCS) 和 iLO (HPE)。管理和部署工具也可能广泛使用 BMC，尤其是通过 IPMI 或 Redfish，因此请考虑带外网络在安全性和管理方面的成本&#x2F;收益权衡。Hypervisor SSH 访问、VM 映像上传、OS 映像安装、管理套接字等会给网络带来巨大的负载。运行三个网络似乎有点矫枉过正，但每个流量路径都代表潜在的容量、吞吐量和&#x2F;或性能瓶颈，您在部署大规模数据集群之前应该仔细考虑。</p>
<h3><span id="故障域">故障域</span></h3><p>故障域是阻止访问一个或多个 OSD 的任何故障。那可能是主机上停止的守护进程；硬盘故障、操作系统崩溃、网卡故障、电源故障、网络中断、停电等等。在规划您的硬件需求时，您必须权衡通过将过多的责任置于过少的故障域中来降低成本的诱惑与隔离每个潜在故障域的额外成本之间的平衡。</p>
<h4><span id="最低硬件建议">最低硬件建议</span></h4><p>Ceph 可以在廉价的商用硬件上运行。小型生产集群和开发集群可以使用适度的硬件成功运行。</p>
<table>
    <tr>
        <td>过程</td>
        <td>标准</td>
        <td>最低推荐</td>
    </tr>
    <tr>
        <td rowspan="5">ceph-osd</td>
    </tr>
        <tr>
            <td>处理器</td>
            <td>最少 1 个核心 <br>  每 200-500 MB/s 1 个核心 <br> 每 1000-3000 IOPS 1 个核心  <br>  结果在复制之前。 <br>  结果可能因不同的 CPU 型号和 Ceph 功能而异。（纠删码、压缩等）。<br> ARM 处理器可能特别需要额外的内核。 <br> 实际性能取决于许多因素，包括驱动器、网络和客户端吞吐量和延迟。
强烈建议进行基准测试。 </td>
        </tr>
        <tr>
            <td>内存</td>
            <td>每个守护进程 4GB+（越多越好）<br> 2-4GB 经常运行（可能很慢） <br> 小于 2GB 不推荐 </td>
        </tr>
        <tr>
            <td>卷存储</td>
            <td>每个守护进程 1 个存储驱动器 </td>
        </tr>
        <tr>
            <td>数据库/文件</td>
            <td>每个守护进程 1 个 SSD 分区（可选） </td>
        </tr>
        <tr>
            <td>网络</td>
            <td>1x 1GbE+ NIC（推荐 10GbE+） </td>
        </tr>
    <tr>
        <td rowspan="4">ceph-mon</td>
    </tr>
        <tr>
            <td>处理器</td>
            <td>最少 2 个核心 </td>
        </tr>
        <tr>
            <td>内存</td>
            <td>每个守护进程 2-4GB+ </td>
        </tr>
        <tr>
        <td>磁盘空间</td>
        <td>每个守护进程 60 GB </td>
        </tr>
        <tr>
            <td>网络</td>
            <td>1x 1GbE+ NIC</td>
        </tr>
    <tr>
        <td rowspan="4">ceph-mds</td>
    </tr>
        <tr>
            <td>处理器</td>
            <td>最少 2 个核心 </td>
        </tr>
        <tr>
            <td>内存</td>
            <td>每个守护进程 2GB+ </td>
        </tr>
        <tr>
            <td>磁盘空间</td>
            <td>每个守护进程 1MB</td>
        </tr>
        <tr>
            <td>网络</td>
            <td>1x 1GbE+ NIC</td>
        </tr>
</table>

<blockquote>
<p>如果您使用单个磁盘运行 OSD，请为您的卷存储创建一个分区，该分区与包含操作系统的分区分开。通常，我们建议操作系统和卷存储使用单独的磁盘。</p>
</blockquote>
<h2><span id="操作系统建议">操作系统建议</span></h2><h3><span id="ceph-依赖项">CEPH 依赖项</span></h3><p>作为一般规则，我们建议在较新版本的 Linux 上部署 Ceph。我们还建议在具有长期支持的版本上进行部署。</p>
<h4><span id="内核">内核</span></h4><blockquote>
<p>Ceph 内核客户端</p>
</blockquote>
<p>如果您使用内核客户端映射 RBD 块设备或挂载 CephFS，一般建议是使用 <a target="_blank" rel="noopener" href="http://kernel.org/">http://kernel.org</a> 或您在任何客户端上的 Linux 发行版提供的“稳定”或“长期维护”内核系列主机。</p>
<p>对于 RBD，如果选择跟踪长期内核，我们目前推荐基于 4.x 的“长期维护”内核系列或更高版本：</p>
<p>对于 RBD，如果选择跟踪长期内核，我们目前推荐基于 4.x 的“长期维护”内核系列或更高版本：</p>
<ul>
<li>4.19.z</li>
<li>4.14.z</li>
<li>5.x</li>
</ul>
<p>对于 CephFS，请参阅有关使用内核驱动程序安装 CephFS 的部分 以获取内核版本指南。</p>
<p>较旧的内核客户端版本可能不支持您的CRUSH 可调配置文件或 Ceph 集群的其他较新功能，需要将存储集群配置为禁用这些功能。</p>
<h3><span id="平台">平台</span></h3><p>下面的图表显示了 Ceph 的需求如何映射到各种 Linux 平台。一般而言，对内核和系统初始化包（即 sysvinit、systemd）之外的特定发行版的依赖性非常小。</p>
<table>
    <tr>
        <td>Release Name</td>
        <td>Tag</td>
        <td>CentOS</td>
        <td>Ubuntu</td>
        <td>OpenSUSE C</td>
        <td>Debian C</td>
    </tr>
        <tr>
            <td>Quincy</td>
            <td>17.2.z</td>
            <td>8 A</td>
            <td>20.04 A</td>
            <td>15.3</td>
            <td>11</td>
        </tr>
        <tr>
            <td>Pacific</td>
            <td>16.2.z</td>
            <td>8 A</td>
            <td>18.04 C,<br>20.04 A</td>
            <td>15.2</td>
            <td>10<br>11</td>
        </tr>
        <tr>
            <td>Octopus</td>
            <td>15.2.z</td>
            <td>7 B <br> 8 A</td>
            <td>18.04 C,<br>20.04 A</td>
            <td>15.2</td>
            <td>10</td>
        </tr>
</table>

<ul>
<li>A : Ceph 提供软件包，并对其中的软件进行了全面的测试。</li>
<li>B : Ceph 提供了软件包，并对其中的软件做了基本的测试。</li>
<li>C : Ceph 只提供包。尚未对这些版本进行任何测试。</li>
</ul>
<blockquote>
<p>Centos 7 用户：Btrfs在 Octopus 版本中不再在 Centos 7 上进行测试。我们建议bluestore改用。</p>
</blockquote>
<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">
      
    </div>
    <footer class="article-footer">
      <a data-url="https://github.com/zhangboc/zhangboc.github.io.git/2023/04/10/Ceph/2.Ceph%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84/Ceph-%E7%AE%80%E4%BB%8B/" data-id="clgd2kqbb000aso5f84215zp8" data-title="Ceph简介" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Ceph%E7%AE%80%E4%BB%8B/" rel="tag">Ceph简介</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/04/10/Ceph/0.Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Ceph故障排查
        
      </div>
    </a>
  
  
    <a href="/2023/04/08/chatjs/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hexo博客中插入 Chart 动态图表</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Ceph/">Ceph</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ceph/" rel="tag">Ceph</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ceph-FS/" rel="tag">Ceph FS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ceph%E7%AE%80%E4%BB%8B/" rel="tag">Ceph简介</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RBD/" rel="tag">RBD</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RGW/" rel="tag">RGW</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/chatjs/" rel="tag">chatjs</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5/" rel="tag">故障排查</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Ceph/" style="font-size: 20px;">Ceph</a> <a href="/tags/Ceph-FS/" style="font-size: 10px;">Ceph FS</a> <a href="/tags/Ceph%E7%AE%80%E4%BB%8B/" style="font-size: 10px;">Ceph简介</a> <a href="/tags/RBD/" style="font-size: 15px;">RBD</a> <a href="/tags/RGW/" style="font-size: 10px;">RGW</a> <a href="/tags/chatjs/" style="font-size: 10px;">chatjs</a> <a href="/tags/%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5/" style="font-size: 10px;">故障排查</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/04/11/Ceph/3.Ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/Ceph-%E5%AE%89%E8%A3%85/">Ceph-安装</a>
          </li>
        
          <li>
            <a href="/2023/04/11/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B/">RBD写入流程</a>
          </li>
        
          <li>
            <a href="/2023/04/11/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%9D%97%E5%88%9B%E5%BB%BA%E5%8F%8A%E6%98%A0%E5%B0%84/">RBD块创建及映射</a>
          </li>
        
          <li>
            <a href="/2023/04/11/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%9D%97%E5%AD%98%E5%82%A8%E6%89%A9%E5%AE%B9%E5%8F%8A%E7%BC%A9%E5%AE%B9/">RBD存储扩容/缩容</a>
          </li>
        
          <li>
            <a href="/2023/04/11/Ceph/5.RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/">RGW对象存储</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script src="/live2D/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2D/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":true,"model":{"jsonPath":"/live2D/assets/xiaomai.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":1},"log":false});</script></body>
</html>