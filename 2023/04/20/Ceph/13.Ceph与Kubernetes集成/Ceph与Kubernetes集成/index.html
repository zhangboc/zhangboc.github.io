<!DOCTYPE html>

<html lang="en">

<head>
  
  <meta name="baidu-site-verification" content="code-J1Qg17G6wT" />
  <title>Ceph与Kubernetes集成 - Hexo</title>
  <meta charset="UTF-8">
  <meta name="description" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5">
  
  

    <!-- Site Verification -->
    <meta name="baidu-site-verification" content="code-J1Qg17G6wT" />

  <link rel="shortcut icon" href="/images/head/head.jpg" type="image/png" />
  <meta property="og:type" content="article">
<meta property="og:title" content="Ceph与Kubernetes集成">
<meta property="og:url" content="https://zhangboc.github.io/2023/04/20/Ceph/13.Ceph%E4%B8%8EKubernetes%E9%9B%86%E6%88%90/Ceph%E4%B8%8EKubernetes%E9%9B%86%E6%88%90/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhangboc.github.io/images/Ceph/Ceph%E4%B8%8Ekubernetes%E9%9B%86%E6%88%90/1.jpg">
<meta property="og:image" content="https://zhangboc.github.io/images/Ceph/Ceph%E4%B8%8Ekubernetes%E9%9B%86%E6%88%90/2.jpg">
<meta property="article:published_time" content="2023-04-19T16:00:00.000Z">
<meta property="article:modified_time" content="2023-04-20T01:47:27.415Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Ceph">
<meta property="article:tag" content="Kubernetes">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhangboc.github.io/images/Ceph/Ceph%E4%B8%8Ekubernetes%E9%9B%86%E6%88%90/1.jpg">
  <link rel="stylesheet" href="https://lib.baomitu.com/highlight.js/9.15.8/styles/atom-one-dark.min.css" crossorigin>
  <link rel="stylesheet" href="/lib/mdui_043tiny/css/mdui.css">
  <link rel="stylesheet" href="/lib/iconfont/iconfont.css">
  <link rel="stylesheet" href="/lib/fancybox/css/jquery.fancybox.min.css">
  <link rel="stylesheet" href="https://lib.baomitu.com/justifiedGallery/3.8.1/css/justifiedGallery.min.css">
  
    <link rel="stylesheet" href="//at.alicdn.com/t/font_2421060_8z08qcz5sq3.css">
  
  <link rel="stylesheet" href="/css/style.css?v=1682237265509">
<meta name="generator" content="Hexo 6.3.0"></head>

<body class="mdui-drawer-body-left">
  
  <div id="nexmoe-background">
    <div class="nexmoe-bg" style="background-image: url(/images/background/xiaomai.jpg)"></div>
    <div class="nexmoe-small" style="background-image: url(/images/background/lihui.png)"></div>
    <div class="mdui-appbar mdui-shadow-0">
      <div class="mdui-toolbar">
        <a mdui-drawer="{target: '#drawer', swipe: true}" title="menu" class="mdui-btn mdui-btn-icon mdui-ripple"><i class="mdui-icon nexmoefont icon-menu"></i></a>
        <div class="mdui-toolbar-spacer"></div>
        <!--<a href="javascript:;" class="mdui-btn mdui-btn-icon"><i class="mdui-icon material-icons">search</i></a>-->
        <a href="/" title="John Doe" class="mdui-btn mdui-btn-icon"><img src="/images/head/head.jpg" alt="John Doe"></a>
       </div>
    </div>
  </div>
  <div id="nexmoe-header">
      <div class="nexmoe-drawer mdui-drawer" id="drawer">
    <div class="nexmoe-avatar mdui-ripple">
        <a href="/" title="John Doe">
            <img src="/images/head/head.jpg" alt="John Doe" alt="John Doe">
        </a>
    </div>
    <div class="nexmoe-count">
        <div class="nexmoe-count-item"><span>文章</span>26 <div class="item-radius"></div><div class="item-radius item-right"></div> </div>
        <div class="nexmoe-count-item"><span>标签</span>20<div class="item-radius"></div><div class="item-radius item-right"></div></div>
        <div class="nexmoe-count-item"><span>分类</span>7<div class="item-radius"></div><div class="item-radius item-right"></div></div>
    </div>
    <div class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}">
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/" title="回到首页">
            <i class="mdui-list-item-icon nexmoefont icon-meishi"></i>
            <div class="mdui-list-item-content">
                回到首页
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/archives.html" title="文章归档">
            <i class="mdui-list-item-icon nexmoefont icon-hanbao1"></i>
            <div class="mdui-list-item-content">
                文章归档
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/about/index.html" title="关于我">
            <i class="mdui-list-item-icon nexmoefont icon-jiubei1"></i>
            <div class="mdui-list-item-content">
                关于我
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/friend/index.html" title="我的朋友">
            <i class="mdui-list-item-icon nexmoefont icon-cola"></i>
            <div class="mdui-list-item-content">
                我的朋友
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/download/index.html" title="下载中心">
            <i class="mdui-list-item-icon nexmoefont icon-tangguo"></i>
            <div class="mdui-list-item-content">
                下载中心
            </div>
        </a>
        
    </div>
    <aside id="nexmoe-sidebar">
  
  
<!-- 站内搜索 -->

<div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-search" >
        <form id="search-form">
            <label><input type="text" id="local-search-input" name="q" results="0" placeholder="站内搜索" class="input form-control" autocomplete="off" autocorrect="off"/></label>
            <!-- 清空/重置搜索框 -->
            <i class="fa fa-times" onclick="resetSearch()"></i>
        </form>
    </div>
    <div id="local-search-result"></div> <!-- 搜索结果区 -->
    <!-- <p class='no-result'></p> 无匹配时显示，注意在 CSS 中设置默认隐藏 -->
</div>


  
  <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-social">
        <a class="mdui-ripple" href="http://wpa.qq.com/msgrd?v=3&uin=1605643129&Site=%E5%8C%97%E4%BA%ACSEO&Menu=yes" target="_blank" mdui-tooltip="{content: 'QQ'}" style="color: rgb(64, 196, 255);background-color: rgba(64, 196, 255, .1);">
            <i class="nexmoefont icon-QQ"></i>
        </a><a class="mdui-ripple" href="mailto:1605643129@qq.com" target="_blank" mdui-tooltip="{content: 'mail'}" style="color: rgb(249,8,8);background-color: rgba(249,8,8,.1);">
            <i class="nexmoefont icon-mail-fill"></i>
        </a><a class="mdui-ripple" href="https://blog.csdn.net/qq_40855827?type=blog" target="_blank" mdui-tooltip="{content: 'CSDN'}" style="color: rgb(199,29,35);background-color: rgba(199,29,35,.1);">
            <i class="nexmoefont icon-csdn"></i>
        </a><a class="mdui-ripple" href="https://home.cnblogs.com/u/1882665" target="_blank" mdui-tooltip="{content: '博客园'}" style="color: rgb(66, 214, 29);background-color: rgba(66, 214, 29, .1);">
            <i class="nexmoefont icon-bokeyuan"></i>
        </a><a class="mdui-ripple" href="https://github.com/zhangboc" target="_blank" mdui-tooltip="{content: 'GitHub'}" style="color: rgb(25, 23, 23);background-color: rgba(25, 23, 23, .15);">
            <i class="nexmoefont icon-github"></i>
        </a><a class="mdui-ripple" href="https://gitee.com/with-the-wind-yue" target="_blank" mdui-tooltip="{content: 'gitee'}" style="color: rgb(255, 255, 255);background-color: rgb(199,29,35);">
            <i class="nexmoefont icon-mayun"></i>
        </a>
    </div>
</div>
  
  
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">分类</h3>
    <div class="nexmoe-widget">

      <ul class="category-list">

        


        

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/Ceph/">Ceph</a>
          <span class="category-list-count">19</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/KVM/">KVM</a>
          <span class="category-list-count">7</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/Ceph/Kubernetes/">Kubernetes</a>
          <span class="category-list-count">1</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/Ceph/OpenStack/">OpenStack</a>
          <span class="category-list-count">2</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/Ceph/SDK/">SDK</a>
          <span class="category-list-count">1</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/hexo/">hexo</a>
          <span class="category-list-count">1</span>
        </li>

        
      </ul>

    </div>
  </div>


  
  
  <div class="nexmoe-widget-wrap">
    <div id="randomtagcloud" class="nexmoe-widget tagcloud nexmoe-rainbow">
      <a href="/tags/Ceph/" style="font-size: 20px;">Ceph</a> <a href="/tags/CephFS/" style="font-size: 10px;">CephFS</a> <a href="/tags/Ceph%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B/" style="font-size: 10px;">Ceph守护进程</a> <a href="/tags/Ceph%E7%AE%80%E4%BB%8B/" style="font-size: 10px;">Ceph简介</a> <a href="/tags/Cinder/" style="font-size: 10px;">Cinder</a> <a href="/tags/CrushMap/" style="font-size: 10px;">CrushMap</a> <a href="/tags/KVM/" style="font-size: 17.5px;">KVM</a> <a href="/tags/KVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/" style="font-size: 10px;">KVM虚拟机迁移</a> <a href="/tags/KVM%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/" style="font-size: 10px;">KVM虚拟网络高级配置</a> <a href="/tags/Kubernetes/" style="font-size: 10px;">Kubernetes</a> <a href="/tags/Linux-HA/" style="font-size: 15px;">Linux HA</a> <a href="/tags/NFS/" style="font-size: 10px;">NFS</a> <a href="/tags/OSD/" style="font-size: 10px;">OSD</a> <a href="/tags/OpenStack/" style="font-size: 12.5px;">OpenStack</a> <a href="/tags/RBD/" style="font-size: 12.5px;">RBD</a> <a href="/tags/RGW/" style="font-size: 12.5px;">RGW</a> <a href="/tags/chatjs/" style="font-size: 10px;">chatjs</a> <a href="/tags/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/" style="font-size: 10px;">基于iSCSI的KVM群集构建</a> <a href="/tags/%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5/" style="font-size: 10px;">故障排查</a> <a href="/tags/%E9%9B%86%E7%BE%A4%E6%B5%8B%E8%AF%95/" style="font-size: 10px;">集群测试</a>
    </div>
    
  </div>

  
  
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">文章</h3>
    <div class="nexmoe-widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/">2023</a><span class="archive-list-count">26</span></li></ul>
    </div>
  </div>


<style>
.nexmoe-widget .archive-list-count{
	position : absolute;
	right: 15px;
	top:9px;
	color: #DDD;
}
</style>

  
</aside>
    <div class="nexmoe-copyright">
        &copy; 2023 John Doe
        Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
        & <a href="https://github.com/zhangboc/hexo.github.io/" target="_blank">ZhangboCheng</a><br/>
        <a href="http://beian.miit.gov.cn" target="_blank">辽ICP备2021002341号</a><br/>
        
        <div style="font-size: 12px">
            <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            本站总访问量  <a id="busuanzi_value_site_pv"></a> 次<br />
            本站访客数<a id="busuanzi_value_site_uv"></a>人次
        </div>
        <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
        <script>
            var now = new Date(); 
            function createtime() { 
                var grt= new Date("08/10/2018 17:38:00");//在此处修改你的建站时间，格式：月/日/年 时:分:秒
                now.setTime(now.getTime()+250); 
                days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
                hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
                if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
                mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
                seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
                snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
                document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; 
                document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
setInterval("createtime()",250);
</script>


        
        
    </div>

</div><!-- .nexmoe-drawer -->

  </div>
  <div id="nexmoe-content">
    <div class="nexmoe-primary">
        <div class="nexmoe-post">
    
        <div class="nexmoe-post-cover" style="padding-bottom: 26.666666666666668%;">
            <img data-src="https://img1.baidu.com/it/u=413643897,2296924942&fm=253&fmt=auto&app=138&f=JPEG?w=800&h=500" data-sizes="auto" alt="Ceph与Kubernetes集成" class="lazyload">
            <h1>Ceph与Kubernetes集成</h1>
        </div>
    

        <div class="nexmoe-post-meta nexmoe-rainbow" style="margin:10px 0!important;">
    <a><i class="nexmoefont icon-calendar-fill"></i>2023年04月20日</a>
    <a><i class="nexmoefont icon-areachart"></i>11.5k 字</a>
    <a><i class="nexmoefont icon-time-circle-fill"></i>大概 66 分钟</a>
</div>

        <div class="nexmoe-post-right">
            
        </div>

        <article>
            <h1><span id></span></h1><span id="more"></span>

<!-- toc -->

<ul>
<li><a href="#%E4%B8%80-%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3">一、参考文档：</a></li>
<li><a href="#%E4%BA%8C-%E9%9B%86%E6%88%90%E6%A6%82%E8%BF%B0">二、集成概述：</a></li>
<li><a href="#%E4%B8%89-ceph%E4%B8%8Evolumes%E7%BB%93%E5%90%88">三、Ceph与volumes结合</a><ul>
<li><a href="#1-%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C">1、准备工作</a><ul>
<li><a href="#11-%E5%88%9B%E5%BB%BApool%E5%92%8C%E7%94%A8%E6%88%B7">1.1、创建pool和用户</a></li>
<li><a href="#12-%E5%88%9B%E5%BB%BA%E8%AE%A4%E8%AF%81%E7%94%A8%E6%88%B7">1.2、创建认证用户</a></li>
<li><a href="#13-%E5%88%9B%E5%BB%BAsecrets%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E5%B0%86ceph%E7%9A%84%E8%AE%A4%E8%AF%81key%E5%AD%98%E5%82%A8%E5%9C%A8secrets%E4%B8%AD">1.3、创建secrets对象存储将Ceph的认证key存储在Secrets中</a></li>
</ul>
</li>
<li><a href="#2-%E5%AE%B9%E5%99%A8%E4%B8%AD%E8%B0%83%E7%94%A8rbd-volumes">2、容器中调用Rbd Volumes</a><ul>
<li><a href="#21-%E5%88%9B%E5%BB%BArbd%E5%9D%97">2.1、创建RBD块</a></li>
<li><a href="#22-pod%E4%B8%AD%E5%BC%95%E7%94%A8rbd-volumes">2.2、pod中引用RBD volumes</a></li>
</ul>
</li>
<li><a href="#3%E6%B5%8B%E8%AF%95%E9%AA%8C%E8%AF%81">3.测试验证</a><ul>
<li><a href="#31-%E7%94%9F%E6%88%90pod">3.1、生成pod</a></li>
<li><a href="#32-%E6%9F%A5%E7%9C%8B%E6%8C%82%E8%BD%BD%E7%9A%84%E6%83%85%E5%86%B5">3.2、 查看挂载的情况</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%9B%9B-ceph%E4%B8%8Epvpvc%E9%9B%86%E6%88%90">四、Ceph与PV&#x2F;PVC集成</a><ul>
<li><a href="#1-%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C-1">1、准备工作</a></li>
<li><a href="#2-%E5%AE%9A%E4%B9%89pv%E5%92%8Cpvc">2、定义PV和PVC</a><ul>
<li><a href="#21-pv%E5%AE%9A%E4%B9%89%E5%AE%9A%E4%B9%89%E4%B8%80%E5%9D%97%E5%AD%98%E5%82%A8%E6%8A%BD%E8%B1%A1%E5%8C%96%E4%B8%BApv">2.1、PV定义，定义一块存储，抽象化为PV</a></li>
<li><a href="#22-pvc%E5%AE%9A%E4%B9%89%E5%BC%95%E7%94%A8pv">2.2、PVC定义，引用PV</a></li>
<li><a href="#23-%E7%94%9F%E6%88%90pv%E5%92%8Cpvc">2.3、生成PV和PVC</a></li>
</ul>
</li>
<li><a href="#3-pod%E4%B8%AD%E5%BC%95%E7%94%A8pvc">3、Pod中引用PVC</a></li>
</ul>
</li>
<li><a href="#%E4%BA%94-ceph%E4%B8%8Estorageclass%E9%9B%86%E6%88%90">五、Ceph与StorageClass集成</a></li>
<li><a href="#1-%E5%88%9B%E5%BB%BA%E6%B1%A0">1、创建池</a></li>
<li><a href="#2-%E8%AE%BE%E7%BD%AE-ceph-%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BA%AB%E4%BB%BD%E9%AA%8C%E8%AF%81">2、设置 CEPH 客户端身份验证</a></li>
<li><a href="#3-%E7%94%9F%E6%88%90ceph-csi-configmap">3、生成CEPH-CSI CONFIGMAP</a></li>
<li><a href="#4-%E7%94%9F%E6%88%90ceph-csi-cephx-secret">4、生成CEPH-CSI CEPHX SECRET</a></li>
<li><a href="#5-%E9%85%8D%E7%BD%AEceph-csi%E6%8F%92%E4%BB%B6">5、配置CEPH-CSI插件</a></li>
<li><a href="#6-%E4%BD%BF%E7%94%A8-ceph-%E5%9D%97%E8%AE%BE%E5%A4%87">6、使用 CEPH 块设备</a><ul>
<li><a href="#1-%E5%88%9B%E5%BB%BA%E5%AD%98%E5%82%A8%E7%B1%BB">1、创建存储类</a></li>
<li><a href="#2-%E5%88%9B%E5%BB%BApersistentvolumeclaim%E4%BD%BF%E7%94%A8pv%E5%88%9B%E5%BB%BA">2、创建PERSISTENTVOLUMECLAIM（使用PV创建）</a><ul>
<li><a href="#1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3">1、官方文档</a></li>
<li><a href="#2-%E5%AE%9E%E6%93%8D">2、实操</a></li>
<li><a href="#3-%E5%AE%B9%E5%99%A8%E4%BD%BF%E7%94%A8">3、容器使用</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%85%AD-statefulset">六、StatefulSet</a></li>
</ul>
<!-- tocstop -->

<h1><span id="一-参考文档">一、参考文档：</span></h1><p><a target="_blank" rel="noopener" href="https://blog.51cto.com/happylab/2488904">https://blog.51cto.com/happylab/2488904</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/acommoners/p/15988974.html">https://www.cnblogs.com/acommoners/p/15988974.html</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/DANTE54/article/details/106471848/">https://blog.csdn.net/DANTE54/article/details/106471848/</a><br><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/92d2c31d0f4b">https://www.jianshu.com/p/92d2c31d0f4b</a></p>
<h1><span id="二-集成概述">二、集成概述：</span></h1><p>Kubernetes和Ceph集成提供了三种实现方式：</p>
<ol>
<li>Volumes 存储卷</li>
<li>PV&#x2F;PVC 持久化卷&#x2F;持久化卷声名</li>
<li>StorageClass 动态存储类，动态创建PV和PVC</li>
</ol>
<p>相关参考：</p>
<p>Volumes结合：<a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/concepts/storage/volumes/">https://kubernetes.io/zh-cn/docs/concepts/storage/volumes/</a>  <a target="_blank" rel="noopener" href="https://github.com/kubernetes/examples/tree/master/volumes/rbd">https://github.com/kubernetes/examples/tree/master/volumes/rbd</a><br>PV&#x2F;PVC结合：<a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/">https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/</a><br>Storage Class结合：<a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/concepts/storage/storage-classes/">https://kubernetes.io/zh-cn/docs/concepts/storage/storage-classes/</a><br>Ceph 官网：<a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rbd/rbd-kubernetes/">https://docs.ceph.com/en/quincy/rbd/rbd-kubernetes/</a></p>
<h1><span id="三-ceph与volumes结合">三、Ceph与volumes结合</span></h1><h2><span id="1-准备工作">1、准备工作</span></h2><h3><span id="11-创建pool和用户">1.1、创建pool和用户</span></h3><pre><code>[root@node-1 ~]# ceph osd pool create kubernetes 8 8 
</code></pre>
<h3><span id="12-创建认证用户">1.2、创建认证用户</span></h3><pre><code>[root@node-1 docker]# ceph auth get-or-create client.kubernetes mon &#39;profile rbd&#39; osd &#39;profile rbd pool=kubernetes&#39;
[client.kubernetes]
    key = AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ==
 [root@node-1 docker]#  ceph auth list | grep kubernetes 
installed auth entries:

client.kubernetes
    caps: [osd] profile rbd pool=kubernetes
</code></pre>
<h3><span id="13-创建secrets对象存储将ceph的认证key存储在secrets中">1.3、创建secrets对象存储将Ceph的认证key存储在Secrets中</span></h3><blockquote>
<p>获取步骤2生成的key，并将其加密为base64格式</p>
</blockquote>
<pre><code>[root@node-1 ~]# echo AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ== | base64 
QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQo=
</code></pre>
<p>创建定义secrets对象</p>
<pre><code>[root@node-1 docker]# mkdir /etc/ceph/volumes
[root@node-1 docker]# vim secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: &quot;kubernetes.io/rbd&quot;
data:
  key: QVFETXVwMWVtWk1GT2hBQUJsbW5aRkUyZkY4cHVIZUlodStVUGc9PQo=
</code></pre>
<p>生成secrets</p>
<pre><code>[root@node-1 volumes]# kubectl apply -f secret.yaml 
secret/ceph-secret created

[root@node-1 volumes]#  kubectl get secret
NAME                  TYPE                                  DATA   AGE
ceph-secret           kubernetes.io/rbd                     1      85s
default-token-p5dsd   kubernetes.io/service-account-token   3      13h
[root@node-1 volumes]# kubectl get secret ceph-secret -o yaml
apiVersion: v1
data:
  key: QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQo=
kind: Secret
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;key&quot;:&quot;QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQo=&quot;&#125;,&quot;kind&quot;:&quot;Secret&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;ceph-secret&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;type&quot;:&quot;kubernetes.io/rbd&quot;&#125;
  creationTimestamp: &quot;2023-03-15T04:08:56Z&quot;
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: &#123;&#125;
        f:key: &#123;&#125;
      f:metadata:
        f:annotations:
          .: &#123;&#125;
          f:kubectl.kubernetes.io/last-applied-configuration: &#123;&#125;
      f:type: &#123;&#125;
    manager: kubectl-client-side-apply
    operation: Update
    time: &quot;2023-03-15T04:08:56Z&quot;
  name: ceph-secret
  namespace: default
  resourceVersion: &quot;119093&quot;
  selfLink: /api/v1/namespaces/default/secrets/ceph-secret
  uid: 5028d511-5ff5-4466-bad2-36e7c4b4b14c
type: kubernetes.io/rbd
</code></pre>
<h2><span id="2-容器中调用rbd-volumes">2、容器中调用Rbd Volumes</span></h2><h3><span id="21-创建rbd块">2.1、创建RBD块</span></h3><pre><code>[root@node-1 volumes]# kubectl apply -f secret.yaml 
secret/ceph-secret created

[root@node-1 volumes]#  kubectl get secret
NAME                  TYPE                                  DATA   AGE
ceph-secret           kubernetes.io/rbd                     1      85s
default-token-p5dsd   kubernetes.io/service-account-token   3      13h
[root@node-1 volumes]# kubectl get secret ceph-secret -o yaml
apiVersion: v1
data:
  key: QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQo=
kind: Secret
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;key&quot;:&quot;QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQo=&quot;&#125;,&quot;kind&quot;:&quot;Secret&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;ceph-secret&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;type&quot;:&quot;kubernetes.io/rbd&quot;&#125;
  creationTimestamp: &quot;2023-03-15T04:08:56Z&quot;
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: &#123;&#125;
        f:key: &#123;&#125;
      f:metadata:
        f:annotations:
          .: &#123;&#125;
          f:kubectl.kubernetes.io/last-applied-configuration: &#123;&#125;
      f:type: &#123;&#125;
    manager: kubectl-client-side-apply
    operation: Update
    time: &quot;2023-03-15T04:08:56Z&quot;
  name: ceph-secret
  namespace: default
  resourceVersion: &quot;119093&quot;
  selfLink: /api/v1/namespaces/default/secrets/ceph-secret
  uid: 5028d511-5ff5-4466-bad2-36e7c4b4b14c
type: kubernetes.io/rbd
</code></pre>
<h3><span id="22-pod中引用rbd-volumes">2.2、pod中引用RBD volumes</span></h3><pre><code>[root@node-1 volumes]# cat pods.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: volume-rbd-demo
spec:
  containers:
  - name: pod-with-rbd
    image: nginx:1.7.9
    imagePullPolicy: IfNotPresent
    ports:
    - name: www
      containerPort: 80
      protocol: TCP
    volumeMounts:
    - name: rbd-demo
      mountPath: /data
  volumes:
  - name: rbd-demo
    rbd:
      monitors:
      - 192.168.187.201:6789
      - 192.168.187.202:6789
      - 192.168.187.203:6789
      pool: kubernetes
      image: rbd.img 
      fsType: ext4 
      user: kubernetes
      secretRef:
        name: ceph-secret
</code></pre>
<h2><span id="3测试验证">3.测试验证</span></h2><h3><span id="31-生成pod">3.1、生成pod</span></h3><pre><code>[root@node-1 volumes]# kubectl apply -f pods.yaml 
pod/volume-rbd-demo created
[root@node-1 volumes]# kubectl get pods -o wide
NAME                     READY   STATUS    RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES
nginx-6799fc88d8-jhzz7   1/1     Running   2          14h     10.244.247.19   node-2   &lt;none&gt;           &lt;none&gt;
volume-rbd-demo          1/1     Running   0          4m15s   10.244.247.22   node-2   &lt;none&gt;           &lt;none&gt;
[root@node-1 volumes]# kubectl describe pods volume-rbd-demo 
Name:         volume-rbd-demo
Namespace:    default
Priority:     0
Node:         node-2/192.168.199.202
Start Time:   Wed, 15 Mar 2023 13:28:44 +0800
Labels:       &lt;none&gt;
Annotations:  cni.projectcalico.org/podIP: 10.244.247.22/32
              cni.projectcalico.org/podIPs: 10.244.247.22/32
Status:       Running
IP:           10.244.247.22
IPs:
  IP:  10.244.247.22
Containers:
  pod-with-rbd:
    Container ID:   docker://19572ad6d343cb1fca6a8326bc5d1d02cefa24eb582d994f9f01de5227a555fb
    Image:          nginx:1.7.9
    Image ID:       docker-pullable://nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 15 Mar 2023 13:29:29 +0800
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /data from rbd-demo (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-p5dsd (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  rbd-demo:
    Type:          RBD (a Rados Block Device mount on the host that shares a pod&#39;s lifetime)
    CephMonitors:  [192.168.187.201:6789 192.168.187.202:6789 192.168.187.203:6789]
    RBDImage:      rbd.img
    FSType:        ext4
    RBDPool:       kubernetes
    RadosUser:     kubernetes
    Keyring:       /etc/ceph/keyring
    SecretRef:     &amp;LocalObjectReference&#123;Name:ceph-secret,&#125;
    ReadOnly:      false
  default-token-p5dsd:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-p5dsd
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason                  Age   From                     Message
  ----    ------                  ----  ----                     -------
  Normal  Scheduled               56s   default-scheduler        Successfully assigned default/volume-rbd-demo to node-2
  Normal  SuccessfulAttachVolume  56s   attachdetach-controller  AttachVolume.Attach succeeded for volume &quot;rbd-demo&quot;
  Normal  Pulling                 46s   kubelet, node-2          Pulling image &quot;nginx:1.7.9&quot;
  Normal  Pulled                  11s   kubelet, node-2          Successfully pulled image &quot;nginx:1.7.9&quot; in 34.176359944s
  Normal  Created                 11s   kubelet, node-2          Created container pod-with-rbd
  Normal  Started                 11s   kubelet, node-2          Started container pod-with-rbd
[root@node-1 volumes]# kubectl exec -it volume-rbd-demo /bin/bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@volume-rbd-demo:/# 
root@volume-rbd-demo:/# 
root@volume-rbd-demo:/# df -HT
Filesystem              Type     Size  Used Avail Use% Mounted on
overlay                 overlay   19G  4.5G   14G  25% /
tmpfs                   tmpfs     68M     0   68M   0% /dev
tmpfs                   tmpfs    2.0G     0  2.0G   0% /sys/fs/cgroup
/dev/rbd0               ext4      11G   38M   11G   1% /data
/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /dev/termination-log
/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /etc/resolv.conf
/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /etc/hostname
/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /etc/hosts
shm                     tmpfs     68M     0   68M   0% /dev/shm
/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /var/cache/nginx
tmpfs                   tmpfs    2.0G   13k  2.0G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                   tmpfs    2.0G     0  2.0G   0% /proc/acpi
tmpfs                   tmpfs     68M     0   68M   0% /proc/kcore
tmpfs                   tmpfs     68M     0   68M   0% /proc/keys
tmpfs                   tmpfs     68M     0   68M   0% /proc/timer_list
tmpfs                   tmpfs     68M     0   68M   0% /proc/timer_stats
tmpfs                   tmpfs     68M     0   68M   0% /proc/sched_debug
tmpfs                   tmpfs    2.0G     0  2.0G   0% /proc/scsi
tmpfs                   tmpfs    2.0G     0  2.0G   0% /sys/firmware
root@volume-rbd-demo:/# cd /data/
root@volume-rbd-demo:/data# ls
lost+found
root@volume-rbd-demo:/data# touch testfile
root@volume-rbd-demo:/data# echo &quot;test&quot; &gt;&gt; testfile 
root@volume-rbd-demo:/data# ls
lost+found  testfile
root@volume-rbd-demo:/data# cat testfile 
test
root@volume-rbd-demo:/data# ls
lost+found  testfile
root@volume-rbd-demo:/data# exit
</code></pre>
<h3><span id="32-查看挂载的情况">3.2、 查看挂载的情况</span></h3><pre><code>[root@node-1 volumes]# kubectl exec -it volume-rbd-demo -- df -h
Filesystem      Size  Used Avail Use% Mounted on
rootfs           50G  6.7G   41G  15% /
overlay          50G  6.7G   41G  15% /
tmpfs            64M     0   64M   0% /dev
tmpfs           920M     0  920M   0% /sys/fs/cgroup
/dev/rbd0       9.8G   37M  9.7G   1% /data
[root@node-1 volumes]# kubectl exec -it volume-rbd-demo /bin/bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@volume-rbd-demo:/# 
root@volume-rbd-demo:/# 
root@volume-rbd-demo:/# df -HT
Filesystem              Type     Size  Used Avail Use% Mounted on
overlay                 overlay   19G  4.5G   14G  25% /
tmpfs                   tmpfs     68M     0   68M   0% /dev
tmpfs                   tmpfs    2.0G     0  2.0G   0% /sys/fs/cgroup
/dev/rbd0               ext4      11G   38M   11G   1% /data
/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /dev/termination-log
/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /etc/resolv.conf
/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /etc/hostname
/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /etc/hosts
shm                     tmpfs     68M     0   68M   0% /dev/shm
/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /var/cache/nginx
tmpfs                   tmpfs    2.0G   13k  2.0G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                   tmpfs    2.0G     0  2.0G   0% /proc/acpi
tmpfs                   tmpfs     68M     0   68M   0% /proc/kcore
tmpfs                   tmpfs     68M     0   68M   0% /proc/keys
tmpfs                   tmpfs     68M     0   68M   0% /proc/timer_list
tmpfs                   tmpfs     68M     0   68M   0% /proc/timer_stats
tmpfs                   tmpfs     68M     0   68M   0% /proc/sched_debug
tmpfs                   tmpfs    2.0G     0  2.0G   0% /proc/scsi
tmpfs                   tmpfs    2.0G     0  2.0G   0% /sys/firmware
[root@node-2 ~]# df -HT |grep rbd0 #node02中查看
/dev/rbd0               ext4       11G   38M   11G    1% /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/kubernetes-image-rbd.img
[root@node-2 ~]# rbd device list 
id pool       namespace image   snap device    
0  kubernetes           rbd.img -    /dev/rbd0
[root@node-2 mounts]# cd /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/kubernetes-image-rbd.img
[root@node-2 kubernetes-image-rbd.img]# ls
lost+found  testfile
[root@node-2 kubernetes-image-rbd.img]# cat testfile 
test
</code></pre>
<h1><span id="四-ceph与pvx2fpvc集成">四、Ceph与PV&#x2F;PVC集成</span></h1><h2><span id="1-准备工作">1、准备工作</span></h2><p><strong>参考步骤一，创建好pool，镜像，用户认证，secrets</strong></p>
<h2><span id="2-定义pv和pvc">2、定义PV和PVC</span></h2><h3><span id="21-pv定义定义一块存储抽象化为pv">2.1、PV定义，定义一块存储，抽象化为PV</span></h3><pre><code>[root@node-1 pv_and_pvc]# cat pv.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: rbd-demo
spec:
  accessModes:
   - ReadWriteOnce #定义一个客户端同时进行读写
  capacity:
    storage: 10G
 #定义大小
  rbd:
 #定义RBD相关的驱动
    monitors:
 #定义Ceph-Mon地址和端口
     - 192.168.187.201:6789
     - 192.168.187.202:6789
     - 192.168.187.203:6789
    pool: kubernetes #定义ceph中的pool
    image: demo-1.img  #定义镜像
    fsType: ext4 #定义镜像格式
    user: kubernetes #定义访问用户
    secretRef:
      name: ceph-secret
  persistentVolumeReclaimPolicy: Retain #定义持久化存储的策略，可以重复利用，pool删除以后PV不受影响
  storageClassName: rbd
[root@node-1 volumes]# rbd create -p kubernetes --image-feature layering demo-1.img --size 10G

[root@node-1 volumes]# rbd info kubernetes/demo-1.img
rbd image &#39;demo-1.img&#39;:
    size 10 GiB in 2560 objects
    order 22 (4 MiB objects)
    snapshot_count: 0
    id: 4091dd5acdf1
    block_name_prefix: rbd_data.4091dd5acdf1
    format: 2
    features: layering
    op_features: 
    flags: 
    create_timestamp: Wed Mar 15 13:54:41 2023
    access_timestamp: Wed Mar 15 13:54:41 2023
    modify_timestamp: Wed Mar 15 13:54:41 2023
[root@node-1 volumes]# rbd -p kubernetes ls 
demo-1.img
rbd.img
</code></pre>
<h3><span id="22-pvc定义引用pv">2.2、PVC定义，引用PV</span></h3><pre><code>[root@node-1 pv_and_pvc]# cat pvc.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-demo
spec:
  accessModes:
   - ReadWriteOnce #访问模式与PV一致
  volumeName: rbd-demo #定义一个名称
  resources:
    requests:
      storage: 10G
 #申请空间大小，可小于RBD大小，但是不能超过
  storageClassName: rbd
</code></pre>
<h3><span id="23-生成pv和pvc">2.3、生成PV和PVC</span></h3><pre><code>[root@node-1 volumes]# kubectl apply -f pv.yaml 
persistentvolume/rbd-demo created

[root@node-1 volumes]# kubectl get pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
rbd-demo   10G        RWO            Retain           Available           rbd                     14s

[root@node-1 pv_and_pvc]# kubectl apply -f pvc.yaml 
persistentvolumeclaim/pvc-demo created
[root@node-1 volumes]# kubectl get pvc
NAME       STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-demo   Bound    rbd-demo   10G        RWO            rbd            7m37s
[root@node-1 volumes]# kubectl describe pvc pvc-demo 
Name:          pvc-demo
Namespace:     default
StorageClass:  rbd
Status:        Bound
Volume:        rbd-demo
Labels:        &lt;none&gt;
Annotations:   pv.kubernetes.io/bind-completed: yes
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      10G
Access Modes:  RWO
VolumeMode:    Filesystem
Mounted By:    &lt;none&gt;
Events:        &lt;none&gt;
[root@node-1 volumes]# kubectl get pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   REASON   AGE
rbd-demo   10G        RWO            Retain           Bound    default/pvc-demo   rbd                     11m
</code></pre>
<h2><span id="3-pod中引用pvc">3、Pod中引用PVC</span></h2><pre><code>[root@node-1 pv_and_pvc]# cat pod-demo.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: pod-demo
spec:
  containers:
  - name: demo
    image: nginx:1.7.9
    imagePullPolicy: IfNotPresent
    ports:
    - name: www
      protocol: TCP
      containerPort: 80
    volumeMounts:
    - name: rbd
      mountPath: /data #挂载点
  volumes:
  - name: rbd
    persistentVolumeClaim:
      claimName: pvc-demo #PVC的名称
[root@node-1 volumes]# kubectl apply -f pod-demo.yaml  #应用yaml文件
pod/pod-demo created
[root@node-1 volumes]# kubectl describe pods pod-demo #查看创建情况，已经将容器部署到node-3
  Normal  Created                 54s   kubelet, node-3          Created container demo
  Normal  Started                 53s   kubelet, node-3          Started container demo
[root@node-1 volumes]# kubectl get pods #查看pods运行情况
NAME                     READY   STATUS    RESTARTS   AGE
nginx-6799fc88d8-jhzz7   1/1     Running   2          2d19h
pod-demo                 1/1     Running   0          4m2s
volume-rbd-demo          1/1     Running   0          2d4h
[root@node-1 volumes]# kubectl exec -it pod-demo /bin/bash  #进入容器查看 挂载情况
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@pod-demo:/# df -HT #查看挂载情况
Filesystem              Type     Size  Used Avail Use% Mounted on
overlay                 overlay   19G  4.1G   15G  23% /
tmpfs                   tmpfs     68M     0   68M   0% /dev
tmpfs                   tmpfs    954M     0  954M   0% /sys/fs/cgroup
/dev/rbd0               ext4      11G   38M   11G   1% /data #已经正常挂载
[root@node-1 volumes]# ssh node-3 
#远程到node-3
Last login: Fri Mar 17 17:34:15 2023 from 192.168.187.1
[root@node-3 ~]# df -HT |grep rbd
#宿主机查看到映射情况
/dev/rbd0               ext4       11G   38M   11G    1% /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/kubernetes-image-demo-1.img
[root@node-3 ~]# rbd device list 
#查看rbd映射情况
id pool       namespace image      snap device    
0  kubernetes           demo-1.img -    /dev/rbd0 
[root@node-3 ~]# docker ps -a |grep nginx #查看容器运行状况
c38756d298c8   nginx                                                &quot;nginx -g &#39;daemon of…&quot;   11 minutes ago   Up 11 minutes                     k8s_demo_pod-demo_default_625a64e7-bac1-4128-a7d8-40cbc9956eed_0
</code></pre>
<h1><span id="五-ceph与storageclass集成">五、Ceph与StorageClass集成</span></h1><p><img src="/images/Ceph/Ceph%E4%B8%8Ekubernetes%E9%9B%86%E6%88%90/1.jpg"></p>
<h1><span id="1-创建池">1、创建池</span></h1><p>默认情况下，Ceph 块设备使用rbd池。为 Kubernetes 卷存储创建一个池。确保您的 Ceph 集群正在运行，然后创建池。<br>新创建的池必须在使用前进行初始化。使用该rbd pool init工具初始化池：</p>
<pre><code>[root@node-1 volumes]ceph osd pool create kubernetes #上述配过不需要重新创建
[root@node-1 volumes]rbd pool init kubernetes #上述配过不需要初始化
</code></pre>
<h1><span id="2-设置-ceph-客户端身份验证">2、设置 CEPH 客户端身份验证</span></h1><p>为 Kubernetes 和ceph-csi创建一个新用户。执行以下命令并记录生成的密钥：</p>
<blockquote>
<p>kubernetes 用户在与PV&#x2F;PVC集成时已经创建过，然后下列命令可以不执行，并且此kubernetes用户在与CSI集成时会出现BUG，需要用ceph的admin用户集成，然后下列命令在创建时不能指定去访问mgr的存储池，也会报错</p>
</blockquote>
<blockquote>
<p>经过实际测试，目前没有出现视频中发现的BUG，可能和安装的版本有关，最大的可能就是ceph-csi的版本</p>
</blockquote>
<pre><code>[root@node-1 volumes]# kubectl version
Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;19&quot;, GitVersion:&quot;v1.19.0&quot;, GitCommit:&quot;e19964183377d0ec2052d1f1fa930c4d7575bd50&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2020-08-26T14:30:33Z&quot;, GoVersion:&quot;go1.15&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;
Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;19&quot;, GitVersion:&quot;v1.19.0&quot;, GitCommit:&quot;e19964183377d0ec2052d1f1fa930c4d7575bd50&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2020-08-26T14:23:04Z&quot;, GoVersion:&quot;go1.15&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;
[root@node-1 volumes]# ceph -v
ceph version 14.2.22 (ca74598065096e6fcbd8433c8779a2be0c889351) nautilus (stable)
</code></pre>
<pre><code>[root@node-1 volumes]# ceph auth list #可以看到该用户已经创建过，不需要重新创建
client.kubernetes
key: AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ==
caps: [mon] profile rbd
caps: [osd] profile rbd pool=kubernetes

[root@node-1 volumes]$ ceph auth get-or-create client.kubernetes mon &#39;profile rbd&#39; osd &#39;profile rbd pool=kubernetes&#39; mgr &#39;profile rbd pool=kubernetes&#39;
[client.kubernetes]
    key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==
</code></pre>
<h1><span id="3-生成ceph-csi-configmap">3、生成CEPH-CSI CONFIGMAP</span></h1><p>ceph -csi需要一个存储在 Kubernetes 中的ConfigMap对象来定义 Ceph 集群的 Ceph 监视器地址。收集 Ceph 集群唯一fsid和监视器地址：</p>
<pre><code>[root@node-1 volumes]# ceph mon dump  #查看集群唯一ID和MON的地址
epoch 3
fsid b8e58b30-4568-4032-a9f4-837ed3fa9529 #唯一ID
last_changed 2023-03-07 13:57:20.988318
created 2023-03-07 13:41:49.055353
min_mon_release 14 (nautilus)
0: [v2:192.168.187.201:3300/0,v1:192.168.187.201:6789/0] mon.node-1
1: [v2:192.168.187.202:3300/0,v1:192.168.187.202:6789/0] mon.node-2
2: [v2:192.168.187.203:3300/0,v1:192.168.187.203:6789/0] mon.node-3
dumped monmap epoch 3
[root@node-1 volumes]# cat &lt;&lt;EOF &gt; csi-config-map.yaml #创建配置文件
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      &#123;
        &quot;clusterID&quot;: &quot;b9127830-b0cc-4e34-aa47-9d1a2e9949a8&quot;,
        &quot;monitors&quot;: [
          &quot;192.168.1.1:6789&quot;,
          &quot;192.168.1.2:6789&quot;,
          &quot;192.168.1.3:6789&quot;
        ]
      &#125;
    ]
metadata:
  name: ceph-csi-config
EOF
[root@node-1 volumes]# vim csi-config-map.yaml #修改配置文件，根据自身集群情况进行修改
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      &#123;
        &quot;clusterID&quot;: &quot;b8e58b30-4568-4032-a9f4-837ed3fa9529&quot;,
        &quot;monitors&quot;: [
          &quot;192.168.187.201:6789&quot;,
          &quot;192.168.187.202:6789&quot;,
          &quot;192.168.187.203:6789&quot;
        ]
      &#125;
    ]
metadata:
  name: ceph-csi-config
[root@node-1 volumes]# kubectl apply -f csi-config-map.yaml #将编辑好的配置文件生成config map
configmap/ceph-csi-config created
[root@node-1 volumes]# kubectl get configmaps #查看配置文件生成情况
NAME              DATA   AGE
ceph-csi-config   1      37s
[root@node-1 volumes]# kubectl get configmaps ceph-csi-config -o yaml #查看生成的配置文件具体内容
apiVersion: v1
data: #在这里定义的
  config.json: |-
    [
      &#123;
        &quot;clusterID&quot;: &quot;b8e58b30-4568-4032-a9f4-837ed3fa9529&quot;,
        &quot;monitors&quot;: [
          &quot;192.168.187.201:6789&quot;,
          &quot;192.168.187.202:6789&quot;,
          &quot;192.168.187.203:6789&quot;
        ]
      &#125;
    ]
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;config.json&quot;:&quot;[\n  &#123;\n    \&quot;clusterID\&quot;: \&quot;b8e58b30-4568-4032-a9f4-837ed3fa9529\&quot;,\n    \&quot;monitors\&quot;: [\n      \&quot;192.168.187.201:6789\&quot;,\n      \&quot;192.168.187.202:6789\&quot;,\n      \&quot;192.168.187.203:6789\&quot;\n    ]\n  &#125;\n]&quot;&#125;,&quot;kind&quot;:&quot;ConfigMap&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;ceph-csi-config&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;&#125;
  creationTimestamp: &quot;2023-03-18T04:11:51Z&quot;
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: &#123;&#125;
        f:config.json: &#123;&#125;
      f:metadata:
        f:annotations:
          .: &#123;&#125;
          f:kubectl.kubernetes.io/last-applied-configuration: &#123;&#125;
    manager: kubectl-client-side-apply
    operation: Update
    time: &quot;2023-03-18T04:11:51Z&quot;
  name: ceph-csi-config
  namespace: default
  resourceVersion: &quot;589956&quot;
  selfLink: /api/v1/namespaces/default/configmaps/ceph-csi-config
  uid: 30e98e2d-46eb-40bb-9d58-cb410012c31a
</code></pre>
<h1><span id="4-生成ceph-csi-cephx-secret">4、生成CEPH-CSI CEPHX SECRET</span></h1><p>ceph-csi需要 cephx 凭据才能与 Ceph 集群通信。使用新创建的 Kubernetes 用户 ID 和 cephx 密钥生成类似于以下示例的csi-rbd-secret.yaml文件：</p>
<pre><code>[root@node-1 volumes]#cat &lt;&lt;EOF &gt; csi-rbd-secret.yaml #配置认证信息
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
stringData:
  userID: kubernetes
  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==
EOF
[root@node-1 volumes]#ceph auth list #获取用户ID
client.kubernetes
    key: AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ==
    caps: [mon] profile rbd
    caps: [osd] profile rbd pool=kubernetes
[root@node-1 volumes]# vim csi-rbd-secret.yaml #根据自身集群情况进行修改
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
stringData:
  userID: kubernetes
  userKey: AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ== #这个密钥不用base 64加密，因为使用的时stringData模式，会自动加密  
[root@node-1 volumes]# kubectl apply -f csi-rbd-secret.yaml #生成这个对象
secret/csi-rbd-secret created
[root@node-1 volumes]# kubectl get secrets  #查看生成结果
NAME                  TYPE                                  DATA   AGE
ceph-secret           kubernetes.io/rbd                     1      3d
csi-rbd-secret        Opaque                                2      32s
default-token-p5dsd   kubernetes.io/service-account-token   3      3d14h
[root@node-1 volumes]# kubectl get secrets csi-rbd-secret -o yaml #查看生成的具体内容
apiVersion: v1
data: #data显示的就是base 64加密后的，stringData显示的就是不加密的
  userID: a3ViZXJuZXRlcw== #定义的用户名
  userKey: QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQ== #自动加密的key
kind: Secret
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Secret&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;csi-rbd-secret&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;stringData&quot;:&#123;&quot;userID&quot;:&quot;kubernetes&quot;,&quot;userKey&quot;:&quot;AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ==&quot;&#125;&#125;
  creationTimestamp: &quot;2023-03-18T04:38:20Z&quot;
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: &#123;&#125;
        f:userID: &#123;&#125;
        f:userKey: &#123;&#125;
      f:metadata:
        f:annotations:
          .: &#123;&#125;
          f:kubectl.kubernetes.io/last-applied-configuration: &#123;&#125;
      f:type: &#123;&#125;
    manager: kubectl-client-side-apply
    operation: Update
    time: &quot;2023-03-18T04:38:20Z&quot;
  name: csi-rbd-secret
  namespace: default
  resourceVersion: &quot;593749&quot;
  selfLink: /api/v1/namespaces/default/secrets/csi-rbd-secret
  uid: 712d89bf-d3b2-44e5-bf91-973f76b15830
type: Opaque
[root@node-1 volumes]# echo a3ViZXJuZXRlcw== | base64 -d #解密userID后得出用户名称
kubernetes[root@node-1 volumes]#
kubernetes[root@node-1 volumes]# echo QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQ== | base64 -d #解密userkey后得出实际的key
AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ==[root@node-1 volumes]# 
</code></pre>
<h1><span id="5-配置ceph-csi插件">5、配置CEPH-CSI插件</span></h1><p>创建所需的ServiceAccount和 RBAC ClusterRole &#x2F; ClusterRoleBinding Kubernetes 对象。这些对象不一定需要为您的 Kubernetes 环境定制，因此可以从ceph-csi 部署 YAML 中按原样使用：</p>
<pre><code>[root@node-1 volumes]# wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
[root@node-1 volumes]# wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml
[root@node-1 volumes]# ls -la | grep rbac.yaml #下载成功
-rw-r--r-- 1 root root 1193 3月  18 12:49 csi-nodeplugin-rbac.yaml #节点驱动的
-rw-r--r-- 1 root root 3347 3月  18 12:48 csi-provisioner-rbac.yaml #提供者的
[root@node-1 volumes]#  kubectl apply -f csi-provisioner-rbac.yaml #进行提交
serviceaccount/rbd-csi-provisioner created
clusterrole.rbac.authorization.k8s.io/rbd-external-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/rbd-csi-provisioner-role created
role.rbac.authorization.k8s.io/rbd-external-provisioner-cfg created
rolebinding.rbac.authorization.k8s.io/rbd-csi-provisioner-role-cfg created


[root@node-1 volumes]# kubectl apply -f csi-nodeplugin-rbac.yaml #进行提交
serviceaccount/rbd-csi-nodeplugin created
clusterrole.rbac.authorization.k8s.io/rbd-csi-nodeplugin created
clusterrolebinding.rbac.authorization.k8s.io/rbd-csi-nodeplugin created
</code></pre>
<p>最后，创建ceph-csi provisioner 和节点插件。除了ceph-csi容器发布版本之外，这些对象不一定需要为您的 Kubernetes 环境定制，因此可以从 ceph- csi部署 YAMLs 中按原样使用：</p>
<pre><code>[root@node-1 volumes]# wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
[root@node-1 volumes]# wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml
[root@node-1 volumes]# ls -la | grep rbdplugin #下载成功
-rw-r--r-- 1 root root 8125 3月  18 12:54 csi-rbdplugin-provisioner.yaml
-rw-r--r-- 1 root root 7736 3月  18 12:54 csi-rbdplugin.yaml
[root@node-1 volumes]# kubectl apply -f csi-rbdplugin-provisioner.yaml #提交
service/csi-rbdplugin-provisioner created
deployment.apps/csi-rbdplugin-provisioner created

[root@node-1 volumes]# kubectl apply -f csi-rbdplugin.yaml#提交
daemonset.apps/csi-rbdplugin created
service/csi-metrics-rbdplugin created
[root@node-1 volumes]# kubectl get deployments.apps  #查看状态
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
csi-rbdplugin-provisioner   0/3     3            0           53s
nginx                       1/1     1            1           3d14h
[root@node-1 volumes]#  kubectl get pods #查看正在创建
NAME                                         READY   STATUS              RESTARTS   AGE
csi-rbdplugin-g9qc7                          0/3     ContainerCreating   0          73s
csi-rbdplugin-n7l5f                          0/3     ContainerCreating   0          73s
csi-rbdplugin-provisioner-6748c759b4-7mjfg   0/7     ContainerCreating   0          85s
csi-rbdplugin-provisioner-6748c759b4-8g2pr   0/7     ContainerCreating   0          85s
csi-rbdplugin-provisioner-6748c759b4-9hgt4   0/7     Pending             0          85s
nginx-6799fc88d8-jhzz7                       1/1     Running             3          3d14h
pod-demo                                     1/1     Running             1          19h
volume-rbd-demo                              1/1     Running             1          2d23h
[root@node-1 volumes]# kubectl describe pods csi-rbdplugin-provisioner-6748c759b4-7mjfg  #查看创建失败，缺少ceph-csi-encryption-kms-config的配置文件

  Normal   Scheduled    2m36s                default-scheduler  Successfully assigned default/csi-rbdplugin-provisioner-6748c759b4-7mjfg to node-3
  Warning  FailedMount  92s (x8 over 2m36s)  kubelet, node-3    MountVolume.SetUp failed for volume &quot;ceph-csi-encryption-kms-config&quot; : configmap &quot;ceph-csi-encryption-kms-config&quot; not found
  Warning  FailedMount  92s (x8 over 2m36s)  kubelet, node-3    MountVolume.SetUp failed for volume &quot;ceph-config&quot; : configmap &quot;ceph-config&quot; not found
  Warning  FailedMount  92s (x8 over 2m36s)  kubelet, node-3    MountVolume.SetUp failed for volume &quot;oidc-token&quot; : failed to fetch token: the API server does not have TokenRequest endpoints enabled
  Warning  FailedMount  33s                  kubelet, node-3    Unable to attach or mount volumes: unmounted volumes=[ceph-csi-encryption-kms-config oidc-token ceph-config], unattached volumes=[ceph-csi-encryption-kms-config keys-tmp-dir oidc-token host-sys rbd-csi-provisioner-token-kpbsw host-dev lib-modules ceph-csi-config socket-dir ceph-config]: timed out waiting for the condition
</code></pre>
<p><strong>最新版：</strong><a target="_blank" rel="noopener" href="https://github.com/ceph/ceph-csi/blob/devel/examples/kms/vault/kms-config.yaml">Github</a><br><strong>老版本：</strong></p>
<pre><code>apiVersion: v1
kind: ConfigMap
data:
config.json: |-
&#123;
&quot;vault-test&quot;: &#123;
&quot;encryptionKMSType&quot;: &quot;vault&quot;,
&quot;vaultAddress&quot;: &quot;http://vault.default.svc.cluster.local:8200&quot;,
&quot;vaultAuthPath&quot;: &quot;/v1/auth/kubernetes/login&quot;,
&quot;vaultRole&quot;: &quot;csi-kubernetes&quot;,
&quot;vaultPassphraseRoot&quot;: &quot;/v1/secret&quot;,
&quot;vaultPassphrasePath&quot;: &quot;ceph-csi/&quot;,
&quot;vaultCAVerify&quot;: &quot;false&quot;
&#125;
&#125;
metadata:
name: ceph-csi-encryption-kms-config
</code></pre>
<pre><code>[root@node-1 volumes]#cat &lt;&lt;EOF &gt; kms-config.yaml #创建
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    &#123;
      &quot;vault-test&quot;: &#123;
        &quot;encryptionKMSType&quot;: &quot;vault&quot;,
        &quot;vaultAddress&quot;: &quot;http://vault.default.svc.cluster.local:8200&quot;,
        &quot;vaultAuthPath&quot;: &quot;/v1/auth/kubernetes/login&quot;,
        &quot;vaultRole&quot;: &quot;csi-kubernetes&quot;,
        &quot;vaultPassphraseRoot&quot;: &quot;/v1/secret&quot;,
        &quot;vaultPassphrasePath&quot;: &quot;ceph-csi/&quot;,
        &quot;vaultCAVerify&quot;: &quot;false&quot;
      &#125;
    &#125;
metadata:
  name: ceph-csi-encryption-kms-config
EOF
[root@node-1 volumes]# kubectl apply -f kms-config.yaml #提交
configmap/ceph-csi-encryption-kms-config created
[root@node-1 volumes]# kubectl get configmaps  #查看配置文件
NAME                             DATA   AGE
ceph-csi-config                  1      57m
ceph-csi-encryption-kms-config   1      18s
[root@node-1 volumes]# kubectl delete -f csi-rbdplugin.yaml #上述查看创建过程中看到已经超时，所以需要删除重新提交
daemonset.apps &quot;csi-rbdplugin&quot; deleted
service &quot;csi-metrics-rbdplugin&quot; deleted

[root@node-1 volumes]# kubectl delete -f csi-rbdplugin-provisioner.yaml #上述查看创建过程中看到已经超时，所以需要删除重新提交
service &quot;csi-rbdplugin-provisioner&quot; deleted
deployment.apps &quot;csi-rbdplugin-provisioner&quot; deleted

[root@node-1 volumes]# kubectl apply -f csi-rbdplugin-provisioner.yaml #重新提交
service/csi-rbdplugin-provisioner created
deployment.apps/csi-rbdplugin-provisioner created

[root@node-1 volumes]# kubectl apply -f csi-rbdplugin.yaml #重新提交
daemonset.apps/csi-rbdplugin created
service/csi-metrics-rbdplugin created
[root@node-1 volumes]# kubectl describe pods csi-rbdplugin-fl6tx #查看还是报错，缺少配置文件ceph-config
  Warning  FailedMount  119s (x13 over 12m)  kubelet, node-3  MountVolume.SetUp failed for volume &quot;ceph-config&quot; : configmap &quot;ceph-config&quot; not found
</code></pre>
<blockquote>
<p>文档来源：<a target="_blank" rel="noopener" href="https://docs.ceph.com/en/quincy/rbd/rbd-kubernetes/">https://docs.ceph.com/en/quincy/rbd/rbd-kubernetes/</a><br>在ceph的官方文档中查看最新版本的ceph-csi还需要另一个ConfigMap对象来定义 Ceph 配置以添加到 CSI 容器内的 ceph.conf 文件中：</p>
</blockquote>
<pre><code>[root@node-1 volumes]# cat &lt;&lt;EOF &gt; ceph-config-map.yaml #创建ceph-config文件
---
apiVersion: v1
kind: ConfigMap
data:
  ceph.conf: |
    [global]
    auth_cluster_required = cephx
    auth_service_required = cephx
    auth_client_required = cephx
  # keyring is a required key and its value should be empty
  keyring: |
metadata:
  name: ceph-config
EOF
[root@node-1 volumes]# kubectl apply -f ceph-config-map.yaml #提交
configmap/ceph-config created
[root@node-1 volumes]# kubectl delete -f csi-rbdplugin-provisioner.yaml #再次删除之前提交的内容
service &quot;csi-rbdplugin-provisioner&quot; deleted
deployment.apps &quot;csi-rbdplugin-provisioner&quot; deleted

[root@node-1 volumes]# kubectl delete -f csi-rbdplugin.yaml #再次删除
daemonset.apps &quot;csi-rbdplugin&quot; deleted
service &quot;csi-metrics-rbdplugin&quot; deleted
[root@node-1 volumes]# kubectl apply -f csi-rbdplugin-provisioner.yaml #重新提交
service/csi-rbdplugin-provisioner created
deployment.apps/csi-rbdplugin-provisioner created
[root@node-1 volumes]# kubectl apply -f csi-rbdplugin.yaml #重新提交
daemonset.apps/csi-rbdplugin created
service/csi-metrics-rbdplugin created
[root@node-1 volumes]# kubectl get pods #查看正在创建中
NAME                                         READY   STATUS              RESTARTS   AGE
csi-rbdplugin-bhlk2                          0/3     ContainerCreating   0          37s
csi-rbdplugin-provisioner-6748c759b4-6xqrs   0/7     Pending             0          49s
csi-rbdplugin-provisioner-6748c759b4-8gz5g   0/7     ContainerCreating   0          49s
csi-rbdplugin-provisioner-6748c759b4-fkxk5   0/7     ContainerCreating   0          49s
csi-rbdplugin-ttc95                          0/3     ContainerCreating   0          37s
nginx-6799fc88d8-jhzz7                       1/1     Running             3          3d15h
pod-demo                                     1/1     Running             1          19h
volume-rbd-demo                              1/1     Running             1          3d

[root@node-1 volumes]# kubectl describe pods csi-rbdplugin-bhlk2 #查看详情，可以看到已经没有报错信息了，正在下载镜像中
Events:
  Type    Reason     Age   From             Message
  ----    ------     ----  ----             -------
  Normal  Scheduled  48s                    Successfully assigned default/csi-rbdplugin-bhlk2 to node-2
  Normal  Pulling    47s   kubelet, node-2  Pulling image &quot;registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.2&quot;
  
  [root@node-1 volumes]# kubectl describe pods csi-rbdplugin-bhlk2 #查看详情，镜像下载失败，怀疑和下载源有关系
  Warning  Failed     53s (x2 over 118s)   kubelet, node-2  Error: ErrImagePull
  Warning  Failed     53s                  kubelet, node-2  Failed to pull image &quot;registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.2&quot;: rpc error: code = Unknown desc = Error response from daemon: Head &quot;https://asia-east1-docker.pkg.dev/v2/k8s-artifacts-prod/images/sig-storage/csi-node-driver-registrar/manifests/v2.6.2&quot;: dial tcp 108.177.97.82:443: connect: connection refused
  Normal   BackOff    39s (x3 over 117s)   kubelet, node-2  Back-off pulling image &quot;registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.2&quot;
</code></pre>
<p><strong>排障过程</strong></p>
<pre><code>#使用Docker 命令下载镜像，可以看到下载失败，基本确定已经是源地址镜像问题，导致无法正常启动
[root@node-1 volumes]# docker pull registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.2
Error response from daemon: Head &quot;https://asia-east1-docker.pkg.dev/v2/k8s-artifacts-prod/images/sig-storage/csi-node-driver-registrar/manifests/v2.6.2&quot;: dial tcp 74.125.204.82:443: connect: connection refused
[root@node-1 volumes]# docker pull registry.aliyuncs.com/google_containers/csi-node-driver-registrar:v2.6.2 #更换成阿里云地址，可以看到能够正常下载，记得Ctrl+c取消下载
v2.6.2: Pulling from google_containers/csi-node-driver-registrar
8fdb1fc20e24: Pull complete 
a13c28052fb0: Downloading [==============================&gt;                    ]  5.706MB/9.273MB
[root@node-1 volumes]# cat csi-rbdplugin-provisioner.yaml  #将配置文件中所有需要从registry.k8s.io下载的镜像替换成阿里云
---
kind: Service
apiVersion: v1
metadata:
  name: csi-rbdplugin-provisioner
  # replace with non-default namespace name
  namespace: default
  labels:
    app: csi-metrics
spec:
  selector:
    app: csi-rbdplugin-provisioner
  ports:
    - name: http-metrics
      port: 8080
      protocol: TCP
      targetPort: 8680

---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: csi-rbdplugin-provisioner
  # replace with non-default namespace name
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: csi-rbdplugin-provisioner
  template:
    metadata:
      labels:
        app: csi-rbdplugin-provisioner
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - csi-rbdplugin-provisioner
              topologyKey: &quot;kubernetes.io/hostname&quot;
      serviceAccountName: rbd-csi-provisioner
      priorityClassName: system-cluster-critical
      containers:
        - name: csi-provisioner
          image: registry.aliyuncs.com/google_containers/csi-provisioner:v3.3.0
          args:
            - &quot;--csi-address=$(ADDRESS)&quot;
            - &quot;--v=1&quot;
            - &quot;--timeout=150s&quot;
            - &quot;--retry-interval-start=500ms&quot;
            - &quot;--leader-election=true&quot;
            #  set it to true to use topology based provisioning
            - &quot;--feature-gates=Topology=false&quot;
            - &quot;--feature-gates=HonorPVReclaimPolicy=true&quot;
            - &quot;--prevent-volume-mode-conversion=true&quot;
            # if fstype is not specified in storageclass, ext4 is default
            - &quot;--default-fstype=ext4&quot;
            - &quot;--extra-create-metadata=true&quot;
          env:
            - name: ADDRESS
              value: unix:///csi/csi-provisioner.sock
          imagePullPolicy: &quot;IfNotPresent&quot;
          volumeMounts:
            - name: socket-dir
              mountPath: /csi
        - name: csi-snapshotter
          image: registry.aliyuncs.com/google_containers/csi-snapshotter:v6.1.0
          args:
            - &quot;--csi-address=$(ADDRESS)&quot;
            - &quot;--v=1&quot;
            - &quot;--timeout=150s&quot;
            - &quot;--leader-election=true&quot;
            - &quot;--extra-create-metadata=true&quot;
          env:
            - name: ADDRESS
              value: unix:///csi/csi-provisioner.sock
          imagePullPolicy: &quot;IfNotPresent&quot;
          volumeMounts:
            - name: socket-dir
              mountPath: /csi
        - name: csi-attacher
          image: registry.aliyuncs.com/google_containers/csi-attacher:v4.0.0
          args:
            - &quot;--v=1&quot;
            - &quot;--csi-address=$(ADDRESS)&quot;
            - &quot;--leader-election=true&quot;
            - &quot;--retry-interval-start=500ms&quot;
            - &quot;--default-fstype=ext4&quot;
          env:
            - name: ADDRESS
              value: /csi/csi-provisioner.sock
          imagePullPolicy: &quot;IfNotPresent&quot;
          volumeMounts:
            - name: socket-dir
              mountPath: /csi
        - name: csi-resizer
          image: registry.aliyuncs.com/google_containers/csi-resizer:v1.6.0
          args:
            - &quot;--csi-address=$(ADDRESS)&quot;
            - &quot;--v=1&quot;
            - &quot;--timeout=150s&quot;
            - &quot;--leader-election&quot;
            - &quot;--retry-interval-start=500ms&quot;
            - &quot;--handle-volume-inuse-error=false&quot;
            - &quot;--feature-gates=RecoverVolumeExpansionFailure=true&quot;
          env:
            - name: ADDRESS
              value: unix:///csi/csi-provisioner.sock
          imagePullPolicy: &quot;IfNotPresent&quot;
          volumeMounts:
            - name: socket-dir
              mountPath: /csi
        - name: csi-rbdplugin
          # for stable functionality replace canary with latest release version
          image: quay.io/cephcsi/cephcsi:canary
          args:
            - &quot;--nodeid=$(NODE_ID)&quot;
            - &quot;--type=rbd&quot;
            - &quot;--controllerserver=true&quot;
            - &quot;--endpoint=$(CSI_ENDPOINT)&quot;
            - &quot;--csi-addons-endpoint=$(CSI_ADDONS_ENDPOINT)&quot;
            - &quot;--v=5&quot;
            - &quot;--drivername=rbd.csi.ceph.com&quot;
            - &quot;--pidlimit=-1&quot;
            - &quot;--rbdhardmaxclonedepth=8&quot;
            - &quot;--rbdsoftmaxclonedepth=4&quot;
            - &quot;--enableprofiling=false&quot;
            - &quot;--setmetadata=true&quot;
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NODE_ID
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            # - name: KMS_CONFIGMAP_NAME
            #   value: encryptionConfig
            - name: CSI_ENDPOINT
              value: unix:///csi/csi-provisioner.sock
            - name: CSI_ADDONS_ENDPOINT
              value: unix:///csi/csi-addons.sock
          imagePullPolicy: &quot;IfNotPresent&quot;
          volumeMounts:
            - name: socket-dir
              mountPath: /csi
            - mountPath: /dev
              name: host-dev
            - mountPath: /sys
              name: host-sys
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
            - name: ceph-csi-config
              mountPath: /etc/ceph-csi-config/
            - name: ceph-csi-encryption-kms-config
              mountPath: /etc/ceph-csi-encryption-kms-config/
            - name: keys-tmp-dir
              mountPath: /tmp/csi/keys
            - name: ceph-config
              mountPath: /etc/ceph/
            - name: oidc-token
              mountPath: /run/secrets/tokens
              readOnly: true
        - name: csi-rbdplugin-controller
          # for stable functionality replace canary with latest release version
          image: quay.io/cephcsi/cephcsi:canary
          args:
            - &quot;--type=controller&quot;
            - &quot;--v=5&quot;
            - &quot;--drivername=rbd.csi.ceph.com&quot;
            - &quot;--drivernamespace=$(DRIVER_NAMESPACE)&quot;
            - &quot;--setmetadata=true&quot;
          env:
            - name: DRIVER_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          imagePullPolicy: &quot;IfNotPresent&quot;
          volumeMounts:
            - name: ceph-csi-config
              mountPath: /etc/ceph-csi-config/
            - name: keys-tmp-dir
              mountPath: /tmp/csi/keys
            - name: ceph-config
              mountPath: /etc/ceph/
        - name: liveness-prometheus
          image: quay.io/cephcsi/cephcsi:canary
          args:
            - &quot;--type=liveness&quot;
            - &quot;--endpoint=$(CSI_ENDPOINT)&quot;
            - &quot;--metricsport=8680&quot;
            - &quot;--metricspath=/metrics&quot;
            - &quot;--polltime=60s&quot;
            - &quot;--timeout=3s&quot;
          env:
            - name: CSI_ENDPOINT
              value: unix:///csi/csi-provisioner.sock
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - name: socket-dir
              mountPath: /csi
          imagePullPolicy: &quot;IfNotPresent&quot;
      volumes:
        - name: host-dev
          hostPath:
            path: /dev
        - name: host-sys
          hostPath:
            path: /sys
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: socket-dir
          emptyDir: &#123;
            medium: &quot;Memory&quot;
          &#125;
        - name: ceph-config
          configMap:
            name: ceph-config
        - name: ceph-csi-config
          configMap:
            name: ceph-csi-config
        - name: ceph-csi-encryption-kms-config
          configMap:
            name: ceph-csi-encryption-kms-config
        - name: keys-tmp-dir
          emptyDir: &#123;
            medium: &quot;Memory&quot;
          &#125;
        - name: oidc-token
          projected:
            sources:
              - serviceAccountToken:
                  path: oidc-token
                  expirationSeconds: 3600
                  audience: ceph-csi-kms
[root@node-1 volumes]# cat csi-rbdplugin.yaml  #将配置文件中所有需要从registry.k8s.io下载的镜像替换成阿里云
---
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: csi-rbdplugin
  # replace with non-default namespace name
  namespace: default
spec:
  selector:
    matchLabels:
      app: csi-rbdplugin
  template:
    metadata:
      labels:
        app: csi-rbdplugin
    spec:
      serviceAccountName: rbd-csi-nodeplugin
      hostNetwork: true
      hostPID: true
      priorityClassName: system-node-critical
      # to use e.g. Rook orchestrated cluster, and mons&#39; FQDN is
      # resolved through k8s service, set dns policy to cluster first
      dnsPolicy: ClusterFirstWithHostNet
      containers:
        - name: driver-registrar
          # This is necessary only for systems with SELinux, where
          # non-privileged sidecar containers cannot access unix domain socket
          # created by privileged CSI driver container.
          securityContext:
            privileged: true
            allowPrivilegeEscalation: true
          image: registry.aliyuncs.com/google_containers/csi-node-driver-registrar:v2.6.2
          args:
            - &quot;--v=1&quot;
            - &quot;--csi-address=/csi/csi.sock&quot;
            - &quot;--kubelet-registration-path=/var/lib/kubelet/plugins/rbd.csi.ceph.com/csi.sock&quot;
          env:
            - name: KUBE_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          volumeMounts:
            - name: socket-dir
              mountPath: /csi
            - name: registration-dir
              mountPath: /registration
        - name: csi-rbdplugin
          securityContext:
            privileged: true
            capabilities:
              add: [&quot;SYS_ADMIN&quot;]
            allowPrivilegeEscalation: true
          # for stable functionality replace canary with latest release version
          image: quay.io/cephcsi/cephcsi:canary
          args:
            - &quot;--nodeid=$(NODE_ID)&quot;
            - &quot;--pluginpath=/var/lib/kubelet/plugins&quot;
            - &quot;--stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/&quot;
            - &quot;--type=rbd&quot;
            - &quot;--nodeserver=true&quot;
            - &quot;--endpoint=$(CSI_ENDPOINT)&quot;
            - &quot;--csi-addons-endpoint=$(CSI_ADDONS_ENDPOINT)&quot;
            - &quot;--v=5&quot;
            - &quot;--drivername=rbd.csi.ceph.com&quot;
            - &quot;--enableprofiling=false&quot;
            # If topology based provisioning is desired, configure required
            # node labels representing the nodes topology domain
            # and pass the label names below, for CSI to consume and advertise
            # its equivalent topology domain
            # - &quot;--domainlabels=failure-domain/region,failure-domain/zone&quot;
            #
            # Options to enable read affinity.
            # If enabled Ceph CSI will fetch labels from kubernetes node and
            # pass `read_from_replica=localize,crush_location=type:value` during
            # rbd map command. refer:
            # https://docs.ceph.com/en/latest/man/8/rbd/#kernel-rbd-krbd-options
            # for more details.
            # - &quot;--enable-read-affinity=true&quot;
            # - &quot;--crush-location-labels=topology.io/zone,topology.io/rack&quot;
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NODE_ID
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            # - name: KMS_CONFIGMAP_NAME
            #   value: encryptionConfig
            - name: CSI_ENDPOINT
              value: unix:///csi/csi.sock
            - name: CSI_ADDONS_ENDPOINT
              value: unix:///csi/csi-addons.sock
          imagePullPolicy: &quot;IfNotPresent&quot;
          volumeMounts:
            - name: socket-dir
              mountPath: /csi
            - mountPath: /dev
              name: host-dev
            - mountPath: /sys
              name: host-sys
            - mountPath: /run/mount
              name: host-mount
            - mountPath: /etc/selinux
              name: etc-selinux
              readOnly: true
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
            - name: ceph-csi-config
              mountPath: /etc/ceph-csi-config/
            - name: ceph-csi-encryption-kms-config
              mountPath: /etc/ceph-csi-encryption-kms-config/
            - name: plugin-dir
              mountPath: /var/lib/kubelet/plugins
              mountPropagation: &quot;Bidirectional&quot;
            - name: mountpoint-dir
              mountPath: /var/lib/kubelet/pods
              mountPropagation: &quot;Bidirectional&quot;
            - name: keys-tmp-dir
              mountPath: /tmp/csi/keys
            - name: ceph-logdir
              mountPath: /var/log/ceph
            - name: ceph-config
              mountPath: /etc/ceph/
            - name: oidc-token
              mountPath: /run/secrets/tokens
              readOnly: true
        - name: liveness-prometheus
          securityContext:
            privileged: true
            allowPrivilegeEscalation: true
          image: quay.io/cephcsi/cephcsi:canary
          args:
            - &quot;--type=liveness&quot;
            - &quot;--endpoint=$(CSI_ENDPOINT)&quot;
            - &quot;--metricsport=8680&quot;
            - &quot;--metricspath=/metrics&quot;
            - &quot;--polltime=60s&quot;
            - &quot;--timeout=3s&quot;
          env:
            - name: CSI_ENDPOINT
              value: unix:///csi/csi.sock
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          volumeMounts:
            - name: socket-dir
              mountPath: /csi
          imagePullPolicy: &quot;IfNotPresent&quot;
      volumes:
        - name: socket-dir
          hostPath:
            path: /var/lib/kubelet/plugins/rbd.csi.ceph.com
            type: DirectoryOrCreate
        - name: plugin-dir
          hostPath:
            path: /var/lib/kubelet/plugins
            type: Directory
        - name: mountpoint-dir
          hostPath:
            path: /var/lib/kubelet/pods
            type: DirectoryOrCreate
        - name: ceph-logdir
          hostPath:
            path: /var/log/ceph
            type: DirectoryOrCreate
        - name: registration-dir
          hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: Directory
        - name: host-dev
          hostPath:
            path: /dev
        - name: host-sys
          hostPath:
            path: /sys
        - name: etc-selinux
          hostPath:
            path: /etc/selinux
        - name: host-mount
          hostPath:
            path: /run/mount
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: ceph-config
          configMap:
            name: ceph-config
        - name: ceph-csi-config
          configMap:
            name: ceph-csi-config
        - name: ceph-csi-encryption-kms-config
          configMap:
            name: ceph-csi-encryption-kms-config
        - name: keys-tmp-dir
          emptyDir: &#123;
            medium: &quot;Memory&quot;
          &#125;
        - name: oidc-token
          projected:
            sources:
              - serviceAccountToken:
                  path: oidc-token
                  expirationSeconds: 3600
                  audience: ceph-csi-kms
---
# This is a service to expose the liveness metrics
apiVersion: v1
kind: Service
metadata:
  name: csi-metrics-rbdplugin
  # replace with non-default namespace name
  namespace: default
  labels:
    app: csi-metrics
spec:
  ports:
    - name: http-metrics
      port: 8080
      protocol: TCP
      targetPort: 8680
  selector:
    app: csi-rbdplugin

[root@node-1 volumes]#kubectl delete -f csi-rbdplugin-provisioner.yaml #删除原来提交的
[root@node-1 volumes]#kubectl delete -f csi-rbdplugin.yaml #删除原来提交的
[root@node-1 volumes]#kubectl apply -f csi-rbdplugin-provisioner.yaml #重新提交
[root@node-1 volumes]#kubectl apply -f csi-rbdplugin.yaml #重新提交
[root@node-1 volumes]# kubectl get pods -o wide
NAME                                         READY   STATUS    RESTARTS   AGE     IP                NODE     NOMINATED NODE   READINESS GATES
csi-rbdplugin-mzbgp                          3/3     Running   0          6m14s   192.168.199.203   node-3   &lt;none&gt;           &lt;none&gt;
csi-rbdplugin-provisioner-79565c5f56-4hsnk   0/7     Pending   0          6m11s   &lt;none&gt;            &lt;none&gt;   &lt;none&gt;           &lt;none&gt;
csi-rbdplugin-provisioner-79565c5f56-dmnmv   7/7     Running   0          6m11s   10.244.139.78     node-3   &lt;none&gt;           &lt;none&gt;
csi-rbdplugin-provisioner-79565c5f56-j499w   7/7     Running   0          6m11s   10.244.247.36     node-2   &lt;none&gt;           &lt;none&gt;
csi-rbdplugin-z7tq8                          3/3     Running   0          6m14s   192.168.199.202   node-2   &lt;none&gt;           &lt;none&gt;
nginx-6799fc88d8-jhzz7                       1/1     Running   3          3d16h   10.244.247.24     node-2   &lt;none&gt;           &lt;none&gt;
pod-demo                                     1/1     Running   1          21h     10.244.139.71     node-3   &lt;none&gt;           &lt;none&gt;
volume-rbd-demo                              1/1     Running   1          3d1h    10.244.247.29     node-2   &lt;none&gt;           &lt;none&gt;
</code></pre>
<blockquote>
<p>查看创建过程中发现仍然无法成功创建，由于我当前k8s环境只有三个机器，一个master，两个node，所以第三个csi创建的时候不能部署到master节点，一直报错，这属于k8s的污染，将配置重新修改后即可</p>
</blockquote>
<pre><code>[root@node-1 volumes]# kubectl describe pods csi-rbdplugin-provisioner-79565c5f56-4hsnk
Events:
  Type     Reason            Age    From  Message
  ----     ------            ----   ----  -------
  Warning  FailedScheduling  6m28s        0/3 nodes are available: 1 node(s) had taint &#123;node-role.kubernetes.io/master: &#125;, that the pod didn&#39;t tolerate, 2 node(s) didn&#39;t match pod affinity/anti-affinity, 2 node(s) didn&#39;t match pod anti-affinity rules.
  Warning  FailedScheduling  6m27s        0/3 nodes are available: 1 node(s) had taint &#123;node-role.kubernetes.io/master: &#125;, that the pod didn&#39;t tolerate, 2 node(s) didn&#39;t match pod affinity/anti-affinity, 2 node(s) didn&#39;t match pod anti-affinity rules.
 
#####这个方法没试，可以试试
 在ceph-csi/deploy/rbd/kubernetes/csi-rbdplugin.yaml中的DaemonSet的spec中添加如下配置：
  tolerations:
   - key: node-role.kubernetes.io/master
     effect: NoSchedule
#####
     
[root@node-1 volumes]# kubectl describe nodes node-1 |grep -E &#39;(Roles|Taints)&#39; #查看默认设置了master节点不允许部署pod，将该配置取消即可
Roles:              master
Taints:             node-role.kubernetes.io/master:NoSchedule
[root@node-1 volumes]# kubectl taint node node-1 node-role.kubernetes.io/master- #删除标记，使Master节点允许部署pod
node/node-1 untainted
# 后续可这两个个命令重新标记
[root@node-1 volumes]# kubectl taint nodes $node_name node-role.kubernetes.io/master=:NoSchedule #重新添加标记，使Master节点不允许部署pod

[root@node-1 volumes]# kubectl taint nodes $node_name node-role.kubernetes.io/control-plane=:NoSchedule #重新添加标记，使Master节点不允许部署pod
[root@node-1 volumes]# kubectl describe nodes node-1 |grep -E &#39;(Roles|Taints)&#39; #查看标记已经被清除
Roles:              master
Taints:             &lt;none&gt;
[root@node-1 volumes]# kubectl get pods #查看状态
NAME                                         READY   STATUS    RESTARTS   AGE
csi-rbdplugin-2bbdf                          3/3     Running   0          7m35s
csi-rbdplugin-ghb7k                          3/3     Running   0          7m35s
csi-rbdplugin-provisioner-79565c5f56-clkbd   7/7     Running   0          7m44s
csi-rbdplugin-provisioner-79565c5f56-mdggw   7/7     Running   0          7m44s
csi-rbdplugin-provisioner-79565c5f56-zvxpx   7/7     Running   0          7m44s
csi-rbdplugin-z27pz                          3/3     Running   0          7m35s
nginx-6799fc88d8-jhzz7                       1/1     Running   3          3d16h
pod-demo                                     1/1     Running   1          21h
volume-rbd-demo                              1/1     Running   1          3d2h
[root@node-1 volumes]# kubectl get deployments.apps  #查看三个机器的csi全部正常运行
NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
csi-rbdplugin-provisioner   3/3     3            3           8m13s
nginx                       1/1     1            1           3d16h
</code></pre>
<h1><span id="6-使用-ceph-块设备">6、使用 CEPH 块设备</span></h1><h2><span id="1-创建存储类">1、创建存储类</span></h2><blockquote>
<p>Kubernetes StorageClass定义了一类存储。可以创建多个StorageClass 对象以映射到不同的服务质量级别（即 NVMe 与基于 HDD 的池）和功能。<br>例如，要创建映射到上面创建的kubernetes池的ceph-csi StorageClass ，在确保“clusterID”属性与您的 Ceph 集群的fsid 匹配后，可以使用以下 YAML 文件：</p>
</blockquote>
<pre><code>[root@node-1 volumes]#cat &lt;&lt;EOF &gt; csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8 #ceph的集群ID
   pool: kubernetes #ceph的存储池
   imageFeatures: layering
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret #指定使用的key
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
   csi.storage.k8s.io/controller-expand-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete #回收的策略
allowVolumeExpansion: true
mountOptions:
   - discard
EOF
[root@node-1 volumes]# vim csi-rbd-sc.yaml #编辑配置文件
   clusterID: b8e58b30-4568-4032-a9f4-837ed3fa9529 #将此ID更换成本地Ceph的集群ID       
[root@node-1 volumes]# kubectl get secrets  #查看上述配置文件中指定的key
NAME                              TYPE                                  DATA   AGE
ceph-secret                       kubernetes.io/rbd                     1      3d4h
csi-rbd-secret                    Opaque                                2      3h36m
default-token-p5dsd               kubernetes.io/service-account-token   3      3d17h
rbd-csi-nodeplugin-token-4fvjf    kubernetes.io/service-account-token   3      3h21m
rbd-csi-provisioner-token-kpbsw   kubernetes.io/service-account-token   3      3h21m
[root@node-1 volumes]# kubectl get secrets csi-rbd-secret -o yaml #查看key的具体信息，就是上述创建的
apiVersion: v1
data:
  userID: a3ViZXJuZXRlcw==
  userKey: QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQ==
kind: Secret
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Secret&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;csi-rbd-secret&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;stringData&quot;:&#123;&quot;userID&quot;:&quot;kubernetes&quot;,&quot;userKey&quot;:&quot;AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ==&quot;&#125;&#125;
  creationTimestamp: &quot;2023-03-18T04:38:20Z&quot;
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: &#123;&#125;
        f:userID: &#123;&#125;
        f:userKey: &#123;&#125;
      f:metadata:
        f:annotations:
          .: &#123;&#125;
          f:kubectl.kubernetes.io/last-applied-configuration: &#123;&#125;
      f:type: &#123;&#125;
    manager: kubectl-client-side-apply
    operation: Update
    time: &quot;2023-03-18T04:38:20Z&quot;
  name: csi-rbd-secret
  namespace: default
  resourceVersion: &quot;593749&quot;
  selfLink: /api/v1/namespaces/default/secrets/csi-rbd-secret
  uid: 712d89bf-d3b2-44e5-bf91-973f76b15830
type: Opaque

[root@node-1 volumes]#  kubectl apply -f csi-rbd-sc.yaml  #提交
storageclass.storage.k8s.io/csi-rbd-sc created 
[root@node-1 volumes]# kubectl get sc #查看状态，sc是StorageClass的简写
NAME         PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
csi-rbd-sc   rbd.csi.ceph.com   Delete          Immediate           true                   31s
</code></pre>
<blockquote>
<p>请注意，在 Kubernetes v1.14 和 v1.15 中，卷扩展功能处于 alpha 状态，需要启用ExpandCSIVolumes功能门。</p>
</blockquote>
<h2><span id="2-创建persistentvolumeclaim使用pv创建">2、创建PERSISTENTVOLUMECLAIM（使用PV创建）</span></h2><blockquote>
<p>PersistentVolumeClaim是用户对抽象存储资源的请求。然后， PersistentVolumeClaim将关联到Pod资源以提供一个PersistentVolume ，该PersistentVolume将由 Ceph 块镜像支持。可以包含一个可选的volumeMode以在已安装的文件系统（默认）或基于原始块设备的卷之间进行选择。<br>使用ceph-csi ，为volumeMode指定Filesystem可以同时支持 ReadWriteOnce和ReadOnlyMany accessMode声明， 为volumeMode指定Block可以支持ReadWriteOnce，ReadWriteMany和 ReadOnlyMany accessMode声明。</p>
</blockquote>
<blockquote>
<p>例如，要创建一个基于块的PersistentVolumeClaim，它利用上面创建的基于ceph-csi的StorageClass，可以使用以下 YAML 从csi-rbd-sc StorageClass请求原始块存储：</p>
</blockquote>
<h3><span id="1-官方文档">1、官方文档</span></h3><pre><code>$ cat &lt;&lt;EOF &gt; raw-block-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: raw-block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
$ kubectl apply -f raw-block-pvc.yaml
</code></pre>
<blockquote>
<p>以下演示和示例将上述 PersistentVolumeClaim绑定到Pod资源作为原始块设备：</p>
</blockquote>
<pre><code>$ cat &lt;&lt;EOF &gt; raw-block-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-raw-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: [&quot;/bin/sh&quot;, &quot;-c&quot;]
      args: [&quot;tail -f /dev/null&quot;]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: raw-block-pvc
EOF
$ kubectl apply -f raw-block-pod.yaml
</code></pre>
<blockquote>
<p>要使用上面创建 的基于ceph-csi的StorageClass创建基于文件系统的PersistentVolumeClaim，可以使用以下 YAML 从 csi -rbd-sc StorageClass请求挂载的文件系统（由 RBD 映像支持） ：</p>
</blockquote>
<pre><code>$ cat &lt;&lt;EOF &gt; pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
$ kubectl apply -f pvc.yaml
</code></pre>
<blockquote>
<p>下面演示和示例将上述 PersistentVolumeClaim绑定到Pod资源作为挂载的文件系统：</p>
</blockquote>
<pre><code>$ cat &lt;&lt;EOF &gt; pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: csi-rbd-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: rbd-pvc
        readOnly: false
EOF
$ kubectl apply -f pod.yaml
</code></pre>
<h3><span id="2-实操">2、实操</span></h3><blockquote>
<p>使用文件系统级别去声明，BLOCK方式测试创建成功，但是不明白啥意思，后续可以查找一下相关文档</p>
</blockquote>
<pre><code>[root@node-1 volumes]#cat &lt;&lt;EOF &gt; pvc.yaml #创建文件，申请1G的空间
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc #名称
spec:
  accessModes:
    - ReadWriteOnce #一个客户端对它进行读写
  volumeMode: Filesystem #类型是文件系统
  resources:
    requests:
      storage: 1Gi #申请的空间是1G
  storageClassName: csi-rbd-sc #SC的名称
EOF
[root@node-1 volumes]# kubectl apply -f pvc.yaml  #提交申请
persistentvolumeclaim/rbd-pvc created
[root@node-1 volumes]# kubectl get pvc #查看自动创建的PVC
NAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-demo   Bound    rbd-demo                                   10G        RWO            rbd            3d2h
rbd-pvc    Bound    pvc-986ef488-0f94-4e3f-b963-3990e3bf0000   1Gi        RWO            csi-rbd-sc     22s
[root@node-1 volumes]# kubectl get pv  #查看PVC创建的PV
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   REASON   AGE
pvc-986ef488-0f94-4e3f-b963-3990e3bf0000   1Gi        RWO            Delete           Bound    default/rbd-pvc    csi-rbd-sc              72s
rbd-demo                                   10G        RWO            Retain           Bound    default/pvc-demo   rbd                     3d2h
[root@node-1 volumes]# rbd ls kubernetes  #查看ceph存储中的设备
csi-vol-9141d3e3-1b67-4ef0-b29f-c93e69254a51
demo-1.img
rbd.img
[root@node-1 volumes]# rbd info kubernetes/csi-vol-9141d3e3-1b67-4ef0-b29f-c93e69254a51 #查看大小为1G
rbd image &#39;csi-vol-9141d3e3-1b67-4ef0-b29f-c93e69254a51&#39;:
    size 1 GiB in 256 objects
    order 22 (4 MiB objects)
    snapshot_count: 0
    id: 4f248f369f6cc
    block_name_prefix: rbd_data.4f248f369f6cc
    format: 2
    features: layering
    op_features: 
    flags: 
    create_timestamp: Sat Mar 18 16:47:54 2023
    access_timestamp: Sat Mar 18 16:47:54 2023
    modify_timestamp: Sat Mar 18 16:47:54 2023
</code></pre>
<blockquote>
<p>如果创建不成功可能就是版本的问题，创建的Kubernetes用户无法接入到ceph中，提示权限错误，这里我没有遇到，如果遇到了可以编辑下面的配置文件，将Kubernetes的用户信息替换成ceph的admin用户，并重新提交即可</p>
</blockquote>
<p><img src="/images/Ceph/Ceph%E4%B8%8Ekubernetes%E9%9B%86%E6%88%90/2.jpg"></p>
<h3><span id="3-容器使用">3、容器使用</span></h3><p>下面演示和示例将上述 PersistentVolumeClaim绑定到Pod资源作为挂载的文件系统：</p>
<pre><code>[root@node-1 volumes]#cat &lt;&lt;EOF &gt; pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: csi-rbd-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html #设置挂载的地方
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: rbd-pvc #调用创建的PVC
        readOnly: false
EOF    
[root@node-1 volumes]# kubectl apply -f pod.yaml
[root@node-1 volumes]# kubectl exec -ti csi-rbd-demo-pod bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@csi-rbd-demo-pod:/# 
root@csi-rbd-demo-pod:/# 
root@csi-rbd-demo-pod:/# df -HT
Filesystem              Type     Size  Used Avail Use% Mounted on
overlay                 overlay   19G  6.2G   13G  34% /
tmpfs                   tmpfs     68M     0   68M   0% /dev
tmpfs                   tmpfs    954M     0  954M   0% /sys/fs/cgroup
/dev/mapper/centos-root xfs       19G  6.2G   13G  34% /etc/hosts
shm                     tmpfs     68M     0   68M   0% /dev/shm
/dev/rbd1               ext4     1.1G  2.7M  1.1G   1% /var/lib/www/html
tmpfs                   tmpfs    954M   13k  954M   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                   tmpfs    954M     0  954M   0% /proc/acpi
tmpfs                   tmpfs    954M     0  954M   0% /proc/scsi
tmpfs                   tmpfs    954M     0  954M   0% /sys/firmware
</code></pre>
<blockquote>
<p>如果无法创建容器，看到是因为无法挂载导致的，可能是因为下面的配置没有定义好，需要重新修改并上传</p>
</blockquote>
<pre><code>[root@node-1 volumes]# cat csi-rbd-sc.yaml 
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: b8e58b30-4568-4032-a9f4-837ed3fa9529
   pool: kubernetes
   imageFeatures: layering #这个地方没有定义的话创建容器的时候无法正常挂载PVC
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
   csi.storage.k8s.io/controller-expand-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
   - discard
</code></pre>
<h1><span id="六-statefulset">六、StatefulSet</span></h1><p><a target="_blank" rel="noopener" href="https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/statefulset/">参考文档</a></p>
<pre><code>[root@node-1 volumes]# kubectl get sc #获取SC的名称
NAME         PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
csi-rbd-sc   rbd.csi.ceph.com   Delete          Immediate           true                   57m
[root@node-1 volumes]# vim nginx.yaml  #创建nginx.yaml的文件
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  type: NodePort
  #clusterIP: None #这个参数报错，需要注释掉，增加上面的type: NodePort
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx
  serviceName: &quot;nginx&quot; # 必须匹配 .spec.template.metadata.labels
  replicas: 3 # 默认值是 1
  minReadySeconds: 10 # 默认值是 0 ，最好把这个命令去掉，要不会无法提交
  template:
    metadata:
      labels:
        app: nginx
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: registry.aliyuncs.com/google_containers/nginx-slim:0.8 #需要将镜像地址替换成阿里云的
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates: #声明使用StorageClass存储
  - metadata:
      name: www
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      storageClassName: &quot;csi-rbd-sc&quot; #需要将sc 的名称修改成上述创建的SC名称
      resources:
        requests:
          storage: 1Gi
[root@node-1 volumes]# kubectl get sts
NAME   READY   AGE
web    3/3     17m
[root@node-1 volumes]# kubectl get pods
NAME                                         READY   STATUS    RESTARTS   AGE
csi-rbd-demo-pod                             1/1     Running   0          49m
csi-rbdplugin-2bbdf                          3/3     Running   0          149m
csi-rbdplugin-ghb7k                          3/3     Running   0          149m
csi-rbdplugin-provisioner-79565c5f56-clkbd   7/7     Running   0          149m
csi-rbdplugin-provisioner-79565c5f56-mdggw   7/7     Running   0          149m
csi-rbdplugin-provisioner-79565c5f56-zvxpx   7/7     Running   0          149m
csi-rbdplugin-z27pz                          3/3     Running   0          149m
nginx-6799fc88d8-4f4kw                       1/1     Running   0          29m
pod-demo                                     1/1     Running   1          24h
volume-rbd-demo                              1/1     Running   1          3d4h
web-0                                        1/1     Running   0          6m30s
web-1                                        1/1     Running   0          6m15s
web-2                                        1/1     Running   0          5m53s
[root@node-1 volumes]# kubectl exec -ti web-0 bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@web-0:/# df -HT 
Filesystem              Type     Size  Used Avail Use% Mounted on
overlay                 overlay   19G  6.6G   12G  36% /
tmpfs                   tmpfs     68M     0   68M   0% /dev
tmpfs                   tmpfs    2.0G     0  2.0G   0% /sys/fs/cgroup
/dev/mapper/centos-root xfs       19G  6.6G   12G  36% /etc/hosts
shm                     tmpfs     68M     0   68M   0% /dev/shm
/dev/rbd1               ext4     1.1G  2.7M  1.1G   1% /usr/share/nginx/html
tmpfs                   tmpfs    2.0G   13k  2.0G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                   tmpfs    2.0G     0  2.0G   0% /proc/acpi
tmpfs                   tmpfs    2.0G     0  2.0G   0% /proc/scsi
tmpfs                   tmpfs    2.0G     0  2.0G   0% /sys/firmware
[root@node-2 ~]# rbd device list 
id pool       namespace image                                        snap device    
0  kubernetes           rbd.img                                      -    /dev/rbd0 
1  kubernetes           csi-vol-d8c2ae42-d57b-4312-88d6-dcfd769c9b11 -    /dev/rbd1 
[root@node-1 volumes]# kubectl get pvc
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-demo    Bound    rbd-demo                                   10G        RWO            rbd            3d3h
rbd-pvc     Bound    pvc-986ef488-0f94-4e3f-b963-3990e3bf0000   1Gi        RWO            csi-rbd-sc     64m
www-web-0   Bound    pvc-d542bca8-04b3-4525-acd3-60aa9d39d8f3   1Gi        RWO            csi-rbd-sc     19m
www-web-1   Bound    pvc-8dbc24e4-bede-44e5-9efa-eae32d55f7e2   1Gi        RWO            csi-rbd-sc     7m54s
www-web-2   Bound    pvc-c63540da-d4f4-4c05-8db4-2636ca290dac   1Gi        RWO            csi-rbd-sc     7m32s
[root@node-1 volumes]# rbd ls kubernetes
csi-vol-50dfd42d-4e69-4cfa-8dd3-627bcd80f287
csi-vol-9141d3e3-1b67-4ef0-b29f-c93e69254a51
csi-vol-9f73997b-40b7-498c-8ae5-6e633ea7e192
csi-vol-d8c2ae42-d57b-4312-88d6-dcfd769c9b11
demo-1.img
rbd.img
[root@node-1 volumes]#  kubectl get pv 
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS   REASON   AGE
pvc-8dbc24e4-bede-44e5-9efa-eae32d55f7e2   1Gi        RWO            Delete           Bound    default/www-web-1   csi-rbd-sc              8m59s
pvc-986ef488-0f94-4e3f-b963-3990e3bf0000   1Gi        RWO            Delete           Bound    default/rbd-pvc     csi-rbd-sc              66m
pvc-c63540da-d4f4-4c05-8db4-2636ca290dac   1Gi        RWO            Delete           Bound    default/www-web-2   csi-rbd-sc              8m37s
pvc-d542bca8-04b3-4525-acd3-60aa9d39d8f3   1Gi        RWO            Delete           Bound    default/www-web-0   csi-rbd-sc              20m
rbd-demo                                   10G        RWO            Retain           Bound    default/pvc-demo    rbd                     3d3h
</code></pre>
<script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">
        </article>

        
            
  <div class="nexmoe-post-copyright">
    <strong>Author：</strong>张博丞<br>
    
      <strong>From：</strong><a href="/%E5%8E%9F%E5%88%9B" title="原创" target="_blank" rel="noopener">原创</a><br>
    
    <strong>Link：</strong><a href="https://zhangboc.github.io/2023/04/20/Ceph/13.Ceph%E4%B8%8EKubernetes%E9%9B%86%E6%88%90/Ceph%E4%B8%8EKubernetes%E9%9B%86%E6%88%90/" title="https:&#x2F;&#x2F;zhangboc.github.io&#x2F;2023&#x2F;04&#x2F;20&#x2F;Ceph&#x2F;13.Ceph%E4%B8%8EKubernetes%E9%9B%86%E6%88%90&#x2F;Ceph%E4%B8%8EKubernetes%E9%9B%86%E6%88%90&#x2F;" target="_blank" rel="noopener">https:&#x2F;&#x2F;zhangboc.github.io&#x2F;2023&#x2F;04&#x2F;20&#x2F;Ceph&#x2F;13.Ceph%E4%B8%8EKubernetes%E9%9B%86%E6%88%90&#x2F;Ceph%E4%B8%8EKubernetes%E9%9B%86%E6%88%90&#x2F;</a><br>

    
      <strong>版权声明：</strong>本文采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank">CC BY-NC-SA 3.0 CN</a> 协议进行许可
    
  </div>


        

        <div class="nexmoe-post-meta nexmoe-rainbow">
    
        <a class="nexmoefont icon-appstore-fill -link" href="/categories/Ceph/">Ceph</a><a class="nexmoefont icon-appstore-fill -link" href="/categories/Ceph/Kubernetes/">Kubernetes</a>
    
    
        <a class="nexmoefont icon-tag-fill -none-link" href="/tags/Ceph/" rel="tag">Ceph</a> <a class="nexmoefont icon-tag-fill -none-link" href="/tags/Kubernetes/" rel="tag">Kubernetes</a>
    
</div>

    <div class="nexmoe-post-footer">
        <section class="nexmoe-comment">
    <div class="valine"></div>
<script src='https://lib.baomitu.com/valine/1.3.9/Valine.min.js'></script>
<script>
    // 使用方法 https://valine.js.org/quickstart.html
    new Valine({
        el: '.valine',
        appId: 'r5zxC0st0DDjPA9auXzMV7HY-gzGzoHsz',
        appKey: '3bqCsovpyfTPHUzTHovd3V3V'
    })
</script>
</section>
    </div>
</div>
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>
<!-- 代码语言 -->
<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>
<!-- 代码块复制 -->
<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>
<script type="text/javascript" src="/libs/codeBlock/clipboard.min.js"></script>
<!-- 代码块收缩 -->
<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script> 
<!-- 代码块折行 -->
<style type="text/css">code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }</style>

        <div class="nexmoe-post-right">
          
            <div class="nexmoe-fixed">
              <div class="nexmoe-tool">
                <a href="#" aria-label="回到顶部" title="top"><button class="mdui-fab mdui-ripple"><i class="nexmoefont icon-caret-top"></i></button></a>
              </div>
            </div>
          
        </div>
    </div>
  </div>
  <div id="nexmoe-pendant">
    <div class="nexmoe-drawer mdui-drawer nexmoe-pd" id="drawer">
        
            <div class="nexmoe-pd-item">
                <div class="clock">
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="memory"></div>
        <div class="needle" id="hours"></div>
        <div class="needle" id="minutes"></div>
        <div class="needle" id="seconds"></div>
        <div class="clock_logo">

        </div>

    </div>
<style>
    .clock {
        background-color: #ffffff;
        width: 70vw;
        height: 70vw;
        max-width: 70vh;
        max-height: 70vh;
        border: solid 2.8vw #242424;
        position: relative;
        overflow: hidden;
        border-radius: 50%;
        box-sizing: border-box;
        box-shadow: 0 1.4vw 2.8vw rgba(0, 0, 0, 0.8);
        zoom:0.2
    }

    .memory {
        position: absolute;
        top: 50%;
        left: 50%;
        transform-origin: center;
    }

    .memory:nth-child(1) {
        background-color: #424242;
        width: 2%;
        height: 8%;
        transform: translate(-50%, -50%) rotate(0deg) translateY(-520%);
    }

    .memory:nth-child(2) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(6deg) translateY(-1461%);
    }

    .memory:nth-child(3) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(12deg) translateY(-1461%);
    }

    .memory:nth-child(4) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(18deg) translateY(-1461%);
    }

    .memory:nth-child(5) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(24deg) translateY(-1461%);
    }

    .memory:nth-child(6) {
        background-color: #424242;
        width: 2%;
        height: 8%;
        transform: translate(-50%, -50%) rotate(30deg) translateY(-520%);
    }

    .memory:nth-child(7) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(36deg) translateY(-1461%);
    }

    .memory:nth-child(8) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(42deg) translateY(-1461%);
    }

    .memory:nth-child(9) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(48deg) translateY(-1461%);
    }

    .memory:nth-child(10) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(54deg) translateY(-1461%);
    }

    .memory:nth-child(11) {
        background-color: #424242;
        width: 2%;
        height: 8%;
        transform: translate(-50%, -50%) rotate(60deg) translateY(-520%);
    }

    .memory:nth-child(12) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(66deg) translateY(-1461%);
    }

    .memory:nth-child(13) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(72deg) translateY(-1461%);
    }

    .memory:nth-child(14) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(78deg) translateY(-1461%);
    }

    .memory:nth-child(15) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(84deg) translateY(-1461%);
    }

    .memory:nth-child(16) {
        background-color: #424242;
        width: 2%;
        height: 8%;
        transform: translate(-50%, -50%) rotate(90deg) translateY(-520%);
    }

    .memory:nth-child(17) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(96deg) translateY(-1461%);
    }

    .memory:nth-child(18) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(102deg) translateY(-1461%);
    }

    .memory:nth-child(19) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(108deg) translateY(-1461%);
    }

    .memory:nth-child(20) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(114deg) translateY(-1461%);
    }

    .memory:nth-child(21) {
        background-color: #424242;
        width: 2%;
        height: 8%;
        transform: translate(-50%, -50%) rotate(120deg) translateY(-520%);
    }

    .memory:nth-child(22) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(126deg) translateY(-1461%);
    }

    .memory:nth-child(23) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(132deg) translateY(-1461%);
    }

    .memory:nth-child(24) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(138deg) translateY(-1461%);
    }

    .memory:nth-child(25) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(144deg) translateY(-1461%);
    }

    .memory:nth-child(26) {
        background-color: #424242;
        width: 2%;
        height: 8%;
        transform: translate(-50%, -50%) rotate(150deg) translateY(-520%);
    }

    .memory:nth-child(27) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(156deg) translateY(-1461%);
    }

    .memory:nth-child(28) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(162deg) translateY(-1461%);
    }

    .memory:nth-child(29) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(168deg) translateY(-1461%);
    }

    .memory:nth-child(30) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(174deg) translateY(-1461%);
    }

    .memory:nth-child(31) {
        background-color: #424242;
        width: 2%;
        height: 8%;
        transform: translate(-50%, -50%) rotate(180deg) translateY(-520%);
    }

    .memory:nth-child(32) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(186deg) translateY(-1461%);
    }

    .memory:nth-child(33) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(192deg) translateY(-1461%);
    }

    .memory:nth-child(34) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(198deg) translateY(-1461%);
    }

    .memory:nth-child(35) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(204deg) translateY(-1461%);
    }

    .memory:nth-child(36) {
        background-color: #424242;
        width: 2%;
        height: 8%;
        transform: translate(-50%, -50%) rotate(210deg) translateY(-520%);
    }

    .memory:nth-child(37) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(216deg) translateY(-1461%);
    }

    .memory:nth-child(38) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(222deg) translateY(-1461%);
    }

    .memory:nth-child(39) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(228deg) translateY(-1461%);
    }

    .memory:nth-child(40) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(234deg) translateY(-1461%);
    }

    .memory:nth-child(41) {
        background-color: #424242;
        width: 2%;
        height: 8%;
        transform: translate(-50%, -50%) rotate(240deg) translateY(-520%);
    }

    .memory:nth-child(42) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(246deg) translateY(-1461%);
    }

    .memory:nth-child(43) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(252deg) translateY(-1461%);
    }

    .memory:nth-child(44) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(258deg) translateY(-1461%);
    }

    .memory:nth-child(45) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(264deg) translateY(-1461%);
    }

    .memory:nth-child(46) {
        background-color: #424242;
        width: 2%;
        height: 8%;
        transform: translate(-50%, -50%) rotate(270deg) translateY(-520%);
    }

    .memory:nth-child(47) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(276deg) translateY(-1461%);
    }

    .memory:nth-child(48) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(282deg) translateY(-1461%);
    }

    .memory:nth-child(49) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(288deg) translateY(-1461%);
    }

    .memory:nth-child(50) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(294deg) translateY(-1461%);
    }

    .memory:nth-child(51) {
        background-color: #424242;
        width: 2%;
        height: 8%;
        transform: translate(-50%, -50%) rotate(300deg) translateY(-520%);
    }

    .memory:nth-child(52) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(306deg) translateY(-1461%);
    }

    .memory:nth-child(53) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(312deg) translateY(-1461%);
    }

    .memory:nth-child(54) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(318deg) translateY(-1461%);
    }

    .memory:nth-child(55) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(324deg) translateY(-1461%);
    }

    .memory:nth-child(56) {
        background-color: #424242;
        width: 2%;
        height: 8%;
        transform: translate(-50%, -50%) rotate(330deg) translateY(-520%);
    }

    .memory:nth-child(57) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(336deg) translateY(-1461%);
    }

    .memory:nth-child(58) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(342deg) translateY(-1461%);
    }

    .memory:nth-child(59) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(348deg) translateY(-1461%);
    }

    .memory:nth-child(60) {
        background-color: #949494;
        width: 1%;
        height: 3%;
        transform: translate(-50%, -50%) rotate(354deg) translateY(-1461%);
    }

    .needle {
        position: absolute;
        top: 50%;
        left: 50%;
        transform-origin: center;
    }

    .needle#hours {
        background-color: #1f1f1f;
        width: 4%;
        height: 30%;
        transform-origin: center 75%;
        transform: translate(-50%, -75%);
    }

    .needle#hours.moving {
        transition: transform 150ms ease-out;
    }

    .needle#hours:after {
        content: '';
        background-color: #1f1f1f;
        width: 4vw;
        height: 4vw;
        max-width: 4vh;
        max-height: 4vh;
        display: block;
        position: absolute;
        top: 75%;
        left: 50%;
        box-sizing: border-box;
        border-radius: 50%;
        transform: translate(-50%, -50%);
    }

    .needle#minutes {
        background-color: #1f1f1f;
        width: 2%;
        height: 45%;
        transform-origin: center 75%;
        transform: translate(-50%, -75%);
    }

    .needle#minutes.moving {
        transition: transform 150ms ease-out;
    }

    .needle#minutes:after {
        content: '';
        background-color: #1f1f1f;
        width: 4vw;
        height: 4vw;
        max-width: 4vh;
        max-height: 4vh;
        display: block;
        position: absolute;
        top: 75%;
        left: 50%;
        box-sizing: border-box;
        border-radius: 50%;
        transform: translate(-50%, -50%);
    }

    .needle#seconds {
        background-color: #cb2f2f;
        width: 1%;
        height: 50%;
        transform-origin: center 75%;
        transform: translate(-50%, -75%);
    }

    .needle#seconds.moving {
        transition: transform 150ms ease-out;
    }

    .needle#seconds:after {
        content: '';
        background-color: #cb2f2f;
        width: 2.5vw;
        height: 2.5vw;
        max-width: 2.5vh;
        max-height: 2.5vh;
        display: block;
        position: absolute;
        top: 75%;
        left: 50%;
        box-sizing: border-box;
        border-radius: 50%;
        transform: translate(-50%, -50%);
    }
    .clock_logo{
        width: 10vw;
        height: 10vw;
        max-width: 10vh;
        max-height: 10vh;
        position: absolute;
        top: 50%;
        left: 50%;
        box-sizing: border-box;
        border-radius: 50%;
        transform: translate(-50%, -50%);
        background-size: 100% 100%;
        background-repeat: no-repeat;
    }
    @media (min-width: 100vh) {
        .clock {
            border: solid 2.8vh #242424;
            box-shadow: 0 1.4vh 2.8vh rgba(0, 0, 0, 0.8);
        }
    }

</style>





            </div>
        
            <div class="nexmoe-pd-item">
                <div class="qweather" >
    <div id="he-plugin-standard"></div>
    <div class="qweather-logo">

    </div>

</div>
<style>
    .qweather{
        position: relative;
    }
    .qweather-logo{
        position: absolute;
        right: 0;
        top: -15px;
        width: 40px;
        height: 40px;
        background-size: 100% 100%;
        background-repeat: no-repeat;
    }
</style>
<script>
  WIDGET = {
    "CONFIG": {
      "layout": "2",
      "width": "260",
      "height": "220",
      "background": "5",
      "dataColor": "e67249",
      "borderRadius": "15",
      "key": "f74d1e1690e6432d801e97fa2f05a162"
    }
  }
</script>
<script src="https://widget.qweather.net/standard/static/js/he-standard-common.js?v=2.0"></script>

            </div>
        
</div>
<style>
    .nexmoe-pd {
        left: auto;
        top: 40px;
        right: 0;
    }
    .nexmoe-pd-item{
       display: flex;
        justify-content: center;
        margin-bottom: 30px;
    }
</style>

  </div>
  <script src="https://lib.baomitu.com/lazysizes/5.1.0/lazysizes.min.js"></script>
<script src="https://lib.baomitu.com/highlight.js/10.0.0/highlight.min.js"></script>
<script src="https://lib.baomitu.com/mdui/0.4.3/js/mdui.min.js"></script>

<script>
	hljs.initHighlightingOnLoad();
</script>

<script src="https://lib.baomitu.com/jquery/3.5.1/jquery.slim.min.js"></script>
<script src="/lib/fancybox/js/jquery.fancybox.min.js"></script>


<script src="/js/app.js?v=1682237265514"></script>

<script src="https://lib.baomitu.com/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>
<script>
	$(".justified-gallery").justifiedGallery({
		rowHeight: 160,
		margins: 10,
	});
</script>

  





<!-- hexo injector body_end start -->
<script src="/js/clock.js"></script>

<script src="https://lib.baomitu.com/clipboard.js/2.0.8/clipboard.min.js"></script>

<script src="/lib/codeBlock/codeBlockFuction.js"></script>

<script src="/lib/codeBlock/codeLang.js"></script>

<script src="/lib/codeBlock/codeCopy.js"></script>

<script src="/lib/codeBlock/codeShrink.js"></script>

<link rel="stylesheet" href="/lib/codeBlock/matery.css">

<script src="https://code.jquery.com/jquery-3.6.0.js"></script>

<script src="/js/search.js"></script>

<script src="/js/webapp.js"></script>
<!-- hexo injector body_end end --><script src="/live2D/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2D/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":true,"model":{"jsonPath":"/live2D/assets/xiaomai.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":1},"log":false});</script></body>
</html>

<script>(function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/5e784416.js","daovoice")</script>
<script>
  daovoice('init', {
    app_id: "5e784416"
  });
  daovoice('update');
</script>

