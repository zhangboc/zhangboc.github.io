<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Ceph-SDK开发与排障</title>
      <link href="/2023/04/23/Ceph/17.SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/"/>
      <url>/2023/04/23/Ceph/17.SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3">一、官方文档</a><ul><li><a href="#1-ceph-rest-%E9%A3%8E%E6%A0%BC%E7%9A%84-api">1、CEPH REST 风格的 API:</a></li><li><a href="#ceph-%E5%AD%98%E5%82%A8%E9%9B%86%E7%BE%A4-api">CEPH 存储集群 API :</a></li><li><a href="#ceph-%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-api">CEPH 文件系统 API:</a></li><li><a href="#ceph-%E5%9D%97%E8%AE%BE%E5%A4%87-api">CEPH 块设备 API :</a></li><li><a href="#eph-rados-%E7%BD%91%E5%85%B3-api">EPH RADOS 网关 API:</a></li><li><a href="#ceph-%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8-api">CEPH 对象存储 API:</a><ul><li><a href="#1-%E5%8F%82%E8%A7%81s3-%E5%85%BC%E5%AE%B9-apihttpsdocscephcomenpacificradosgws3">1、参见<a href="https://docs.ceph.com/en/pacific/radosgw/s3/">S3 兼容 API</a></a></li><li><a href="#2-%E5%8F%82%E8%A7%81%E4%B8%8E-swift-%E5%85%BC%E5%AE%B9%E7%9A%84-apihttpsdocscephcomenpacificradosgwswift">2、参见<a href="https://docs.ceph.com/en/pacific/radosgw/swift/">与 Swift 兼容的 API</a>。</a></li><li><a href="#3-%E5%8F%82%E8%A7%81%E7%AE%A1%E7%90%86%E6%93%8D%E4%BD%9C-apihttpsdocscephcomenpacificradosgwadminops">3、参见<a href="https://docs.ceph.com/en/pacific/radosgw/adminops">管理操作 API</a></a></li></ul></li><li><a href="#ceph-mon-%E5%91%BD%E4%BB%A4-api">CEPH MON 命令 API</a></li></ul></li><li><a href="#%E4%BA%8C-ceph-storage-cluster-api">二、Ceph Storage Cluster API</a><ul><li><a href="#%E7%AC%AC-1-%E6%AD%A5%E8%8E%B7%E5%8F%96-librados">第 1 步：获取 LIBRADOS</a><ul><li><a href="#1-%E4%B8%BA-cc-%E8%8E%B7%E5%8F%96-librados">1、为 C&#x2F;C++ 获取 LIBRADOS</a></li><li><a href="#2-%E4%B8%BA-python-%E8%8E%B7%E5%8F%96-librados">2、为 PYTHON 获取 LIBRADOS</a></li><li><a href="#3-%E4%B8%BA-java-%E8%8E%B7%E5%8F%96-librados">3、为 JAVA 获取 LIBRADOS</a></li><li><a href="#4-%E4%B8%BA-php-%E8%8E%B7%E5%8F%96-librados">4、为 PHP 获取 LIBRADOS</a></li></ul></li><li><a href="#%E7%AC%AC-2-%E6%AD%A5%E9%85%8D%E7%BD%AE%E9%9B%86%E7%BE%A4%E5%8F%A5%E6%9F%84">第 2 步：配置集群句柄</a><ul><li><a href="#1-c-%E7%A4%BA%E4%BE%8B">1、C 示例</a></li><li><a href="#2-c-%E7%A4%BA%E4%BE%8B">2、C++ 示例</a></li><li><a href="#3-python-%E7%A4%BA%E4%BE%8B">3、PYTHON 示例</a></li><li><a href="#4-java-%E7%A4%BA%E4%BE%8B">4、JAVA 示例</a></li><li><a href="#5-php-%E7%A4%BA%E4%BE%8B">5、PHP 示例</a></li></ul></li><li><a href="#%E7%AC%AC-3-%E6%AD%A5%E5%88%9B%E5%BB%BA-io-%E4%B8%8A%E4%B8%8B%E6%96%87">第 3 步：创建 I&#x2F;O 上下文</a><ul><li><a href="#1-c-%E7%A4%BA%E4%BE%8B-1">1、C 示例</a></li><li><a href="#2-c-%E7%A4%BA%E4%BE%8B-1">2、C++ 示例</a></li><li><a href="#3-python-%E7%A4%BA%E4%BE%8B-1">3、PYTHON 示例</a></li><li><a href="#4-java-%E7%A4%BA%E4%BE%8B-1">4、JAVA 示例</a></li><li><a href="#5-php-%E7%A4%BA%E4%BE%8B-1">5、PHP 示例</a></li></ul></li><li><a href="#%E7%AC%AC-4-%E6%AD%A5%E7%BB%93%E6%9D%9F%E9%93%BE%E6%8E%A5">第 4 步：结束链接</a><ul><li><a href="#1-c-%E7%A4%BA%E4%BE%8B-2">1、C 示例</a></li><li><a href="#2-c-%E7%A4%BA%E4%BE%8B-2">2、C++ 示例</a></li><li><a href="#3-java-%E7%A4%BA%E4%BE%8B">3、JAVA 示例</a></li><li><a href="#4-python-%E7%A4%BA%E4%BE%8B">4、PYTHON 示例</a></li><li><a href="#4-php-%E7%A4%BA%E4%BE%8B">4、PHP 示例</a></li></ul></li><li><a href="#%E7%AC%AC5%E6%AD%A5python%E6%B5%8B%E8%AF%95">第5步：Python测试</a></li></ul></li><li><a href="#%E4%B8%89-ceph-%E5%9D%97%E8%AE%BE%E5%A4%87-api">三、CEPH 块设备 API</a><ul><li><a href="#rados%E7%9A%84%E5%AE%A2%E6%88%B7%E7%AB%AFapi">RADOS的客户端API</a></li><li><a href="#1-librbd-librados%E4%BB%8B%E7%BB%8D">1. librbd  &amp;  librados介绍</a></li><li><a href="#11-%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0">1.1 架构概述</a></li><li><a href="#12-%E6%BA%90%E7%A0%81%E7%BB%93%E6%9E%84">1.2 源码结构</a><ul><li><a href="#121-librbd">1.2.1 librbd</a></li><li><a href="#122-librados">1.2.2 librados</a></li></ul></li><li><a href="#2-librados-librbd-api%E4%BB%8B%E7%BB%8D">2、librados &amp; librbd API介绍</a><ul><li><a href="#21-%E6%BA%90%E7%A0%81%E6%A6%82%E8%A7%88">2.1 源码概览</a></li><li><a href="#22-%E4%B8%BB%E8%A6%81%E6%93%8D%E4%BD%9C%E4%BB%8B%E7%BB%8D">2.2 主要操作介绍</a></li><li><a href="#23-%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">2.3 参考资料</a></li></ul></li><li><a href="#%E5%AE%9E%E8%B7%B5">实践</a></li><li><a href="#%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3">官方文档：</a><ul><li><a href="#%E7%A4%BA%E4%BE%8B%E5%88%9B%E5%BB%BA%E5%92%8C%E5%86%99%E5%85%A5%E5%9B%BE%E5%83%8F">示例：创建和写入图像</a></li></ul></li></ul></li><li><a href="#ragw-api">RAGW API</a><ul><li><a href="#%E4%B8%80-s3-compatible-api">一、S3-compatible API</a><ul><li><a href="#1-api">1、API</a></li><li><a href="#2-%E5%8A%9F%E8%83%BD%E6%94%AF%E6%8C%81">2、功能支持</a></li><li><a href="#3-%E4%B8%8D%E6%94%AF%E6%8C%81%E7%9A%84%E6%A0%87%E9%A2%98%E5%AD%97%E6%AE%B5">3、不支持的标题字段</a></li><li><a href="#4-%E6%B5%8B%E8%AF%95">4、测试：</a></li></ul></li><li><a href="#%E4%BA%8C-swift-compatible-api">二、Swift-compatible API</a><ul><li><a href="#1-api-1">1、API</a></li><li><a href="#2-%E5%8A%9F%E8%83%BD%E6%94%AF%E6%8C%81-1">2、功能支持</a></li></ul></li><li><a href="#%E4%B8%89-%E5%88%9B%E5%BB%BA%E8%BF%9E%E6%8E%A5">三、创建连接</a><ul><li><a href="#1-%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%AE%B9%E5%99%A8">1、创建一个容器</a></li><li><a href="#2-%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%AF%B9%E8%B1%A1">2、创建一个对象</a></li><li><a href="#3-%E5%88%97%E5%87%BA%E6%8B%A5%E6%9C%89%E7%9A%84%E5%AE%B9%E5%99%A8">3、列出拥有的容器</a></li><li><a href="#4-%E5%88%97%E5%87%BA%E4%B8%80%E4%B8%AA%E5%AE%B9%E5%99%A8%E7%9A%84%E5%86%85%E5%AE%B9">4、列出一个容器的内容</a></li><li><a href="#5-%E6%A3%80%E7%B4%A2%E5%AF%B9%E8%B1%A1">5、检索对象</a></li><li><a href="#6-%E5%88%A0%E9%99%A4%E5%AF%B9%E8%B1%A1">6、删除对象</a></li><li><a href="#7-%E5%88%A0%E9%99%A4%E5%AE%B9%E5%99%A8">7、删除容器</a></li></ul></li></ul></li><li><a href="#ceph-mon%E7%A9%BA%E9%97%B4%E4%B8%8D%E8%B6%B3%E6%8E%92%E9%9A%9C">Ceph MON空间不足排障</a><ul><li><a href="#1-%E6%9F%A5%E7%9C%8B%E5%91%8A%E8%AD%A6%E4%BF%A1%E6%81%AF">1、查看告警信息</a></li><li><a href="#2-%E6%9F%A5%E7%9C%8B%E5%85%B7%E4%BD%93%E4%BF%A1%E6%81%AF">2、查看具体信息</a></li><li><a href="#3-%E6%9F%A5%E7%9C%8B%E7%A1%AC%E7%9B%98%E5%8D%A0%E7%94%A8%E7%A9%BA%E9%97%B4">3、查看硬盘占用空间</a></li><li><a href="#4-%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE30%E5%91%8A%E8%AD%A6">4、参数设置30%告警</a></li><li><a href="#5-%E6%89%A9%E5%AE%B9">5、扩容</a></li><li><a href="#6-%E6%9F%A5%E7%9C%8Bnode-1%E5%91%8A%E8%AD%A6%E6%B6%88%E5%A4%B1">6、查看Node-1告警消失</a></li></ul></li><li><a href="#ceph%E9%9B%86%E7%BE%A4%E9%98%BB%E5%A1%9E%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5">Ceph集群阻塞故障排查</a><ul><li><a href="#%E4%B8%80-%E6%95%85%E9%9A%9C%E7%8E%B0%E8%B1%A1">一、故障现象</a></li><li><a href="#%E4%BA%8C-%E6%9F%A5%E7%9C%8Bnode-1mon%E6%97%A5%E5%BF%97">二、查看Node-1Mon日志</a></li><li><a href="#%E4%B8%89-%E6%9F%A5%E7%9C%8Bnode-2mon%E6%97%A5%E5%BF%97">三、查看Node-2Mon日志</a></li><li><a href="#%E5%9B%9B-%E6%9F%A5%E7%9C%8Bnode-2%E7%9A%84%E7%A1%AC%E7%9B%98%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5">四、查看Node-2的硬盘使用情况</a></li><li><a href="#%E4%BA%94-%E6%9F%A5%E7%9C%8Bnode-2%E7%9A%84mon%E6%9C%8D%E5%8A%A1">五、查看Node-2的Mon服务</a></li><li><a href="#%E5%85%AD-%E8%BF%87%E6%BB%A4%E5%A4%A7%E4%BA%8E50m%E7%9A%84%E6%96%87%E4%BB%B6">六、过滤大于50M的文件</a></li><li><a href="#%E4%B8%83-%E5%BE%97%E5%88%B0%E6%96%87%E4%BB%B6%E5%9F%BA%E6%9C%AC%E9%83%BD%E6%98%AF%E8%A2%ABceph%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE%E5%8D%A0%E7%94%A8%E4%BA%86%E8%BF%99%E4%B8%AA%E4%B8%8D%E8%83%BD%E5%88%A0%E9%99%A4">七、得到文件基本都是被Ceph的元数据占用了，这个不能删除</a></li><li><a href="#%E5%85%AB-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95">八、解决方法</a></li></ul></li><li><a href="#%E6%9C%8D%E5%8A%A1carsh%E5%BD%92%E6%A1%A3%E6%8E%92%E6%9F%A5">服务carsh归档排查</a><ul><li><a href="#%E4%B8%80-%E6%9F%A5%E7%9C%8B%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81">一、查看集群状态</a></li><li><a href="#%E4%BA%8C-%E6%9F%A5%E7%9C%8B%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF">二、查看详细信息</a></li><li><a href="#%E4%B8%89-%E6%9F%A5%E7%9C%8B%E6%97%A5%E5%BF%97">三、查看日志</a></li><li><a href="#%E5%9B%9B-%E6%8F%90%E7%A4%BA80%E7%AB%AF%E5%8F%A3%E8%A2%AB%E5%8D%A0%E7%94%A8">四、提示80端口被占用</a></li><li><a href="#%E4%BA%94-%E6%9F%A5%E7%9C%8B%E5%91%8A%E8%AD%A6%E4%BF%A1%E6%81%AF">五、查看告警信息</a></li><li><a href="#%E5%85%AD-%E6%9F%A5%E7%9C%8B%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF">六、查看详细信息</a></li><li><a href="#%E4%B8%83-%E5%BD%92%E6%A1%A3%E4%B8%80%E4%B8%AA%E5%91%8A%E8%AD%A6%E4%BF%A1%E6%81%AF">七、归档一个告警信息</a></li><li><a href="#%E5%85%AB-%E5%BD%92%E6%A1%A3%E6%89%80%E6%9C%89%E5%91%8A%E8%AD%A6%E4%BF%A1%E6%81%AF">八、归档所有告警信息</a></li></ul></li><li><a href="#%E6%97%B6%E9%92%9F%E5%81%8F%E7%A7%BB%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5">时钟偏移故障排查</a></li></ul><!-- tocstop --><h1><span id="一-官方文档">一、官方文档</span></h1><h2><span id="1-ceph-rest-风格的-api">1、CEPH REST 风格的 API:</span></h2><p>请参阅<a href="https://docs.ceph.com/en/pacific/mgr/ceph_api/">Ceph REST API</a></p><h2><span id="ceph-存储集群-api">CEPH 存储集群 API :</span></h2><p>请参阅<a href="https://docs.ceph.com/en/pacific/rados/api/">Ceph 存储集群 API</a></p><h2><span id="ceph-文件系统-api">CEPH 文件系统 API:</span></h2><p>参见<a href="https://docs.ceph.com/en/pacific/cephfs/api">libcephfs</a></p><h2><span id="ceph-块设备-api">CEPH 块设备 API :</span></h2><p>参见<a href="https://docs.ceph.com/en/pacific/rbd/api/librbdpy">librbdpy</a></p><h2><span id="eph-rados-网关-api">EPH RADOS 网关 API:</span></h2><p>参见<a href="https://docs.ceph.com/en/pacific/radosgw/api">librgw-py</a></p><h2><span id="ceph-对象存储-api">CEPH 对象存储 API:</span></h2><h3><span id="1-参见s3-兼容-api">1、参见</span></h3><h3><span id="2-参见与-swift-兼容的-api">2、参见。</span></h3><h3><span id="3-参见管理操作-api">3、参见</span></h3><h2><span id="ceph-mon-命令-api">CEPH MON 命令 API</span></h2><p>参见<a href="https://docs.ceph.com/en/pacific/api/mon_command_api">Mon 命令 API</a></p><h1><span id="二-ceph-storage-cluster-api">二、Ceph Storage Cluster API</span></h1><p>Ceph存储集群有一个消息层协议，使客户端能够与Ceph Monitor和Ceph OSD Daemon交互。 以库的形式向Ceph Clientlibrados提供此功能。所有 Ceph 客户端都使用或封装了相同的功能来与对象存储交互。例如， 利用此功能。您可以 直接与 Ceph 交互（例如，与 Ceph 对话的应用程序、您自己的 Ceph 接口等）。</p><p>Ceph 存储集群提供基本的存储服务，使 Ceph能够在一个统一的系统中以独特的方式提供对象、块和文件存储。但是，您不限于使用 RESTful、块或 POSIX 接口。该 API基于RADOS，librados使您能够创建自己的 Ceph 存储集群接口。<br>APIlibrados使您能够与 Ceph 存储集群中的两种类型的守护进程进行交互：</p><p>Ceph Monitor，维护集群映射的主副本。<br>Ceph OSD 守护进程</p><p>( OSD)，它将数据作为对象存储在存储节点上。</p><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/1.jpg"></p><p>本指南对使用librados. 有关 Ceph 存储集群的更多详细信息，请参阅体系结构。要使用 API，您需要一个正在运行的 Ceph 存储集群。有关详细信息，请参阅安装（快速）。</p><h2><span id="第-1-步获取-librados">第 1 步：获取 LIBRADOS</span></h2><p>您的客户端应用程序必须绑定librados以连接到 Ceph 存储集群。您必须安装librados任何必需的包才能编写使用librados. APIlibrados是用 C++ 编写的，具有针对 C、Python、Java 和 PHP 的附加绑定。</p><h3><span id="1-为-cx2fc-获取-librados">1、为 C&#x2F;C++ 获取 LIBRADOS</span></h3><p>要在 Debian&#x2F;Ubuntu 发行版上安装libradosC&#x2F;C++ 的开发支持文件，请执行以下命令：</p><pre><code>sudo apt-get install librados-dev</code></pre><p>要在 RHEL&#x2F;CentOS 发行版上安装libradosC&#x2F;C++ 的开发支持文件，请执行以下命令：</p><pre><code>sudo yum install librados2-devel</code></pre><p>为开发人员安装后librados，您可以在以下位置找到 C&#x2F;C++ 所需的标头&#x2F;usr&#x2F;include&#x2F;rados：</p><pre><code>ls /usr/include/rados</code></pre><h3><span id="2-为-python-获取-librados">2、为 PYTHON 获取 LIBRADOS</span></h3><p>该模块为 Python 应用程序rados提供支持。Debian&#x2F;Ubuntu 的软件包和libradosRHEL &#x2F;CentOS 的软件包将为 您安装软件包。你 也可以直接安装。librados-devlibrados2-develpython-radospython-rados</p><p>要librados在 Debian&#x2F;Ubuntu 发行版上安装 Python 的开发支持文件，请执行以下命令：</p><pre><code>sudo apt-get install python3-rados</code></pre><p>要在 RHEL&#x2F;CentOS 发行版上安装libradosPython 的开发支持文件，请执行以下命令：</p><pre><code>sudo yum install python-rados</code></pre><p>要在 SLE&#x2F;openSUSE 发行版上安装libradosPython 的开发支持文件，请执行以下命令：</p><pre><code>sudo zypper install python3-rados</code></pre><p>&#x2F;usr&#x2F;share&#x2F;pyshared您可以在 Debian 系统下或 CentOS&#x2F;RHEL 系统下找到该模块&#x2F;usr&#x2F;lib&#x2F;python*&#x2F;site-packages。</p><h3><span id="3-为-java-获取-librados">3、为 JAVA 获取 LIBRADOS</span></h3><p>要为 Java 安装librados，您需要执行以下过程：<br>安装<br>jna.jar。对于 Debian&#x2F;Ubuntu，执行：</p><pre><code>sudo apt-get install libjna-java</code></pre><p>对于 CentOS&#x2F;RHEL，执行：</p><pre><code>sudo yum install jna</code></pre><p>JAR 文件位于&#x2F;usr&#x2F;share&#x2F;java.<br>克隆<br>rados-java存储库：</p><pre><code>git clone --recursive https://github.com/ceph/rados-java.git</code></pre><p>构建<br>rados-java存储库：</p><pre><code>cd rados-javaant</code></pre><p>JAR 文件位于rados-java&#x2F;target.<br>将 RADOS 的 JAR 复制到一个公共位置（例如，<br>&#x2F;usr&#x2F;share&#x2F;java）并确保它和 JNA JAR 在您的 JVM 类路径中。例如：</p><pre><code>sudo cp target/rados-0.1.3.jar /usr/share/java/rados-0.1.3.jarsudo ln -s /usr/share/java/jna-3.2.7.jar /usr/lib/jvm/default-java/jre/lib/ext/jna-3.2.7.jarsudo ln -s /usr/share/java/rados-0.1.3.jar  /usr/lib/jvm/default-java/jre/lib/ext/rados-0.1.3.jar</code></pre><p>要构建文档，请执行以下命令：</p><pre><code>ant docs</code></pre><h3><span id="4-为-php-获取-librados">4、为 PHP 获取 LIBRADOS</span></h3><p>要安装libradosPHP 扩展，您需要执行以下过程：<br>安装 php-dev。<br>对于 Debian&#x2F;Ubuntu，执行：</p><pre><code>sudo apt-get install php5-dev build-essential</code></pre><p>对于 CentOS&#x2F;RHEL，执行：</p><pre><code>sudo yum install php-devel</code></pre><p>克隆<br>phprados存储库：</p><pre><code>git clone https://github.com/ceph/phprados.git</code></pre><p>构建<br>phprados：</p><pre><code>cd phpradosphpize./configuremakesudo make install</code></pre><p>phprados<br>通过将以下行添加到启用php.ini：</p><pre><code>extension=rados.so</code></pre><h2><span id="第-2-步配置集群句柄">第 2 步：配置集群句柄</span></h2><p>Ceph 客户端通过直接librados与 OSD 交互以存储和检索数据。要与 OSD 交互，客户端应用程序必须调用 librados 并连接到 Ceph Monitor。连接后，从 Ceph Monitorlibrados 检索 Cluster Map 。当客户端应用程序想要读取或写入数据时，它会创建一个 I&#x2F;O 上下文并绑定到一个 Pool。该池有一个关联的CRUSH 规则，该规则定义它将如何将数据放置在存储集群中。通过 I&#x2F;O 上下文，客户端将对象名称提供给librados，它获取对象名称和集群映射（即集群的拓扑结构）并计算归置组和OSD 用于定位数据。然后客户端应用程序可以读取或写入数据。客户端应用程序不需要直接了解集群的拓扑结构。</p><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/2.jpg"></p><p>因此，从您的应用程序使用集群的第一步是 1) 创建一个集群句柄，您的应用程序将使用该句柄连接到存储集群，然后 2) 使用该句柄进行连接。要连接到集群，应用程序必须提供监视器地址、用户名和身份验证密钥（默认情况下启用 cephx）。</p><p>与不同的 Ceph 存储集群通信——或与不同用户的同一个集群通信——需要不同的集群句柄。<br>RADOS 提供了多种方法供您设置所需的值。对于监视器和加密密钥设置，一种简单的处理方法是确保您的 Ceph 配置文件包含keyring密钥环文件的路径和至少一个监视器地址（例如，）。例如：mon host</p><pre><code>[global]mon host = 192.168.1.1keyring = /etc/ceph/ceph.client.admin.keyring</code></pre><p>创建句柄后，您可以读取 Ceph 配置文件来配置句柄。您还可以将参数传递给您的应用程序，并使用用于解析命令行参数的函数（例如，rados_conf_parse_argv()）或解析 Ceph 环境变量（例如，rados_conf_parse_env()）来解析它们。一些包装器可能不实现便利方法，因此您可能需要实现这些功能。下图提供了初始连接的高级流程</p><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/3.jpg"></p><p>连接后，您的应用程序可以仅使用集群句柄调用影响整个集群的函数。例如，一旦你有了一个集群句柄，你就可以：<br>获取集群统计信息<br>使用池操作（存在、创建、列出、删除）<br>获取和设置配置</p><p>Ceph 的强大功能之一是能够绑定到不同的池。每个池可能有不同数量的归置组、对象副本和复制策略。例如，可以将池设置为使用 SSD 存储常用对象的“热”池或使用纠删码的“冷”池。<br>各种librados绑定的主要区别在于 C 与 C++、Java 和 Python 的面向对象绑定之间。面向对象的绑定使用对象来表示集群句柄、IO 上下文、迭代器、异常等。</p><h3><span id="1-c-示例">1、C 示例</span></h3><p>对于 C，使用用户创建一个简单的集群句柄admin，配置它并连接到集群可能看起来像这样：</p><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;rados/librados.h&gt;int main (int argc, const char **argv)&#123;        /* Declare the cluster handle and required arguments. */        rados_t cluster;        char cluster_name[] = &quot;ceph&quot;;        char user_name[] = &quot;client.admin&quot;;        uint64_t flags = 0;        /* Initialize the cluster handle with the &quot;ceph&quot; cluster name and the &quot;client.admin&quot; user */        int err;        err = rados_create2(&amp;cluster, cluster_name, user_name, flags);        if (err &lt; 0) &#123;                fprintf(stderr, &quot;%s: Couldn&#39;t create the cluster handle! %s\n&quot;, argv[0], strerror(-err));                exit(EXIT_FAILURE);        &#125; else &#123;                printf(&quot;\nCreated a cluster handle.\n&quot;);        &#125;        /* Read a Ceph configuration file to configure the cluster handle. */        err = rados_conf_read_file(cluster, &quot;/etc/ceph/ceph.conf&quot;);        if (err &lt; 0) &#123;                fprintf(stderr, &quot;%s: cannot read config file: %s\n&quot;, argv[0], strerror(-err));                exit(EXIT_FAILURE);        &#125; else &#123;                printf(&quot;\nRead the config file.\n&quot;);        &#125;        /* Read command line arguments */        err = rados_conf_parse_argv(cluster, argc, argv);        if (err &lt; 0) &#123;                fprintf(stderr, &quot;%s: cannot parse command line arguments: %s\n&quot;, argv[0], strerror(-err));                exit(EXIT_FAILURE);        &#125; else &#123;                printf(&quot;\nRead the command line arguments.\n&quot;);        &#125;        /* Connect to the cluster */        err = rados_connect(cluster);        if (err &lt; 0) &#123;                fprintf(stderr, &quot;%s: cannot connect to cluster: %s\n&quot;, argv[0], strerror(-err));                exit(EXIT_FAILURE);        &#125; else &#123;                printf(&quot;\nConnected to the cluster.\n&quot;);        &#125;&#125;</code></pre><p>编译您的客户端并链接到librados使用-lrados. 例如：</p><pre><code>gcc ceph-client.c -lrados -o ceph-client</code></pre><h3><span id="2-c-示例">2、C++ 示例</span></h3><p>Ceph 项目在ceph&#x2F;examples&#x2F;librados 目录中提供了一个 C++ 示例。对于 C++，使用用户的简单簇句柄admin需要您初始化librados::Rados簇句柄对象：</p><pre><code>#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;rados/librados.hpp&gt;int main(int argc, const char **argv)&#123;        int ret = 0;        /* Declare the cluster handle and required variables. */        librados::Rados cluster;        char cluster_name[] = &quot;ceph&quot;;        char user_name[] = &quot;client.admin&quot;;        uint64_t flags = 0;        /* Initialize the cluster handle with the &quot;ceph&quot; cluster name and &quot;client.admin&quot; user */        &#123;                ret = cluster.init2(user_name, cluster_name, flags);                if (ret &lt; 0) &#123;                        std::cerr &lt;&lt; &quot;Couldn&#39;t initialize the cluster handle! error &quot; &lt;&lt; ret &lt;&lt; std::endl;                        return EXIT_FAILURE;                &#125; else &#123;                        std::cout &lt;&lt; &quot;Created a cluster handle.&quot; &lt;&lt; std::endl;                &#125;        &#125;        /* Read a Ceph configuration file to configure the cluster handle. */        &#123;                ret = cluster.conf_read_file(&quot;/etc/ceph/ceph.conf&quot;);                if (ret &lt; 0) &#123;                        std::cerr &lt;&lt; &quot;Couldn&#39;t read the Ceph configuration file! error &quot; &lt;&lt; ret &lt;&lt; std::endl;                        return EXIT_FAILURE;                &#125; else &#123;                        std::cout &lt;&lt; &quot;Read the Ceph configuration file.&quot; &lt;&lt; std::endl;                &#125;        &#125;        /* Read command line arguments */        &#123;                ret = cluster.conf_parse_argv(argc, argv);                if (ret &lt; 0) &#123;                        std::cerr &lt;&lt; &quot;Couldn&#39;t parse command line options! error &quot; &lt;&lt; ret &lt;&lt; std::endl;                        return EXIT_FAILURE;                &#125; else &#123;                        std::cout &lt;&lt; &quot;Parsed command line options.&quot; &lt;&lt; std::endl;                &#125;        &#125;        /* Connect to the cluster */        &#123;                ret = cluster.connect();                if (ret &lt; 0) &#123;                        std::cerr &lt;&lt; &quot;Couldn&#39;t connect to cluster! error &quot; &lt;&lt; ret &lt;&lt; std::endl;                        return EXIT_FAILURE;                &#125; else &#123;                        std::cout &lt;&lt; &quot;Connected to the cluster.&quot; &lt;&lt; std::endl;                &#125;        &#125;        return 0;&#125;</code></pre><p>编译源码；然后，链接librados使用-lrados。例如：</p><pre><code>g++ -g -c ceph-client.cc -o ceph-client.og++ -g ceph-client.o -lrados -o ceph-client</code></pre><h3><span id="3-python-示例">3、PYTHON 示例</span></h3><p>Python默认使用adminid和集群名，如果conffile参数设置为空字符串，会读取标准文件。Python 绑定将 C++ 错误转换为异常。</p><pre><code>import radostry:        cluster = rados.Rados(conffile=&#39;&#39;)except TypeError as e:        print &#39;Argument validation error: &#39;, e        raise eprint &quot;Created cluster handle.&quot;try:        cluster.connect()except Exception as e:        print &quot;connection error: &quot;, e        raise efinally:        print &quot;Connected to the cluster.&quot;</code></pre><p>执行示例以验证它是否连接到您的集群：</p><pre><code>python ceph-client.py</code></pre><h3><span id="4-java-示例">4、JAVA 示例</span></h3><p>Java 要求您指定用户 ID ( admin) 或用户名 ( client.admin)，并ceph默认使用集群名称。Java 绑定将基于 C++ 的错误转换为异常</p><pre><code>import com.ceph.rados.Rados;import com.ceph.rados.RadosException;import java.io.File;public class CephClient &#123;        public static void main (String args[])&#123;                try &#123;                        Rados cluster = new Rados(&quot;admin&quot;);                        System.out.println(&quot;Created cluster handle.&quot;);                        File f = new File(&quot;/etc/ceph/ceph.conf&quot;);                        cluster.confReadFile(f);                        System.out.println(&quot;Read the configuration file.&quot;);                        cluster.connect();                        System.out.println(&quot;Connected to the cluster.&quot;);                &#125; catch (RadosException e) &#123;                        System.out.println(e.getMessage() + &quot;: &quot; + e.getReturnValue());                &#125;        &#125;&#125;</code></pre><p>编译源码；然后，运行它。如果您已将 JAR 复制到 &#x2F;usr&#x2F;share&#x2F;java您的目录并从中链接了 sym ext，则无需指定类路径。例如：</p><pre><code>javac CephClient.javajava CephClient</code></pre><h3><span id="5-php-示例">5、PHP 示例</span></h3><p>在 PHP 中启用 RADOS 扩展后，您可以非常轻松地开始创建新的集群句柄：</p><pre><code>&lt;?php$r = rados_create();rados_conf_read_file($r, &#39;/etc/ceph/ceph.conf&#39;);if (!rados_connect($r)) &#123;        echo &quot;Failed to connect to Ceph cluster&quot;;&#125; else &#123;        echo &quot;Successfully connected to Ceph cluster&quot;;&#125;</code></pre><p>将其保存为 rados.php 并运行代码：</p><pre><code>php rados.php</code></pre><h2><span id="第-3-步创建-ix2fo-上下文">第 3 步：创建 I&#x2F;O 上下文</span></h2><p>一旦你的应用程序有了一个集群句柄并连接到 Ceph 存储集群，你就可以创建一个 I&#x2F;O 上下文并开始读写数据。I&#x2F;O 上下文将连接绑定到特定池。用户必须具有适当的 CAPS权限才能访问指定的池。例如，具有读取权限但没有写入权限的用户将只能读取数据。I&#x2F;O 上下文功能包括：</p><ul><li>写入&#x2F;读取数据和扩展属性</li><li>列出并迭代对象和扩展属性</li><li>快照池、列表快照等。</li></ul><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/4.jpg"></p><p>RADOS 使您能够同步和异步交互。一旦您的应用程序拥有 I&#x2F;O 上下文，读&#x2F;写操作只需要您知道对象&#x2F;xattr 名称。封装的 CRUSH 算法librados使用 cluster map 来识别合适的 OSD。OSD 守护进程处理复制，如Smart Daemons Enable Hyperscale中所述。该librados库还将对象映射到归置组，如 计算 PG ID中所述。</p><p>以下示例使用默认data池。但是，您也可以使用 API 列出池，确保它们存在，或者创建和删除池。对于写操作，示例说明了如何使用同步模式。对于读取操作，示例说明了如何使用异步模式。</p><blockquote><p>使用此 API 删除池时要小心。如果删除一个池，该池和池中的所有数据都将丢失。</p></blockquote><h3><span id="1-c-示例">1、C 示例</span></h3><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;rados/librados.h&gt;int main (int argc, const char **argv)&#123;        /*         * Continued from previous C example, where cluster handle and         * connection are established. First declare an I/O Context.         */        rados_ioctx_t io;        char *poolname = &quot;data&quot;;        err = rados_ioctx_create(cluster, poolname, &amp;io);        if (err &lt; 0) &#123;                fprintf(stderr, &quot;%s: cannot open rados pool %s: %s\n&quot;, argv[0], poolname, strerror(-err));                rados_shutdown(cluster);                exit(EXIT_FAILURE);        &#125; else &#123;                printf(&quot;\nCreated I/O context.\n&quot;);        &#125;        /* Write data to the cluster synchronously. */        err = rados_write(io, &quot;hw&quot;, &quot;Hello World!&quot;, 12, 0);        if (err &lt; 0) &#123;                fprintf(stderr, &quot;%s: Cannot write object \&quot;hw\&quot; to pool %s: %s\n&quot;, argv[0], poolname, strerror(-err));                rados_ioctx_destroy(io);                rados_shutdown(cluster);                exit(1);        &#125; else &#123;                printf(&quot;\nWrote \&quot;Hello World\&quot; to object \&quot;hw\&quot;.\n&quot;);        &#125;        char xattr[] = &quot;en_US&quot;;        err = rados_setxattr(io, &quot;hw&quot;, &quot;lang&quot;, xattr, 5);        if (err &lt; 0) &#123;                fprintf(stderr, &quot;%s: Cannot write xattr to pool %s: %s\n&quot;, argv[0], poolname, strerror(-err));                rados_ioctx_destroy(io);                rados_shutdown(cluster);                exit(1);        &#125; else &#123;                printf(&quot;\nWrote \&quot;en_US\&quot; to xattr \&quot;lang\&quot; for object \&quot;hw\&quot;.\n&quot;);        &#125;        /*         * Read data from the cluster asynchronously.         * First, set up asynchronous I/O completion.         */        rados_completion_t comp;        err = rados_aio_create_completion(NULL, NULL, NULL, &amp;comp);        if (err &lt; 0) &#123;                fprintf(stderr, &quot;%s: Could not create aio completion: %s\n&quot;, argv[0], strerror(-err));                rados_ioctx_destroy(io);                rados_shutdown(cluster);                exit(1);        &#125; else &#123;                printf(&quot;\nCreated AIO completion.\n&quot;);        &#125;        /* Next, read data using rados_aio_read. */        char read_res[100];        err = rados_aio_read(io, &quot;hw&quot;, comp, read_res, 12, 0);        if (err &lt; 0) &#123;                fprintf(stderr, &quot;%s: Cannot read object. %s %s\n&quot;, argv[0], poolname, strerror(-err));                rados_ioctx_destroy(io);                rados_shutdown(cluster);                exit(1);        &#125; else &#123;                printf(&quot;\nRead object \&quot;hw\&quot;. The contents are:\n %s \n&quot;, read_res);        &#125;        /* Wait for the operation to complete */        rados_aio_wait_for_complete(comp);        /* Release the asynchronous I/O complete handle to avoid memory leaks. */        rados_aio_release(comp);        char xattr_res[100];        err = rados_getxattr(io, &quot;hw&quot;, &quot;lang&quot;, xattr_res, 5);        if (err &lt; 0) &#123;                fprintf(stderr, &quot;%s: Cannot read xattr. %s %s\n&quot;, argv[0], poolname, strerror(-err));                rados_ioctx_destroy(io);                rados_shutdown(cluster);                exit(1);        &#125; else &#123;                printf(&quot;\nRead xattr \&quot;lang\&quot; for object \&quot;hw\&quot;. The contents are:\n %s \n&quot;, xattr_res);        &#125;        err = rados_rmxattr(io, &quot;hw&quot;, &quot;lang&quot;);        if (err &lt; 0) &#123;                fprintf(stderr, &quot;%s: Cannot remove xattr. %s %s\n&quot;, argv[0], poolname, strerror(-err));                rados_ioctx_destroy(io);                rados_shutdown(cluster);                exit(1);        &#125; else &#123;                printf(&quot;\nRemoved xattr \&quot;lang\&quot; for object \&quot;hw\&quot;.\n&quot;);        &#125;        err = rados_remove(io, &quot;hw&quot;);        if (err &lt; 0) &#123;                fprintf(stderr, &quot;%s: Cannot remove object. %s %s\n&quot;, argv[0], poolname, strerror(-err));                rados_ioctx_destroy(io);                rados_shutdown(cluster);                exit(1);        &#125; else &#123;                printf(&quot;\nRemoved object \&quot;hw\&quot;.\n&quot;);        &#125;&#125;</code></pre><h3><span id="2-c-示例">2、C++ 示例</span></h3><pre><code>#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;rados/librados.hpp&gt;int main(int argc, const char **argv)&#123;        /* Continued from previous C++ example, where cluster handle and         * connection are established. First declare an I/O Context.         */        librados::IoCtx io_ctx;        const char *pool_name = &quot;data&quot;;        &#123;                ret = cluster.ioctx_create(pool_name, io_ctx);                if (ret &lt; 0) &#123;                        std::cerr &lt;&lt; &quot;Couldn&#39;t set up ioctx! error &quot; &lt;&lt; ret &lt;&lt; std::endl;                        exit(EXIT_FAILURE);                &#125; else &#123;                        std::cout &lt;&lt; &quot;Created an ioctx for the pool.&quot; &lt;&lt; std::endl;                &#125;        &#125;        /* Write an object synchronously. */        &#123;                librados::bufferlist bl;                bl.append(&quot;Hello World!&quot;);                ret = io_ctx.write_full(&quot;hw&quot;, bl);                if (ret &lt; 0) &#123;                        std::cerr &lt;&lt; &quot;Couldn&#39;t write object! error &quot; &lt;&lt; ret &lt;&lt; std::endl;                        exit(EXIT_FAILURE);                &#125; else &#123;                        std::cout &lt;&lt; &quot;Wrote new object &#39;hw&#39; &quot; &lt;&lt; std::endl;                &#125;        &#125;        /*         * Add an xattr to the object.         */        &#123;                librados::bufferlist lang_bl;                lang_bl.append(&quot;en_US&quot;);                ret = io_ctx.setxattr(&quot;hw&quot;, &quot;lang&quot;, lang_bl);                if (ret &lt; 0) &#123;                        std::cerr &lt;&lt; &quot;failed to set xattr version entry! error &quot;                        &lt;&lt; ret &lt;&lt; std::endl;                        exit(EXIT_FAILURE);                &#125; else &#123;                        std::cout &lt;&lt; &quot;Set the xattr &#39;lang&#39; on our object!&quot; &lt;&lt; std::endl;                &#125;        &#125;        /*         * Read the object back asynchronously.         */        &#123;                librados::bufferlist read_buf;                int read_len = 4194304;                //Create I/O Completion.                librados::AioCompletion *read_completion = librados::Rados::aio_create_completion();                //Send read request.                ret = io_ctx.aio_read(&quot;hw&quot;, read_completion, &amp;read_buf, read_len, 0);                if (ret &lt; 0) &#123;                        std::cerr &lt;&lt; &quot;Couldn&#39;t start read object! error &quot; &lt;&lt; ret &lt;&lt; std::endl;                        exit(EXIT_FAILURE);                &#125;                // Wait for the request to complete, and check that it succeeded.                read_completion-&gt;wait_for_complete();                ret = read_completion-&gt;get_return_value();                if (ret &lt; 0) &#123;                        std::cerr &lt;&lt; &quot;Couldn&#39;t read object! error &quot; &lt;&lt; ret &lt;&lt; std::endl;                        exit(EXIT_FAILURE);                &#125; else &#123;                        std::cout &lt;&lt; &quot;Read object hw asynchronously with contents.\n&quot;                        &lt;&lt; read_buf.c_str() &lt;&lt; std::endl;                &#125;        &#125;        /*         * Read the xattr.         */        &#123;                librados::bufferlist lang_res;                ret = io_ctx.getxattr(&quot;hw&quot;, &quot;lang&quot;, lang_res);                if (ret &lt; 0) &#123;                        std::cerr &lt;&lt; &quot;failed to get xattr version entry! error &quot;                        &lt;&lt; ret &lt;&lt; std::endl;                        exit(EXIT_FAILURE);                &#125; else &#123;                        std::cout &lt;&lt; &quot;Got the xattr &#39;lang&#39; from object hw!&quot;                        &lt;&lt; lang_res.c_str() &lt;&lt; std::endl;                &#125;        &#125;        /*         * Remove the xattr.         */        &#123;                ret = io_ctx.rmxattr(&quot;hw&quot;, &quot;lang&quot;);                if (ret &lt; 0) &#123;                        std::cerr &lt;&lt; &quot;Failed to remove xattr! error &quot;                        &lt;&lt; ret &lt;&lt; std::endl;                        exit(EXIT_FAILURE);                &#125; else &#123;                        std::cout &lt;&lt; &quot;Removed the xattr &#39;lang&#39; from our object!&quot; &lt;&lt; std::endl;                &#125;        &#125;        /*         * Remove the object.         */        &#123;                ret = io_ctx.remove(&quot;hw&quot;);                if (ret &lt; 0) &#123;                        std::cerr &lt;&lt; &quot;Couldn&#39;t remove object! error &quot; &lt;&lt; ret &lt;&lt; std::endl;                        exit(EXIT_FAILURE);                &#125; else &#123;                        std::cout &lt;&lt; &quot;Removed object &#39;hw&#39;.&quot; &lt;&lt; std::endl;                &#125;        &#125;&#125;</code></pre><h3><span id="3-python-示例">3、PYTHON 示例</span></h3><pre><code>print &quot;\n\nI/O Context and Object Operations&quot;print &quot;=================================&quot;print &quot;\nCreating a context for the &#39;data&#39; pool&quot;if not cluster.pool_exists(&#39;data&#39;):        raise RuntimeError(&#39;No data pool exists&#39;)ioctx = cluster.open_ioctx(&#39;data&#39;)print &quot;\nWriting object &#39;hw&#39; with contents &#39;Hello World!&#39; to pool &#39;data&#39;.&quot;ioctx.write(&quot;hw&quot;, &quot;Hello World!&quot;)print &quot;Writing XATTR &#39;lang&#39; with value &#39;en_US&#39; to object &#39;hw&#39;&quot;ioctx.set_xattr(&quot;hw&quot;, &quot;lang&quot;, &quot;en_US&quot;)print &quot;\nWriting object &#39;bm&#39; with contents &#39;Bonjour tout le monde!&#39; to pool &#39;data&#39;.&quot;ioctx.write(&quot;bm&quot;, &quot;Bonjour tout le monde!&quot;)print &quot;Writing XATTR &#39;lang&#39; with value &#39;fr_FR&#39; to object &#39;bm&#39;&quot;ioctx.set_xattr(&quot;bm&quot;, &quot;lang&quot;, &quot;fr_FR&quot;)print &quot;\nContents of object &#39;hw&#39;\n------------------------&quot;print ioctx.read(&quot;hw&quot;)print &quot;\n\nGetting XATTR &#39;lang&#39; from object &#39;hw&#39;&quot;print ioctx.get_xattr(&quot;hw&quot;, &quot;lang&quot;)print &quot;\nContents of object &#39;bm&#39;\n------------------------&quot;print ioctx.read(&quot;bm&quot;)print &quot;Getting XATTR &#39;lang&#39; from object &#39;bm&#39;&quot;print ioctx.get_xattr(&quot;bm&quot;, &quot;lang&quot;)print &quot;\nRemoving object &#39;hw&#39;&quot;ioctx.remove_object(&quot;hw&quot;)print &quot;Removing object &#39;bm&#39;&quot;ioctx.remove_object(&quot;bm&quot;)</code></pre><h3><span id="4-java-示例">4、JAVA 示例</span></h3><pre><code>import com.ceph.rados.Rados;import com.ceph.rados.RadosException;import java.io.File;import com.ceph.rados.IoCTX;public class CephClient &#123;        public static void main (String args[])&#123;                try &#123;                        Rados cluster = new Rados(&quot;admin&quot;);                        System.out.println(&quot;Created cluster handle.&quot;);                        File f = new File(&quot;/etc/ceph/ceph.conf&quot;);                        cluster.confReadFile(f);                        System.out.println(&quot;Read the configuration file.&quot;);                        cluster.connect();                        System.out.println(&quot;Connected to the cluster.&quot;);                        IoCTX io = cluster.ioCtxCreate(&quot;data&quot;);                        String oidone = &quot;hw&quot;;                        String contentone = &quot;Hello World!&quot;;                        io.write(oidone, contentone);                        String oidtwo = &quot;bm&quot;;                        String contenttwo = &quot;Bonjour tout le monde!&quot;;                        io.write(oidtwo, contenttwo);                        String[] objects = io.listObjects();                        for (String object: objects)                                System.out.println(object);                        io.remove(oidone);                        io.remove(oidtwo);                        cluster.ioCtxDestroy(io);                &#125; catch (RadosException e) &#123;                        System.out.println(e.getMessage() + &quot;: &quot; + e.getReturnValue());                &#125;        &#125;&#125;</code></pre><h3><span id="5-php-示例">5、PHP 示例</span></h3><pre><code>&lt;?php$io = rados_ioctx_create($r, &quot;mypool&quot;);rados_write_full($io, &quot;oidOne&quot;, &quot;mycontents&quot;);rados_remove(&quot;oidOne&quot;);rados_ioctx_destroy($io);</code></pre><h2><span id="第-4-步结束链接">第 4 步：结束链接</span></h2><p>一旦你的应用程序完成了 I&#x2F;O 上下文和集群句柄，应用程序应该关闭连接并关闭句柄。对于异步 I&#x2F;O，应用程序还应确保挂起的异步操作已完成。</p><h3><span id="1-c-示例">1、C 示例</span></h3><pre><code>rados_ioctx_destroy(io);rados_shutdown(cluster);</code></pre><h3><span id="2-c-示例">2、C++ 示例</span></h3><pre><code>io_ctx.close();cluster.shutdown();</code></pre><h3><span id="3-java-示例">3、JAVA 示例</span></h3><pre><code>cluster.ioCtxDestroy(io);cluster.shutDown();</code></pre><h3><span id="4-python-示例">4、PYTHON 示例</span></h3><pre><code>print &quot;\nClosing the connection.&quot;ioctx.close()print &quot;Shutting down the handle.&quot;cluster.shutdown()</code></pre><h3><span id="4-php-示例">4、PHP 示例</span></h3><pre><code>rados_shutdown($r);</code></pre><h2><span id="第5步python测试">第5步：Python测试</span></h2><pre><code>[root@node-1 python]# ceph osd pool create data 8 8 pool &#39;data&#39; created[root@node-1 python]# cat ceph.py #!/usr/bin/python# -*- coding: utf-8 -*-import radostry:        cluster = rados.Rados(conffile=&#39;/etc/ceph/ceph.conf&#39;)except TypeError as e:        print &#39;Argument validation error: &#39;, e        raise eprint &quot;Created cluster handle.&quot;try:        cluster.connect()except Exception as e:        print &quot;connection error: &quot;, e        raise efinally:        print &quot;Connected to the cluster.&quot;print &quot;\n\nI/O Context and Object Operations&quot;print &quot;=================================&quot;print &quot;\nCreating a context for the &#39;data&#39; pool&quot;if not cluster.pool_exists(&#39;data&#39;):        raise RuntimeError(&#39;No data pool exists&#39;)ioctx = cluster.open_ioctx(&#39;data&#39;)print &quot;\nWriting object &#39;hw&#39; with contents &#39;Hello World!&#39; to pool &#39;data&#39;.&quot;ioctx.write(&quot;hw&quot;, &quot;Hello World!&quot;)print &quot;Writing XATTR &#39;lang&#39; with value &#39;en_US&#39; to object &#39;hw&#39;&quot;ioctx.set_xattr(&quot;hw&quot;, &quot;lang&quot;, &quot;en_US&quot;)print &quot;\nWriting object &#39;bm&#39; with contents &#39;Bonjour tout le monde!&#39; to pool &#39;data&#39;.&quot;ioctx.write(&quot;bm&quot;, &quot;Bonjour tout le monde!&quot;)print &quot;Writing XATTR &#39;lang&#39; with value &#39;fr_FR&#39; to object &#39;bm&#39;&quot;ioctx.set_xattr(&quot;bm&quot;, &quot;lang&quot;, &quot;fr_FR&quot;)print &quot;\nContents of object &#39;hw&#39;\n------------------------&quot;print ioctx.read(&quot;hw&quot;)print &quot;\n\nGetting XATTR &#39;lang&#39; from object &#39;hw&#39;&quot;print ioctx.get_xattr(&quot;hw&quot;, &quot;lang&quot;)print &quot;\nContents of object &#39;bm&#39;\n------------------------&quot;print ioctx.read(&quot;bm&quot;)print &quot;Getting XATTR &#39;lang&#39; from object &#39;bm&#39;&quot;print ioctx.get_xattr(&quot;bm&quot;, &quot;lang&quot;)print &quot;\nRemoving object &#39;hw&#39;&quot;ioctx.remove_object(&quot;hw&quot;)print &quot;Removing object &#39;bm&#39;&quot;ioctx.remove_object(&quot;bm&quot;)print &quot;\nClosing the connection.&quot;ioctx.close()print &quot;Shutting down the handle.&quot;cluster.shutdown()[root@node-1 python]# python ceph.py Created cluster handle.Connected to the cluster.I/O Context and Object Operations=================================Creating a context for the &#39;data&#39; poolWriting object &#39;hw&#39; with contents &#39;Hello World!&#39; to pool &#39;data&#39;.Writing XATTR &#39;lang&#39; with value &#39;en_US&#39; to object &#39;hw&#39;Writing object &#39;bm&#39; with contents &#39;Bonjour tout le monde!&#39; to pool &#39;data&#39;.Writing XATTR &#39;lang&#39; with value &#39;fr_FR&#39; to object &#39;bm&#39;Contents of object &#39;hw&#39;------------------------Hello World!Getting XATTR &#39;lang&#39; from object &#39;hw&#39;en_USContents of object &#39;bm&#39;------------------------Bonjour tout le monde!Getting XATTR &#39;lang&#39; from object &#39;bm&#39;fr_FRRemoving object &#39;hw&#39;Removing object &#39;bm&#39;Closing the connection.Shutting down the handle.</code></pre><h1><span id="三-ceph-块设备-api">三、CEPH 块设备 API</span></h1><p>Ceph的客户端是系统对外提供的功能接口，上层应用通过它访问ceph存储系统。</p><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/5.jpg"><br><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/6.jpg"></p><h2><span id="rados的客户端api">RADOS的客户端API</span></h2><p>上述接口几乎包括了对Ceph集群和其中数据的所有访问功能。其中所谓集群的整体访问包括连接集群、创建存储池、删除存储池和获取集群状态等等。所谓对象访问是之对存储池中对象的访问，包括创建删除对象、向对象写数据或者追加数据和读对象数据等接口。上述功能通过Rados和IoCtx两个类实现，两个类的主要函数如图2所示（这里仅是示例，实际接口数量要多很多，具体参考源代码）。</p><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/7.jpg"><br>图2 访问接口类图<br><a href="https://www.freesion.com/article/9714523649/">https://www.freesion.com/article/9714523649/</a><br>Librados 架构图</p><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/8.jpg"></p><h2><span id="1-librbd-amp-librados介绍">1. librbd  &amp;  librados介绍</span></h2><p>Librbd模块实现了RBD接口，其基于Librados实现了对RBD的基本操作。</p><h2><span id="11-架构概述">1.1 架构概述</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/9.jpg"><br> 上层应用层访问RBD块设备有两种途径：librbd和krbd。其中librbd是一个属于librados的用户态接口库，而krbd是继承在GNU&#x2F;Linux内核中的一个内核模块，通过用户态的rbd命令行工具，可以将RBD块设备映射为本地的一个块设备文件。</p><p>从RBD的架构图中可以看到，RBD块设备由于元数据信息少且访问不频繁，故RBD在Ceph集群中不需要daemon守护进程将元数据加载到内存进行元数据访问加速，所有的元数据和数据操作直接与集群中的MON服务和OSD服务交互。其架构图如下：</p><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/10.jpg"><br> 最上层是librbd层，模块cls_rbd是一个Cls扩展模块，实现了RBD的元数据相关的操作。RBD的数访问直接通过librados来访问。在最底层是OSDC层完成数据的发送。</p><h2><span id="12-源码结构">1.2 源码结构</span></h2><h3><span id="121-librbd">1.2.1 librbd</span></h3><p>在src&#x2F;librbd下为librbd的源码，其中分为.cc文件和.h文件和子目录，文件的大致内容为：</p><ol><li>.cc和.h 文件：对应组件的源码；<br>2.子目录： 子目录中文件为针对每个模块的请求，如针对librbd&#x2F;operation.cc组件，存在路径为operation，路径中文件为：<br><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/11.jpg"></li></ol><p>可见相应目录中为对应组件的请求的实现，这些请求发送到rados，进而实现对osd的操作。</p><h3><span id="122-librados">1.2.2 librados</span></h3><p> src&#x2F;librados中为librados的源码，内容如下：<br> <img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/12.jpg"></p><p> 与librbd目录下源码相比，librados中的源码内容相对较少，librados.cc中负责接口的封装，具体的实现细节在IoCtxImpl.cc中。</p><h2><span id="2-librados-amp-librbd-api介绍">2、librados &amp; librbd API介绍</span></h2><h3><span id="21-源码概览">2.1 源码概览</span></h3><p>ceph封装的接口源码位于src&#x2F;pybind路径下，其中子目录下的.pyx文件为api接口源码，文件结构如图：<br> <img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/13.jpg"></p><h3><span id="22-主要操作介绍">2.2 主要操作介绍</span></h3><p> 对于librbd和librados提供的python api接口，本文重点关注以下操作：</p><pre><code>1 1.  pool的创建和删除；2 2.  image的创建和删除；3 3.  snapshot的创建、利用snapshot进行clone操作、snapshot的删除；4 4.  image flatten操作。</code></pre><p> 下文将按照{create pool} -&gt; {create image} -&gt; {create snapshot} -&gt; {image clone} -&gt; {image flatten} -&gt; {delete snapshot} -&gt; {delete image} -&gt; {delete pool}的顺序来依次介绍上述操作的api接口调用。</p><h4><span id="221-准备工作创建链接">2.2.1 准备工作：创建链接</span></h4><p>在所有操作之前，首先需要创建对集群的连接，才能进行进一步的操作：</p><pre><code>1 import rados2 import rbd3 # connect to cluster4 cluster = rados.Rados(conffile=&#39;/etc/ceph/ceph.conf&#39;)5 cluster.connect()</code></pre><h4><span id="222-创建存储池并获取ioctx">2.2.2 创建存储池，并获取ioctx</span></h4><p>创建了对集群的connection之后，就可以创建存储池，并获取ioctx。</p><pre><code> 1 #list pool 2 before_create = cluster.list_pools() 3 print(&quot;before create pools:&#123;0&#125;.format(before_create)) 4  5 #create pool 6 cluster.create_pool(POOL) 7  8 #list pool 9 after_create = cluster.list_pools()10 print(&quot;after create pools:&#123;0&#125;.format(after_create))11 12 #get_ioctx13 ioctx = cluster.open_ioctx(POOL)</code></pre><blockquote><p>这里create_pool()接口的输入不包括pg和pgp，因为pgp和pg的数量在api中是根据ceph.conf中参数设置。</p></blockquote><pre><code>1 # /etc/ceph/ceph.conf2 osd_pool_default_pg_num = &#123;int&#125;3 osd_pool_default_pgp_num = &#123;int&#125;</code></pre><h4><span id="223-创建image">2.2.3 创建image</span></h4><p>创建存储池并获取ioctx，就可以开始创建rbd image了</p><pre><code>1 rbd_inst = rbd.RBD()2 size = 4* 1024 * 33 4 rbd_inst.create(ioctx, &quot;myimage&quot;, size)5 img_list = rbd_inst.list( ioctx)6 print(&quot;after create images &#123;0&#125;&quot;.format(img_list))</code></pre><blockquote><p>1.在哪个pool中创建image，ioctx就从哪个pool中获取；<br>2.image创建必须显示指定大小，size计算值为bytes。</p></blockquote><h4><span id="224-创建snapshot">2.2.4 创建snapshot</span></h4><p>rbd image创建之后，就可以根据image创建snapshot了。</p><pre><code>1 # create snapshot2 print(&quot;start to create snapshot&quot;)3 rbd_img = rbd.Image(ioctx, name=IMG)4 rbd_img.create_snap(SNP)5 # list snapshot6 print(&quot;list snapshot&quot;)7 snap_list = rbd_img.list_snaps()8 for item in snap_list:9     print(item[&#39;name&#39;])</code></pre><blockquote><p>1.使用的是img.Image()类，与2.2.2区分；<br>2.snap_list的类型为rbd.SnapIterator(Image image），使用for循环迭代的元素为dict，其中有三个key： id（int：numeric identifier of the snapshot), size(int, size of the image at the time of snapshot (in bytes)); name ( str: name of the snapshot).</p></blockquote><h4><span id="225-使用snapshot-clone-rbd-image">2.2.5 使用snapshot clone rbd image</span></h4><p>如果需要使用snapshot克隆image，需要首先对snapshot进行保护，之后才能进行clone操作。</p><pre><code>1 # protect snapshot2 rbd_img.protect_snap(SNP)3 is_protect = rbd_img.is_protected_snap(SNP)4 print(&quot;&#123;0&#125; protected status:&#123;1&#125;&quot;.format(SNP, is_protect))5 # image clone6 print(&quot;clone image&quot;)7 # parent_ioctx, parent_image_name,parent_snap_name, child_ioctx, child_image_name8 rbd_inst.clone(ioctx, “myimage”,“mysnapshot”, ioctx, “clnimg”)9 img_list = rbd_inst.list(ioctx)</code></pre><blockquote><p>1.rbd_image() 需要与保护的snapshot一致（即在初始化时需要提供该snapshot隶属的pool的ioctx和image的name）：<br>2.clone() 函数必须传入的五个变量</p></blockquote><blockquote><blockquote><p>(1) parent_ioctx: snapshot 父类的ioctx;(2) parent_image_name: 父image的名称;(3) parent_snap_name: 父snapshot的名称;(4) child_ioctx: 子image的ioctx;(5) child_image_name: 子image名称;</p></blockquote></blockquote><blockquote><blockquote><blockquote><p>如果clone之后的image和原来的image位于同一个pool，则两者的ioctx相同，反之需要从子image所属的pool获取子ioctx。</p></blockquote></blockquote></blockquote><h4><span id="226-image-flatten">2.2.6 image flatten</span></h4><p>image flatten操作使clone生成的rbd image摆脱对snapshot的依赖</p><pre><code>1 # image flatten2 print(&quot;flatten image&quot;)3 clone_img = rbd.Image(ioctx, name=”clnimg”)4 clone_img.flatten()</code></pre><blockquote><p>因为需要访问的是clone生成的薪image，故需要新初始化一个rbd.Image（）访问。</p></blockquote><h4><span id="227-删除-snapshot-x2f-image-x2f-pool">2.2.7 删除 snapshot &#x2F; image &#x2F; pool</span></h4><pre><code> 1 # unprotect image 2 print(&quot;unprotect snapshot&quot;) 3 rbd_img.unprotect_snap(SNP) 4 is_protect = rbd_img.is_protected_snap(SNP) 5 print(&quot;&#123;0&#125; protected status:&#123;1&#125;&quot;.format(SNP, is_protect)) 6   7 # purge snapshot 8 print(&quot;remove snapshot&quot;) 9 rbd_img.remove_snap(SNP)10  11 # close image connection12 rbd_img.clone()13 clone_img.close()14  15 # remove rbd image16 print(&quot;remove rbd image&quot;)17 rbd_inst.remove(ioctx, IMG)18 rbd_inst.remove(ioctx, CLN)</code></pre><blockquote><p>1.只有在snapshot下的所有image都进行flatten操作之后，才能够unprotect该snapshot；<br>2.需要先关闭rbd.Image()对image的访问之后，才能移除rbd image。</p></blockquote><h4><span id="228-关闭链接终止对集群的访问">2.2.8 关闭链接，终止对集群的访问</span></h4><pre><code> 1 # close ioctx 2 print(&quot;close ioctx&quot;) 3 ioctx.close() 4   5 # delete pool 6 print(&quot;delete pool&quot;) 7 cluster.delete_pool(POOL) 8 after_delete = cluster.list_pools() 9 print(&quot;after delete pools:  &#123;0&#125;&quot;.format(after_delete))10  11 # close connection12 print(&quot;close connection&quot;)13 cluster.shutdown()</code></pre><blockquote><p>顺序：<br>{关闭ioctx} -&gt; {删除pool}-&gt; {关闭cluster链接}</p></blockquote><h3><span id="23-参考资料">2.3 参考资料</span></h3><ol><li>librbd(python)<br><a href="http://docs.ceph.com/docs/master/rbd/api/librbdpy/">http://docs.ceph.com/docs/master/rbd/api/librbdpy/</a></li><li>librados(python)<br><a href="http://docs.ceph.com/docs/master/rados/api/python/">http://docs.ceph.com/docs/master/rados/api/python/</a></li></ol><p><a href="https://www.cnblogs.com/yi-mu-xi/p/10368990.html">https://www.cnblogs.com/yi-mu-xi/p/10368990.html</a></p><h2><span id="实践">实践</span></h2><pre><code>[root@node-1 python]# cat ceph-rbd.py #!/usr/bin/python# -*- coding: utf-8 -*-import radosimport rbdcluster = rados.Rados(conffile=&#39;/etc/ceph/ceph.conf&#39;)try:    cluster.connect()    ioctx = cluster.open_ioctx(&#39;data&#39;)    try:        rbd_inst = rbd.RBD()        size = 4 * 1024**3  # 4 GiB        rbd_inst.create(ioctx, &#39;myimage&#39;, size)        image = rbd.Image(ioctx, &#39;myimage&#39;)        try:            data = &#39;foo&#39; * 200            image.write(data, 0)        finally:            image.close()    finally:        ioctx.close()finally:    cluster.shutdown()[root@node-1 python]# python ceph-rbd.py^C[root@node-1 python]# rbd ls datamyimage[root@node-1 python]# rbd info data/myimagerbd image &#39;myimage&#39;:    size 4 GiB in 1024 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 652ecdcdc583b    block_name_prefix: rbd_data.652ecdcdc583b    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Sun Mar 19 19:39:38 2023    access_timestamp: Sun Mar 19 19:39:38 2023    modify_timestamp: Sun Mar 19 19:39:38 2023</code></pre><h2><span id="官方文档">官方文档：</span></h2><p>rbd python模块提供对 RBD 图像的类文件访问。</p><h3><span id="示例创建和写入图像">示例：创建和写入图像</span></h3><p>要使用rbd，您必须首先连接到 RADOS 并打开一个 IO 上下文：</p><pre><code>cluster = rados.Rados(conffile=&#39;my_ceph.conf&#39;)cluster.connect()ioctx = cluster.open_ioctx(&#39;mypool&#39;)</code></pre><p>然后你实例化一个 :class:rbd.RBD 对象，你用它来创建图像：</p><pre><code>rbd_inst = rbd.RBD()size = 4 * 1024**3  # 4 GiBrbd_inst.create(ioctx, &#39;myimage&#39;, size)</code></pre><p>要对图像执行 I&#x2F;O，您需要实例化一个 :class:rbd.Image 对象：</p><pre><code>image = rbd.Image(ioctx, &#39;myimage&#39;)data = &#39;foo&#39; * 200image.write(data, 0)</code></pre><p>这会将“foo”写入图像的前 600 个字节。请注意，数据不能是 :type:unicode - Librbd不知道如何处理比 :c:type:char 宽的字符。<br>最后，您需要关闭图像、IO 上下文和与 RADOS 的连接：</p><pre><code>image.close()ioctx.close()cluster.shutdown()</code></pre><p>为了安全起见，这些调用中的每一个都需要在一个单独的 :finally 块中：</p><pre><code>cluster = rados.Rados(conffile=&#39;my_ceph_conf&#39;)try:    cluster.connect()    ioctx = cluster.open_ioctx(&#39;my_pool&#39;)    try:        rbd_inst = rbd.RBD()        size = 4 * 1024**3  # 4 GiB        rbd_inst.create(ioctx, &#39;myimage&#39;, size)        image = rbd.Image(ioctx, &#39;myimage&#39;)        try:            data = &#39;foo&#39; * 200            image.write(data, 0)        finally:            image.close()    finally:        ioctx.close()finally:    cluster.shutdown()</code></pre><p>这可能很麻烦，因此Rados、Ioctx和 Image类可以用作自动关闭&#x2F;关闭的上下文管理器（<a href="https://www.python.org/dev/peps/pep-0343">请参阅</a>）。使用它们作为上下文管理器，上面的例子变成：</p><pre><code>with rados.Rados(conffile=&#39;my_ceph.conf&#39;) as cluster:    with cluster.open_ioctx(&#39;mypool&#39;) as ioctx:        rbd_inst = rbd.RBD()        size = 4 * 1024**3  # 4 GiB        rbd_inst.create(ioctx, &#39;myimage&#39;, size)        with rbd.Image(ioctx, &#39;myimage&#39;) as image:            data = &#39;foo&#39; * 200            image.write(data, 0)</code></pre><h1><span id="ragw-api">RAGW API</span></h1><p>Ceph 支持与Amazon S3 API的基本数据访问模型兼容的 RESTful API 。</p><h2><span id="一-s3-compatible-api">一、S3-compatible API</span></h2><h3><span id="1-api">1、API</span></h3><ul><li>Common</li><li>Authentication</li><li>Service Ops</li><li>Bucket Ops</li><li>Object Ops</li><li>C++</li><li>C#</li><li>Java</li><li>Perl</li><li>PHP</li><li>Python</li><li>Ruby AWS::SDK Examples (aws-sdk gem ~&gt;2)</li><li>Ruby AWS::S3 Examples (aws-s3 gem)</li></ul><h3><span id="2-功能支持">2、功能支持</span></h3><p>下表描述了当前 Amazon S3 功能特性的支持状态：</p><table><thead><tr><th>特征</th><th>是否支持</th></tr></thead><tbody><tr><td>列出桶</td><td>支持的</td></tr><tr><td>删除桶</td><td>支持的</td></tr><tr><td>创建桶</td><td>支持的</td></tr><tr><td>桶生命周期</td><td>支持的</td></tr><tr><td>桶复制</td><td>支持的</td></tr><tr><td>策略（桶、对象）</td><td>支持的</td></tr><tr><td>桶网站</td><td>支持的</td></tr><tr><td>存储桶 ACL（获取、放置）</td><td>支持的</td></tr><tr><td>桶位置</td><td>支持的</td></tr><tr><td>桶通知</td><td>支持的</td></tr><tr><td>桶对象版本</td><td>支持的</td></tr><tr><td>获取桶信息 (HEAD)</td><td>支持的</td></tr><tr><td>桶请求付款</td><td>支持的</td></tr><tr><td>放置对象</td><td>支持的</td></tr><tr><td>删除对象</td><td>支持的</td></tr><tr><td>获取对象</td><td>支持的</td></tr><tr><td>对象 ACL（获取、放置）</td><td>支持的</td></tr><tr><td>获取对象信息 (HEAD)</td><td>支持的</td></tr><tr><td>POST 对象</td><td>支持的</td></tr><tr><td>复制对象</td><td>支持的</td></tr><tr><td>分段上传</td><td>支持的</td></tr><tr><td>对象标记</td><td>支持的</td></tr><tr><td>桶标记</td><td>支持的</td></tr><tr><td>存储类</td><td>支持的</td></tr></tbody></table><h3><span id="3-不支持的标题字段">3、不支持的标题字段</span></h3><p>不支持以下常见请求头字段：</p><table><thead><tr><th>特征</th><th>类型</th></tr></thead><tbody><tr><td>服务器</td><td>回复</td></tr><tr><td>x-amz-删除标记</td><td>回复</td></tr><tr><td>x-amz-id-2</td><td>回复</td></tr><tr><td>x-amz 版本 ID</td><td>回复</td></tr></tbody></table><h3><span id="4-测试">4、测试：</span></h3><pre><code>[root@node-1 python]# vim s3client.py #创建一个叫做s3-sdk-demo的bucket#!/usr/bin/python# -*- coding: utf-8 -*-import botoimport boto.s3.connectionaccess_key = &quot;OCK1875BBTEIYZ40P0BD&quot;secret_key = &quot;CAubxcn0GBQUk650D1Kx3syDKZB4zWDAjDBwxFLQ&quot;conn = boto.connect_s3(        aws_access_key_id = access_key,        aws_secret_access_key = secret_key,        host=&#39;node-1&#39;,        port=80,        is_secure=False,                         calling_format = boto.s3.connection.OrdinaryCallingFormat(),        )conn.create_bucket(&#39;s3-sdk-demo&#39;)for bucket in conn.get_all_buckets():        print &quot;&#123;name&#125;\t&#123;created&#125;&quot;.format(                name = bucket.name,                created = bucket.creation_date,        )~                                                                                                                                          ~                                                                                                                                          ~                                                                                                                                          ~                                                                                                                                          ~                                                                                                                                          ~                                                                                                                                          &quot;s3client.py&quot; 24L, 651C 已写入您在 /var/spool/mail/root 中有新邮件[root@node-1 python]# python s3client.py  #创建成功ceph-s3-bucket    2023-03-09T10:33:21.666Zs3-sdk-demo    2023-03-19T13:44:25.673Zs3cmd-demo    2023-03-09T10:36:26.010Zswift-demo    2023-03-09T10:39:31.869Z[root@node-1 python]# cat s3client.py  #往桶里创建文件默认会放在最后一个桶，有时间可进行修改脚本#!/usr/bin/python# -*- coding: utf-8 -*-import botoimport boto.s3.connectionaccess_key = &quot;OCK1875BBTEIYZ40P0BD&quot;secret_key = &quot;CAubxcn0GBQUk650D1Kx3syDKZB4zWDAjDBwxFLQ&quot;conn = boto.connect_s3(        aws_access_key_id = access_key,        aws_secret_access_key = secret_key,        host=&#39;node-1&#39;,        port=80,        is_secure=False,                       calling_format = boto.s3.connection.OrdinaryCallingFormat(),        )bucket=conn.get_bucket(&#39;s3-sdk-demo&#39;)for bucket in conn.get_all_buckets():        print &quot;&#123;name&#125;\t&#123;created&#125;&quot;.format(                name = bucket.name,                created = bucket.creation_date,        )for key in bucket.list():        print &quot;&#123;name&#125;\t&#123;size&#125;\t&#123;modified&#125;&quot;.format(                name = key.name,                size = key.size,                modified = key.last_modified,                )key = bucket.new_key(&#39;zhangy2.txt&#39;)key.set_contents_from_string(&#39;Hello ZhangY!&#39;)</code></pre><h2><span id="二-swift-compatible-api">二、Swift-compatible API</span></h2><h3><span id="1-api">1、API</span></h3><ul><li>Authentication</li><li>Service Ops</li><li>Container Ops</li><li>Object Ops</li><li>Temp URL Ops</li><li>Tutorial</li><li>Java</li><li>Python</li><li>Ruby</li></ul><h3><span id="2-功能支持">2、功能支持</span></h3><p>下表描述了当前 Swift 功能特性的支持状态：</p><table><thead><tr><th>特征</th><th>是否支持</th></tr></thead><tbody><tr><td>验证</td><td>支持</td></tr><tr><td>获取帐户元数据</td><td>支持</td></tr><tr><td>Swift ACL</td><td>支持</td></tr><tr><td>列出容器</td><td>支持</td></tr><tr><td>删除容器</td><td>支持</td></tr><tr><td>创建容器</td><td>支持</td></tr><tr><td>获取容器元数据</td><td>支持</td></tr><tr><td>更新容器元数据</td><td>支持</td></tr><tr><td>删除容器元数据</td><td>支持</td></tr><tr><td>列出对象</td><td>支持</td></tr><tr><td>静态网站</td><td>支持</td></tr><tr><td>创建对象</td><td>支持</td></tr><tr><td>创建大对象</td><td>支持</td></tr><tr><td>删除对象</td><td>支持</td></tr><tr><td>获取对象</td><td>支持</td></tr><tr><td>复制对象</td><td>支持</td></tr><tr><td>获取对象元数据</td><td>支持</td></tr><tr><td>更新对象元数据</td><td>支持</td></tr><tr><td>过期对象</td><td>支持</td></tr><tr><td>临时网址</td><td>支持</td></tr><tr><td>对象版本控制</td><td>支持</td></tr><tr><td>CORS</td><td>支持</td></tr></tbody></table><h2><span id="三-创建连接">三、创建连接</span></h2><p>这将创建一个连接，以便您可以与服务器交互：</p><pre><code>import swiftclientuser = &#39;account_name:username&#39;key = &#39;your_api_key&#39;conn = swiftclient.Connection(        user=user,        key=key,        authurl=&#39;https://objects.dreamhost.com/auth&#39;,)</code></pre><h3><span id="1-创建一个容器">1、创建一个容器</span></h3><p>这将创建一个名为的新容器my-new-container：</p><pre><code>container_name = &#39;my-new-container&#39;conn.put_container(container_name)</code></pre><h3><span id="2-创建一个对象">2、创建一个对象</span></h3><p>这将从hello.txt名为的文件创建一个文件my_hello.txt：</p><pre><code>with open(&#39;hello.txt&#39;, &#39;r&#39;) as hello_file:        conn.put_object(container_name, &#39;hello.txt&#39;,                                        contents= hello_file.read(),                                        content_type=&#39;text/plain&#39;)</code></pre><h3><span id="3-列出拥有的容器">3、列出拥有的容器</span></h3><p>这将获取您拥有的容器列表，并打印出容器名称：</p><pre><code>for container in conn.get_account()[1]:        print container[&#39;name&#39;]</code></pre><p>输出看起来像这样：</p><pre><code>mahbuckat1mahbuckat2mahbuckat3</code></pre><h3><span id="4-列出一个容器的内容">4、列出一个容器的内容</span></h3><p>这将获取容器中的对象列表，并打印出每个对象的名称、文件大小和最后修改日期：</p><pre><code>for data in conn.get_container(container_name)[1]:        print &#39;&#123;0&#125;\t&#123;1&#125;\t&#123;2&#125;&#39;.format(data[&#39;name&#39;], data[&#39;bytes&#39;], data[&#39;last_modified&#39;])</code></pre><p>输出看起来像这样：</p><pre><code>myphoto1.jpg 251262  2011-08-08T21:35:48.000Zmyphoto2.jpg 262518  2011-08-08T21:38:01.000Z</code></pre><h3><span id="5-检索对象">5、检索对象</span></h3><p>这将下载对象hello.txt并将其保存在 .&#x2F;my_hello.txt：</p><pre><code>obj_tuple = conn.get_object(container_name, &#39;hello.txt&#39;)with open(&#39;my_hello.txt&#39;, &#39;w&#39;) as my_hello:        my_hello.write(obj_tuple[1])</code></pre><h3><span id="6-删除对象">6、删除对象</span></h3><p>这将删除对象hello.txt：</p><pre><code>conn.delete_object(container_name, &#39;hello.txt&#39;)</code></pre><h3><span id="7-删除容器">7、删除容器</span></h3><blockquote><p>容器必须是空的！否则请求将无效！</p></blockquote><pre><code>conn.delete_container(container_name)</code></pre><h1><span id="ceph-mon空间不足排障">Ceph MON空间不足排障</span></h1><h2><span id="1-查看告警信息">1、查看告警信息</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/14.jpg"></p><h2><span id="2-查看具体信息">2、查看具体信息</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/15.jpg"></p><h2><span id="3-查看硬盘占用空间">3、查看硬盘占用空间</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/16.jpg"></p><h2><span id="4-参数设置30告警">4、参数设置30%告警</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/17.jpg"></p><h2><span id="5-扩容">5、扩容</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/18.jpg"></p><h2><span id="6-查看node-1告警消失">6、查看Node-1告警消失</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/19.jpg"></p><h1><span id="ceph集群阻塞故障排查">Ceph集群阻塞故障排查</span></h1><h2><span id="一-故障现象">一、故障现象</span></h2><p>查看集群状态也无法显示，夯住了<br><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/20.jpg"></p><h2><span id="二-查看node-1mon日志">二、查看Node-1Mon日志</span></h2><p>根据报错信息得出没办法分配一个global_id和没办法得到一个健康的状态，并没有有效信息<br><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/21.jpg"></p><h2><span id="三-查看node-2mon日志">三、查看Node-2Mon日志</span></h2><p>查看报错信息得到Node-2的存储空间已经没有了，所以报了一个错误<br><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/22.jpg"></p><h2><span id="四-查看node-2的硬盘使用情况">四、查看Node-2的硬盘使用情况</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/23.jpg"></p><h2><span id="五-查看node-2的mon服务">五、查看Node-2的Mon服务</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/24.jpg"></p><h2><span id="六-过滤大于50m的文件">六、过滤大于50M的文件</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/25.jpg"><br><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/26.jpg"></p><h2><span id="七-得到文件基本都是被ceph的元数据占用了这个不能删除">七、得到文件基本都是被Ceph的元数据占用了，这个不能删除</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/27.jpg"></p><h2><span id="八-解决方法">八、解决方法</span></h2><p>扩容硬盘</p><h1><span id="服务carsh归档排查">服务carsh归档排查</span></h1><h2><span id="一-查看集群状态">一、查看集群状态</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/28.jpg"></p><h2><span id="二-查看详细信息">二、查看详细信息</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/29.jpg"></p><h2><span id="三-查看日志">三、查看日志</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/30.jpg"></p><h2><span id="四-提示80端口被占用">四、提示80端口被占用</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/31.jpg"></p><h2><span id="五-查看告警信息">五、查看告警信息</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/32.jpg"></p><h2><span id="六-查看详细信息">六、查看详细信息</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/33.jpg"></p><h2><span id="七-归档一个告警信息">七、归档一个告警信息</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/34.jpg"></p><h2><span id="八-归档所有告警信息">八、归档所有告警信息</span></h2><p><img src="/images/Ceph/SDK%E5%BC%80%E5%8F%91%E4%B8%8E%E6%8E%92%E9%9A%9C/35.jpg"></p><h1><span id="时钟偏移故障排查">时钟偏移故障排查</span></h1><pre><code>[root@node-1 python]# ceph -s #查看状态  cluster:    id:     b8e58b30-4568-4032-a9f4-837ed3fa9529    health: HEALTH_WARN            clock skew detected on mon.node-2 #看到node-2节点时钟偏移   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 24s)    mgr: node-1(active, since 4h), standbys: node-2, node-3    mds: cephfs-demo:1 &#123;0=node-2=up:active(laggy or crashed)&#125; 2 up:standby    osd: 6 osds: 6 up (since 5h), 6 in (since 8h)    rgw: 2 daemons active (node-1, node-2)   task status:   data:    pools:   12 pools, 272 pgs    objects: 1.07k objects, 2.5 GiB    usage:   14 GiB used, 46 GiB / 60 GiB avail    pgs:     272 active+clean[root@node-1 python]# ceph health detail  #查看已经偏移了194秒HEALTH_WARN 26 slow ops, oldest one blocked for 59 sec, mon.node-2 has slow ops; clock skew detected on mon.node-2SLOW_OPS 26 slow ops, oldest one blocked for 59 sec, mon.node-2 has slow opsMON_CLOCK_SKEW clock skew detected on mon.node-2    mon.node-2 clock skew 194.296s &gt; max 0.05s (latency 0.00344935s) #默认最大不超过0.05秒[root@node-1 python]# ceph --admin-daemon /var/run/ceph/ceph-mon.node-1.asok config show |grep 0.05  #查看相关配置    &quot;bluestore_cache_trim_interval&quot;: &quot;0.050000&quot;,    &quot;mds_cache_reservation&quot;: &quot;0.050000&quot;,    &quot;mon_clock_drift_allowed&quot;: &quot;0.050000&quot;,    &quot;mon_reweight_max_change&quot;: &quot;0.050000&quot;,    &quot;mon_warn_on_slow_ping_ratio&quot;: &quot;0.050000&quot;,    &quot;osd_pool_default_hit_set_bloom_fpp&quot;: &quot;0.050000&quot;,    &quot;paxos_min_wait&quot;: &quot;0.050000&quot;,    &quot;target_max_misplaced_ratio&quot;: &quot;0.050000&quot;,</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
          <category> SDK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph与Kubernetes集成</title>
      <link href="/2023/04/20/Ceph/13.Ceph%E4%B8%8EKubernetes%E9%9B%86%E6%88%90/Ceph%E4%B8%8EKubernetes%E9%9B%86%E6%88%90/"/>
      <url>/2023/04/20/Ceph/13.Ceph%E4%B8%8EKubernetes%E9%9B%86%E6%88%90/Ceph%E4%B8%8EKubernetes%E9%9B%86%E6%88%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3">一、参考文档：</a></li><li><a href="#%E4%BA%8C-%E9%9B%86%E6%88%90%E6%A6%82%E8%BF%B0">二、集成概述：</a></li><li><a href="#%E4%B8%89-ceph%E4%B8%8Evolumes%E7%BB%93%E5%90%88">三、Ceph与volumes结合</a><ul><li><a href="#1-%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C">1、准备工作</a><ul><li><a href="#11-%E5%88%9B%E5%BB%BApool%E5%92%8C%E7%94%A8%E6%88%B7">1.1、创建pool和用户</a></li><li><a href="#12-%E5%88%9B%E5%BB%BA%E8%AE%A4%E8%AF%81%E7%94%A8%E6%88%B7">1.2、创建认证用户</a></li><li><a href="#13-%E5%88%9B%E5%BB%BAsecrets%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E5%B0%86ceph%E7%9A%84%E8%AE%A4%E8%AF%81key%E5%AD%98%E5%82%A8%E5%9C%A8secrets%E4%B8%AD">1.3、创建secrets对象存储将Ceph的认证key存储在Secrets中</a></li></ul></li><li><a href="#2-%E5%AE%B9%E5%99%A8%E4%B8%AD%E8%B0%83%E7%94%A8rbd-volumes">2、容器中调用Rbd Volumes</a><ul><li><a href="#21-%E5%88%9B%E5%BB%BArbd%E5%9D%97">2.1、创建RBD块</a></li><li><a href="#22-pod%E4%B8%AD%E5%BC%95%E7%94%A8rbd-volumes">2.2、pod中引用RBD volumes</a></li></ul></li><li><a href="#3%E6%B5%8B%E8%AF%95%E9%AA%8C%E8%AF%81">3.测试验证</a><ul><li><a href="#31-%E7%94%9F%E6%88%90pod">3.1、生成pod</a></li><li><a href="#32-%E6%9F%A5%E7%9C%8B%E6%8C%82%E8%BD%BD%E7%9A%84%E6%83%85%E5%86%B5">3.2、 查看挂载的情况</a></li></ul></li></ul></li><li><a href="#%E5%9B%9B-ceph%E4%B8%8Epvpvc%E9%9B%86%E6%88%90">四、Ceph与PV&#x2F;PVC集成</a><ul><li><a href="#1-%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C-1">1、准备工作</a></li><li><a href="#2-%E5%AE%9A%E4%B9%89pv%E5%92%8Cpvc">2、定义PV和PVC</a><ul><li><a href="#21-pv%E5%AE%9A%E4%B9%89%E5%AE%9A%E4%B9%89%E4%B8%80%E5%9D%97%E5%AD%98%E5%82%A8%E6%8A%BD%E8%B1%A1%E5%8C%96%E4%B8%BApv">2.1、PV定义，定义一块存储，抽象化为PV</a></li><li><a href="#22-pvc%E5%AE%9A%E4%B9%89%E5%BC%95%E7%94%A8pv">2.2、PVC定义，引用PV</a></li><li><a href="#23-%E7%94%9F%E6%88%90pv%E5%92%8Cpvc">2.3、生成PV和PVC</a></li></ul></li><li><a href="#3-pod%E4%B8%AD%E5%BC%95%E7%94%A8pvc">3、Pod中引用PVC</a></li></ul></li><li><a href="#%E4%BA%94-ceph%E4%B8%8Estorageclass%E9%9B%86%E6%88%90">五、Ceph与StorageClass集成</a></li><li><a href="#1-%E5%88%9B%E5%BB%BA%E6%B1%A0">1、创建池</a></li><li><a href="#2-%E8%AE%BE%E7%BD%AE-ceph-%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BA%AB%E4%BB%BD%E9%AA%8C%E8%AF%81">2、设置 CEPH 客户端身份验证</a></li><li><a href="#3-%E7%94%9F%E6%88%90ceph-csi-configmap">3、生成CEPH-CSI CONFIGMAP</a></li><li><a href="#4-%E7%94%9F%E6%88%90ceph-csi-cephx-secret">4、生成CEPH-CSI CEPHX SECRET</a></li><li><a href="#5-%E9%85%8D%E7%BD%AEceph-csi%E6%8F%92%E4%BB%B6">5、配置CEPH-CSI插件</a></li><li><a href="#6-%E4%BD%BF%E7%94%A8-ceph-%E5%9D%97%E8%AE%BE%E5%A4%87">6、使用 CEPH 块设备</a><ul><li><a href="#1-%E5%88%9B%E5%BB%BA%E5%AD%98%E5%82%A8%E7%B1%BB">1、创建存储类</a></li><li><a href="#2-%E5%88%9B%E5%BB%BApersistentvolumeclaim%E4%BD%BF%E7%94%A8pv%E5%88%9B%E5%BB%BA">2、创建PERSISTENTVOLUMECLAIM（使用PV创建）</a><ul><li><a href="#1-%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3">1、官方文档</a></li><li><a href="#2-%E5%AE%9E%E6%93%8D">2、实操</a></li><li><a href="#3-%E5%AE%B9%E5%99%A8%E4%BD%BF%E7%94%A8">3、容器使用</a></li></ul></li></ul></li><li><a href="#%E5%85%AD-statefulset">六、StatefulSet</a></li></ul><!-- tocstop --><h1><span id="一-参考文档">一、参考文档：</span></h1><p><a href="https://blog.51cto.com/happylab/2488904">https://blog.51cto.com/happylab/2488904</a><br><a href="https://www.cnblogs.com/acommoners/p/15988974.html">https://www.cnblogs.com/acommoners/p/15988974.html</a><br><a href="https://blog.csdn.net/DANTE54/article/details/106471848/">https://blog.csdn.net/DANTE54/article/details/106471848/</a><br><a href="https://www.jianshu.com/p/92d2c31d0f4b">https://www.jianshu.com/p/92d2c31d0f4b</a></p><h1><span id="二-集成概述">二、集成概述：</span></h1><p>Kubernetes和Ceph集成提供了三种实现方式：</p><ol><li>Volumes 存储卷</li><li>PV&#x2F;PVC 持久化卷&#x2F;持久化卷声名</li><li>StorageClass 动态存储类，动态创建PV和PVC</li></ol><p>相关参考：</p><p>Volumes结合：<a href="https://kubernetes.io/zh-cn/docs/concepts/storage/volumes/">https://kubernetes.io/zh-cn/docs/concepts/storage/volumes/</a>  <a href="https://github.com/kubernetes/examples/tree/master/volumes/rbd">https://github.com/kubernetes/examples/tree/master/volumes/rbd</a><br>PV&#x2F;PVC结合：<a href="https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/">https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/</a><br>Storage Class结合：<a href="https://kubernetes.io/zh-cn/docs/concepts/storage/storage-classes/">https://kubernetes.io/zh-cn/docs/concepts/storage/storage-classes/</a><br>Ceph 官网：<a href="https://docs.ceph.com/en/quincy/rbd/rbd-kubernetes/">https://docs.ceph.com/en/quincy/rbd/rbd-kubernetes/</a></p><h1><span id="三-ceph与volumes结合">三、Ceph与volumes结合</span></h1><h2><span id="1-准备工作">1、准备工作</span></h2><h3><span id="11-创建pool和用户">1.1、创建pool和用户</span></h3><pre><code>[root@node-1 ~]# ceph osd pool create kubernetes 8 8 </code></pre><h3><span id="12-创建认证用户">1.2、创建认证用户</span></h3><pre><code>[root@node-1 docker]# ceph auth get-or-create client.kubernetes mon &#39;profile rbd&#39; osd &#39;profile rbd pool=kubernetes&#39;[client.kubernetes]    key = AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ== [root@node-1 docker]#  ceph auth list | grep kubernetes installed auth entries:client.kubernetes    caps: [osd] profile rbd pool=kubernetes</code></pre><h3><span id="13-创建secrets对象存储将ceph的认证key存储在secrets中">1.3、创建secrets对象存储将Ceph的认证key存储在Secrets中</span></h3><blockquote><p>获取步骤2生成的key，并将其加密为base64格式</p></blockquote><pre><code>[root@node-1 ~]# echo AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ== | base64 QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQo=</code></pre><p>创建定义secrets对象</p><pre><code>[root@node-1 docker]# mkdir /etc/ceph/volumes[root@node-1 docker]# vim secret.yamlapiVersion: v1kind: Secretmetadata:  name: ceph-secrettype: &quot;kubernetes.io/rbd&quot;data:  key: QVFETXVwMWVtWk1GT2hBQUJsbW5aRkUyZkY4cHVIZUlodStVUGc9PQo=</code></pre><p>生成secrets</p><pre><code>[root@node-1 volumes]# kubectl apply -f secret.yaml secret/ceph-secret created[root@node-1 volumes]#  kubectl get secretNAME                  TYPE                                  DATA   AGEceph-secret           kubernetes.io/rbd                     1      85sdefault-token-p5dsd   kubernetes.io/service-account-token   3      13h[root@node-1 volumes]# kubectl get secret ceph-secret -o yamlapiVersion: v1data:  key: QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQo=kind: Secretmetadata:  annotations:    kubectl.kubernetes.io/last-applied-configuration: |      &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;key&quot;:&quot;QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQo=&quot;&#125;,&quot;kind&quot;:&quot;Secret&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;ceph-secret&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;type&quot;:&quot;kubernetes.io/rbd&quot;&#125;  creationTimestamp: &quot;2023-03-15T04:08:56Z&quot;  managedFields:  - apiVersion: v1    fieldsType: FieldsV1    fieldsV1:      f:data:        .: &#123;&#125;        f:key: &#123;&#125;      f:metadata:        f:annotations:          .: &#123;&#125;          f:kubectl.kubernetes.io/last-applied-configuration: &#123;&#125;      f:type: &#123;&#125;    manager: kubectl-client-side-apply    operation: Update    time: &quot;2023-03-15T04:08:56Z&quot;  name: ceph-secret  namespace: default  resourceVersion: &quot;119093&quot;  selfLink: /api/v1/namespaces/default/secrets/ceph-secret  uid: 5028d511-5ff5-4466-bad2-36e7c4b4b14ctype: kubernetes.io/rbd</code></pre><h2><span id="2-容器中调用rbd-volumes">2、容器中调用Rbd Volumes</span></h2><h3><span id="21-创建rbd块">2.1、创建RBD块</span></h3><pre><code>[root@node-1 volumes]# kubectl apply -f secret.yaml secret/ceph-secret created[root@node-1 volumes]#  kubectl get secretNAME                  TYPE                                  DATA   AGEceph-secret           kubernetes.io/rbd                     1      85sdefault-token-p5dsd   kubernetes.io/service-account-token   3      13h[root@node-1 volumes]# kubectl get secret ceph-secret -o yamlapiVersion: v1data:  key: QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQo=kind: Secretmetadata:  annotations:    kubectl.kubernetes.io/last-applied-configuration: |      &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;key&quot;:&quot;QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQo=&quot;&#125;,&quot;kind&quot;:&quot;Secret&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;ceph-secret&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;type&quot;:&quot;kubernetes.io/rbd&quot;&#125;  creationTimestamp: &quot;2023-03-15T04:08:56Z&quot;  managedFields:  - apiVersion: v1    fieldsType: FieldsV1    fieldsV1:      f:data:        .: &#123;&#125;        f:key: &#123;&#125;      f:metadata:        f:annotations:          .: &#123;&#125;          f:kubectl.kubernetes.io/last-applied-configuration: &#123;&#125;      f:type: &#123;&#125;    manager: kubectl-client-side-apply    operation: Update    time: &quot;2023-03-15T04:08:56Z&quot;  name: ceph-secret  namespace: default  resourceVersion: &quot;119093&quot;  selfLink: /api/v1/namespaces/default/secrets/ceph-secret  uid: 5028d511-5ff5-4466-bad2-36e7c4b4b14ctype: kubernetes.io/rbd</code></pre><h3><span id="22-pod中引用rbd-volumes">2.2、pod中引用RBD volumes</span></h3><pre><code>[root@node-1 volumes]# cat pods.yaml apiVersion: v1kind: Podmetadata:  name: volume-rbd-demospec:  containers:  - name: pod-with-rbd    image: nginx:1.7.9    imagePullPolicy: IfNotPresent    ports:    - name: www      containerPort: 80      protocol: TCP    volumeMounts:    - name: rbd-demo      mountPath: /data  volumes:  - name: rbd-demo    rbd:      monitors:      - 192.168.187.201:6789      - 192.168.187.202:6789      - 192.168.187.203:6789      pool: kubernetes      image: rbd.img       fsType: ext4       user: kubernetes      secretRef:        name: ceph-secret</code></pre><h2><span id="3测试验证">3.测试验证</span></h2><h3><span id="31-生成pod">3.1、生成pod</span></h3><pre><code>[root@node-1 volumes]# kubectl apply -f pods.yaml pod/volume-rbd-demo created[root@node-1 volumes]# kubectl get pods -o wideNAME                     READY   STATUS    RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATESnginx-6799fc88d8-jhzz7   1/1     Running   2          14h     10.244.247.19   node-2   &lt;none&gt;           &lt;none&gt;volume-rbd-demo          1/1     Running   0          4m15s   10.244.247.22   node-2   &lt;none&gt;           &lt;none&gt;[root@node-1 volumes]# kubectl describe pods volume-rbd-demo Name:         volume-rbd-demoNamespace:    defaultPriority:     0Node:         node-2/192.168.199.202Start Time:   Wed, 15 Mar 2023 13:28:44 +0800Labels:       &lt;none&gt;Annotations:  cni.projectcalico.org/podIP: 10.244.247.22/32              cni.projectcalico.org/podIPs: 10.244.247.22/32Status:       RunningIP:           10.244.247.22IPs:  IP:  10.244.247.22Containers:  pod-with-rbd:    Container ID:   docker://19572ad6d343cb1fca6a8326bc5d1d02cefa24eb582d994f9f01de5227a555fb    Image:          nginx:1.7.9    Image ID:       docker-pullable://nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451    Port:           80/TCP    Host Port:      0/TCP    State:          Running      Started:      Wed, 15 Mar 2023 13:29:29 +0800    Ready:          True    Restart Count:  0    Environment:    &lt;none&gt;    Mounts:      /data from rbd-demo (rw)      /var/run/secrets/kubernetes.io/serviceaccount from default-token-p5dsd (ro)Conditions:  Type              Status  Initialized       True   Ready             True   ContainersReady   True   PodScheduled      True Volumes:  rbd-demo:    Type:          RBD (a Rados Block Device mount on the host that shares a pod&#39;s lifetime)    CephMonitors:  [192.168.187.201:6789 192.168.187.202:6789 192.168.187.203:6789]    RBDImage:      rbd.img    FSType:        ext4    RBDPool:       kubernetes    RadosUser:     kubernetes    Keyring:       /etc/ceph/keyring    SecretRef:     &amp;LocalObjectReference&#123;Name:ceph-secret,&#125;    ReadOnly:      false  default-token-p5dsd:    Type:        Secret (a volume populated by a Secret)    SecretName:  default-token-p5dsd    Optional:    falseQoS Class:       BestEffortNode-Selectors:  &lt;none&gt;Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300sEvents:  Type    Reason                  Age   From                     Message  ----    ------                  ----  ----                     -------  Normal  Scheduled               56s   default-scheduler        Successfully assigned default/volume-rbd-demo to node-2  Normal  SuccessfulAttachVolume  56s   attachdetach-controller  AttachVolume.Attach succeeded for volume &quot;rbd-demo&quot;  Normal  Pulling                 46s   kubelet, node-2          Pulling image &quot;nginx:1.7.9&quot;  Normal  Pulled                  11s   kubelet, node-2          Successfully pulled image &quot;nginx:1.7.9&quot; in 34.176359944s  Normal  Created                 11s   kubelet, node-2          Created container pod-with-rbd  Normal  Started                 11s   kubelet, node-2          Started container pod-with-rbd[root@node-1 volumes]# kubectl exec -it volume-rbd-demo /bin/bashkubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.root@volume-rbd-demo:/# root@volume-rbd-demo:/# root@volume-rbd-demo:/# df -HTFilesystem              Type     Size  Used Avail Use% Mounted onoverlay                 overlay   19G  4.5G   14G  25% /tmpfs                   tmpfs     68M     0   68M   0% /devtmpfs                   tmpfs    2.0G     0  2.0G   0% /sys/fs/cgroup/dev/rbd0               ext4      11G   38M   11G   1% /data/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /dev/termination-log/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /etc/resolv.conf/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /etc/hostname/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /etc/hostsshm                     tmpfs     68M     0   68M   0% /dev/shm/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /var/cache/nginxtmpfs                   tmpfs    2.0G   13k  2.0G   1% /run/secrets/kubernetes.io/serviceaccounttmpfs                   tmpfs    2.0G     0  2.0G   0% /proc/acpitmpfs                   tmpfs     68M     0   68M   0% /proc/kcoretmpfs                   tmpfs     68M     0   68M   0% /proc/keystmpfs                   tmpfs     68M     0   68M   0% /proc/timer_listtmpfs                   tmpfs     68M     0   68M   0% /proc/timer_statstmpfs                   tmpfs     68M     0   68M   0% /proc/sched_debugtmpfs                   tmpfs    2.0G     0  2.0G   0% /proc/scsitmpfs                   tmpfs    2.0G     0  2.0G   0% /sys/firmwareroot@volume-rbd-demo:/# cd /data/root@volume-rbd-demo:/data# lslost+foundroot@volume-rbd-demo:/data# touch testfileroot@volume-rbd-demo:/data# echo &quot;test&quot; &gt;&gt; testfile root@volume-rbd-demo:/data# lslost+found  testfileroot@volume-rbd-demo:/data# cat testfile testroot@volume-rbd-demo:/data# lslost+found  testfileroot@volume-rbd-demo:/data# exit</code></pre><h3><span id="32-查看挂载的情况">3.2、 查看挂载的情况</span></h3><pre><code>[root@node-1 volumes]# kubectl exec -it volume-rbd-demo -- df -hFilesystem      Size  Used Avail Use% Mounted onrootfs           50G  6.7G   41G  15% /overlay          50G  6.7G   41G  15% /tmpfs            64M     0   64M   0% /devtmpfs           920M     0  920M   0% /sys/fs/cgroup/dev/rbd0       9.8G   37M  9.7G   1% /data[root@node-1 volumes]# kubectl exec -it volume-rbd-demo /bin/bashkubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.root@volume-rbd-demo:/# root@volume-rbd-demo:/# root@volume-rbd-demo:/# df -HTFilesystem              Type     Size  Used Avail Use% Mounted onoverlay                 overlay   19G  4.5G   14G  25% /tmpfs                   tmpfs     68M     0   68M   0% /devtmpfs                   tmpfs    2.0G     0  2.0G   0% /sys/fs/cgroup/dev/rbd0               ext4      11G   38M   11G   1% /data/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /dev/termination-log/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /etc/resolv.conf/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /etc/hostname/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /etc/hostsshm                     tmpfs     68M     0   68M   0% /dev/shm/dev/mapper/centos-root xfs       19G  4.5G   14G  25% /var/cache/nginxtmpfs                   tmpfs    2.0G   13k  2.0G   1% /run/secrets/kubernetes.io/serviceaccounttmpfs                   tmpfs    2.0G     0  2.0G   0% /proc/acpitmpfs                   tmpfs     68M     0   68M   0% /proc/kcoretmpfs                   tmpfs     68M     0   68M   0% /proc/keystmpfs                   tmpfs     68M     0   68M   0% /proc/timer_listtmpfs                   tmpfs     68M     0   68M   0% /proc/timer_statstmpfs                   tmpfs     68M     0   68M   0% /proc/sched_debugtmpfs                   tmpfs    2.0G     0  2.0G   0% /proc/scsitmpfs                   tmpfs    2.0G     0  2.0G   0% /sys/firmware[root@node-2 ~]# df -HT |grep rbd0 #node02中查看/dev/rbd0               ext4       11G   38M   11G    1% /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/kubernetes-image-rbd.img[root@node-2 ~]# rbd device list id pool       namespace image   snap device    0  kubernetes           rbd.img -    /dev/rbd0[root@node-2 mounts]# cd /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/kubernetes-image-rbd.img[root@node-2 kubernetes-image-rbd.img]# lslost+found  testfile[root@node-2 kubernetes-image-rbd.img]# cat testfile test</code></pre><h1><span id="四-ceph与pvx2fpvc集成">四、Ceph与PV&#x2F;PVC集成</span></h1><h2><span id="1-准备工作">1、准备工作</span></h2><p><strong>参考步骤一，创建好pool，镜像，用户认证，secrets</strong></p><h2><span id="2-定义pv和pvc">2、定义PV和PVC</span></h2><h3><span id="21-pv定义定义一块存储抽象化为pv">2.1、PV定义，定义一块存储，抽象化为PV</span></h3><pre><code>[root@node-1 pv_and_pvc]# cat pv.yaml apiVersion: v1kind: PersistentVolumemetadata:  name: rbd-demospec:  accessModes:   - ReadWriteOnce #定义一个客户端同时进行读写  capacity:    storage: 10G #定义大小  rbd: #定义RBD相关的驱动    monitors: #定义Ceph-Mon地址和端口     - 192.168.187.201:6789     - 192.168.187.202:6789     - 192.168.187.203:6789    pool: kubernetes #定义ceph中的pool    image: demo-1.img  #定义镜像    fsType: ext4 #定义镜像格式    user: kubernetes #定义访问用户    secretRef:      name: ceph-secret  persistentVolumeReclaimPolicy: Retain #定义持久化存储的策略，可以重复利用，pool删除以后PV不受影响  storageClassName: rbd[root@node-1 volumes]# rbd create -p kubernetes --image-feature layering demo-1.img --size 10G[root@node-1 volumes]# rbd info kubernetes/demo-1.imgrbd image &#39;demo-1.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 4091dd5acdf1    block_name_prefix: rbd_data.4091dd5acdf1    format: 2    features: layering    op_features:     flags:     create_timestamp: Wed Mar 15 13:54:41 2023    access_timestamp: Wed Mar 15 13:54:41 2023    modify_timestamp: Wed Mar 15 13:54:41 2023[root@node-1 volumes]# rbd -p kubernetes ls demo-1.imgrbd.img</code></pre><h3><span id="22-pvc定义引用pv">2.2、PVC定义，引用PV</span></h3><pre><code>[root@node-1 pv_and_pvc]# cat pvc.yaml apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: pvc-demospec:  accessModes:   - ReadWriteOnce #访问模式与PV一致  volumeName: rbd-demo #定义一个名称  resources:    requests:      storage: 10G #申请空间大小，可小于RBD大小，但是不能超过  storageClassName: rbd</code></pre><h3><span id="23-生成pv和pvc">2.3、生成PV和PVC</span></h3><pre><code>[root@node-1 volumes]# kubectl apply -f pv.yaml persistentvolume/rbd-demo created[root@node-1 volumes]# kubectl get pvNAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGErbd-demo   10G        RWO            Retain           Available           rbd                     14s[root@node-1 pv_and_pvc]# kubectl apply -f pvc.yaml persistentvolumeclaim/pvc-demo created[root@node-1 volumes]# kubectl get pvcNAME       STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGEpvc-demo   Bound    rbd-demo   10G        RWO            rbd            7m37s[root@node-1 volumes]# kubectl describe pvc pvc-demo Name:          pvc-demoNamespace:     defaultStorageClass:  rbdStatus:        BoundVolume:        rbd-demoLabels:        &lt;none&gt;Annotations:   pv.kubernetes.io/bind-completed: yesFinalizers:    [kubernetes.io/pvc-protection]Capacity:      10GAccess Modes:  RWOVolumeMode:    FilesystemMounted By:    &lt;none&gt;Events:        &lt;none&gt;[root@node-1 volumes]# kubectl get pvNAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   REASON   AGErbd-demo   10G        RWO            Retain           Bound    default/pvc-demo   rbd                     11m</code></pre><h2><span id="3-pod中引用pvc">3、Pod中引用PVC</span></h2><pre><code>[root@node-1 pv_and_pvc]# cat pod-demo.yaml apiVersion: v1kind: Podmetadata:  name: pod-demospec:  containers:  - name: demo    image: nginx:1.7.9    imagePullPolicy: IfNotPresent    ports:    - name: www      protocol: TCP      containerPort: 80    volumeMounts:    - name: rbd      mountPath: /data #挂载点  volumes:  - name: rbd    persistentVolumeClaim:      claimName: pvc-demo #PVC的名称[root@node-1 volumes]# kubectl apply -f pod-demo.yaml  #应用yaml文件pod/pod-demo created[root@node-1 volumes]# kubectl describe pods pod-demo #查看创建情况，已经将容器部署到node-3  Normal  Created                 54s   kubelet, node-3          Created container demo  Normal  Started                 53s   kubelet, node-3          Started container demo[root@node-1 volumes]# kubectl get pods #查看pods运行情况NAME                     READY   STATUS    RESTARTS   AGEnginx-6799fc88d8-jhzz7   1/1     Running   2          2d19hpod-demo                 1/1     Running   0          4m2svolume-rbd-demo          1/1     Running   0          2d4h[root@node-1 volumes]# kubectl exec -it pod-demo /bin/bash  #进入容器查看 挂载情况kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.root@pod-demo:/# df -HT #查看挂载情况Filesystem              Type     Size  Used Avail Use% Mounted onoverlay                 overlay   19G  4.1G   15G  23% /tmpfs                   tmpfs     68M     0   68M   0% /devtmpfs                   tmpfs    954M     0  954M   0% /sys/fs/cgroup/dev/rbd0               ext4      11G   38M   11G   1% /data #已经正常挂载[root@node-1 volumes]# ssh node-3 #远程到node-3Last login: Fri Mar 17 17:34:15 2023 from 192.168.187.1[root@node-3 ~]# df -HT |grep rbd#宿主机查看到映射情况/dev/rbd0               ext4       11G   38M   11G    1% /var/lib/kubelet/plugins/kubernetes.io/rbd/mounts/kubernetes-image-demo-1.img[root@node-3 ~]# rbd device list #查看rbd映射情况id pool       namespace image      snap device    0  kubernetes           demo-1.img -    /dev/rbd0 [root@node-3 ~]# docker ps -a |grep nginx #查看容器运行状况c38756d298c8   nginx                                                &quot;nginx -g &#39;daemon of…&quot;   11 minutes ago   Up 11 minutes                     k8s_demo_pod-demo_default_625a64e7-bac1-4128-a7d8-40cbc9956eed_0</code></pre><h1><span id="五-ceph与storageclass集成">五、Ceph与StorageClass集成</span></h1><p><img src="/images/Ceph/Ceph%E4%B8%8Ekubernetes%E9%9B%86%E6%88%90/1.jpg"></p><h1><span id="1-创建池">1、创建池</span></h1><p>默认情况下，Ceph 块设备使用rbd池。为 Kubernetes 卷存储创建一个池。确保您的 Ceph 集群正在运行，然后创建池。<br>新创建的池必须在使用前进行初始化。使用该rbd pool init工具初始化池：</p><pre><code>[root@node-1 volumes]ceph osd pool create kubernetes #上述配过不需要重新创建[root@node-1 volumes]rbd pool init kubernetes #上述配过不需要初始化</code></pre><h1><span id="2-设置-ceph-客户端身份验证">2、设置 CEPH 客户端身份验证</span></h1><p>为 Kubernetes 和ceph-csi创建一个新用户。执行以下命令并记录生成的密钥：</p><blockquote><p>kubernetes 用户在与PV&#x2F;PVC集成时已经创建过，然后下列命令可以不执行，并且此kubernetes用户在与CSI集成时会出现BUG，需要用ceph的admin用户集成，然后下列命令在创建时不能指定去访问mgr的存储池，也会报错</p></blockquote><blockquote><p>经过实际测试，目前没有出现视频中发现的BUG，可能和安装的版本有关，最大的可能就是ceph-csi的版本</p></blockquote><pre><code>[root@node-1 volumes]# kubectl versionClient Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;19&quot;, GitVersion:&quot;v1.19.0&quot;, GitCommit:&quot;e19964183377d0ec2052d1f1fa930c4d7575bd50&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2020-08-26T14:30:33Z&quot;, GoVersion:&quot;go1.15&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;19&quot;, GitVersion:&quot;v1.19.0&quot;, GitCommit:&quot;e19964183377d0ec2052d1f1fa930c4d7575bd50&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2020-08-26T14:23:04Z&quot;, GoVersion:&quot;go1.15&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;[root@node-1 volumes]# ceph -vceph version 14.2.22 (ca74598065096e6fcbd8433c8779a2be0c889351) nautilus (stable)</code></pre><pre><code>[root@node-1 volumes]# ceph auth list #可以看到该用户已经创建过，不需要重新创建client.kuberneteskey: AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ==caps: [mon] profile rbdcaps: [osd] profile rbd pool=kubernetes[root@node-1 volumes]$ ceph auth get-or-create client.kubernetes mon &#39;profile rbd&#39; osd &#39;profile rbd pool=kubernetes&#39; mgr &#39;profile rbd pool=kubernetes&#39;[client.kubernetes]    key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==</code></pre><h1><span id="3-生成ceph-csi-configmap">3、生成CEPH-CSI CONFIGMAP</span></h1><p>ceph -csi需要一个存储在 Kubernetes 中的ConfigMap对象来定义 Ceph 集群的 Ceph 监视器地址。收集 Ceph 集群唯一fsid和监视器地址：</p><pre><code>[root@node-1 volumes]# ceph mon dump  #查看集群唯一ID和MON的地址epoch 3fsid b8e58b30-4568-4032-a9f4-837ed3fa9529 #唯一IDlast_changed 2023-03-07 13:57:20.988318created 2023-03-07 13:41:49.055353min_mon_release 14 (nautilus)0: [v2:192.168.187.201:3300/0,v1:192.168.187.201:6789/0] mon.node-11: [v2:192.168.187.202:3300/0,v1:192.168.187.202:6789/0] mon.node-22: [v2:192.168.187.203:3300/0,v1:192.168.187.203:6789/0] mon.node-3dumped monmap epoch 3[root@node-1 volumes]# cat &lt;&lt;EOF &gt; csi-config-map.yaml #创建配置文件---apiVersion: v1kind: ConfigMapdata:  config.json: |-    [      &#123;        &quot;clusterID&quot;: &quot;b9127830-b0cc-4e34-aa47-9d1a2e9949a8&quot;,        &quot;monitors&quot;: [          &quot;192.168.1.1:6789&quot;,          &quot;192.168.1.2:6789&quot;,          &quot;192.168.1.3:6789&quot;        ]      &#125;    ]metadata:  name: ceph-csi-configEOF[root@node-1 volumes]# vim csi-config-map.yaml #修改配置文件，根据自身集群情况进行修改---apiVersion: v1kind: ConfigMapdata:  config.json: |-    [      &#123;        &quot;clusterID&quot;: &quot;b8e58b30-4568-4032-a9f4-837ed3fa9529&quot;,        &quot;monitors&quot;: [          &quot;192.168.187.201:6789&quot;,          &quot;192.168.187.202:6789&quot;,          &quot;192.168.187.203:6789&quot;        ]      &#125;    ]metadata:  name: ceph-csi-config[root@node-1 volumes]# kubectl apply -f csi-config-map.yaml #将编辑好的配置文件生成config mapconfigmap/ceph-csi-config created[root@node-1 volumes]# kubectl get configmaps #查看配置文件生成情况NAME              DATA   AGEceph-csi-config   1      37s[root@node-1 volumes]# kubectl get configmaps ceph-csi-config -o yaml #查看生成的配置文件具体内容apiVersion: v1data: #在这里定义的  config.json: |-    [      &#123;        &quot;clusterID&quot;: &quot;b8e58b30-4568-4032-a9f4-837ed3fa9529&quot;,        &quot;monitors&quot;: [          &quot;192.168.187.201:6789&quot;,          &quot;192.168.187.202:6789&quot;,          &quot;192.168.187.203:6789&quot;        ]      &#125;    ]kind: ConfigMapmetadata:  annotations:    kubectl.kubernetes.io/last-applied-configuration: |      &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;config.json&quot;:&quot;[\n  &#123;\n    \&quot;clusterID\&quot;: \&quot;b8e58b30-4568-4032-a9f4-837ed3fa9529\&quot;,\n    \&quot;monitors\&quot;: [\n      \&quot;192.168.187.201:6789\&quot;,\n      \&quot;192.168.187.202:6789\&quot;,\n      \&quot;192.168.187.203:6789\&quot;\n    ]\n  &#125;\n]&quot;&#125;,&quot;kind&quot;:&quot;ConfigMap&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;ceph-csi-config&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;&#125;  creationTimestamp: &quot;2023-03-18T04:11:51Z&quot;  managedFields:  - apiVersion: v1    fieldsType: FieldsV1    fieldsV1:      f:data:        .: &#123;&#125;        f:config.json: &#123;&#125;      f:metadata:        f:annotations:          .: &#123;&#125;          f:kubectl.kubernetes.io/last-applied-configuration: &#123;&#125;    manager: kubectl-client-side-apply    operation: Update    time: &quot;2023-03-18T04:11:51Z&quot;  name: ceph-csi-config  namespace: default  resourceVersion: &quot;589956&quot;  selfLink: /api/v1/namespaces/default/configmaps/ceph-csi-config  uid: 30e98e2d-46eb-40bb-9d58-cb410012c31a</code></pre><h1><span id="4-生成ceph-csi-cephx-secret">4、生成CEPH-CSI CEPHX SECRET</span></h1><p>ceph-csi需要 cephx 凭据才能与 Ceph 集群通信。使用新创建的 Kubernetes 用户 ID 和 cephx 密钥生成类似于以下示例的csi-rbd-secret.yaml文件：</p><pre><code>[root@node-1 volumes]#cat &lt;&lt;EOF &gt; csi-rbd-secret.yaml #配置认证信息---apiVersion: v1kind: Secretmetadata:  name: csi-rbd-secret  namespace: defaultstringData:  userID: kubernetes  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==EOF[root@node-1 volumes]#ceph auth list #获取用户IDclient.kubernetes    key: AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ==    caps: [mon] profile rbd    caps: [osd] profile rbd pool=kubernetes[root@node-1 volumes]# vim csi-rbd-secret.yaml #根据自身集群情况进行修改---apiVersion: v1kind: Secretmetadata:  name: csi-rbd-secret  namespace: defaultstringData:  userID: kubernetes  userKey: AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ== #这个密钥不用base 64加密，因为使用的时stringData模式，会自动加密  [root@node-1 volumes]# kubectl apply -f csi-rbd-secret.yaml #生成这个对象secret/csi-rbd-secret created[root@node-1 volumes]# kubectl get secrets  #查看生成结果NAME                  TYPE                                  DATA   AGEceph-secret           kubernetes.io/rbd                     1      3dcsi-rbd-secret        Opaque                                2      32sdefault-token-p5dsd   kubernetes.io/service-account-token   3      3d14h[root@node-1 volumes]# kubectl get secrets csi-rbd-secret -o yaml #查看生成的具体内容apiVersion: v1data: #data显示的就是base 64加密后的，stringData显示的就是不加密的  userID: a3ViZXJuZXRlcw== #定义的用户名  userKey: QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQ== #自动加密的keykind: Secretmetadata:  annotations:    kubectl.kubernetes.io/last-applied-configuration: |      &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Secret&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;csi-rbd-secret&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;stringData&quot;:&#123;&quot;userID&quot;:&quot;kubernetes&quot;,&quot;userKey&quot;:&quot;AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ==&quot;&#125;&#125;  creationTimestamp: &quot;2023-03-18T04:38:20Z&quot;  managedFields:  - apiVersion: v1    fieldsType: FieldsV1    fieldsV1:      f:data:        .: &#123;&#125;        f:userID: &#123;&#125;        f:userKey: &#123;&#125;      f:metadata:        f:annotations:          .: &#123;&#125;          f:kubectl.kubernetes.io/last-applied-configuration: &#123;&#125;      f:type: &#123;&#125;    manager: kubectl-client-side-apply    operation: Update    time: &quot;2023-03-18T04:38:20Z&quot;  name: csi-rbd-secret  namespace: default  resourceVersion: &quot;593749&quot;  selfLink: /api/v1/namespaces/default/secrets/csi-rbd-secret  uid: 712d89bf-d3b2-44e5-bf91-973f76b15830type: Opaque[root@node-1 volumes]# echo a3ViZXJuZXRlcw== | base64 -d #解密userID后得出用户名称kubernetes[root@node-1 volumes]#kubernetes[root@node-1 volumes]# echo QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQ== | base64 -d #解密userkey后得出实际的keyAQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ==[root@node-1 volumes]# </code></pre><h1><span id="5-配置ceph-csi插件">5、配置CEPH-CSI插件</span></h1><p>创建所需的ServiceAccount和 RBAC ClusterRole &#x2F; ClusterRoleBinding Kubernetes 对象。这些对象不一定需要为您的 Kubernetes 环境定制，因此可以从ceph-csi 部署 YAML 中按原样使用：</p><pre><code>[root@node-1 volumes]# wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml[root@node-1 volumes]# wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml[root@node-1 volumes]# ls -la | grep rbac.yaml #下载成功-rw-r--r-- 1 root root 1193 3月  18 12:49 csi-nodeplugin-rbac.yaml #节点驱动的-rw-r--r-- 1 root root 3347 3月  18 12:48 csi-provisioner-rbac.yaml #提供者的[root@node-1 volumes]#  kubectl apply -f csi-provisioner-rbac.yaml #进行提交serviceaccount/rbd-csi-provisioner createdclusterrole.rbac.authorization.k8s.io/rbd-external-provisioner-runner createdclusterrolebinding.rbac.authorization.k8s.io/rbd-csi-provisioner-role createdrole.rbac.authorization.k8s.io/rbd-external-provisioner-cfg createdrolebinding.rbac.authorization.k8s.io/rbd-csi-provisioner-role-cfg created[root@node-1 volumes]# kubectl apply -f csi-nodeplugin-rbac.yaml #进行提交serviceaccount/rbd-csi-nodeplugin createdclusterrole.rbac.authorization.k8s.io/rbd-csi-nodeplugin createdclusterrolebinding.rbac.authorization.k8s.io/rbd-csi-nodeplugin created</code></pre><p>最后，创建ceph-csi provisioner 和节点插件。除了ceph-csi容器发布版本之外，这些对象不一定需要为您的 Kubernetes 环境定制，因此可以从 ceph- csi部署 YAMLs 中按原样使用：</p><pre><code>[root@node-1 volumes]# wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml[root@node-1 volumes]# wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml[root@node-1 volumes]# ls -la | grep rbdplugin #下载成功-rw-r--r-- 1 root root 8125 3月  18 12:54 csi-rbdplugin-provisioner.yaml-rw-r--r-- 1 root root 7736 3月  18 12:54 csi-rbdplugin.yaml[root@node-1 volumes]# kubectl apply -f csi-rbdplugin-provisioner.yaml #提交service/csi-rbdplugin-provisioner createddeployment.apps/csi-rbdplugin-provisioner created[root@node-1 volumes]# kubectl apply -f csi-rbdplugin.yaml#提交daemonset.apps/csi-rbdplugin createdservice/csi-metrics-rbdplugin created[root@node-1 volumes]# kubectl get deployments.apps  #查看状态NAME                        READY   UP-TO-DATE   AVAILABLE   AGEcsi-rbdplugin-provisioner   0/3     3            0           53snginx                       1/1     1            1           3d14h[root@node-1 volumes]#  kubectl get pods #查看正在创建NAME                                         READY   STATUS              RESTARTS   AGEcsi-rbdplugin-g9qc7                          0/3     ContainerCreating   0          73scsi-rbdplugin-n7l5f                          0/3     ContainerCreating   0          73scsi-rbdplugin-provisioner-6748c759b4-7mjfg   0/7     ContainerCreating   0          85scsi-rbdplugin-provisioner-6748c759b4-8g2pr   0/7     ContainerCreating   0          85scsi-rbdplugin-provisioner-6748c759b4-9hgt4   0/7     Pending             0          85snginx-6799fc88d8-jhzz7                       1/1     Running             3          3d14hpod-demo                                     1/1     Running             1          19hvolume-rbd-demo                              1/1     Running             1          2d23h[root@node-1 volumes]# kubectl describe pods csi-rbdplugin-provisioner-6748c759b4-7mjfg  #查看创建失败，缺少ceph-csi-encryption-kms-config的配置文件  Normal   Scheduled    2m36s                default-scheduler  Successfully assigned default/csi-rbdplugin-provisioner-6748c759b4-7mjfg to node-3  Warning  FailedMount  92s (x8 over 2m36s)  kubelet, node-3    MountVolume.SetUp failed for volume &quot;ceph-csi-encryption-kms-config&quot; : configmap &quot;ceph-csi-encryption-kms-config&quot; not found  Warning  FailedMount  92s (x8 over 2m36s)  kubelet, node-3    MountVolume.SetUp failed for volume &quot;ceph-config&quot; : configmap &quot;ceph-config&quot; not found  Warning  FailedMount  92s (x8 over 2m36s)  kubelet, node-3    MountVolume.SetUp failed for volume &quot;oidc-token&quot; : failed to fetch token: the API server does not have TokenRequest endpoints enabled  Warning  FailedMount  33s                  kubelet, node-3    Unable to attach or mount volumes: unmounted volumes=[ceph-csi-encryption-kms-config oidc-token ceph-config], unattached volumes=[ceph-csi-encryption-kms-config keys-tmp-dir oidc-token host-sys rbd-csi-provisioner-token-kpbsw host-dev lib-modules ceph-csi-config socket-dir ceph-config]: timed out waiting for the condition</code></pre><p><strong>最新版：</strong><a href="https://github.com/ceph/ceph-csi/blob/devel/examples/kms/vault/kms-config.yaml">Github</a><br><strong>老版本：</strong></p><pre><code>apiVersion: v1kind: ConfigMapdata:config.json: |-&#123;&quot;vault-test&quot;: &#123;&quot;encryptionKMSType&quot;: &quot;vault&quot;,&quot;vaultAddress&quot;: &quot;http://vault.default.svc.cluster.local:8200&quot;,&quot;vaultAuthPath&quot;: &quot;/v1/auth/kubernetes/login&quot;,&quot;vaultRole&quot;: &quot;csi-kubernetes&quot;,&quot;vaultPassphraseRoot&quot;: &quot;/v1/secret&quot;,&quot;vaultPassphrasePath&quot;: &quot;ceph-csi/&quot;,&quot;vaultCAVerify&quot;: &quot;false&quot;&#125;&#125;metadata:name: ceph-csi-encryption-kms-config</code></pre><pre><code>[root@node-1 volumes]#cat &lt;&lt;EOF &gt; kms-config.yaml #创建---apiVersion: v1kind: ConfigMapdata:  config.json: |-    &#123;      &quot;vault-test&quot;: &#123;        &quot;encryptionKMSType&quot;: &quot;vault&quot;,        &quot;vaultAddress&quot;: &quot;http://vault.default.svc.cluster.local:8200&quot;,        &quot;vaultAuthPath&quot;: &quot;/v1/auth/kubernetes/login&quot;,        &quot;vaultRole&quot;: &quot;csi-kubernetes&quot;,        &quot;vaultPassphraseRoot&quot;: &quot;/v1/secret&quot;,        &quot;vaultPassphrasePath&quot;: &quot;ceph-csi/&quot;,        &quot;vaultCAVerify&quot;: &quot;false&quot;      &#125;    &#125;metadata:  name: ceph-csi-encryption-kms-configEOF[root@node-1 volumes]# kubectl apply -f kms-config.yaml #提交configmap/ceph-csi-encryption-kms-config created[root@node-1 volumes]# kubectl get configmaps  #查看配置文件NAME                             DATA   AGEceph-csi-config                  1      57mceph-csi-encryption-kms-config   1      18s[root@node-1 volumes]# kubectl delete -f csi-rbdplugin.yaml #上述查看创建过程中看到已经超时，所以需要删除重新提交daemonset.apps &quot;csi-rbdplugin&quot; deletedservice &quot;csi-metrics-rbdplugin&quot; deleted[root@node-1 volumes]# kubectl delete -f csi-rbdplugin-provisioner.yaml #上述查看创建过程中看到已经超时，所以需要删除重新提交service &quot;csi-rbdplugin-provisioner&quot; deleteddeployment.apps &quot;csi-rbdplugin-provisioner&quot; deleted[root@node-1 volumes]# kubectl apply -f csi-rbdplugin-provisioner.yaml #重新提交service/csi-rbdplugin-provisioner createddeployment.apps/csi-rbdplugin-provisioner created[root@node-1 volumes]# kubectl apply -f csi-rbdplugin.yaml #重新提交daemonset.apps/csi-rbdplugin createdservice/csi-metrics-rbdplugin created[root@node-1 volumes]# kubectl describe pods csi-rbdplugin-fl6tx #查看还是报错，缺少配置文件ceph-config  Warning  FailedMount  119s (x13 over 12m)  kubelet, node-3  MountVolume.SetUp failed for volume &quot;ceph-config&quot; : configmap &quot;ceph-config&quot; not found</code></pre><blockquote><p>文档来源：<a href="https://docs.ceph.com/en/quincy/rbd/rbd-kubernetes/">https://docs.ceph.com/en/quincy/rbd/rbd-kubernetes/</a><br>在ceph的官方文档中查看最新版本的ceph-csi还需要另一个ConfigMap对象来定义 Ceph 配置以添加到 CSI 容器内的 ceph.conf 文件中：</p></blockquote><pre><code>[root@node-1 volumes]# cat &lt;&lt;EOF &gt; ceph-config-map.yaml #创建ceph-config文件---apiVersion: v1kind: ConfigMapdata:  ceph.conf: |    [global]    auth_cluster_required = cephx    auth_service_required = cephx    auth_client_required = cephx  # keyring is a required key and its value should be empty  keyring: |metadata:  name: ceph-configEOF[root@node-1 volumes]# kubectl apply -f ceph-config-map.yaml #提交configmap/ceph-config created[root@node-1 volumes]# kubectl delete -f csi-rbdplugin-provisioner.yaml #再次删除之前提交的内容service &quot;csi-rbdplugin-provisioner&quot; deleteddeployment.apps &quot;csi-rbdplugin-provisioner&quot; deleted[root@node-1 volumes]# kubectl delete -f csi-rbdplugin.yaml #再次删除daemonset.apps &quot;csi-rbdplugin&quot; deletedservice &quot;csi-metrics-rbdplugin&quot; deleted[root@node-1 volumes]# kubectl apply -f csi-rbdplugin-provisioner.yaml #重新提交service/csi-rbdplugin-provisioner createddeployment.apps/csi-rbdplugin-provisioner created[root@node-1 volumes]# kubectl apply -f csi-rbdplugin.yaml #重新提交daemonset.apps/csi-rbdplugin createdservice/csi-metrics-rbdplugin created[root@node-1 volumes]# kubectl get pods #查看正在创建中NAME                                         READY   STATUS              RESTARTS   AGEcsi-rbdplugin-bhlk2                          0/3     ContainerCreating   0          37scsi-rbdplugin-provisioner-6748c759b4-6xqrs   0/7     Pending             0          49scsi-rbdplugin-provisioner-6748c759b4-8gz5g   0/7     ContainerCreating   0          49scsi-rbdplugin-provisioner-6748c759b4-fkxk5   0/7     ContainerCreating   0          49scsi-rbdplugin-ttc95                          0/3     ContainerCreating   0          37snginx-6799fc88d8-jhzz7                       1/1     Running             3          3d15hpod-demo                                     1/1     Running             1          19hvolume-rbd-demo                              1/1     Running             1          3d[root@node-1 volumes]# kubectl describe pods csi-rbdplugin-bhlk2 #查看详情，可以看到已经没有报错信息了，正在下载镜像中Events:  Type    Reason     Age   From             Message  ----    ------     ----  ----             -------  Normal  Scheduled  48s                    Successfully assigned default/csi-rbdplugin-bhlk2 to node-2  Normal  Pulling    47s   kubelet, node-2  Pulling image &quot;registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.2&quot;    [root@node-1 volumes]# kubectl describe pods csi-rbdplugin-bhlk2 #查看详情，镜像下载失败，怀疑和下载源有关系  Warning  Failed     53s (x2 over 118s)   kubelet, node-2  Error: ErrImagePull  Warning  Failed     53s                  kubelet, node-2  Failed to pull image &quot;registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.2&quot;: rpc error: code = Unknown desc = Error response from daemon: Head &quot;https://asia-east1-docker.pkg.dev/v2/k8s-artifacts-prod/images/sig-storage/csi-node-driver-registrar/manifests/v2.6.2&quot;: dial tcp 108.177.97.82:443: connect: connection refused  Normal   BackOff    39s (x3 over 117s)   kubelet, node-2  Back-off pulling image &quot;registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.2&quot;</code></pre><p><strong>排障过程</strong></p><pre><code>#使用Docker 命令下载镜像，可以看到下载失败，基本确定已经是源地址镜像问题，导致无法正常启动[root@node-1 volumes]# docker pull registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.2Error response from daemon: Head &quot;https://asia-east1-docker.pkg.dev/v2/k8s-artifacts-prod/images/sig-storage/csi-node-driver-registrar/manifests/v2.6.2&quot;: dial tcp 74.125.204.82:443: connect: connection refused[root@node-1 volumes]# docker pull registry.aliyuncs.com/google_containers/csi-node-driver-registrar:v2.6.2 #更换成阿里云地址，可以看到能够正常下载，记得Ctrl+c取消下载v2.6.2: Pulling from google_containers/csi-node-driver-registrar8fdb1fc20e24: Pull complete a13c28052fb0: Downloading [==============================&gt;                    ]  5.706MB/9.273MB[root@node-1 volumes]# cat csi-rbdplugin-provisioner.yaml  #将配置文件中所有需要从registry.k8s.io下载的镜像替换成阿里云---kind: ServiceapiVersion: v1metadata:  name: csi-rbdplugin-provisioner  # replace with non-default namespace name  namespace: default  labels:    app: csi-metricsspec:  selector:    app: csi-rbdplugin-provisioner  ports:    - name: http-metrics      port: 8080      protocol: TCP      targetPort: 8680---kind: DeploymentapiVersion: apps/v1metadata:  name: csi-rbdplugin-provisioner  # replace with non-default namespace name  namespace: defaultspec:  replicas: 3  selector:    matchLabels:      app: csi-rbdplugin-provisioner  template:    metadata:      labels:        app: csi-rbdplugin-provisioner    spec:      affinity:        podAntiAffinity:          requiredDuringSchedulingIgnoredDuringExecution:            - labelSelector:                matchExpressions:                  - key: app                    operator: In                    values:                      - csi-rbdplugin-provisioner              topologyKey: &quot;kubernetes.io/hostname&quot;      serviceAccountName: rbd-csi-provisioner      priorityClassName: system-cluster-critical      containers:        - name: csi-provisioner          image: registry.aliyuncs.com/google_containers/csi-provisioner:v3.3.0          args:            - &quot;--csi-address=$(ADDRESS)&quot;            - &quot;--v=1&quot;            - &quot;--timeout=150s&quot;            - &quot;--retry-interval-start=500ms&quot;            - &quot;--leader-election=true&quot;            #  set it to true to use topology based provisioning            - &quot;--feature-gates=Topology=false&quot;            - &quot;--feature-gates=HonorPVReclaimPolicy=true&quot;            - &quot;--prevent-volume-mode-conversion=true&quot;            # if fstype is not specified in storageclass, ext4 is default            - &quot;--default-fstype=ext4&quot;            - &quot;--extra-create-metadata=true&quot;          env:            - name: ADDRESS              value: unix:///csi/csi-provisioner.sock          imagePullPolicy: &quot;IfNotPresent&quot;          volumeMounts:            - name: socket-dir              mountPath: /csi        - name: csi-snapshotter          image: registry.aliyuncs.com/google_containers/csi-snapshotter:v6.1.0          args:            - &quot;--csi-address=$(ADDRESS)&quot;            - &quot;--v=1&quot;            - &quot;--timeout=150s&quot;            - &quot;--leader-election=true&quot;            - &quot;--extra-create-metadata=true&quot;          env:            - name: ADDRESS              value: unix:///csi/csi-provisioner.sock          imagePullPolicy: &quot;IfNotPresent&quot;          volumeMounts:            - name: socket-dir              mountPath: /csi        - name: csi-attacher          image: registry.aliyuncs.com/google_containers/csi-attacher:v4.0.0          args:            - &quot;--v=1&quot;            - &quot;--csi-address=$(ADDRESS)&quot;            - &quot;--leader-election=true&quot;            - &quot;--retry-interval-start=500ms&quot;            - &quot;--default-fstype=ext4&quot;          env:            - name: ADDRESS              value: /csi/csi-provisioner.sock          imagePullPolicy: &quot;IfNotPresent&quot;          volumeMounts:            - name: socket-dir              mountPath: /csi        - name: csi-resizer          image: registry.aliyuncs.com/google_containers/csi-resizer:v1.6.0          args:            - &quot;--csi-address=$(ADDRESS)&quot;            - &quot;--v=1&quot;            - &quot;--timeout=150s&quot;            - &quot;--leader-election&quot;            - &quot;--retry-interval-start=500ms&quot;            - &quot;--handle-volume-inuse-error=false&quot;            - &quot;--feature-gates=RecoverVolumeExpansionFailure=true&quot;          env:            - name: ADDRESS              value: unix:///csi/csi-provisioner.sock          imagePullPolicy: &quot;IfNotPresent&quot;          volumeMounts:            - name: socket-dir              mountPath: /csi        - name: csi-rbdplugin          # for stable functionality replace canary with latest release version          image: quay.io/cephcsi/cephcsi:canary          args:            - &quot;--nodeid=$(NODE_ID)&quot;            - &quot;--type=rbd&quot;            - &quot;--controllerserver=true&quot;            - &quot;--endpoint=$(CSI_ENDPOINT)&quot;            - &quot;--csi-addons-endpoint=$(CSI_ADDONS_ENDPOINT)&quot;            - &quot;--v=5&quot;            - &quot;--drivername=rbd.csi.ceph.com&quot;            - &quot;--pidlimit=-1&quot;            - &quot;--rbdhardmaxclonedepth=8&quot;            - &quot;--rbdsoftmaxclonedepth=4&quot;            - &quot;--enableprofiling=false&quot;            - &quot;--setmetadata=true&quot;          env:            - name: POD_IP              valueFrom:                fieldRef:                  fieldPath: status.podIP            - name: NODE_ID              valueFrom:                fieldRef:                  fieldPath: spec.nodeName            - name: POD_NAMESPACE              valueFrom:                fieldRef:                  fieldPath: metadata.namespace            # - name: KMS_CONFIGMAP_NAME            #   value: encryptionConfig            - name: CSI_ENDPOINT              value: unix:///csi/csi-provisioner.sock            - name: CSI_ADDONS_ENDPOINT              value: unix:///csi/csi-addons.sock          imagePullPolicy: &quot;IfNotPresent&quot;          volumeMounts:            - name: socket-dir              mountPath: /csi            - mountPath: /dev              name: host-dev            - mountPath: /sys              name: host-sys            - mountPath: /lib/modules              name: lib-modules              readOnly: true            - name: ceph-csi-config              mountPath: /etc/ceph-csi-config/            - name: ceph-csi-encryption-kms-config              mountPath: /etc/ceph-csi-encryption-kms-config/            - name: keys-tmp-dir              mountPath: /tmp/csi/keys            - name: ceph-config              mountPath: /etc/ceph/            - name: oidc-token              mountPath: /run/secrets/tokens              readOnly: true        - name: csi-rbdplugin-controller          # for stable functionality replace canary with latest release version          image: quay.io/cephcsi/cephcsi:canary          args:            - &quot;--type=controller&quot;            - &quot;--v=5&quot;            - &quot;--drivername=rbd.csi.ceph.com&quot;            - &quot;--drivernamespace=$(DRIVER_NAMESPACE)&quot;            - &quot;--setmetadata=true&quot;          env:            - name: DRIVER_NAMESPACE              valueFrom:                fieldRef:                  fieldPath: metadata.namespace          imagePullPolicy: &quot;IfNotPresent&quot;          volumeMounts:            - name: ceph-csi-config              mountPath: /etc/ceph-csi-config/            - name: keys-tmp-dir              mountPath: /tmp/csi/keys            - name: ceph-config              mountPath: /etc/ceph/        - name: liveness-prometheus          image: quay.io/cephcsi/cephcsi:canary          args:            - &quot;--type=liveness&quot;            - &quot;--endpoint=$(CSI_ENDPOINT)&quot;            - &quot;--metricsport=8680&quot;            - &quot;--metricspath=/metrics&quot;            - &quot;--polltime=60s&quot;            - &quot;--timeout=3s&quot;          env:            - name: CSI_ENDPOINT              value: unix:///csi/csi-provisioner.sock            - name: POD_IP              valueFrom:                fieldRef:                  fieldPath: status.podIP          volumeMounts:            - name: socket-dir              mountPath: /csi          imagePullPolicy: &quot;IfNotPresent&quot;      volumes:        - name: host-dev          hostPath:            path: /dev        - name: host-sys          hostPath:            path: /sys        - name: lib-modules          hostPath:            path: /lib/modules        - name: socket-dir          emptyDir: &#123;            medium: &quot;Memory&quot;          &#125;        - name: ceph-config          configMap:            name: ceph-config        - name: ceph-csi-config          configMap:            name: ceph-csi-config        - name: ceph-csi-encryption-kms-config          configMap:            name: ceph-csi-encryption-kms-config        - name: keys-tmp-dir          emptyDir: &#123;            medium: &quot;Memory&quot;          &#125;        - name: oidc-token          projected:            sources:              - serviceAccountToken:                  path: oidc-token                  expirationSeconds: 3600                  audience: ceph-csi-kms[root@node-1 volumes]# cat csi-rbdplugin.yaml  #将配置文件中所有需要从registry.k8s.io下载的镜像替换成阿里云---kind: DaemonSetapiVersion: apps/v1metadata:  name: csi-rbdplugin  # replace with non-default namespace name  namespace: defaultspec:  selector:    matchLabels:      app: csi-rbdplugin  template:    metadata:      labels:        app: csi-rbdplugin    spec:      serviceAccountName: rbd-csi-nodeplugin      hostNetwork: true      hostPID: true      priorityClassName: system-node-critical      # to use e.g. Rook orchestrated cluster, and mons&#39; FQDN is      # resolved through k8s service, set dns policy to cluster first      dnsPolicy: ClusterFirstWithHostNet      containers:        - name: driver-registrar          # This is necessary only for systems with SELinux, where          # non-privileged sidecar containers cannot access unix domain socket          # created by privileged CSI driver container.          securityContext:            privileged: true            allowPrivilegeEscalation: true          image: registry.aliyuncs.com/google_containers/csi-node-driver-registrar:v2.6.2          args:            - &quot;--v=1&quot;            - &quot;--csi-address=/csi/csi.sock&quot;            - &quot;--kubelet-registration-path=/var/lib/kubelet/plugins/rbd.csi.ceph.com/csi.sock&quot;          env:            - name: KUBE_NODE_NAME              valueFrom:                fieldRef:                  fieldPath: spec.nodeName          volumeMounts:            - name: socket-dir              mountPath: /csi            - name: registration-dir              mountPath: /registration        - name: csi-rbdplugin          securityContext:            privileged: true            capabilities:              add: [&quot;SYS_ADMIN&quot;]            allowPrivilegeEscalation: true          # for stable functionality replace canary with latest release version          image: quay.io/cephcsi/cephcsi:canary          args:            - &quot;--nodeid=$(NODE_ID)&quot;            - &quot;--pluginpath=/var/lib/kubelet/plugins&quot;            - &quot;--stagingpath=/var/lib/kubelet/plugins/kubernetes.io/csi/&quot;            - &quot;--type=rbd&quot;            - &quot;--nodeserver=true&quot;            - &quot;--endpoint=$(CSI_ENDPOINT)&quot;            - &quot;--csi-addons-endpoint=$(CSI_ADDONS_ENDPOINT)&quot;            - &quot;--v=5&quot;            - &quot;--drivername=rbd.csi.ceph.com&quot;            - &quot;--enableprofiling=false&quot;            # If topology based provisioning is desired, configure required            # node labels representing the nodes topology domain            # and pass the label names below, for CSI to consume and advertise            # its equivalent topology domain            # - &quot;--domainlabels=failure-domain/region,failure-domain/zone&quot;            #            # Options to enable read affinity.            # If enabled Ceph CSI will fetch labels from kubernetes node and            # pass `read_from_replica=localize,crush_location=type:value` during            # rbd map command. refer:            # https://docs.ceph.com/en/latest/man/8/rbd/#kernel-rbd-krbd-options            # for more details.            # - &quot;--enable-read-affinity=true&quot;            # - &quot;--crush-location-labels=topology.io/zone,topology.io/rack&quot;          env:            - name: POD_IP              valueFrom:                fieldRef:                  fieldPath: status.podIP            - name: NODE_ID              valueFrom:                fieldRef:                  fieldPath: spec.nodeName            - name: POD_NAMESPACE              valueFrom:                fieldRef:                  fieldPath: metadata.namespace            # - name: KMS_CONFIGMAP_NAME            #   value: encryptionConfig            - name: CSI_ENDPOINT              value: unix:///csi/csi.sock            - name: CSI_ADDONS_ENDPOINT              value: unix:///csi/csi-addons.sock          imagePullPolicy: &quot;IfNotPresent&quot;          volumeMounts:            - name: socket-dir              mountPath: /csi            - mountPath: /dev              name: host-dev            - mountPath: /sys              name: host-sys            - mountPath: /run/mount              name: host-mount            - mountPath: /etc/selinux              name: etc-selinux              readOnly: true            - mountPath: /lib/modules              name: lib-modules              readOnly: true            - name: ceph-csi-config              mountPath: /etc/ceph-csi-config/            - name: ceph-csi-encryption-kms-config              mountPath: /etc/ceph-csi-encryption-kms-config/            - name: plugin-dir              mountPath: /var/lib/kubelet/plugins              mountPropagation: &quot;Bidirectional&quot;            - name: mountpoint-dir              mountPath: /var/lib/kubelet/pods              mountPropagation: &quot;Bidirectional&quot;            - name: keys-tmp-dir              mountPath: /tmp/csi/keys            - name: ceph-logdir              mountPath: /var/log/ceph            - name: ceph-config              mountPath: /etc/ceph/            - name: oidc-token              mountPath: /run/secrets/tokens              readOnly: true        - name: liveness-prometheus          securityContext:            privileged: true            allowPrivilegeEscalation: true          image: quay.io/cephcsi/cephcsi:canary          args:            - &quot;--type=liveness&quot;            - &quot;--endpoint=$(CSI_ENDPOINT)&quot;            - &quot;--metricsport=8680&quot;            - &quot;--metricspath=/metrics&quot;            - &quot;--polltime=60s&quot;            - &quot;--timeout=3s&quot;          env:            - name: CSI_ENDPOINT              value: unix:///csi/csi.sock            - name: POD_IP              valueFrom:                fieldRef:                  fieldPath: status.podIP          volumeMounts:            - name: socket-dir              mountPath: /csi          imagePullPolicy: &quot;IfNotPresent&quot;      volumes:        - name: socket-dir          hostPath:            path: /var/lib/kubelet/plugins/rbd.csi.ceph.com            type: DirectoryOrCreate        - name: plugin-dir          hostPath:            path: /var/lib/kubelet/plugins            type: Directory        - name: mountpoint-dir          hostPath:            path: /var/lib/kubelet/pods            type: DirectoryOrCreate        - name: ceph-logdir          hostPath:            path: /var/log/ceph            type: DirectoryOrCreate        - name: registration-dir          hostPath:            path: /var/lib/kubelet/plugins_registry/            type: Directory        - name: host-dev          hostPath:            path: /dev        - name: host-sys          hostPath:            path: /sys        - name: etc-selinux          hostPath:            path: /etc/selinux        - name: host-mount          hostPath:            path: /run/mount        - name: lib-modules          hostPath:            path: /lib/modules        - name: ceph-config          configMap:            name: ceph-config        - name: ceph-csi-config          configMap:            name: ceph-csi-config        - name: ceph-csi-encryption-kms-config          configMap:            name: ceph-csi-encryption-kms-config        - name: keys-tmp-dir          emptyDir: &#123;            medium: &quot;Memory&quot;          &#125;        - name: oidc-token          projected:            sources:              - serviceAccountToken:                  path: oidc-token                  expirationSeconds: 3600                  audience: ceph-csi-kms---# This is a service to expose the liveness metricsapiVersion: v1kind: Servicemetadata:  name: csi-metrics-rbdplugin  # replace with non-default namespace name  namespace: default  labels:    app: csi-metricsspec:  ports:    - name: http-metrics      port: 8080      protocol: TCP      targetPort: 8680  selector:    app: csi-rbdplugin[root@node-1 volumes]#kubectl delete -f csi-rbdplugin-provisioner.yaml #删除原来提交的[root@node-1 volumes]#kubectl delete -f csi-rbdplugin.yaml #删除原来提交的[root@node-1 volumes]#kubectl apply -f csi-rbdplugin-provisioner.yaml #重新提交[root@node-1 volumes]#kubectl apply -f csi-rbdplugin.yaml #重新提交[root@node-1 volumes]# kubectl get pods -o wideNAME                                         READY   STATUS    RESTARTS   AGE     IP                NODE     NOMINATED NODE   READINESS GATEScsi-rbdplugin-mzbgp                          3/3     Running   0          6m14s   192.168.199.203   node-3   &lt;none&gt;           &lt;none&gt;csi-rbdplugin-provisioner-79565c5f56-4hsnk   0/7     Pending   0          6m11s   &lt;none&gt;            &lt;none&gt;   &lt;none&gt;           &lt;none&gt;csi-rbdplugin-provisioner-79565c5f56-dmnmv   7/7     Running   0          6m11s   10.244.139.78     node-3   &lt;none&gt;           &lt;none&gt;csi-rbdplugin-provisioner-79565c5f56-j499w   7/7     Running   0          6m11s   10.244.247.36     node-2   &lt;none&gt;           &lt;none&gt;csi-rbdplugin-z7tq8                          3/3     Running   0          6m14s   192.168.199.202   node-2   &lt;none&gt;           &lt;none&gt;nginx-6799fc88d8-jhzz7                       1/1     Running   3          3d16h   10.244.247.24     node-2   &lt;none&gt;           &lt;none&gt;pod-demo                                     1/1     Running   1          21h     10.244.139.71     node-3   &lt;none&gt;           &lt;none&gt;volume-rbd-demo                              1/1     Running   1          3d1h    10.244.247.29     node-2   &lt;none&gt;           &lt;none&gt;</code></pre><blockquote><p>查看创建过程中发现仍然无法成功创建，由于我当前k8s环境只有三个机器，一个master，两个node，所以第三个csi创建的时候不能部署到master节点，一直报错，这属于k8s的污染，将配置重新修改后即可</p></blockquote><pre><code>[root@node-1 volumes]# kubectl describe pods csi-rbdplugin-provisioner-79565c5f56-4hsnkEvents:  Type     Reason            Age    From  Message  ----     ------            ----   ----  -------  Warning  FailedScheduling  6m28s        0/3 nodes are available: 1 node(s) had taint &#123;node-role.kubernetes.io/master: &#125;, that the pod didn&#39;t tolerate, 2 node(s) didn&#39;t match pod affinity/anti-affinity, 2 node(s) didn&#39;t match pod anti-affinity rules.  Warning  FailedScheduling  6m27s        0/3 nodes are available: 1 node(s) had taint &#123;node-role.kubernetes.io/master: &#125;, that the pod didn&#39;t tolerate, 2 node(s) didn&#39;t match pod affinity/anti-affinity, 2 node(s) didn&#39;t match pod anti-affinity rules. #####这个方法没试，可以试试 在ceph-csi/deploy/rbd/kubernetes/csi-rbdplugin.yaml中的DaemonSet的spec中添加如下配置：  tolerations:   - key: node-role.kubernetes.io/master     effect: NoSchedule#####     [root@node-1 volumes]# kubectl describe nodes node-1 |grep -E &#39;(Roles|Taints)&#39; #查看默认设置了master节点不允许部署pod，将该配置取消即可Roles:              masterTaints:             node-role.kubernetes.io/master:NoSchedule[root@node-1 volumes]# kubectl taint node node-1 node-role.kubernetes.io/master- #删除标记，使Master节点允许部署podnode/node-1 untainted# 后续可这两个个命令重新标记[root@node-1 volumes]# kubectl taint nodes $node_name node-role.kubernetes.io/master=:NoSchedule #重新添加标记，使Master节点不允许部署pod[root@node-1 volumes]# kubectl taint nodes $node_name node-role.kubernetes.io/control-plane=:NoSchedule #重新添加标记，使Master节点不允许部署pod[root@node-1 volumes]# kubectl describe nodes node-1 |grep -E &#39;(Roles|Taints)&#39; #查看标记已经被清除Roles:              masterTaints:             &lt;none&gt;[root@node-1 volumes]# kubectl get pods #查看状态NAME                                         READY   STATUS    RESTARTS   AGEcsi-rbdplugin-2bbdf                          3/3     Running   0          7m35scsi-rbdplugin-ghb7k                          3/3     Running   0          7m35scsi-rbdplugin-provisioner-79565c5f56-clkbd   7/7     Running   0          7m44scsi-rbdplugin-provisioner-79565c5f56-mdggw   7/7     Running   0          7m44scsi-rbdplugin-provisioner-79565c5f56-zvxpx   7/7     Running   0          7m44scsi-rbdplugin-z27pz                          3/3     Running   0          7m35snginx-6799fc88d8-jhzz7                       1/1     Running   3          3d16hpod-demo                                     1/1     Running   1          21hvolume-rbd-demo                              1/1     Running   1          3d2h[root@node-1 volumes]# kubectl get deployments.apps  #查看三个机器的csi全部正常运行NAME                        READY   UP-TO-DATE   AVAILABLE   AGEcsi-rbdplugin-provisioner   3/3     3            3           8m13snginx                       1/1     1            1           3d16h</code></pre><h1><span id="6-使用-ceph-块设备">6、使用 CEPH 块设备</span></h1><h2><span id="1-创建存储类">1、创建存储类</span></h2><blockquote><p>Kubernetes StorageClass定义了一类存储。可以创建多个StorageClass 对象以映射到不同的服务质量级别（即 NVMe 与基于 HDD 的池）和功能。<br>例如，要创建映射到上面创建的kubernetes池的ceph-csi StorageClass ，在确保“clusterID”属性与您的 Ceph 集群的fsid 匹配后，可以使用以下 YAML 文件：</p></blockquote><pre><code>[root@node-1 volumes]#cat &lt;&lt;EOF &gt; csi-rbd-sc.yaml---apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:   name: csi-rbd-scprovisioner: rbd.csi.ceph.comparameters:   clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8 #ceph的集群ID   pool: kubernetes #ceph的存储池   imageFeatures: layering   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret #指定使用的key   csi.storage.k8s.io/provisioner-secret-namespace: default   csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret   csi.storage.k8s.io/controller-expand-secret-namespace: default   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret   csi.storage.k8s.io/node-stage-secret-namespace: defaultreclaimPolicy: Delete #回收的策略allowVolumeExpansion: truemountOptions:   - discardEOF[root@node-1 volumes]# vim csi-rbd-sc.yaml #编辑配置文件   clusterID: b8e58b30-4568-4032-a9f4-837ed3fa9529 #将此ID更换成本地Ceph的集群ID       [root@node-1 volumes]# kubectl get secrets  #查看上述配置文件中指定的keyNAME                              TYPE                                  DATA   AGEceph-secret                       kubernetes.io/rbd                     1      3d4hcsi-rbd-secret                    Opaque                                2      3h36mdefault-token-p5dsd               kubernetes.io/service-account-token   3      3d17hrbd-csi-nodeplugin-token-4fvjf    kubernetes.io/service-account-token   3      3h21mrbd-csi-provisioner-token-kpbsw   kubernetes.io/service-account-token   3      3h21m[root@node-1 volumes]# kubectl get secrets csi-rbd-secret -o yaml #查看key的具体信息，就是上述创建的apiVersion: v1data:  userID: a3ViZXJuZXRlcw==  userKey: QVFBRVFSRmtyWVNyQnhBQVVOWlJUNkJ4TFMwakh6SXpNUWc0Q1E9PQ==kind: Secretmetadata:  annotations:    kubectl.kubernetes.io/last-applied-configuration: |      &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Secret&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;csi-rbd-secret&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;stringData&quot;:&#123;&quot;userID&quot;:&quot;kubernetes&quot;,&quot;userKey&quot;:&quot;AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ==&quot;&#125;&#125;  creationTimestamp: &quot;2023-03-18T04:38:20Z&quot;  managedFields:  - apiVersion: v1    fieldsType: FieldsV1    fieldsV1:      f:data:        .: &#123;&#125;        f:userID: &#123;&#125;        f:userKey: &#123;&#125;      f:metadata:        f:annotations:          .: &#123;&#125;          f:kubectl.kubernetes.io/last-applied-configuration: &#123;&#125;      f:type: &#123;&#125;    manager: kubectl-client-side-apply    operation: Update    time: &quot;2023-03-18T04:38:20Z&quot;  name: csi-rbd-secret  namespace: default  resourceVersion: &quot;593749&quot;  selfLink: /api/v1/namespaces/default/secrets/csi-rbd-secret  uid: 712d89bf-d3b2-44e5-bf91-973f76b15830type: Opaque[root@node-1 volumes]#  kubectl apply -f csi-rbd-sc.yaml  #提交storageclass.storage.k8s.io/csi-rbd-sc created [root@node-1 volumes]# kubectl get sc #查看状态，sc是StorageClass的简写NAME         PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGEcsi-rbd-sc   rbd.csi.ceph.com   Delete          Immediate           true                   31s</code></pre><blockquote><p>请注意，在 Kubernetes v1.14 和 v1.15 中，卷扩展功能处于 alpha 状态，需要启用ExpandCSIVolumes功能门。</p></blockquote><h2><span id="2-创建persistentvolumeclaim使用pv创建">2、创建PERSISTENTVOLUMECLAIM（使用PV创建）</span></h2><blockquote><p>PersistentVolumeClaim是用户对抽象存储资源的请求。然后， PersistentVolumeClaim将关联到Pod资源以提供一个PersistentVolume ，该PersistentVolume将由 Ceph 块镜像支持。可以包含一个可选的volumeMode以在已安装的文件系统（默认）或基于原始块设备的卷之间进行选择。<br>使用ceph-csi ，为volumeMode指定Filesystem可以同时支持 ReadWriteOnce和ReadOnlyMany accessMode声明， 为volumeMode指定Block可以支持ReadWriteOnce，ReadWriteMany和 ReadOnlyMany accessMode声明。</p></blockquote><blockquote><p>例如，要创建一个基于块的PersistentVolumeClaim，它利用上面创建的基于ceph-csi的StorageClass，可以使用以下 YAML 从csi-rbd-sc StorageClass请求原始块存储：</p></blockquote><h3><span id="1-官方文档">1、官方文档</span></h3><pre><code>$ cat &lt;&lt;EOF &gt; raw-block-pvc.yaml---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: raw-block-pvcspec:  accessModes:    - ReadWriteOnce  volumeMode: Block  resources:    requests:      storage: 1Gi  storageClassName: csi-rbd-scEOF$ kubectl apply -f raw-block-pvc.yaml</code></pre><blockquote><p>以下演示和示例将上述 PersistentVolumeClaim绑定到Pod资源作为原始块设备：</p></blockquote><pre><code>$ cat &lt;&lt;EOF &gt; raw-block-pod.yaml---apiVersion: v1kind: Podmetadata:  name: pod-with-raw-block-volumespec:  containers:    - name: fc-container      image: fedora:26      command: [&quot;/bin/sh&quot;, &quot;-c&quot;]      args: [&quot;tail -f /dev/null&quot;]      volumeDevices:        - name: data          devicePath: /dev/xvda  volumes:    - name: data      persistentVolumeClaim:        claimName: raw-block-pvcEOF$ kubectl apply -f raw-block-pod.yaml</code></pre><blockquote><p>要使用上面创建 的基于ceph-csi的StorageClass创建基于文件系统的PersistentVolumeClaim，可以使用以下 YAML 从 csi -rbd-sc StorageClass请求挂载的文件系统（由 RBD 映像支持） ：</p></blockquote><pre><code>$ cat &lt;&lt;EOF &gt; pvc.yaml---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: rbd-pvcspec:  accessModes:    - ReadWriteOnce  volumeMode: Filesystem  resources:    requests:      storage: 1Gi  storageClassName: csi-rbd-scEOF$ kubectl apply -f pvc.yaml</code></pre><blockquote><p>下面演示和示例将上述 PersistentVolumeClaim绑定到Pod资源作为挂载的文件系统：</p></blockquote><pre><code>$ cat &lt;&lt;EOF &gt; pod.yaml---apiVersion: v1kind: Podmetadata:  name: csi-rbd-demo-podspec:  containers:    - name: web-server      image: nginx      volumeMounts:        - name: mypvc          mountPath: /var/lib/www/html  volumes:    - name: mypvc      persistentVolumeClaim:        claimName: rbd-pvc        readOnly: falseEOF$ kubectl apply -f pod.yaml</code></pre><h3><span id="2-实操">2、实操</span></h3><blockquote><p>使用文件系统级别去声明，BLOCK方式测试创建成功，但是不明白啥意思，后续可以查找一下相关文档</p></blockquote><pre><code>[root@node-1 volumes]#cat &lt;&lt;EOF &gt; pvc.yaml #创建文件，申请1G的空间---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: rbd-pvc #名称spec:  accessModes:    - ReadWriteOnce #一个客户端对它进行读写  volumeMode: Filesystem #类型是文件系统  resources:    requests:      storage: 1Gi #申请的空间是1G  storageClassName: csi-rbd-sc #SC的名称EOF[root@node-1 volumes]# kubectl apply -f pvc.yaml  #提交申请persistentvolumeclaim/rbd-pvc created[root@node-1 volumes]# kubectl get pvc #查看自动创建的PVCNAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGEpvc-demo   Bound    rbd-demo                                   10G        RWO            rbd            3d2hrbd-pvc    Bound    pvc-986ef488-0f94-4e3f-b963-3990e3bf0000   1Gi        RWO            csi-rbd-sc     22s[root@node-1 volumes]# kubectl get pv  #查看PVC创建的PVNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   REASON   AGEpvc-986ef488-0f94-4e3f-b963-3990e3bf0000   1Gi        RWO            Delete           Bound    default/rbd-pvc    csi-rbd-sc              72srbd-demo                                   10G        RWO            Retain           Bound    default/pvc-demo   rbd                     3d2h[root@node-1 volumes]# rbd ls kubernetes  #查看ceph存储中的设备csi-vol-9141d3e3-1b67-4ef0-b29f-c93e69254a51demo-1.imgrbd.img[root@node-1 volumes]# rbd info kubernetes/csi-vol-9141d3e3-1b67-4ef0-b29f-c93e69254a51 #查看大小为1Grbd image &#39;csi-vol-9141d3e3-1b67-4ef0-b29f-c93e69254a51&#39;:    size 1 GiB in 256 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 4f248f369f6cc    block_name_prefix: rbd_data.4f248f369f6cc    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar 18 16:47:54 2023    access_timestamp: Sat Mar 18 16:47:54 2023    modify_timestamp: Sat Mar 18 16:47:54 2023</code></pre><blockquote><p>如果创建不成功可能就是版本的问题，创建的Kubernetes用户无法接入到ceph中，提示权限错误，这里我没有遇到，如果遇到了可以编辑下面的配置文件，将Kubernetes的用户信息替换成ceph的admin用户，并重新提交即可</p></blockquote><p><img src="/images/Ceph/Ceph%E4%B8%8Ekubernetes%E9%9B%86%E6%88%90/2.jpg"></p><h3><span id="3-容器使用">3、容器使用</span></h3><p>下面演示和示例将上述 PersistentVolumeClaim绑定到Pod资源作为挂载的文件系统：</p><pre><code>[root@node-1 volumes]#cat &lt;&lt;EOF &gt; pod.yaml---apiVersion: v1kind: Podmetadata:  name: csi-rbd-demo-podspec:  containers:    - name: web-server      image: nginx      volumeMounts:        - name: mypvc          mountPath: /var/lib/www/html #设置挂载的地方  volumes:    - name: mypvc      persistentVolumeClaim:        claimName: rbd-pvc #调用创建的PVC        readOnly: falseEOF    [root@node-1 volumes]# kubectl apply -f pod.yaml[root@node-1 volumes]# kubectl exec -ti csi-rbd-demo-pod bashkubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.root@csi-rbd-demo-pod:/# root@csi-rbd-demo-pod:/# root@csi-rbd-demo-pod:/# df -HTFilesystem              Type     Size  Used Avail Use% Mounted onoverlay                 overlay   19G  6.2G   13G  34% /tmpfs                   tmpfs     68M     0   68M   0% /devtmpfs                   tmpfs    954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs       19G  6.2G   13G  34% /etc/hostsshm                     tmpfs     68M     0   68M   0% /dev/shm/dev/rbd1               ext4     1.1G  2.7M  1.1G   1% /var/lib/www/htmltmpfs                   tmpfs    954M   13k  954M   1% /run/secrets/kubernetes.io/serviceaccounttmpfs                   tmpfs    954M     0  954M   0% /proc/acpitmpfs                   tmpfs    954M     0  954M   0% /proc/scsitmpfs                   tmpfs    954M     0  954M   0% /sys/firmware</code></pre><blockquote><p>如果无法创建容器，看到是因为无法挂载导致的，可能是因为下面的配置没有定义好，需要重新修改并上传</p></blockquote><pre><code>[root@node-1 volumes]# cat csi-rbd-sc.yaml ---apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:   name: csi-rbd-scprovisioner: rbd.csi.ceph.comparameters:   clusterID: b8e58b30-4568-4032-a9f4-837ed3fa9529   pool: kubernetes   imageFeatures: layering #这个地方没有定义的话创建容器的时候无法正常挂载PVC   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret   csi.storage.k8s.io/provisioner-secret-namespace: default   csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret   csi.storage.k8s.io/controller-expand-secret-namespace: default   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret   csi.storage.k8s.io/node-stage-secret-namespace: defaultreclaimPolicy: DeleteallowVolumeExpansion: truemountOptions:   - discard</code></pre><h1><span id="六-statefulset">六、StatefulSet</span></h1><p><a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/statefulset/">参考文档</a></p><pre><code>[root@node-1 volumes]# kubectl get sc #获取SC的名称NAME         PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGEcsi-rbd-sc   rbd.csi.ceph.com   Delete          Immediate           true                   57m[root@node-1 volumes]# vim nginx.yaml  #创建nginx.yaml的文件apiVersion: v1kind: Servicemetadata:  name: nginx  labels:    app: nginxspec:  ports:  - port: 80    name: web  type: NodePort  #clusterIP: None #这个参数报错，需要注释掉，增加上面的type: NodePort  selector:    app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata:  name: webspec:  selector:    matchLabels:      app: nginx  serviceName: &quot;nginx&quot; # 必须匹配 .spec.template.metadata.labels  replicas: 3 # 默认值是 1  minReadySeconds: 10 # 默认值是 0 ，最好把这个命令去掉，要不会无法提交  template:    metadata:      labels:        app: nginx    spec:      terminationGracePeriodSeconds: 10      containers:      - name: nginx        image: registry.aliyuncs.com/google_containers/nginx-slim:0.8 #需要将镜像地址替换成阿里云的        ports:        - containerPort: 80          name: web        volumeMounts:        - name: www          mountPath: /usr/share/nginx/html  volumeClaimTemplates: #声明使用StorageClass存储  - metadata:      name: www    spec:      accessModes: [ &quot;ReadWriteOnce&quot; ]      storageClassName: &quot;csi-rbd-sc&quot; #需要将sc 的名称修改成上述创建的SC名称      resources:        requests:          storage: 1Gi[root@node-1 volumes]# kubectl get stsNAME   READY   AGEweb    3/3     17m[root@node-1 volumes]# kubectl get podsNAME                                         READY   STATUS    RESTARTS   AGEcsi-rbd-demo-pod                             1/1     Running   0          49mcsi-rbdplugin-2bbdf                          3/3     Running   0          149mcsi-rbdplugin-ghb7k                          3/3     Running   0          149mcsi-rbdplugin-provisioner-79565c5f56-clkbd   7/7     Running   0          149mcsi-rbdplugin-provisioner-79565c5f56-mdggw   7/7     Running   0          149mcsi-rbdplugin-provisioner-79565c5f56-zvxpx   7/7     Running   0          149mcsi-rbdplugin-z27pz                          3/3     Running   0          149mnginx-6799fc88d8-4f4kw                       1/1     Running   0          29mpod-demo                                     1/1     Running   1          24hvolume-rbd-demo                              1/1     Running   1          3d4hweb-0                                        1/1     Running   0          6m30sweb-1                                        1/1     Running   0          6m15sweb-2                                        1/1     Running   0          5m53s[root@node-1 volumes]# kubectl exec -ti web-0 bashkubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.root@web-0:/# df -HT Filesystem              Type     Size  Used Avail Use% Mounted onoverlay                 overlay   19G  6.6G   12G  36% /tmpfs                   tmpfs     68M     0   68M   0% /devtmpfs                   tmpfs    2.0G     0  2.0G   0% /sys/fs/cgroup/dev/mapper/centos-root xfs       19G  6.6G   12G  36% /etc/hostsshm                     tmpfs     68M     0   68M   0% /dev/shm/dev/rbd1               ext4     1.1G  2.7M  1.1G   1% /usr/share/nginx/htmltmpfs                   tmpfs    2.0G   13k  2.0G   1% /run/secrets/kubernetes.io/serviceaccounttmpfs                   tmpfs    2.0G     0  2.0G   0% /proc/acpitmpfs                   tmpfs    2.0G     0  2.0G   0% /proc/scsitmpfs                   tmpfs    2.0G     0  2.0G   0% /sys/firmware[root@node-2 ~]# rbd device list id pool       namespace image                                        snap device    0  kubernetes           rbd.img                                      -    /dev/rbd0 1  kubernetes           csi-vol-d8c2ae42-d57b-4312-88d6-dcfd769c9b11 -    /dev/rbd1 [root@node-1 volumes]# kubectl get pvcNAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGEpvc-demo    Bound    rbd-demo                                   10G        RWO            rbd            3d3hrbd-pvc     Bound    pvc-986ef488-0f94-4e3f-b963-3990e3bf0000   1Gi        RWO            csi-rbd-sc     64mwww-web-0   Bound    pvc-d542bca8-04b3-4525-acd3-60aa9d39d8f3   1Gi        RWO            csi-rbd-sc     19mwww-web-1   Bound    pvc-8dbc24e4-bede-44e5-9efa-eae32d55f7e2   1Gi        RWO            csi-rbd-sc     7m54swww-web-2   Bound    pvc-c63540da-d4f4-4c05-8db4-2636ca290dac   1Gi        RWO            csi-rbd-sc     7m32s[root@node-1 volumes]# rbd ls kubernetescsi-vol-50dfd42d-4e69-4cfa-8dd3-627bcd80f287csi-vol-9141d3e3-1b67-4ef0-b29f-c93e69254a51csi-vol-9f73997b-40b7-498c-8ae5-6e633ea7e192csi-vol-d8c2ae42-d57b-4312-88d6-dcfd769c9b11demo-1.imgrbd.img[root@node-1 volumes]#  kubectl get pv NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS   REASON   AGEpvc-8dbc24e4-bede-44e5-9efa-eae32d55f7e2   1Gi        RWO            Delete           Bound    default/www-web-1   csi-rbd-sc              8m59spvc-986ef488-0f94-4e3f-b963-3990e3bf0000   1Gi        RWO            Delete           Bound    default/rbd-pvc     csi-rbd-sc              66mpvc-c63540da-d4f4-4c05-8db4-2636ca290dac   1Gi        RWO            Delete           Bound    default/www-web-2   csi-rbd-sc              8m37spvc-d542bca8-04b3-4525-acd3-60aa9d39d8f3   1Gi        RWO            Delete           Bound    default/www-web-0   csi-rbd-sc              20mrbd-demo                                   10G        RWO            Retain           Bound    default/pvc-demo    rbd                     3d3h</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph与KVM集成</title>
      <link href="/2023/04/20/Ceph/14.Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/"/>
      <url>/2023/04/20/Ceph/14.Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-%E6%9F%A5%E7%9C%8B%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%98%AF%E5%90%A6%E6%94%AF%E6%8C%81%E8%99%9A%E6%8B%9F%E5%8C%96">一、查看虚拟机是否支持虚拟化</a></li><li><a href="#%E4%BA%8C-%E5%AE%89%E8%A3%85kvm">二、安装KVM</a></li><li><a href="#%E4%B8%89-%E9%80%9A%E8%BF%87kvm%E5%AE%89%E8%A3%85%E8%99%9A%E6%8B%9F%E6%9C%BA">三、通过KVM安装虚拟机</a></li><li><a href="#%E5%9B%9B-qemu%E5%AF%B9%E6%8E%A5rbd%E5%AD%98%E5%82%A8%E5%9D%97">四、QEMU对接RBD存储块</a><ul><li><a href="#%E4%BA%94-libvirt%E5%AF%B9%E6%8E%A5kvm%E8%AF%A6%E8%A7%A3">五、Libvirt对接KVM详解</a></li><li><a href="#51-%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%B1%A0-%E4%BB%A5%E4%B8%8B%E7%A4%BA%E4%BE%8B%E4%BD%BF%E7%94%A8%E6%B1%A0%E5%90%8D%E7%A7%B0libvirt-pool">5.1、创建一个池。以下示例使用池名称libvirt-pool.：</a></li><li><a href="#52-%E4%BD%BF%E7%94%A8rbd%E5%B7%A5%E5%85%B7%E5%88%9D%E5%A7%8B%E5%8C%96%E6%B1%A0%E4%BB%A5%E4%BE%9B-rbd-%E4%BD%BF%E7%94%A8">5.2、使用rbd工具初始化池以供 RBD 使用：</a></li><li><a href="#53-%E5%88%9B%E5%BB%BA-ceph-%E7%94%A8%E6%88%B7%E6%88%96%E7%94%A8%E4%BA%8Eclientadmin097-%E5%8F%8A%E6%9B%B4%E6%97%A9%E7%89%88%E6%9C%AC-%E4%BB%A5%E4%B8%8B%E7%A4%BA%E4%BE%8B%E4%BD%BF%E7%94%A8-ceph-%E7%94%A8%E6%88%B7%E5%90%8Dclientlibvirt-%E5%92%8C%E5%BC%95%E7%94%A8libvirt-pool">5.3、创建 Ceph 用户（或用于client.admin0.9.7 及更早版本）。以下示例使用 Ceph 用户名client.libvirt 和引用libvirt-pool。</a></li><li><a href="#54-%E4%BD%BF%E7%94%A8-qemu%E5%9C%A8-rbd-%E6%B1%A0%E4%B8%AD%E5%88%9B%E5%BB%BA%E6%98%A0%E5%83%8F-%E4%BB%A5%E4%B8%8B%E7%A4%BA%E4%BE%8B%E4%BD%BF%E7%94%A8%E5%9B%BE%E5%83%8F%E5%90%8D%E7%A7%B0new-libvirt-image-%E5%92%8C%E5%BC%95%E7%94%A8libvirt-pool">5.4、使用 QEMU在 RBD 池中创建映像。以下示例使用图像名称new-libvirt-image 和引用libvirt-pool。</a></li><li><a href="#55-%E9%85%8D%E7%BD%AE%E8%99%9A%E6%8B%9F%E6%9C%BA">5.5、配置虚拟机</a><ul><li><a href="#551-%E4%BD%BF%E7%94%A8-virsh-edit%E6%89%93%E5%BC%80%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">5.5.1、使用 virsh edit打开配置文件。</a></li><li><a href="#552-%E5%A6%82%E6%9E%9C%E6%82%A8%E7%9A%84-ceph-%E5%AD%98%E5%82%A8%E9%9B%86%E7%BE%A4%E5%90%AF%E7%94%A8%E4%BA%86ceph-%E8%BA%AB%E4%BB%BD%E9%AA%8C%E8%AF%81%E9%BB%98%E8%AE%A4%E6%83%85%E5%86%B5%E4%B8%8B%E5%90%AF%E7%94%A8%E6%82%A8%E5%BF%85%E9%A1%BB%E7%94%9F%E6%88%90%E4%B8%80%E4%B8%AA%E5%AF%86%E9%92%A5">5.5.2、如果您的 Ceph 存储集群启用了Ceph 身份验证（默认情况下启用），您必须生成一个密钥。</a></li><li><a href="#553-%E5%AE%9A%E4%B9%89%E5%AF%86%E9%92%A5">5.5.3、定义密钥。</a></li><li><a href="#554-%E8%8E%B7%E5%8F%96clientlibvirt%E5%AF%86%E9%92%A5%E5%B9%B6%E5%B0%86%E5%AF%86%E9%92%A5%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%BF%9D%E5%AD%98%E5%88%B0%E6%96%87%E4%BB%B6%E4%B8%AD">5.5.4、获取client.libvirt密钥并将密钥字符串保存到文件中。</a></li><li><a href="#555-%E8%AE%BE%E7%BD%AE%E5%AF%86%E9%92%A5%E7%9A%84-uuid">5.5.5、设置密钥的 UUID。</a></li><li><a href="#556-%E4%BF%AE%E6%94%B9%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%96%87%E4%BB%B6%E5%A2%9E%E5%8A%A0%E8%AE%A4%E8%AF%81">5.5.6、修改虚拟机文件，增加认证</a></li></ul></li></ul></li><li><a href="#%E5%85%AD-kvm%E5%AF%B9%E6%8E%A5%E6%B5%8B%E8%AF%95%E5%8F%8A%E6%89%A9%E5%AE%B9">六、KVM对接测试及扩容</a><ul><li><a href="#61-%E5%AF%B9%E6%8E%A5%E6%B5%8B%E8%AF%95">6.1、对接测试</a></li><li><a href="#62-%E6%89%A9%E5%AE%B9%E5%A4%A7%E5%B0%8F">6.2、扩容大小</a></li></ul></li><li><a href="#%E4%B8%83-%E7%B3%BB%E7%BB%9F%E7%9B%98%E5%AD%98%E5%82%A8%E8%87%B3ceph%E4%B9%9F%E5%8F%AF%E7%94%A8%E4%BA%8E%E8%BF%81%E7%A7%BB%E5%88%B0ceph%E9%9B%86%E7%BE%A4%E4%B8%AD">七、系统盘存储至Ceph（也可用于迁移到ceph集群中）</a><ul><li><a href="#71-%E4%B8%BB%E6%9C%BA%E5%81%9C%E6%9C%BA">7.1、主机停机</a></li><li><a href="#72-%E8%BD%AC%E6%8D%A2%E6%A0%BC%E5%BC%8F%E5%AD%98%E6%94%BE%E5%9C%A8ceph%E9%9B%86%E7%BE%A4%E4%B8%AD">7.2、转换格式存放在ceph集群中</a></li><li><a href="#73-%E4%BF%AE%E6%94%B9%E4%B8%BB%E6%9C%BAxml%E6%96%87%E4%BB%B6">7.3、修改主机xml文件</a></li></ul></li><li><a href="#%E5%85%AB-%E6%8F%AD%E7%A7%98%E4%BA%91%E4%B8%BB%E6%9C%BA%E7%A7%92%E7%BA%A7%E9%83%A8%E7%BD%B2%E5%A5%A5%E7%A7%98">八、揭秘云主机秒级部署奥秘</a><ul><li><a href="#81-%E5%88%9B%E5%BB%BA%E5%BF%AB%E7%85%A7">8.1、创建快照</a></li><li><a href="#82-%E5%AF%B9%E5%BF%AB%E7%85%A7%E5%A2%9E%E5%8A%A0%E4%BF%9D%E6%8A%A4">8.2、对快照增加保护</a></li><li><a href="#83-%E6%9F%A5%E7%9C%8B%E5%BF%AB%E7%85%A7%E4%BF%9D%E6%8A%A4">8.3、查看快照保护</a></li><li><a href="#84-%E5%85%8B%E9%9A%86%E9%95%9C%E5%83%8F">8.4、克隆镜像</a></li><li><a href="#85-%E6%9F%A5%E7%9C%8B%E5%85%8B%E9%9A%86%E9%95%9C%E5%83%8F">8.5、查看克隆镜像</a></li><li><a href="#86-%E9%85%8D%E7%BD%AE%E8%99%9A%E6%8B%9F%E6%9C%BAxml%E6%96%87%E4%BB%B6">8.6、配置虚拟机xml文件</a></li><li><a href="#87-%E5%BC%95%E5%85%A5xml%E6%96%87%E4%BB%B6">8.7、引入XML文件</a></li><li><a href="#88-%E5%BC%80%E6%9C%BA%E5%B9%B6%E6%B5%8B%E8%AF%95">8.8、开机并测试</a></li></ul></li></ul><!-- tocstop --><h1><span id="一-查看虚拟机是否支持虚拟化">一、查看虚拟机是否支持虚拟化</span></h1><p><a href="https://www.cnblogs.com/huangyanqi/p/8591586.html">KVM参考文档</a><br><a href="https://docs.ceph.com/en/octopus/rbd/qemu-rbd/">Ceph与KVM集成官方文档-qemu</a><br><a href="https://docs.ceph.com/en/octopus/rbd/libvirt/">Ceph与KVM集成官方文档-libvirt</a></p><pre><code>[root@node-1 ~]# lscpu | grep vmx #查看并没有开启嵌套虚拟化，需要在VMware中开启[root@node-1 ~]#  poweroff #关机</code></pre><p><img src="/images/Ceph/Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/1.jpg"></p><h1><span id="二-安装kvm">二、安装KVM</span></h1><pre><code>[root@node-1 ~]# yum groupinstall &quot;Virtualization Host&quot; -y启动服务：[root@node-1 ~]# systemctl start libvirtd &amp;&amp; systemctl enable libvirtd查看KVM状态：[root@node-1 ~]# virsh list --all #安装成功 Id    名称                         状态----------------------------------------------------[root@node-1 ~]# yum install virt-manager -y #安装图形化管理工具，适合没有KVM基础的人[root@node-1 ~]#  yum install dejavu-lgc-sans-fonts #如果图形化工具打开乱码可以尝试安装这个包，如果还不行，就修改系统的语言为英文</code></pre><h1><span id="三-通过kvm安装虚拟机">三、通过KVM安装虚拟机</span></h1><pre><code>[root@node-1 ~]# qemu-img create -f qcow2 /var/lib/libvirt/images/demo.img 4G #在images文件夹下创建一个4G的块设备，叫demo.imgFormatting &#39;/var/lib/libvirt/images/demo.img&#39;, fmt=qcow size=4294967296 encryption=off [root@node-1 ~]# file /var/lib/libvirt/images/demo.img #查看文件类型/var/lib/libvirt/images/demo.img: QEMU QCOW Image (v1), 4294967296 bytes[root@node-1 ~]# qemu-img info /var/lib/libvirt/images/demo.img #查看创建信息image: /var/lib/libvirt/images/demo.imgfile format: qcow2virtual size: 4.0G (4294967296 bytes)disk size: 20Kcluster_size: 4096[root@node-1 ~]# yum install libguestfs-tools virt-install.noarch lrzsz -y[root@node-1 ~]# virt-install --helpusage: virt-install --name NAME --memory MB STORAGE INSTALL [options]从指定安装源创建新虚拟机。optional arguments:  -h, --help            show this help message and exit  --version             show program&#39;s version number and exit  --connect URI         通过 libvirt URI 连接到虚拟机管理程序通用选项:  -n NAME, --name NAME  客户机实例名称  --memory MEMORY       Configure guest memory allocation. Ex:                        --memory 1024 (in MiB)                        --memory 512,maxmemory=1024                        --memory 512,maxmemory=1024,hotplugmemorymax=2048,hotplugmemoryslots=2  --vcpus VCPUS         Number of vcpus to configure for your guest. Ex:                        --vcpus 5                        --vcpus 5,maxvcpus=10,cpuset=1-4,6,8                        --vcpus sockets=2,cores=4,threads=2  --cpu CPU             CPU model and features. Ex:                        --cpu coreduo,+x2apic                        --cpu host-passthrough                        --cpu host  --metadata METADATA   配置客户机元数据。例如：                        --metadata name=foo,title=&quot;My pretty title&quot;,uuid=...                        --metadata description=&quot;My nice long description&quot;安装方法选项:  --cdrom CDROM         光驱安装介质  -l LOCATION, --location LOCATION                        安装源 (例如：nfs:host:/path, http://host/path,                        ftp://host/path)  --pxe                 使用 PXE 协议从网络引导  --import              在已有的磁盘镜像中构建客户机  --livecd              将光驱介质视为 Live CD  -x EXTRA_ARGS, --extra-args EXTRA_ARGS                        将附加参数添加到由 --location                        引导的内核中  --initrd-inject INITRD_INJECT                        添加指定文件到由 --location 指定的 initrd                        根中  --os-variant DISTRO_VARIANT                        在客户机上安装的操作系统，例如：&#39;fedor                        a18&#39;、&#39;rhel6&#39;、&#39;winxp&#39; 等。  --boot BOOT           配置客户机引导设置。例如：                        --boot hd,cdrom,menu=on                        --boot init=/sbin/init (针对容器)  --idmap IDMAP         为 LXC 容器启用用户名称空间。例如：                        --idmap uid_start=0,uid_target=1000,uid_count=10设备选项:  --disk DISK           指定存储的各种选项。例如：                        --disk size=10 (在默认位置创建 10GiB 镜像)                        --disk /my/existing/disk,cache=none                        --disk device=cdrom,bus=scsi                        --disk=?  -w NETWORK, --network NETWORK                        配置客户机网络接口。例如：                        --network bridge=mybr0                        --network network=my_libvirt_virtual_net                        --network network=mynet,model=virtio,mac=00:11...                        --network none                        --network help  --graphics GRAPHICS   配置客户机显示设置。例如：                        --graphics vnc                        --graphics spice,port=5901,tlsport=5902                        --graphics none                        --graphics vnc,password=foobar,port=5910,keymap=ja  --controller CONTROLLER                        配置客户机控制器设备。例如：                        --controller type=usb,model=ich9-ehci1  --input INPUT         配置客户机输入设备。例如：                        --input tablet                        --input keyboard,bus=usb  --serial SERIAL       配置客户机串口设备  --parallel PARALLEL   配置客户机并口设备  --channel CHANNEL     配置客户机通信通道  --console CONSOLE     配置文本控制台连接主机与客户机  --hostdev HOSTDEV     配置物理 USB/PCI 等主机设备与客户机共享  --filesystem FILESYSTEM                        传递主机目录到客户机。例如：                        --filesystem /my/source/dir,/dir/in/guest                        --filesystem template_name,/,type=template  --sound [SOUND]       配置客户机声音设备仿真  --watchdog WATCHDOG   配置客户机 watchdog 设备  --video VIDEO         配置客户机视频硬件。  --smartcard SMARTCARD                        配置客户机智能卡设备。例如：                        --smartcard mode=passthrough  --redirdev REDIRDEV   配置客户机重定向设备。例如：                        --redirdev usb,type=tcp,server=192.168.1.1:4000  --memballoon MEMBALLOON                        配置客户机 memballoon 设备。例如：                        --memballoon model=virtio  --tpm TPM             配置客户机 TPM 设备。例如：                        --tpm /dev/tpm  --rng RNG             Configure a guest RNG device. Ex:                        --rng /dev/urandom  --panic PANIC         配置客户机 panic 设备。例如：                        --panic default  --memdev MEMDEV       Configure a guest memory device. Ex:                        --memdev dimm,target_size=1024客户机配置选项:  --security SECURITY   设置域安全驱动配置。  --cputune CPUTUNE     Tune CPU parameters for the domain process.  --numatune NUMATUNE   为域进程调整 NUMA 策略。  --memtune MEMTUNE     为域进程调整内存策略。  --blkiotune BLKIOTUNE                        为域进程调整 blkio 策略。  --memorybacking MEMORYBACKING                        为域进程设置内存后备策略。例如：                        --memorybacking hugepages=on  --features FEATURES   设置域 &lt;features&gt; XML。例如：                        --features acpi=off                        --features apic=on,eoi=on  --clock CLOCK         设置域 &lt;clock&gt; XML。例如：                        --clock offset=localtime,rtc_tickpolicy=catchup  --pm PM               配置 VM 电源管理功能  --events EVENTS       配置 VM 生命周期管理策略  --resource RESOURCE   配置 VM 资源分区(cgroups)  --sysinfo SYSINFO     Configure SMBIOS System Information. Ex:                        --sysinfo emulate                        --sysinfo host                        --sysinfo bios_vendor=Vendor_Inc.,bios_version=1.2.3-abc,...                        --sysinfo system_manufacturer=System_Corp.,system_product=Computer,...                        --sysinfo baseBoard_manufacturer=Baseboard_Corp.,baseBoard_product=Motherboard,...  --qemu-commandline QEMU_COMMANDLINE                        Pass arguments directly to the qemu emulator. Ex:                        --qemu-commandline=&#39;-display gtk,gl=on&#39;                        --qemu-commandline env=DISPLAY=:0.1虚拟化平台选项:  -v, --hvm             这个客户机应该是一个全虚拟化客户机  -p, --paravirt        这个客户机应该是一个半虚拟化客户机  --container           这个客户机应该是一个容器客户机  --virt-type HV_TYPE   要使用的管理程序名称 (kvm, qemu, xen, ...)  --arch ARCH           模拟 CPU 架构  --machine MACHINE     机器类型为仿真类型其它选项:  --autostart           主机启动时自动启动域。  --transient           Create a transient domain.  --wait WAIT           请等待数分钟以便完成安装。  --noautoconsole       不要自动尝试连接到客户端控制台  --noreboot            安装完成后不启动客户机。  --print-xml [XMLONLY]                        打印生成的 XML 域，而不是创建客户机。  --dry-run             运行安装程序，但不创建设备或定义客户                        机。  --check CHECK         启用或禁用验证检查。例如：                        --check path_in_use=off                        --check all=off  -q, --quiet           抑制非错误输出  -d, --debug           输入故障排除信息使用 &#39;--option=?&#39; 或 &#39;--option help&#39; 来查看可用的子选项请参考 man 手册，以便了解示例和完整的选项语法。[root@node-1 ~]# virt-install --name demo --memory 1024 --vcpus 1 --disk /var/lib/libvirt/images/demo.img --network default --graphics vnc,password=redhat,listen=0.0.0.0 --cdrom /root/CentOS-7-x86_64-Minimal-2009.iso --noautoconsole #开始安装WARNING  未检测到操作系统，虚拟机性能可能会受到影响。使用 --os-variant 选项指定操作系统以获得最佳性能。开始安装......ERROR    内部错误：process exited while connecting to monitor: 2023-03-18T14:47:07.198668Z qemu-kvm: -drive file=/root/CentOS-7-x86_64-Minimal-2009.iso,format=raw,if=none,id=drive-ide0-0-1,readonly=on: could not open disk image /root/CentOS-7-x86_64-Minimal-2009.iso: Could not open &#39;/root/CentOS-7-x86_64-Minimal-2009.iso&#39;: Permission denied域安装失败，您可以运行下列命令重启您的域：&#39;virsh start virsh --connect qemu:///system start demo&#39;否则请重新开始安装。</code></pre><pre><code>#通过上述报错后可得出权限不足[root@node-1 ~]# tail -f /var/log/libvirt/qemu/demo.log #查看报错日志[root@node-1 ~]# vim /etc/libvirt/qemu.conf #设置root权限</code></pre><p><img src="/images/Ceph/Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/2.jpg"></p><pre><code>[root@node-1 ~]# virt-install --name demo --memory 1024 --vcpus 1 --disk /var/lib/libvirt/images/demo.img --network default --graphics vnc,password=redhat,listen=0.0.0.0 --cdrom /root/CentOS-7-x86_64-Minimal-2009.iso --noautoconsole #再次执行创建成功WARNING  未检测到操作系统，虚拟机性能可能会受到影响。使用 --os-variant 选项指定操作系统以获得最佳性能。开始安装......域安装仍在进行。您可以重新连接到控制台以便完成安装进程。参数详解：--name：主机名称--memory：内存大小：M--vcpus：CPU核心数：H--disk：硬盘路径--network: 指定默认网络--graphics: 指定使用VNC，并且设置VNC密码为redhat，监听全部地址--cdrom:使用光盘的方式进行安装--noautoconsole:不要自动尝试连接到客户端控制台[root@node-1 ~]# virsh list --all  #查看创建成功 Id    名称                         状态---------------------------------------------------- 1     demo                           running[root@node-1 ~]# virsh vncdisplay demo #查看主机demo的VNC监听地址:0</code></pre><p><a href="https://downloads.realvnc.com/download/file/viewer.files/VNC-Viewer-7.0.1-Windows.exe">VNC-viewer下载地址</a></p><p>输入KVM的地址加监听的端口号，并点击继续:</p><p><img src="/images/Ceph/Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/3.jpg"></p><p>输入密码：redhat</p><p><img src="/images/Ceph/Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/4.jpg"></p><p>安装操作系统</p><p><img src="/images/Ceph/Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/5.jpg"><br><img src="/images/Ceph/Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/6.jpg"></p><pre><code>[root@node-1 ~]# virsh start demo #主机重启后可能需要手动开机域 demo 已开始[root@node-1 ~]# cat /etc/libvirt/qemu/demo.xml  #查看生成的xml文件&lt;!--WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BEOVERWRITTEN AND LOST. Changes to this xml configuration should be made using:  virsh edit demoor other application using the libvirt API.--&gt;&lt;domain type=&#39;kvm&#39;&gt;  &lt;name&gt;demo&lt;/name&gt;  &lt;uuid&gt;a379dd0c-025c-4fd8-bbe0-0835c9ce03a1&lt;/uuid&gt;  &lt;memory unit=&#39;KiB&#39;&gt;1048576&lt;/memory&gt;  &lt;currentMemory unit=&#39;KiB&#39;&gt;1048576&lt;/currentMemory&gt;  &lt;vcpu placement=&#39;static&#39;&gt;1&lt;/vcpu&gt;  &lt;os&gt;    &lt;type arch=&#39;x86_64&#39; machine=&#39;pc-i440fx-rhel7.0.0&#39;&gt;hvm&lt;/type&gt;    &lt;boot dev=&#39;hd&#39;/&gt;  &lt;/os&gt;  &lt;features&gt;    &lt;acpi/&gt;    &lt;apic/&gt;  &lt;/features&gt;  &lt;cpu mode=&#39;custom&#39; match=&#39;exact&#39; check=&#39;partial&#39;&gt;    &lt;model fallback=&#39;allow&#39;&gt;Broadwell-noTSX-IBRS&lt;/model&gt;    &lt;feature policy=&#39;require&#39; name=&#39;md-clear&#39;/&gt;    &lt;feature policy=&#39;require&#39; name=&#39;spec-ctrl&#39;/&gt;    &lt;feature policy=&#39;require&#39; name=&#39;ssbd&#39;/&gt;  &lt;/cpu&gt;  &lt;clock offset=&#39;utc&#39;&gt;    &lt;timer name=&#39;rtc&#39; tickpolicy=&#39;catchup&#39;/&gt;    &lt;timer name=&#39;pit&#39; tickpolicy=&#39;delay&#39;/&gt;    &lt;timer name=&#39;hpet&#39; present=&#39;no&#39;/&gt;  &lt;/clock&gt;  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;  &lt;on_crash&gt;destroy&lt;/on_crash&gt;  &lt;pm&gt;    &lt;suspend-to-mem enabled=&#39;no&#39;/&gt;    &lt;suspend-to-disk enabled=&#39;no&#39;/&gt;  &lt;/pm&gt;  &lt;devices&gt;    &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt;    &lt;disk type=&#39;file&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;qcow2&#39;/&gt;      &lt;source file=&#39;/var/lib/libvirt/images/demo.img&#39;/&gt;      &lt;target dev=&#39;hda&#39; bus=&#39;ide&#39;/&gt;      &lt;address type=&#39;drive&#39; controller=&#39;0&#39; bus=&#39;0&#39; target=&#39;0&#39; unit=&#39;0&#39;/&gt;    &lt;/disk&gt;    &lt;disk type=&#39;file&#39; device=&#39;cdrom&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;target dev=&#39;hdb&#39; bus=&#39;ide&#39;/&gt;      &lt;readonly/&gt;      &lt;address type=&#39;drive&#39; controller=&#39;0&#39; bus=&#39;0&#39; target=&#39;0&#39; unit=&#39;1&#39;/&gt;    &lt;/disk&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-ehci1&#39;&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x7&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-uhci1&#39;&gt;      &lt;master startport=&#39;0&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x0&#39; multifunction=&#39;on&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-uhci2&#39;&gt;      &lt;master startport=&#39;2&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x1&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-uhci3&#39;&gt;      &lt;master startport=&#39;4&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x2&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;pci&#39; index=&#39;0&#39; model=&#39;pci-root&#39;/&gt;    &lt;controller type=&#39;ide&#39; index=&#39;0&#39;&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x01&#39; function=&#39;0x1&#39;/&gt;    &lt;/controller&gt;    &lt;interface type=&#39;network&#39;&gt;      &lt;mac address=&#39;52:54:00:bf:29:21&#39;/&gt;      &lt;source network=&#39;default&#39;/&gt;      &lt;model type=&#39;rtl8139&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x03&#39; function=&#39;0x0&#39;/&gt;    &lt;/interface&gt;    &lt;serial type=&#39;pty&#39;&gt;      &lt;target type=&#39;isa-serial&#39; port=&#39;0&#39;&gt;        &lt;model name=&#39;isa-serial&#39;/&gt;      &lt;/target&gt;    &lt;/serial&gt;    &lt;console type=&#39;pty&#39;&gt;      &lt;target type=&#39;serial&#39; port=&#39;0&#39;/&gt;    &lt;/console&gt;    &lt;input type=&#39;mouse&#39; bus=&#39;ps2&#39;/&gt;    &lt;input type=&#39;keyboard&#39; bus=&#39;ps2&#39;/&gt;    &lt;graphics type=&#39;vnc&#39; port=&#39;-1&#39; autoport=&#39;yes&#39; listen=&#39;0.0.0.0&#39; passwd=&#39;redhat&#39;&gt;      &lt;listen type=&#39;address&#39; address=&#39;0.0.0.0&#39;/&gt;    &lt;/graphics&gt;    &lt;video&gt;      &lt;model type=&#39;cirrus&#39; vram=&#39;16384&#39; heads=&#39;1&#39; primary=&#39;yes&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x02&#39; function=&#39;0x0&#39;/&gt;    &lt;/video&gt;    &lt;memballoon model=&#39;virtio&#39;&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x05&#39; function=&#39;0x0&#39;/&gt;    &lt;/memballoon&gt;  &lt;/devices&gt;&lt;/domain&gt;</code></pre><h1><span id="四-qemu对接rbd存储块">四、QEMU对接RBD存储块</span></h1><p><img src="/images/Ceph/Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/7.jpg"></p><pre><code>[root@node-1 ~]# ceph osd lspools #查看当前pool4 .rgw.root5 default.rgw.control6 default.rgw.meta7 default.rgw.log13 ceph-demo14 default.rgw.buckets.index15 default.rgw.buckets.data16 cephfs_data17 cephfs_metadata18 kubernetes[root@node-1 ~]# qemu-img create -f raw rbd:ceph-demo/kvm-test.img 5G  #通过qemu在ceph的存储池中创建一个RBD块设备Formatting &#39;rbd:ceph-demo/kvm-test.img&#39;, fmt=raw size=5368709120 cluster_size=0 [root@node-1 ~]# rbd info ceph-demo/kvm-test.img #查看创建的块设备rbd image &#39;kvm-test.img&#39;:    size 5 GiB in 1280 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 58e90a051900e    block_name_prefix: rbd_data.58e90a051900e    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Sat Mar 18 23:23:31 2023    access_timestamp: Sat Mar 18 23:23:31 2023    modify_timestamp: Sat Mar 18 23:23:31 2023[root@node-1 ~]# rbd -p ceph-demo ls #查看块设备kvm-test.imgrbd-bakup.imgrbd.imgtest-demotest-demo-1.imgtest-demo-3.img[root@node-1 ~]# qemu-img resize rbd:ceph-demo/kvm-test.img 6G #通过qume调整rbd大小Image resized.[root@node-1 ~]# rbd info ceph-demo/kvm-test.imgrbd image &#39;kvm-test.img&#39;:    size 6 GiB in 1536 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 58e90a051900e    block_name_prefix: rbd_data.58e90a051900e    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Sat Mar 18 23:23:31 2023    access_timestamp: Sat Mar 18 23:27:12 2023    modify_timestamp: Sat Mar 18 23:23:31 2023</code></pre><h2><span id="五-libvirt对接kvm详解">五、Libvirt对接KVM详解</span></h2><p><img src="/images/Ceph/Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/8.jpg"></p><h2><span id="51-创建一个池-以下示例使用池名称libvirt-pool">5.1、创建一个池。以下示例使用池名称libvirt-pool.：</span></h2><pre><code>[root@node-1 ~]# ceph osd pool create libvirt-pool 16 16pool &#39;libvirt-pool&#39; created验证池是否存在。[root@node-1 ~]# ceph osd lspools4 .rgw.root5 default.rgw.control6 default.rgw.meta7 default.rgw.log13 ceph-demo14 default.rgw.buckets.index15 default.rgw.buckets.data16 cephfs_data17 cephfs_metadata18 kubernetes19 libvirt-pool #创建成功</code></pre><h2><span id="52-使用rbd工具初始化池以供-rbd-使用">5.2、使用rbd工具初始化池以供 RBD 使用：</span></h2><p><code>[root@node-1 ~]# rbd pool init libvirt-pool #默认为RBD格式</code></p><h2><span id="53-创建-ceph-用户或用于clientadmin097-及更早版本-以下示例使用-ceph-用户名clientlibvirt-和引用libvirt-pool">5.3、创建 Ceph 用户（或用于client.admin0.9.7 及更早版本）。以下示例使用 Ceph 用户名client.libvirt 和引用libvirt-pool。</span></h2><pre><code>[root@node-1 ~]# ceph auth get-or-create client.libvirt mon &#39;profile rbd&#39; osd &#39;profile rbd pool=libvirt-pool&#39;[client.libvirt]    key = AQBW2hVkmdgYDBAAk/8RRIomBvQ1RjAuwJtRgw==验证名称是否存在。[root@node-1 ~]# ceph auth list mds.node-1    key: AQCs2QpkNSE5LhAA66qFcFeSCcO7eIVGfH2kuA==    caps: [mds] allow    caps: [mon] allow profile mds    caps: [osd] allow rwxmds.node-2    key: AQCt2Qpk7YOGIhAAeXR14T7zOVBW7B8j0t77JQ==    caps: [mds] allow    caps: [mon] allow profile mds    caps: [osd] allow rwxmds.node-3    key: AQCu2Qpkty3yJxAAnSykxl8bEmuHqJ8TKTINDA==    caps: [mds] allow    caps: [mon] allow profile mds    caps: [osd] allow rwxosd.0    key: AQC+0QZkOgr3FhAAQv9XW7FIaZAxchn1osNoLA==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *osd.1    key: AQDN0QZkomSUHBAAzHUP9FglHxndMZwzPlXwrQ==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *osd.2    key: AQDf0QZk0A++IRAAQFRGoyjoaPhB+FgatmxA4A==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *osd.3    key: AQDu0QZkcuv5CBAApo23xWc/aF79lfRu1ZjL0g==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *osd.4    key: AQAL0gZkQylZBBAA7nm9rCSwdqiuKn+rRewcsw==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *osd.5    key: AQAY0gZkU/U3AhAAjgysqCuvYyk4wgkHz/YQQQ==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *client.admin    key: AQCnzgZkNmvADRAAKAZNkmurOI4dLuviFjVvsQ==    caps: [mds] allow *    caps: [mgr] allow *    caps: [mon] allow *    caps: [osd] allow *client.bootstrap-mds    key: AQCnzgZkqHfADRAA7TQKHkEqb3OrkdxV9MF4Uw==    caps: [mon] allow profile bootstrap-mdsclient.bootstrap-mgr    key: AQCnzgZkUoHADRAAdJMFHJU2vzIthSaVj3shaQ==    caps: [mon] allow profile bootstrap-mgrclient.bootstrap-osd    key: AQCnzgZkzonADRAAFrpte0n08NP07vpuCeNp3g==    caps: [mon] allow profile bootstrap-osdclient.bootstrap-rbd    key: AQCnzgZk05LADRAAVNTqEPcfoB7mQ2zoy0FiIQ==    caps: [mon] allow profile bootstrap-rbdclient.bootstrap-rbd-mirror    key: AQCnzgZknsXADRAAjFAgSZT+AsV+cm6CZbP9bw==    caps: [mon] allow profile bootstrap-rbd-mirrorclient.bootstrap-rgw    key: AQCnzgZkK9PADRAAzwV5uTQbWpYyn7vcM8DAQQ==    caps: [mon] allow profile bootstrap-rgwclient.kubernetes    key: AQAEQRFkrYSrBxAAUNZRT6BxLS0jHzIzMQg4CQ==    caps: [mon] profile rbd    caps: [osd] profile rbd pool=kubernetesclient.libvirt    key: AQBW2hVkmdgYDBAAk/8RRIomBvQ1RjAuwJtRgw==    caps: [mon] profile rbd    caps: [osd] profile rbd pool=libvirt-poolclient.rgw.node-1    key: AQAPtQlk8KNzFRAAOinyE+NbWbcHfsmQnSRbyg==    caps: [mon] allow rw    caps: [osd] allow rwxclient.rgw.node-2    key: AQDutwlkmakzHRAAVmxW6zS80nYyqS827qG9zg==    caps: [mon] allow rw    caps: [osd] allow rwxmgr.node-1    key: AQA60AZkz43bERAAjFnhLCkPoQX6AoxPoQZ/hA==    caps: [mds] allow *    caps: [mon] allow profile mgr    caps: [osd] allow *mgr.node-2    key: AQA70AZk33pwBBAABdE45uQJTEtWF0fD26vYkg==    caps: [mds] allow *    caps: [mon] allow profile mgr    caps: [osd] allow *mgr.node-3    key: AQA70AZkQwPlMxAAO/1EZp+zpQ3QB8H2OsN8/g==    caps: [mds] allow *    caps: [mon] allow profile mgr    caps: [osd] allow *installed auth entries:</code></pre><blockquote><p>注意：libvirt将使用 ID 访问 Ceph libvirt，而不是 Ceph 名称client.libvirt。ID和名称的区别详见用户管理-用户和 用户管理-CLI 。</p></blockquote><h2><span id="54-使用-qemu在-rbd-池中创建映像-以下示例使用图像名称new-libvirt-image-和引用libvirt-pool">5.4、使用 QEMU在 RBD 池中创建映像。以下示例使用图像名称new-libvirt-image 和引用libvirt-pool。</span></h2><pre><code>[root@node-1 ~]# qemu-img create -f rbd rbd:libvirt-pool/new-libvirt-image 2G #这里-f指定的rbd也可替换成raw，效果一样Formatting &#39;rbd:libvirt-pool/new-libvirt-image&#39;, fmt=rbd size=2147483648 cluster_size=0验证块是否存在。[root@node-1 ~]# rbd -p libvirt-pool lsnew-libvirt-image[root@node-1 ~]# qemu-img info rbd:libvirt-pool/new-libvirt-imageimage: rbd:libvirt-pool/new-libvirt-imagefile format: rawvirtual size: 2.0G (2147483648 bytes)disk size: unavailable</code></pre><blockquote><p>如果您希望为此客户端启用调试日志和管理套接字，您可以将以下部分添加到&#x2F;etc&#x2F;ceph&#x2F;ceph.conf：</p></blockquote><pre><code>[client.libvirt]log file = /var/log/ceph/qemu-guest-$pid.logadmin socket = /var/run/ceph/$cluster-$type.$id.$pid.$cctid.asok</code></pre><blockquote><p>部分名称client.libvirt应与您在上面创建的 cephx 用户匹配。如果启用了 SELinux 或 AppArmor，请注意这可能会阻止客户端进程（通过 libvirt 的 qemu）执行某些操作，例如写入日志或操作图像或管理套接字到目标位置（或）。&#x2F;var&#x2F; log&#x2F;ceph&#x2F;var&#x2F;run&#x2F;ceph</p></blockquote><h2><span id="55-配置虚拟机">5.5、配置虚拟机</span></h2><h3><span id="551-使用-virsh-edit打开配置文件">5.5.1、使用 virsh edit打开配置文件。</span></h3><pre><code>[root@node-1 ~]# virsh edit demo #增加如下配置    &lt;disk type=&#39;network&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;source protocol=&#39;rbd&#39; name=&#39;libvirt-pool/new-libvirt-image&#39;&gt;        &lt;host name=&#39;192.168.187.201&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.202&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.202&#39; port=&#39;6789&#39;/&gt;      &lt;/source&gt;      &lt;target dev=&#39;vdb&#39; bus=&#39;virtio&#39;/&gt;    &lt;/disk&gt;[root@node-1 ~]# virsh edit demo&lt;domain type=&#39;kvm&#39;&gt;  &lt;name&gt;demo&lt;/name&gt;  &lt;uuid&gt;a379dd0c-025c-4fd8-bbe0-0835c9ce03a1&lt;/uuid&gt;  &lt;memory unit=&#39;KiB&#39;&gt;1048576&lt;/memory&gt;  &lt;currentMemory unit=&#39;KiB&#39;&gt;1048576&lt;/currentMemory&gt;  &lt;vcpu placement=&#39;static&#39;&gt;1&lt;/vcpu&gt;  &lt;os&gt;    &lt;type arch=&#39;x86_64&#39; machine=&#39;pc-i440fx-rhel7.0.0&#39;&gt;hvm&lt;/type&gt;    &lt;boot dev=&#39;hd&#39;/&gt;  &lt;/os&gt;  &lt;features&gt;    &lt;acpi/&gt;    &lt;apic/&gt;  &lt;/features&gt;  &lt;cpu mode=&#39;custom&#39; match=&#39;exact&#39; check=&#39;partial&#39;&gt;    &lt;model fallback=&#39;allow&#39;&gt;Broadwell-noTSX-IBRS&lt;/model&gt;    &lt;feature policy=&#39;require&#39; name=&#39;md-clear&#39;/&gt;    &lt;feature policy=&#39;require&#39; name=&#39;spec-ctrl&#39;/&gt;    &lt;feature policy=&#39;require&#39; name=&#39;ssbd&#39;/&gt;  &lt;/cpu&gt;  &lt;clock offset=&#39;utc&#39;&gt;    &lt;timer name=&#39;rtc&#39; tickpolicy=&#39;catchup&#39;/&gt;    &lt;timer name=&#39;pit&#39; tickpolicy=&#39;delay&#39;/&gt;    &lt;timer name=&#39;hpet&#39; present=&#39;no&#39;/&gt;  &lt;/clock&gt;  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;  &lt;on_crash&gt;destroy&lt;/on_crash&gt;  &lt;pm&gt;    &lt;suspend-to-mem enabled=&#39;no&#39;/&gt;    &lt;suspend-to-disk enabled=&#39;no&#39;/&gt;  &lt;/pm&gt;  &lt;devices&gt;    &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt;    &lt;disk type=&#39;file&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;qcow2&#39;/&gt;      &lt;source file=&#39;/var/lib/libvirt/images/demo.img&#39;/&gt;      &lt;target dev=&#39;hda&#39; bus=&#39;ide&#39;/&gt;      &lt;address type=&#39;drive&#39; controller=&#39;0&#39; bus=&#39;0&#39; target=&#39;0&#39; unit=&#39;0&#39;/&gt;    &lt;/disk&gt;    &lt;disk type=&#39;file&#39; device=&#39;cdrom&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;target dev=&#39;hdb&#39; bus=&#39;ide&#39;/&gt;      &lt;readonly/&gt;      &lt;address type=&#39;drive&#39; controller=&#39;0&#39; bus=&#39;0&#39; target=&#39;0&#39; unit=&#39;1&#39;/&gt;    &lt;/disk&gt;    &lt;disk type=&#39;network&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;source protocol=&#39;rbd&#39; name=&#39;libvirt-pool/new-libvirt-image&#39;&gt;        &lt;host name=&#39;192.168.187.201&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.202&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.203&#39; port=&#39;6789&#39;/&gt;      &lt;/source&gt;      &lt;target dev=&#39;vdb&#39; bus=&#39;virtio&#39;/&gt;    &lt;/disk&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-ehci1&#39;&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x7&#39;/&gt;&quot;/tmp/virshJs10SR.xml&quot; 102L, 3801C written</code></pre><pre><code>&lt;disk type=&#39;network&#39; device=&#39;disk&#39;&gt;        &lt;source protocol=&#39;rbd&#39; name=&#39;libvirt-pool/new-libvirt-image&#39;&gt;                &lt;host name=&#39;&#123;monitor-host&#125;&#39; port=&#39;6789&#39;/&gt;        &lt;/source&gt;        &lt;target dev=&#39;vdb&#39; bus=&#39;virtio&#39;/&gt;&lt;/disk&gt;</code></pre><blockquote><p>替换{monitor-host}为您的主机名，并根据需要替换池和&#x2F;或映像名称。您可以<host> 为 Ceph 监视器添加多个条目。该属性是将出现在VM 目录dev下的逻辑设备名称。&#x2F;dev可选bus属性指示要模拟的磁盘设备的类型。<br>有效设置是特定于驱动程序的（例如，“ide”、“scsi”、“virtio”、“xen”、“usb”或“sata”）。</host></p></blockquote><blockquote><p>重要提示：使用edit而不是vim。如果用文本编辑器 编辑配置文件，可能无法识别更改。如果 下的 XML 文件内容与 的果 不一致，则您的 VM 可能无法正常工作。sudo virsh edit&#x2F;etc&#x2F;libvirt&#x2F;qemulibvirt&#x2F;etc&#x2F;libvirt&#x2F;qemusudo virsh dumpxml {vmdomain-name}</p></blockquote><h3><span id="552-如果您的-ceph-存储集群启用了ceph-身份验证默认情况下启用您必须生成一个密钥">5.5.2、如果您的 Ceph 存储集群启用了Ceph 身份验证（默认情况下启用），您必须生成一个密钥。</span></h3><pre><code>[root@node-1 ~]# cat &gt; secret.xml &lt;&lt;EOF&lt;secret ephemeral=&#39;no&#39; private=&#39;no&#39;&gt;        &lt;usage type=&#39;ceph&#39;&gt;                &lt;name&gt;client.libvirt secret&lt;/name&gt;        &lt;/usage&gt;&lt;/secret&gt;EOF</code></pre><h3><span id="553-定义密钥">5.5.3、定义密钥。</span></h3><pre><code>[root@node-1 ~]# sudo virsh secret-define --file secret.xml 生成 secret efade415-5404-4b67-bd1e-165c6f64cbba[root@node-1 ~]# virsh secret-list  UUID                                  用量-------------------------------------------------------------------------------- efade415-5404-4b67-bd1e-165c6f64cbba  ceph client.libvirt secret[root@node-1 ~]# virsh secret-dumpxml efade415-5404-4b67-bd1e-165c6f64cbba&lt;secret ephemeral=&#39;no&#39; private=&#39;no&#39;&gt;  &lt;uuid&gt;efade415-5404-4b67-bd1e-165c6f64cbba&lt;/uuid&gt;  &lt;usage type=&#39;ceph&#39;&gt;    &lt;name&gt;client.libvirt secret&lt;/name&gt;  &lt;/usage&gt;&lt;/secret&gt;</code></pre><h3><span id="554-获取clientlibvirt密钥并将密钥字符串保存到文件中">5.5.4、获取client.libvirt密钥并将密钥字符串保存到文件中。</span></h3><pre><code>[root@node-1 ~]# ceph auth get-key client.libvirt | sudo tee client.libvirt.keyAQBW2hVkmdgYDBAAk/8RRIomBvQ1RjAuwJtRgw==</code></pre><h3><span id="555-设置密钥的-uuid">5.5.5、设置密钥的 UUID。</span></h3><blockquote><p>这里的 efade415-5404-4b67-bd1e-165c6f64cbba就是virsh secret-list 获取到的，$(cat client.libvirt.key)就是ceph auth get-key client.libvirt 获取的</p></blockquote><pre><code>[root@node-1 ~]# sudo virsh secret-set-value --secret efade415-5404-4b67-bd1e-165c6f64cbba --base64 $(cat client.libvirt.key) &amp;&amp; rm client.libvirt.key secret.xmlsecret 值设定rm：是否删除普通文件 &quot;client.libvirt.key&quot;？yrm：是否删除普通文件 &quot;secret.xml&quot;？y[root@node-1 ~]#  virsh secret-get-value efade415-5404-4b67-bd1e-165c6f64cbba #查看值AQBW2hVkmdgYDBAAk/8RRIomBvQ1RjAuwJtRgw==</code></pre><h3><span id="556-修改虚拟机文件增加认证">5.5.6、修改虚拟机文件，增加认证</span></h3><pre><code>增加如下配置，在source和target之间...&lt;/source&gt;&lt;auth username=&#39;libvirt&#39;&gt;        &lt;secret type=&#39;ceph&#39; uuid=&#39;&#123;uuid of secret&#125;&#39;/&gt;&lt;/auth&gt;&lt;target ...[root@node-1 ~]# virsh edit demo&lt;domain type=&#39;kvm&#39;&gt;  &lt;name&gt;demo&lt;/name&gt;  &lt;uuid&gt;a379dd0c-025c-4fd8-bbe0-0835c9ce03a1&lt;/uuid&gt;  &lt;memory unit=&#39;KiB&#39;&gt;1048576&lt;/memory&gt;  &lt;currentMemory unit=&#39;KiB&#39;&gt;1048576&lt;/currentMemory&gt;  &lt;vcpu placement=&#39;static&#39;&gt;1&lt;/vcpu&gt;  &lt;os&gt;    &lt;type arch=&#39;x86_64&#39; machine=&#39;pc-i440fx-rhel7.0.0&#39;&gt;hvm&lt;/type&gt;    &lt;boot dev=&#39;hd&#39;/&gt;  &lt;/os&gt;  &lt;features&gt;    &lt;acpi/&gt;    &lt;apic/&gt;  &lt;/features&gt;  &lt;cpu mode=&#39;custom&#39; match=&#39;exact&#39; check=&#39;partial&#39;&gt;    &lt;model fallback=&#39;allow&#39;&gt;Broadwell-noTSX-IBRS&lt;/model&gt;    &lt;feature policy=&#39;require&#39; name=&#39;md-clear&#39;/&gt;    &lt;feature policy=&#39;require&#39; name=&#39;spec-ctrl&#39;/&gt;    &lt;feature policy=&#39;require&#39; name=&#39;ssbd&#39;/&gt;  &lt;/cpu&gt;  &lt;clock offset=&#39;utc&#39;&gt;    &lt;timer name=&#39;rtc&#39; tickpolicy=&#39;catchup&#39;/&gt;    &lt;timer name=&#39;pit&#39; tickpolicy=&#39;delay&#39;/&gt;    &lt;timer name=&#39;hpet&#39; present=&#39;no&#39;/&gt;  &lt;/clock&gt;  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;  &lt;on_crash&gt;destroy&lt;/on_crash&gt;  &lt;pm&gt;    &lt;suspend-to-mem enabled=&#39;no&#39;/&gt;    &lt;suspend-to-disk enabled=&#39;no&#39;/&gt;  &lt;/pm&gt;  &lt;devices&gt;    &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt;    &lt;disk type=&#39;file&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;qcow2&#39;/&gt;      &lt;source file=&#39;/var/lib/libvirt/images/demo.img&#39;/&gt;      &lt;target dev=&#39;hda&#39; bus=&#39;ide&#39;/&gt;      &lt;address type=&#39;drive&#39; controller=&#39;0&#39; bus=&#39;0&#39; target=&#39;0&#39; unit=&#39;0&#39;/&gt;    &lt;/disk&gt;    &lt;disk type=&#39;file&#39; device=&#39;cdrom&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;target dev=&#39;hdb&#39; bus=&#39;ide&#39;/&gt;      &lt;readonly/&gt;      &lt;address type=&#39;drive&#39; controller=&#39;0&#39; bus=&#39;0&#39; target=&#39;0&#39; unit=&#39;1&#39;/&gt;    &lt;/disk&gt;    &lt;disk type=&#39;network&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;source protocol=&#39;rbd&#39; name=&#39;libvirt-pool/new-libvirt-image&#39;&gt;        &lt;host name=&#39;192.168.187.201&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.202&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.203&#39; port=&#39;6789&#39;/&gt;      &lt;/source&gt;      &lt;auth username=&#39;libvirt&#39;&gt;        &lt;secret type=&#39;ceph&#39; uuid=&#39;efade415-5404-4b67-bd1e-165c6f64cbba&#39;/&gt;      &lt;/auth&gt;      &lt;target dev=&#39;vdb&#39; bus=&#39;virtio&#39;/&gt;&quot;/tmp/virshBHe7Hw.xml&quot; 106L, 4003C written</code></pre><h1><span id="六-kvm对接测试及扩容">六、KVM对接测试及扩容</span></h1><h2><span id="61-对接测试">6.1、对接测试</span></h2><pre><code>[root@node-1 ~]# virsh destroy demo  #停机域 demo 被删除[root@node-1 ~]# virsh list --all  #查看 Id    名称                         状态---------------------------------------------------- -     demo                           关闭[root@node-1 ~]# virsh start demo #开机域 demo 已开始[root@node-1 ~]# sudo virsh qemu-monitor-command --hmp demo &#39;info block&#39; #查看硬件设备情况，已经能看到drive-virtio-disk1了drive-ide0-0-0: removable=0 io-status=ok file=/var/lib/libvirt/images/demo.img ro=0 drv=qcow2 encrypted=0 bps=0 bps_rd=0 bps_wr=0 iops=0 iops_rd=0 iops_wr=0drive-ide0-0-1: removable=1 locked=0 tray-open=0 io-status=ok [not inserted]drive-virtio-disk1: removable=0 io-status=ok file=rbd:libvirt-pool/new-libvirt-image:id=libvirt:key=AQBW2hVkmdgYDBAAk/8RRIomBvQ1RjAuwJtRgw==:auth_supported=cephx\\;none:mon_host=192.168.187.201\\:6789\\;192.168.187.202\\:6789\\;192.168.187.203\\:6789 ro=0 drv=raw encrypted=0 bps=0 bps_rd=0 bps_wr=0 iops=0 iops_rd=0 iops_wr=0[root@node-1 ~]# virsh domblklist demo --details #更明显的查看类型     Device     目标     源------------------------------------------------file       disk       hda        /var/lib/libvirt/images/demo.imgfile       cdrom      hdb        -network    disk       vdb        libvirt-pool/new-libvirt-image</code></pre><p><img src="/images/Ceph/Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/9.jpg"></p><h2><span id="62-扩容大小">6.2、扩容大小</span></h2><blockquote><p>扩容完需要重启主机，可以在网上找找文档，怎么避免重启</p></blockquote><pre><code>[root@node-1 ~]# qemu-img resize rbd:libvirt-pool/new-libvirt-image 5G Image resized.[root@node-1 ~]# qemu-img info rbd:libvirt-pool/new-libvirt-imageimage: rbd:libvirt-pool/new-libvirt-imagefile format: rawvirtual size: 5.0G (5368709120 bytes)disk size: unavailable</code></pre><p><img src="/images/Ceph/Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/10.jpg"></p><h1><span id="七-系统盘存储至ceph也可用于迁移到ceph集群中">七、系统盘存储至Ceph（也可用于迁移到ceph集群中）</span></h1><h2><span id="71-主机停机">7.1、主机停机</span></h2><pre><code>[root@node-1 ~]# virsh destroy demo 域 demo 被删除</code></pre><h2><span id="72-转换格式存放在ceph集群中">7.2、转换格式存放在ceph集群中</span></h2><pre><code>[root@node-1 ~]# qemu-img convert -f qcow2 /var/lib/libvirt/images/demo.img -O raw rbd:libvirt-pool/demo.img #保留本地镜像，并将格式转换成raw，上传到libvirt-pool中，取名为：demo.img[root@node-1 ~]# rbd ls libvirt-pool #查看上传成功demo.imgnew-libvirt-image[root@node-1 ~]# rbd info libvirt-pool/demo.img #查看具体信息rbd image &#39;demo.img&#39;:    size 4 GiB in 1024 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 58f742a8909ec    block_name_prefix: rbd_data.58f742a8909ec    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Sun Mar 19 00:45:28 2023    access_timestamp: Sun Mar 19 00:45:28 2023    modify_timestamp: Sun Mar 19 00:45:28 2023</code></pre><h2><span id="73-修改主机xml文件">7.3、修改主机xml文件</span></h2><pre><code>    &lt;disk type=&#39;network&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;auth username=&#39;libvirt&#39;&gt;        &lt;secret type=&#39;ceph&#39; uuid=&#39;efade415-5404-4b67-bd1e-165c6f64cbba&#39;/&gt;      &lt;/auth&gt;      &lt;source protocol=&#39;rbd&#39; name=&#39;libvirt-pool/demo.image&#39;&gt;        &lt;host name=&#39;192.168.187.201&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.202&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.203&#39; port=&#39;6789&#39;/&gt;      &lt;/source&gt;      &lt;target dev=&#39;hda&#39; bus=&#39;virtio&#39;/&gt;    &lt;/disk&gt;[root@node-1 ~]# vim /etc/libvirt/qemu/demo.xml #将原来的sda硬盘的配置替换成ceph的格式&lt;!--WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BEOVERWRITTEN AND LOST. Changes to this xml configuration should be made using:  virsh edit demoor other application using the libvirt API.--&gt;&lt;domain type=&#39;kvm&#39;&gt;  &lt;name&gt;demo&lt;/name&gt;  &lt;uuid&gt;a379dd0c-025c-4fd8-bbe0-0835c9ce03a1&lt;/uuid&gt;  &lt;memory unit=&#39;KiB&#39;&gt;1048576&lt;/memory&gt;  &lt;currentMemory unit=&#39;KiB&#39;&gt;1048576&lt;/currentMemory&gt;  &lt;vcpu placement=&#39;static&#39;&gt;1&lt;/vcpu&gt;  &lt;os&gt;    &lt;type arch=&#39;x86_64&#39; machine=&#39;pc-i440fx-rhel7.0.0&#39;&gt;hvm&lt;/type&gt;    &lt;boot dev=&#39;hd&#39;/&gt;  &lt;/os&gt;  &lt;features&gt;    &lt;acpi/&gt;    &lt;apic/&gt;  &lt;/features&gt;  &lt;cpu mode=&#39;custom&#39; match=&#39;exact&#39; check=&#39;partial&#39;&gt;    &lt;model fallback=&#39;allow&#39;&gt;Broadwell-noTSX-IBRS&lt;/model&gt;    &lt;feature policy=&#39;require&#39; name=&#39;md-clear&#39;/&gt;    &lt;feature policy=&#39;require&#39; name=&#39;spec-ctrl&#39;/&gt;    &lt;feature policy=&#39;require&#39; name=&#39;ssbd&#39;/&gt;  &lt;/cpu&gt;  &lt;clock offset=&#39;utc&#39;&gt;    &lt;timer name=&#39;rtc&#39; tickpolicy=&#39;catchup&#39;/&gt;    &lt;timer name=&#39;pit&#39; tickpolicy=&#39;delay&#39;/&gt;    &lt;timer name=&#39;hpet&#39; present=&#39;no&#39;/&gt;  &lt;/clock&gt;  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;  &lt;on_crash&gt;destroy&lt;/on_crash&gt;  &lt;pm&gt;    &lt;suspend-to-mem enabled=&#39;no&#39;/&gt;    &lt;suspend-to-disk enabled=&#39;no&#39;/&gt;  &lt;/pm&gt;  &lt;devices&gt;    &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt;    &lt;disk type=&#39;network&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;auth username=&#39;libvirt&#39;&gt;        &lt;secret type=&#39;ceph&#39; uuid=&#39;efade415-5404-4b67-bd1e-165c6f64cbba&#39;/&gt;      &lt;/auth&gt;      &lt;source protocol=&#39;rbd&#39; name=&#39;libvirt-pool/demo.image&#39;&gt;        &lt;host name=&#39;192.168.187.201&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.202&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.203&#39; port=&#39;6789&#39;/&gt;      &lt;/source&gt;      &lt;target dev=&#39;hda&#39; bus=&#39;virtio&#39;/&gt;    &lt;/disk&gt;    &lt;disk type=&#39;network&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;auth username=&#39;libvirt&#39;&gt;        &lt;secret type=&#39;ceph&#39; uuid=&#39;efade415-5404-4b67-bd1e-165c6f64cbba&#39;/&gt;      &lt;/auth&gt;      &lt;source protocol=&#39;rbd&#39; name=&#39;libvirt-pool/new-libvirt-image&#39;&gt;        &lt;host name=&#39;192.168.187.201&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.202&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.203&#39; port=&#39;6789&#39;/&gt;      &lt;/source&gt;      &lt;target dev=&#39;vdb&#39; bus=&#39;virtio&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x06&#39; function=&#39;0x0&#39;/&gt;    &lt;/disk&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-ehci1&#39;&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x7&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-uhci1&#39;&gt;      &lt;master startport=&#39;0&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x0&#39; multifunction=&#39;on&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-uhci2&#39;&gt;      &lt;master startport=&#39;2&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x1&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-uhci3&#39;&gt;      &lt;master startport=&#39;4&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x2&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;pci&#39; index=&#39;0&#39; model=&#39;pci-root&#39;/&gt;    &lt;controller type=&#39;ide&#39; index=&#39;0&#39;&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x01&#39; function=&#39;0x1&#39;/&gt;    &lt;/controller&gt;    &lt;interface type=&#39;network&#39;&gt;      &lt;mac address=&#39;52:54:00:bf:29:21&#39;/&gt;      &lt;source network=&#39;default&#39;/&gt;      &lt;model type=&#39;rtl8139&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x03&#39; function=&#39;0x0&#39;/&gt;    &lt;/interface&gt;    &lt;serial type=&#39;pty&#39;&gt;      &lt;target type=&#39;isa-serial&#39; port=&#39;0&#39;&gt;        &lt;model name=&#39;isa-serial&#39;/&gt;      &lt;/target&gt;    &lt;/serial&gt;    &lt;console type=&#39;pty&#39;&gt;      &lt;target type=&#39;serial&#39; port=&#39;0&#39;/&gt;    &lt;/console&gt;    &lt;input type=&#39;mouse&#39; bus=&#39;ps2&#39;/&gt;    &lt;input type=&#39;keyboard&#39; bus=&#39;ps2&#39;/&gt;    &lt;graphics type=&#39;vnc&#39; port=&#39;-1&#39; autoport=&#39;yes&#39; listen=&#39;0.0.0.0&#39; passwd=&#39;redhat&#39;&gt;      &lt;listen type=&#39;address&#39; address=&#39;0.0.0.0&#39;/&gt;    &lt;/graphics&gt;    &lt;video&gt;      &lt;model type=&#39;cirrus&#39; vram=&#39;16384&#39; heads=&#39;1&#39; primary=&#39;yes&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x02&#39; function=&#39;0x0&#39;/&gt;    &lt;/video&gt;    &lt;memballoon model=&#39;virtio&#39;&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x05&#39; function=&#39;0x0&#39;/&gt;    &lt;/memballoon&gt;  &lt;/devices&gt;&lt;/domain&gt;[root@node-1 ~]# virsh start demo #主机开机测试[root@node-1 ~]# virsh list  #查看运行正常 Id    名称                         状态---------------------------------------------------- 6     demo                           running</code></pre><p><strong>主机运行正常，且数据完成迁移；</strong></p><p><img src="/images/Ceph/Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/11.jpg"></p><h1><span id="八-揭秘云主机秒级部署奥秘">八、揭秘云主机秒级部署奥秘</span></h1><h2><span id="81-创建快照">8.1、创建快照</span></h2><pre><code>[root@node-1 ~]# rbd snap create libvirt-pool/demo.img@Centos7-template[root@node-1 ~]# rbd snap ls libvirt-pool/demo.imgSNAPID NAME             SIZE  PROTECTED TIMESTAMP                     4 Centos7-template 4 GiB yes       Sun Mar 19 01:02:04 2023</code></pre><h2><span id="82-对快照增加保护">8.2、对快照增加保护</span></h2><pre><code>[root@node-1 ~]# rbd snap protect libvirt-pool/demo.img@Centos7-template</code></pre><h2><span id="83-查看快照保护">8.3、查看快照保护</span></h2><pre><code>[root@node-1 ~]#  rbd info libvirt-pool/demo.img@Centos7-templaterbd image &#39;demo.img&#39;:    size 4 GiB in 1024 objects    order 22 (4 MiB objects)    snapshot_count: 1    id: 58f742a8909ec    block_name_prefix: rbd_data.58f742a8909ec    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Sun Mar 19 00:45:28 2023    access_timestamp: Sun Mar 19 00:45:28 2023    modify_timestamp: Sun Mar 19 00:45:28 2023     protected: True #保护中</code></pre><h2><span id="84-克隆镜像">8.4、克隆镜像</span></h2><pre><code>[root@node-1 ~]# rbd clone libvirt-pool/demo.img@Centos7-template libvirt-pool/clone-vm1.img[root@node-1 ~]# rbd clone libvirt-pool/demo.img@Centos7-template libvirt-pool/clone-vm2.img</code></pre><h2><span id="85-查看克隆镜像">8.5、查看克隆镜像</span></h2><pre><code>[root@node-1 ~]# rbd info libvirt-pool/clone-vm1.imgrbd image &#39;clone-vm1.img&#39;:    size 4 GiB in 1024 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 58ff55e9ad1d0    block_name_prefix: rbd_data.58ff55e9ad1d0    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Sun Mar 19 01:06:58 2023    access_timestamp: Sun Mar 19 01:06:58 2023    modify_timestamp: Sun Mar 19 01:06:58 2023    parent: libvirt-pool/demo.img@Centos7-template #父镜像    overlap: 4 GiB</code></pre><h2><span id="86-配置虚拟机xml文件">8.6、配置虚拟机xml文件</span></h2><p>修改配置文件步骤：</p><ol><li>修改xml中的主机名称</li><li>删除UUI的</li><li>删除vdb的硬盘配置</li><li>修改vba的硬盘配置</li></ol><pre><code>[root@node-1 ~]# cd /etc/libvirt/qemu/[root@node-1 qemu]# lsdemo-bak.xml  demo.xml  networks[root@node-1 qemu]# cp demo.xml clone-vm1.xml [root@node-1 qemu]# cp demo.xml clone-vm2.xml[root@node-1 qemu]# vim clone-vm1.xml&lt;!--WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BEOVERWRITTEN AND LOST. Changes to this xml configuration should be made using:  virsh edit demoor other application using the libvirt API.--&gt;&lt;domain type=&#39;kvm&#39;&gt;  &lt;name&gt;clone-vm1&lt;/name&gt;  &lt;memory unit=&#39;KiB&#39;&gt;1048576&lt;/memory&gt;  &lt;currentMemory unit=&#39;KiB&#39;&gt;1048576&lt;/currentMemory&gt;  &lt;vcpu placement=&#39;static&#39;&gt;1&lt;/vcpu&gt;  &lt;os&gt;    &lt;type arch=&#39;x86_64&#39; machine=&#39;pc-i440fx-rhel7.0.0&#39;&gt;hvm&lt;/type&gt;    &lt;boot dev=&#39;hd&#39;/&gt;  &lt;/os&gt;  &lt;features&gt;    &lt;acpi/&gt;    &lt;apic/&gt;  &lt;/features&gt;  &lt;cpu mode=&#39;custom&#39; match=&#39;exact&#39; check=&#39;partial&#39;&gt;    &lt;model fallback=&#39;allow&#39;&gt;Broadwell-noTSX-IBRS&lt;/model&gt;    &lt;feature policy=&#39;require&#39; name=&#39;md-clear&#39;/&gt;    &lt;feature policy=&#39;require&#39; name=&#39;spec-ctrl&#39;/&gt;    &lt;feature policy=&#39;require&#39; name=&#39;ssbd&#39;/&gt;  &lt;/cpu&gt;  &lt;clock offset=&#39;utc&#39;&gt;    &lt;timer name=&#39;rtc&#39; tickpolicy=&#39;catchup&#39;/&gt;    &lt;timer name=&#39;pit&#39; tickpolicy=&#39;delay&#39;/&gt;    &lt;timer name=&#39;hpet&#39; present=&#39;no&#39;/&gt;  &lt;/clock&gt;  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;  &lt;on_crash&gt;destroy&lt;/on_crash&gt;  &lt;pm&gt;    &lt;suspend-to-mem enabled=&#39;no&#39;/&gt;    &lt;suspend-to-disk enabled=&#39;no&#39;/&gt;  &lt;/pm&gt;  &lt;devices&gt;    &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt;    &lt;disk type=&#39;network&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;auth username=&#39;libvirt&#39;&gt;        &lt;secret type=&#39;ceph&#39; uuid=&#39;efade415-5404-4b67-bd1e-165c6f64cbba&#39;/&gt;      &lt;/auth&gt;      &lt;source protocol=&#39;rbd&#39; name=&#39;libvirt-pool/clone-vm1.img&#39;&gt;        &lt;host name=&#39;192.168.187.201&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.202&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.203&#39; port=&#39;6789&#39;/&gt;      &lt;/source&gt;      &lt;target dev=&#39;hda&#39; bus=&#39;virtio&#39;/&gt;    &lt;/disk&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-ehci1&#39;&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x7&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-uhci1&#39;&gt;      &lt;master startport=&#39;0&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x0&#39; multifunction=&#39;on&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-uhci2&#39;&gt;      &lt;master startport=&#39;2&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x1&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-uhci3&#39;&gt;      &lt;master startport=&#39;4&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x2&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;pci&#39; index=&#39;0&#39; model=&#39;pci-root&#39;/&gt;    &lt;controller type=&#39;ide&#39; index=&#39;0&#39;&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x01&#39; function=&#39;0x1&#39;/&gt;    &lt;/controller&gt;    &lt;interface type=&#39;network&#39;&gt;      &lt;mac address=&#39;52:54:00:bf:29:21&#39;/&gt;      &lt;source network=&#39;default&#39;/&gt;      &lt;model type=&#39;rtl8139&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x03&#39; function=&#39;0x0&#39;/&gt;    &lt;/interface&gt;    &lt;serial type=&#39;pty&#39;&gt;      &lt;target type=&#39;isa-serial&#39; port=&#39;0&#39;&gt;        &lt;model name=&#39;isa-serial&#39;/&gt;      &lt;/target&gt;    &lt;/serial&gt;    &lt;console type=&#39;pty&#39;&gt;      &lt;target type=&#39;serial&#39; port=&#39;0&#39;/&gt;    &lt;/console&gt;    &lt;input type=&#39;mouse&#39; bus=&#39;ps2&#39;/&gt;    &lt;input type=&#39;keyboard&#39; bus=&#39;ps2&#39;/&gt;    &lt;graphics type=&#39;vnc&#39; port=&#39;-1&#39; autoport=&#39;yes&#39; listen=&#39;0.0.0.0&#39; passwd=&#39;redhat&#39;&gt;      &lt;listen type=&#39;address&#39; address=&#39;0.0.0.0&#39;/&gt;    &lt;/graphics&gt;    &lt;video&gt;      &lt;model type=&#39;cirrus&#39; vram=&#39;16384&#39; heads=&#39;1&#39; primary=&#39;yes&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x02&#39; function=&#39;0x0&#39;/&gt;    &lt;/video&gt;    &lt;memballoon model=&#39;virtio&#39;&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x05&#39; function=&#39;0x0&#39;/&gt;    &lt;/memballoon&gt;  &lt;/devices&gt;&lt;/domain&gt;[root@node-1 qemu]# vim clone-vm2.xml&lt;!--WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BEOVERWRITTEN AND LOST. Changes to this xml configuration should be made using:  virsh edit demoor other application using the libvirt API.--&gt;&lt;domain type=&#39;kvm&#39;&gt;  &lt;name&gt;clone-vm2&lt;/name&gt;  &lt;memory unit=&#39;KiB&#39;&gt;1048576&lt;/memory&gt;  &lt;currentMemory unit=&#39;KiB&#39;&gt;1048576&lt;/currentMemory&gt;  &lt;vcpu placement=&#39;static&#39;&gt;1&lt;/vcpu&gt;  &lt;os&gt;    &lt;type arch=&#39;x86_64&#39; machine=&#39;pc-i440fx-rhel7.0.0&#39;&gt;hvm&lt;/type&gt;    &lt;boot dev=&#39;hd&#39;/&gt;  &lt;/os&gt;  &lt;features&gt;    &lt;acpi/&gt;    &lt;apic/&gt;  &lt;/features&gt;  &lt;cpu mode=&#39;custom&#39; match=&#39;exact&#39; check=&#39;partial&#39;&gt;    &lt;model fallback=&#39;allow&#39;&gt;Broadwell-noTSX-IBRS&lt;/model&gt;    &lt;feature policy=&#39;require&#39; name=&#39;md-clear&#39;/&gt;    &lt;feature policy=&#39;require&#39; name=&#39;spec-ctrl&#39;/&gt;    &lt;feature policy=&#39;require&#39; name=&#39;ssbd&#39;/&gt;  &lt;/cpu&gt;  &lt;clock offset=&#39;utc&#39;&gt;    &lt;timer name=&#39;rtc&#39; tickpolicy=&#39;catchup&#39;/&gt;    &lt;timer name=&#39;pit&#39; tickpolicy=&#39;delay&#39;/&gt;    &lt;timer name=&#39;hpet&#39; present=&#39;no&#39;/&gt;  &lt;/clock&gt;  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;  &lt;on_crash&gt;destroy&lt;/on_crash&gt;  &lt;pm&gt;    &lt;suspend-to-mem enabled=&#39;no&#39;/&gt;    &lt;suspend-to-disk enabled=&#39;no&#39;/&gt;  &lt;/pm&gt;  &lt;devices&gt;    &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt;    &lt;disk type=&#39;network&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;auth username=&#39;libvirt&#39;&gt;        &lt;secret type=&#39;ceph&#39; uuid=&#39;efade415-5404-4b67-bd1e-165c6f64cbba&#39;/&gt;      &lt;/auth&gt;      &lt;source protocol=&#39;rbd&#39; name=&#39;libvirt-pool/clone-vm2.img&#39;&gt;        &lt;host name=&#39;192.168.187.201&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.202&#39; port=&#39;6789&#39;/&gt;        &lt;host name=&#39;192.168.187.203&#39; port=&#39;6789&#39;/&gt;      &lt;/source&gt;      &lt;target dev=&#39;hda&#39; bus=&#39;virtio&#39;/&gt;    &lt;/disk&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-ehci1&#39;&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x7&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-uhci1&#39;&gt;      &lt;master startport=&#39;0&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x0&#39; multifunction=&#39;on&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-uhci2&#39;&gt;      &lt;master startport=&#39;2&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x1&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;usb&#39; index=&#39;0&#39; model=&#39;ich9-uhci3&#39;&gt;      &lt;master startport=&#39;4&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x04&#39; function=&#39;0x2&#39;/&gt;    &lt;/controller&gt;    &lt;controller type=&#39;pci&#39; index=&#39;0&#39; model=&#39;pci-root&#39;/&gt;    &lt;controller type=&#39;ide&#39; index=&#39;0&#39;&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x01&#39; function=&#39;0x1&#39;/&gt;    &lt;/controller&gt;    &lt;interface type=&#39;network&#39;&gt;      &lt;mac address=&#39;52:54:00:bf:29:21&#39;/&gt;      &lt;source network=&#39;default&#39;/&gt;      &lt;model type=&#39;rtl8139&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x03&#39; function=&#39;0x0&#39;/&gt;    &lt;/interface&gt;    &lt;serial type=&#39;pty&#39;&gt;      &lt;target type=&#39;isa-serial&#39; port=&#39;0&#39;&gt;        &lt;model name=&#39;isa-serial&#39;/&gt;      &lt;/target&gt;    &lt;/serial&gt;    &lt;console type=&#39;pty&#39;&gt;      &lt;target type=&#39;serial&#39; port=&#39;0&#39;/&gt;    &lt;/console&gt;    &lt;input type=&#39;mouse&#39; bus=&#39;ps2&#39;/&gt;    &lt;input type=&#39;keyboard&#39; bus=&#39;ps2&#39;/&gt;    &lt;graphics type=&#39;vnc&#39; port=&#39;-1&#39; autoport=&#39;yes&#39; listen=&#39;0.0.0.0&#39; passwd=&#39;redhat&#39;&gt;      &lt;listen type=&#39;address&#39; address=&#39;0.0.0.0&#39;/&gt;    &lt;/graphics&gt;    &lt;video&gt;      &lt;model type=&#39;cirrus&#39; vram=&#39;16384&#39; heads=&#39;1&#39; primary=&#39;yes&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x02&#39; function=&#39;0x0&#39;/&gt;    &lt;/video&gt;    &lt;memballoon model=&#39;virtio&#39;&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x05&#39; function=&#39;0x0&#39;/&gt;    &lt;/memballoon&gt;  &lt;/devices&gt;&lt;/domain&gt;</code></pre><h2><span id="87-引入xml文件">8.7、引入XML文件</span></h2><pre><code>[root@node-1 qemu]# virsh define clone-vm1.xml 定义域 clone-vm1（从 clone-vm1.xml）[root@node-1 qemu]# virsh define clone-vm2.xml 定义域 clone-vm2（从 clone-vm2.xml）</code></pre><h2><span id="88-开机并测试">8.8、开机并测试</span></h2><pre><code>[root@node-1 qemu]# virsh list --all  Id    名称                         状态---------------------------------------------------- 6     demo                           running -     clone-vm1                      关闭 -     clone-vm2                      关闭[root@node-1 qemu]# virsh start clone-vm1域 clone-vm1 已开始[root@node-1 qemu]# virsh start clone-vm2域 clone-vm2 已开始[root@node-1 qemu]# virsh domblklist clone-vm1 目标     源------------------------------------------------hda        libvirt-pool/clone-vm1.img[root@node-1 qemu]# virsh domblklist clone-vm2目标     源------------------------------------------------hda        libvirt-pool/clone-vm2.img[root@node-1 qemu]# virsh vncdisplay clone-vm1:1[root@node-1 qemu]# virsh vncdisplay clone-vm2:2</code></pre><p><img src="/images/Ceph/Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/12.jpg"><br><img src="/images/Ceph/Ceph%E4%B8%8EKVM%E9%9B%86%E6%88%90/13.jpg"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KVM </tag>
            
            <tag> Ceph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph管理和监控</title>
      <link href="/2023/04/20/Ceph/16,Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/"/>
      <url>/2023/04/20/Ceph/16,Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-mgr%E7%BB%84%E4%BB%B6%E6%A6%82%E8%BF%B0">一、MGR组件概述</a></li><li><a href="#%E4%BA%8C-%E6%94%AF%E6%8C%81%E7%9A%84%E6%A8%A1%E5%9D%97">二、支持的模块</a></li><li><a href="#%E4%B8%89-%E5%AE%89%E8%A3%85ceph-dashboard">三、安装Ceph-Dashboard</a></li><li><a href="#%E5%9B%9B-%E5%90%AF%E5%8A%A8dashboard%E6%A8%A1%E5%9D%97">四、启动Dashboard模块</a></li><li><a href="#%E4%BA%94-%E7%94%9F%E6%88%90%E8%87%AA%E7%AD%BE%E5%90%8D%E8%AF%81%E4%B9%A6">五、生成自签名证书</a></li><li><a href="#%E5%85%AD-%E9%85%8D%E7%BD%AE%E7%9B%91%E5%90%AC%E7%AB%AF%E5%8F%A3%E5%92%8Cip">六、配置监听端口和IP</a></li><li><a href="#%E4%B8%83-%E5%88%9B%E5%BB%BA%E7%AE%A1%E7%90%86%E5%91%98%E7%94%A8%E6%88%B7">七、创建管理员用户</a></li><li><a href="#%E5%85%AB-%E7%99%BB%E5%BD%95%E6%8E%A7%E5%88%B6%E5%8F%B0">八、登录控制台</a></li><li><a href="#%E4%B9%9D-%E4%BD%BF%E7%94%A8prometheus">九、使用PROMETHEUS</a><ul><li><a href="#1-%E5%90%AF%E7%94%A8prometheus%E6%A8%A1%E5%9D%97">1、启用PROMETHEUS模块</a></li><li><a href="#2-%E5%AE%89%E8%A3%85prometheus">2、安装PROMETHEUS</a></li><li><a href="#3-%E4%BD%BF%E7%94%A8prometheus">3、使用PROMETHEUS</a></li></ul></li><li><a href="#%E5%8D%81-%E4%BD%BF%E7%94%A8grafana%E8%BF%9B%E8%A1%8C%E5%9B%BE%E5%BD%A2%E5%B1%95%E7%A4%BA">十、使用Grafana进行图形展示</a><ul><li><a href="#1-%E5%AE%89%E8%A3%85">1、安装</a></li><li><a href="#2-%E7%99%BB%E5%BD%95">2、登录</a></li><li><a href="#3-%E9%85%8D%E7%BD%AEdata-source">3、配置DATA-source</a></li><li><a href="#4-%E8%B0%83%E7%94%A8ceph-grafana-dashboards%E8%87%AA%E5%B8%A6%E7%9A%84json%E6%96%87%E4%BB%B6">4、调用ceph-grafana-dashboards自带的JSON文件</a></li></ul></li></ul><!-- tocstop --><h1><span id="一-mgr组件概述">一、MGR组件概述</span></h1><p>Ceph Manager守护进程 (ceph-mgr) 与监控守护进程一起运行，为外部监控和管理系统提供额外的监控和接口。</p><p>自 12.x ( luminous ) Ceph 发布以来，正常操作需要 ceph-mgr 守护进程。ceph-mgr 守护进程是 11.x ( kraken ) Ceph 版本中的一个可选组件。</p><p>默认情况下，管理器守护进程不需要额外的配置，除了确保它正在运行。如果没有正在运行的 mgr 守护进程，您将看到一个健康警告，并且在启动mgr 之前， ceph 状态输出中的一些其他信息将丢失或过时。</p><p>使用常规部署工具，例如 ceph-ansible 或 ceph-deploy，在每个 mon 节点上设置 ceph-mgr 守护进程。将 mgr 守护进程与 mons 放在相同的节点上不是强制性的，但它几乎总是明智的。</p><pre><code>[root@node-1 ~]# ceph -s   cluster:    id:     b8e58b30-4568-4032-a9f4-837ed3fa9529    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 50m)    mgr: node-3(active, since 50m), standbys: node-1, node-2    mds: cephfs-demo:1 &#123;0=node-2=up:active&#125; 2 up:standby    osd: 6 osds: 6 up (since 49m), 6 in (since 49m)    rgw: 2 daemons active (node-1, node-2)   task status:   data:    pools:   11 pools, 264 pgs    objects: 1.05k objects, 2.5 GiB    usage:   14 GiB used, 46 GiB / 60 GiB avail    pgs:     264 active+clean   io:    client:   58 KiB/s rd, 0 B/s wr, 58 op/s rd, 38 op/s wr</code></pre><h1><span id="二-支持的模块">二、支持的模块</span></h1><ol><li>Installation and Configuration</li><li>Writing modules</li><li>Writing orchestrator plugins</li><li>Dashboard module</li><li>Alerts module #告警模块</li><li>DiskPrediction module</li><li>Local pool module</li><li>RESTful module</li><li>Zabbix module #zabbix模块</li><li>Prometheus module #普罗米修斯监控模块</li><li>Influx module</li><li>Hello module</li><li>Telegraf module</li><li>Telemetry module</li><li>Iostat module #实时展示集群的IO读写情况，需启用该模块</li><li>Crash module #分析Crash 报错信息并上报</li><li>Orchestrator CLI module</li><li>Rook module</li><li>DeepSea module</li><li>Insights module #巡检模块，能够文字的输出当前集群的一些运行状态</li><li>Ansible module #Ansible模块</li><li>SSH orchestrator</li></ol><h1><span id="三-安装ceph-dashboard">三、安装Ceph-Dashboard</span></h1><pre><code>[root@node-1 ~]# yum install ceph-mgr-dashboard -y  正在安装    : python2-jwt-1.6.1-1.el7.noarch                                                                                         1/3   正在安装    : 2:ceph-grafana-dashboards-14.2.22-0.el7.noarch                                                                         2/3   正在安装    : 2:ceph-mgr-dashboard-14.2.22-0.el7.noarch                                                                              3/3   验证中      : 2:ceph-grafana-dashboards-14.2.22-0.el7.noarch  #安装过程中自动安装了grafana的软件包                                                                       1/3   验证中      : python2-jwt-1.6.1-1.el7.noarch                                                                                         2/3   验证中      : 2:ceph-mgr-dashboard-14.2.22-0.el7.noarch                                                                              3/3 </code></pre><h1><span id="四-启动dashboard模块">四、启动Dashboard模块</span></h1><pre><code>[root@node-1 ~]# ceph mgr module enable dashboard #启动失败Error ENOENT: all mgr daemons do not support module &#39;dashboard&#39;, pass --force to force enablement[root@node-1 ~]# ceph mgr module enable dashboard --force #根据报错信息重启启动[root@node-1 ~]# ceph mgr module ls |less #查看启动情况&#123;    &quot;always_on_modules&quot;: [        &quot;balancer&quot;,        &quot;crash&quot;,        &quot;devicehealth&quot;,        &quot;orchestrator_cli&quot;,        &quot;progress&quot;,        &quot;rbd_support&quot;,        &quot;status&quot;,        &quot;volumes&quot;    ],    &quot;enabled_modules&quot;: [        &quot;dashboard&quot;, #在这里显示说明已经成功启用        &quot;iostat&quot;,        &quot;restful&quot;    ],</code></pre><h1><span id="五-生成自签名证书">五、生成自签名证书</span></h1><pre><code>[root@node-1 ~]# ceph dashboard create-self-signed-cert #如果执行失败，可重新执行ceph mgr module enable dashboard --force，确保mgr的主在当前节点Self-signed certificate created</code></pre><h1><span id="六-配置监听端口和ip">六、配置监听端口和IP</span></h1><pre><code>[root@node-1 ~]# ceph config set mgr mgr/dashboard/server_addr 192.168.187.201 #根据当前ceph的Mon地址进行修改[root@node-1 ~]# ceph config set mgr mgr/dashboard/server_port 8080 #默认就是8080，可修改[root@node-1 ~]# ceph config set mgr mgr/dashboard/ssl_server_port 8443 #默认就是8443，可修改[root@node-1 ~]# ceph mgr services #查看运行情况&#123;    &quot;dashboard&quot;: &quot;https://node-1:8443/&quot;&#125;</code></pre><h1><span id="七-创建管理员用户">七、创建管理员用户</span></h1><pre><code>[root@node-1 ~]# cat file-containing-password 980611[root@node-1 ~]# ceph dashboard ac-user-create ceph-admin -i file-containing-password administrator&#123;&quot;username&quot;: &quot;ceph-admin&quot;, &quot;lastUpdate&quot;: 1679209609, &quot;name&quot;: null, &quot;roles&quot;: [&quot;administrator&quot;], &quot;password&quot;: &quot;$2b$12$H7SV2apfmeA4fk7gZ.oTE.4bCmw9JucjwonB7wsRQp1lSQuOc2qHm&quot;, &quot;email&quot;: null&#125;</code></pre><h1><span id="八-登录控制台">八、登录控制台</span></h1><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/1.jpg"></p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/604584571">汉化教程</a></p></blockquote><h1><span id="九-使用prometheus">九、使用PROMETHEUS</span></h1><h2><span id="1-启用prometheus模块">1、启用PROMETHEUS模块</span></h2><pre><code>[root@node-1 ~]# ceph mgr module enable prometheus您在 /var/spool/mail/root 中有新邮件[root@node-1 ~]# netstat -ntlp | grep 9283tcp6       0      0 :::9283                 :::*                    LISTEN      1481/ceph-mgr       </code></pre><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/2.jpg"></p><h2><span id="2-安装prometheus">2、安装PROMETHEUS</span></h2><blockquote><p><a href="https://www.worldlink.com.cn/en/osdir/prometheus-rpm.html">参考文档</a></p></blockquote><pre><code>配置yum源：[root@node-1 ~]# cat /etc/yum.repos.d/prometheus.repo[prometheus]name=prometheusbaseurl=https://packagecloud.io/prometheus-rpm/release/el/$releasever/$basearchrepo_gpgcheck=1enabled=1gpgkey=https://packagecloud.io/prometheus-rpm/release/gpgkey       https://raw.githubusercontent.com/lest/prometheus-rpm/master/RPM-GPG-KEY-prometheus-rpmgpgcheck=0metadata_expire=300[root@node-1 ~]# yum makecache[root@node-1 ~]# yum install prometheus -y[root@node-1 ~]# systemctl restart prometheus[root@node-1 ~]# systemctl status prometheus● prometheus.service - The Prometheus monitoring system and time series database.   Loaded: loaded (/usr/lib/systemd/system/prometheus.service; enabled; vendor preset: disabled)   Active: active (running) since 日 2023-03-19 18:09:23 CST; 59s ago     Docs: https://prometheus.io Main PID: 11713 (prometheus)   CGroup: /system.slice/prometheus.service           └─11713 /usr/bin/prometheus -config.file=/etc/prometheus/prometheus.yml -storage.local.path=/var/lib/prometheus/data -web.con...3月 19 18:09:23 node-1 systemd[1]: Started The Prometheus monitoring system and time series database..3月 19 18:09:23 node-1 prometheus[11713]: time=&quot;2023-03-19T18:09:23+08:00&quot; level=info msg=&quot;Starting prometheus (version=1.8.2, b....go:87&quot;3月 19 18:09:23 node-1 prometheus[11713]: time=&quot;2023-03-19T18:09:23+08:00&quot; level=info msg=&quot;Build context (go=go1.9.2, user=root@....go:88&quot;3月 19 18:09:23 node-1 prometheus[11713]: time=&quot;2023-03-19T18:09:23+08:00&quot; level=info msg=&quot;Host details (Linux 3.10.0-1160.83.1.....go:89&quot;3月 19 18:09:23 node-1 prometheus[11713]: time=&quot;2023-03-19T18:09:23+08:00&quot; level=info msg=&quot;Loading configuration file /etc/prome...go:254&quot;3月 19 18:09:23 node-1 prometheus[11713]: time=&quot;2023-03-19T18:09:23+08:00&quot; level=info msg=&quot;Listening on :9090&quot; source=&quot;web.go:341&quot;3月 19 18:09:23 node-1 prometheus[11713]: time=&quot;2023-03-19T18:09:23+08:00&quot; level=info msg=&quot;Loading series map and head chunks......go:428&quot;3月 19 18:09:23 node-1 prometheus[11713]: time=&quot;2023-03-19T18:09:23+08:00&quot; level=info msg=&quot;0 series loaded.&quot; source=&quot;storage.go:439&quot;3月 19 18:09:23 node-1 prometheus[11713]: time=&quot;2023-03-19T18:09:23+08:00&quot; level=info msg=&quot;Starting target manager...&quot; source=&quot;t....go:63&quot;3月 19 18:09:23 node-1 prometheus[11713]: time=&quot;2023-03-19T18:09:23+08:00&quot; level=info msg=&quot;Server is Ready to receive requests.&quot;...go:230&quot;Hint: Some lines were ellipsized, use -l to show in full.[root@node-1 ~]# systemctl enable prometheusCreated symlink from /etc/systemd/system/multi-user.target.wants/prometheus.service to /usr/lib/systemd/system/prometheus.service.</code></pre><h2><span id="3-使用prometheus">3、使用PROMETHEUS</span></h2><pre><code>[root@node-1 prometheus]# vim /etc/prometheus/prometheus.yml #修改配置文件，新增如下内容  - job_name: &#39;ceph-exporter&#39;    honor_labels: true    static_configs:    - targets: [&#39;192.168.187.201:9283&#39;]      labels:        instance: ceph-exporter[root@node-1 prometheus]# systemctl restart prometheus.service  #重启服务[root@node-1 prometheus]# tail -n 10 /var/log/messages #查看日志无报错内容Mar 19 18:22:01 node-1 prometheus: time=&quot;2023-03-19T18:22:01+08:00&quot; level=info msg=&quot;Host details (Linux 3.10.0-1160.83.1.el7.x86_64 #1 SMP Wed Jan 25 16:41:43 UTC 2023 x86_64 node-1 (none))&quot; source=&quot;main.go:89&quot;Mar 19 18:22:01 node-1 prometheus: time=&quot;2023-03-19T18:22:01+08:00&quot; level=info msg=&quot;Loading configuration file /etc/prometheus/prometheus.yml&quot; source=&quot;main.go:254&quot;Mar 19 18:22:01 node-1 prometheus: time=&quot;2023-03-19T18:22:01+08:00&quot; level=info msg=&quot;Listening on :9090&quot; source=&quot;web.go:341&quot;Mar 19 18:22:01 node-1 prometheus: time=&quot;2023-03-19T18:22:01+08:00&quot; level=info msg=&quot;Loading series map and head chunks...&quot; source=&quot;storage.go:428&quot;Mar 19 18:22:01 node-1 prometheus: time=&quot;2023-03-19T18:22:01+08:00&quot; level=info msg=&quot;558 series loaded.&quot; source=&quot;storage.go:439&quot;Mar 19 18:22:01 node-1 prometheus: time=&quot;2023-03-19T18:22:01+08:00&quot; level=info msg=&quot;Server is Ready to receive requests.&quot; source=&quot;main.go:230&quot;Mar 19 18:22:01 node-1 prometheus: time=&quot;2023-03-19T18:22:01+08:00&quot; level=info msg=&quot;Starting target manager...&quot; source=&quot;targetmanager.go:63&quot;Mar 19 18:22:12 node-1 ceph-mgr: ::ffff:192.168.187.201 - - [19/Mar/2023:18:22:12] &quot;GET /metrics HTTP/1.1&quot; 200 166317 &quot;&quot; &quot;Prometheus/1.8.2&quot;Mar 19 18:22:27 node-1 ceph-mgr: ::ffff:192.168.187.201 - - [19/Mar/2023:18:22:27] &quot;GET /metrics HTTP/1.1&quot; 200 166316 &quot;&quot; &quot;Prometheus/1.8.2&quot;Mar 19 18:22:42 node-1 ceph-mgr: ::ffff:192.168.187.201 - - [19/Mar/2023:18:22:42] &quot;GET /metrics HTTP/1.1&quot; 200 166320 &quot;&quot; &quot;Prometheus/1.8.2&quot;      [root@node-1 prometheus]# netstat -ntlp | grep prome #查看监听端口为9090，使用浏览器访问看下tcp6       0      0 :::9090                 :::*                    LISTEN      13315/prometheus</code></pre><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/3.jpg"><br><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/4.jpg"></p><h1><span id="十-使用grafana进行图形展示">十、使用Grafana进行图形展示</span></h1><blockquote><p><a href="https://grafana.com/docs/grafana/latest/setup-grafana/installation/rpm/">官方文档</a><br><a href="https://mirrors.cloud.tencent.com/grafana/yum/rpm/Packages/">腾讯云软件地址</a></p></blockquote><h2><span id="1-安装">1、安装</span></h2><pre><code>[root@node-1 prometheus]# yum install https://mirrors.cloud.tencent.com/grafana/yum/rpm/Packages/grafana-5.4.2-1.x86_64.rpm #安装[root@node-1 prometheus]# sudo /bin/systemctl enable grafana-server.service #配置开机启动Created symlink from /etc/systemd/system/multi-user.target.wants/grafana-server.service to /usr/lib/systemd/system/grafana-server.service.[root@node-1 prometheus]# sudo /bin/systemctl start grafana-server.service #启动服务[root@node-1 prometheus]# netstat -antupl | grep grafana #查看监听端口tcp        0      0 192.168.187.201:58338   34.120.177.193:443      ESTABLISHED 14849/grafana-serve tcp6       0      0 :::3000                 :::*                    LISTEN      14849/grafana-serve </code></pre><h2><span id="2-登录">2、登录</span></h2><p><a href="http://192.168.187.201:3000/?orgId=1">http://192.168.187.201:3000/?orgId=1</a><br>用户名密码默认都是：admin</p><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/5.jpg"></p><p>强制修改密码后登录进入</p><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/6.jpg"></p><h2><span id="3-配置data-source">3、配置DATA-source</span></h2><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/7.jpg"></p><p>点击添加</p><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/8.jpg"></p><p>选择prometheus</p><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/9.jpg"></p><p>点击保存并测试</p><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/10.jpg"></p><p>返回上层目录可以看到创建成功</p><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/11.jpg"></p><h2><span id="4-调用ceph-grafana-dashboards自带的json文件">4、调用ceph-grafana-dashboards自带的JSON文件</span></h2><pre><code>[root@node-1 prometheus]# rpm -ql ceph-grafana-dashboards #查看软件包自带的JSON文件/etc/grafana/dashboards/ceph-dashboard/etc/grafana/dashboards/ceph-dashboard/ceph-cluster.json/etc/grafana/dashboards/ceph-dashboard/cephfs-overview.json/etc/grafana/dashboards/ceph-dashboard/host-details.json/etc/grafana/dashboards/ceph-dashboard/hosts-overview.json/etc/grafana/dashboards/ceph-dashboard/osd-device-details.json/etc/grafana/dashboards/ceph-dashboard/osds-overview.json/etc/grafana/dashboards/ceph-dashboard/pool-detail.json/etc/grafana/dashboards/ceph-dashboard/pool-overview.json/etc/grafana/dashboards/ceph-dashboard/radosgw-detail.json/etc/grafana/dashboards/ceph-dashboard/radosgw-overview.json/etc/grafana/dashboards/ceph-dashboard/radosgw-sync-overview.json/etc/grafana/dashboards/ceph-dashboard/rbd-details.json/etc/grafana/dashboards/ceph-dashboard/rbd-overview.json/usr/share/doc/ceph-grafana-dashboards-14.2.22/usr/share/doc/ceph-grafana-dashboards-14.2.22/README/usr/share/doc/ceph-grafana-dashboards-14.2.22/README.md[root@node-1 ceph-dashboard]# pwd 进入目录/etc/grafana/dashboards/ceph-dashboard[root@node-1 ceph-dashboard]# ll #将下面的json文件导出到本地电脑总用量 248-rw-r----- 1 root grafana 29577 6月  30 2021 ceph-cluster.json-rw-r----- 1 root grafana  6414 6月  30 2021 cephfs-overview.json-rw-r----- 1 root grafana 33133 6月  30 2021 host-details.json-rw-r----- 1 root grafana 22126 6月  30 2021 hosts-overview.json-rw-r----- 1 root grafana 19493 6月  30 2021 osd-device-details.json-rw-r----- 1 root grafana 21210 6月  30 2021 osds-overview.json-rw-r----- 1 root grafana 15029 6月  30 2021 pool-detail.json-rw-r----- 1 root grafana 17174 6月  30 2021 pool-overview.json-rw-r----- 1 root grafana 11655 6月  30 2021 radosgw-detail.json-rw-r----- 1 root grafana 14377 6月  30 2021 radosgw-overview.json-rw-r----- 1 root grafana  9399 6月  30 2021 radosgw-sync-overview.json-rw-r----- 1 root grafana  9379 6月  30 2021 rbd-details.json-rw-r----- 1 root grafana 15879 6月  30 2021 rbd-overview.json[root@node-1 ceph-dashboard]# zip ceph-json.zip ./*  adding: ceph-cluster.json (deflated 87%)  adding: cephfs-overview.json (deflated 78%)  adding: host-details.json (deflated 88%)  adding: hosts-overview.json (deflated 87%)  adding: osd-device-details.json (deflated 90%)  adding: osds-overview.json (deflated 86%)  adding: pool-detail.json (deflated 85%)  adding: pool-overview.json (deflated 88%)  adding: radosgw-detail.json (deflated 84%)  adding: radosgw-overview.json (deflated 87%)  adding: radosgw-sync-overview.json (deflated 85%)  adding: rbd-details.json (deflated 85%)  adding: rbd-overview.json (deflated 88%)[root@node-1 ceph-dashboard]# sz ceph-json.zip #使用sz命令导出，如没有此命令可安装lrzsz软件包</code></pre><p>查看已经全部下载到电脑本地</p><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/12.jpg"></p><p>将所有json文件上传</p><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/13.jpg"></p><p>上传完成后可以看到全部文件，随意点击即可获取数值，目前看有个别的json文件有问题，无法获取数据，可以自行查找原因</p><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/14.jpg"></p><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/15.jpg"></p><p>可以使用第三方的模板，例如：<a href="https://grafana.com/grafana/dashboards/917-ceph-cluster/">https://grafana.com/grafana/dashboards/917-ceph-cluster/</a></p><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/16.jpg"></p><p>有网络的情况可以直接使用ID进行导入，没有网络可下载JSON文件，离线导入即可<br>输入：917后即可导入，需修改名称，防止重复</p><p><img src="/images/Ceph/Ceph%E7%AE%A1%E7%90%86%E5%92%8C%E7%9B%91%E6%8E%A7/17.jpg"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KVM虚拟化进阶之基于iSCSI的KVM群集构建</title>
      <link href="/2023/04/19/KVM/6.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/"/>
      <url>/2023/04/19/KVM/6.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#1-%E8%A7%84%E5%88%92%E8%AE%BE%E8%AE%A1">1、规划设计</a></li><li><a href="#2-%E8%8A%82%E7%82%B9%E5%87%86%E5%A4%87">2、节点准备</a><ul><li><a href="#1-%E9%98%B6%E6%AE%B51%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85">1、阶段1：操作系统安装</a></li><li><a href="#2-%E9%98%B6%E6%AE%B52%E7%BE%A4%E9%9B%86%E7%BB%84%E4%BB%B6%E5%AE%89%E8%A3%85">2、阶段2：群集组件安装</a></li><li><a href="#3-%E9%98%B6%E6%AE%B53%E7%BE%A4%E9%9B%86%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85">3、阶段3：群集节点安装</a></li></ul></li><li><a href="#3-%E9%85%8D%E7%BD%AEiscsi-target">3、配置iSCSI Target</a></li><li><a href="#4-%E9%85%8D%E7%BD%AEstonithdisk">4、配置STONITH（DISK）</a></li><li><a href="#5-%E9%85%8D%E7%BD%AEdlm">5、配置DLM</a></li><li><a href="#6-%E9%85%8D%E7%BD%AEclvm">6、配置CLVM</a></li><li><a href="#7-%E9%85%8D%E7%BD%AEgfs2">7、配置GFS2</a></li><li><a href="#8-%E5%90%91%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%B7%BB%E5%8A%A0%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%B5%84%E6%BA%90">8、向集群中添加虚拟机资源</a><ul><li><a href="#1-%E5%B0%86%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E5%88%B0iscsi%E7%9A%84%E5%85%B1%E4%BA%AB%E7%9B%AE%E5%BD%95">1、将虚拟机数据迁移到iSCSI的共享目录</a></li><li><a href="#2-%E6%B5%8B%E8%AF%95%E5%8A%A8%E6%80%81%E8%BF%81%E7%A7%BB">2、测试动态迁移</a></li></ul></li></ul><!-- tocstop --><p>基于iSCSI的KVM群集构建</p><blockquote><p>群集节点约束：DLM &gt; CLVM &gt; File System &gt; Virtual Domain</p></blockquote><h1><span id="1-规划设计">1、规划设计</span></h1><table><thead><tr><th>主机名称</th><th>管理地址</th><th>心跳地址</th><th>存储地址</th></tr></thead><tbody><tr><td>node1</td><td>192.168.200.200</td><td>192.168.0.200</td><td>192.168.1.200</td></tr><tr><td>node2</td><td>192.168.200.201</td><td>192.168.0.201</td><td>192.168.1.201</td></tr><tr><td>stor1</td><td>192.168.200.202</td><td></td><td>192.168.1.202</td></tr></tbody></table><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/80.jpg"></p><h1><span id="2-节点准备">2、节点准备</span></h1><h2><span id="1-阶段1操作系统安装">1、阶段1：操作系统安装</span></h2><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/81.jpg"></p><h2><span id="2-阶段2群集组件安装">2、阶段2：群集组件安装</span></h2><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/82.jpg"></p><h2><span id="3-阶段3群集节点安装">3、阶段3：群集节点安装</span></h2><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/83.jpg"></p><blockquote><p>需要注意各个节点的防火墙配置！</p></blockquote><h1><span id="3-配置iscsi-target">3、配置iSCSI Target</span></h1><p><strong>在stor1服务器中安装targetcli</strong></p><pre><code>[root@stor1 ~]# yum install -y targetcli[root@stor1 ~]# targetcliWarning: Could not load preferences file /root/.targetcli/prefs.bin.targetcli shell version 2.1.53Copyright 2011-2013 by Datera, Inc and others.For help on commands, type &#39;help&#39;./&gt; /&gt; /&gt; lso- / ......................................................................................................................... [...]  o- backstores .............................................................................................................. [...]  | o- block .................................................................................................. [Storage Objects: 0]  | o- fileio ................................................................................................. [Storage Objects: 0]  | o- pscsi .................................................................................................. [Storage Objects: 0]  | o- ramdisk ................................................................................................ [Storage Objects: 0]  o- iscsi ............................................................................................................ [Targets: 0]  o- loopback ......................................................................................................... [Targets: 0]/&gt; [root@stor1 ~]# firewall-cmd --add-service=iscsi-target --permanentsuccess[root@stor1 ~]# firewall-cmd --reloadsuccess[root@stor1 ~]# fdisk -l Disk /dev/sdb: 42.9 GB, 42949672960 bytes, 83886080 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes[root@stor1 ~]# fdisk /dev/sdb Welcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Device does not contain a recognized partition tableBuilding a new DOS disklabel with disk identifier 0x923327a1.Command (m for help): pDisk /dev/sdb: 42.9 GB, 42949672960 bytes, 83886080 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x923327a1   Device Boot      Start         End      Blocks   Id  SystemCommand (m for help): nPartition type:   p   primary (0 primary, 0 extended, 4 free)   e   extendedSelect (default p): pPartition number (1-4, default 1): First sector (2048-83886079, default 2048): Using default value 2048Last sector, +sectors or +size&#123;K,M,G&#125; (2048-83886079, default 83886079): Using default value 83886079Partition 1 of type Linux and of size 40 GiB is setCommand (m for help): pDisk /dev/sdb: 42.9 GB, 42949672960 bytes, 83886080 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x923327a1   Device Boot      Start         End      Blocks   Id  System/dev/sdb1            2048    83886079    41942016   83  LinuxCommand (m for help): tSelected partition 1Hex code (type L to list all codes): 8eChanged type of partition &#39;Linux&#39; to &#39;Linux LVM&#39;Command (m for help): pDisk /dev/sdb: 42.9 GB, 42949672960 bytes, 83886080 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x923327a1   Device Boot      Start         End      Blocks   Id  System/dev/sdb1            2048    83886079    41942016   8e  Linux LVMCommand (m for help): wThe partition table has been altered!Calling ioctl() to re-read partition table.Syncing disks.[root@stor1 ~]# pvcreate /dev/sbd1  Device /dev/sbd1 not found.[root@stor1 ~]# pvcreate /dev/sdb1  Physical volume &quot;/dev/sdb1&quot; successfully created.[root@stor1 ~]# vgcreate stor1 /dev/sdb1  Volume group &quot;stor1&quot; successfully created[root@stor1 ~]# lvcreate -l 100%FREE -n lvstor1 stor1  Logical volume &quot;lvstor1&quot; created.[root@stor1 ~]# lvscan   ACTIVE            &#39;/dev/stor1/lvstor1&#39; [&lt;40.00 GiB] inherit  ACTIVE            &#39;/dev/centos/swap&#39; [&lt;3.88 GiB] inherit  ACTIVE            &#39;/dev/centos/root&#39; [&lt;35.12 GiB] inherit[root@stor1 ~]# mkfs.xfs /dev/stor1/lvstor1 meta-data=/dev/stor1/lvstor1     isize=512    agcount=4, agsize=2621184 blks         =                       sectsz=512   attr=2, projid32bit=1         =                       crc=1        finobt=0, sparse=0data     =                       bsize=4096   blocks=10484736, imaxpct=25         =                       sunit=0      swidth=0 blksnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1log      =internal log           bsize=4096   blocks=5119, version=2         =                       sectsz=512   sunit=0 blks, lazy-count=1realtime =none                   extsz=4096   blocks=0, rtextents=0[root@stor1 ~]# mkdir /stor1[root@stor1 ~]# mount /dev/stor1/lvstor1 /stor1/[root@stor1 ~]# targetclitargetcli shell version 2.1.53Copyright 2011-2013 by Datera, Inc and others.For help on commands, type &#39;help&#39;./&gt; lso- / ......................................................................................................................... [...]  o- backstores .............................................................................................................. [...]  | o- block .................................................................................................. [Storage Objects: 0]  | o- fileio ................................................................................................. [Storage Objects: 0]  | o- pscsi .................................................................................................. [Storage Objects: 0]  | o- ramdisk ................................................................................................ [Storage Objects: 0]  o- iscsi ............................................................................................................ [Targets: 0]  o- loopback ......................................................................................................... [Targets: 0]/&gt; /&gt; cd backstores/fileio/backstores/fileio&gt; create disk01 /stor1/disk01.img 1GCreated fileio disk01 with size 1073741824/backstores/fileio&gt; lso- fileio ..................................................................................................... [Storage Objects: 1]  o- disk01 .................................................................... [/stor1/disk01.img (1.0GiB) write-back deactivated]    o- alua ....................................................................................................... [ALUA Groups: 1]      o- default_tg_pt_gp ........................................................................... [ALUA state: Active/optimized]/backstores/fileio&gt; create disk02 /stor1/disk02.img 20GCreated fileio disk02 with size 21474836480/backstores/fileio&gt; lso- fileio ..................................................................................................... [Storage Objects: 2]  o- disk01 .................................................................... [/stor1/disk01.img (1.0GiB) write-back deactivated]  | o- alua ....................................................................................................... [ALUA Groups: 1]  |   o- default_tg_pt_gp ........................................................................... [ALUA state: Active/optimized]  o- disk02 ................................................................... [/stor1/disk02.img (20.0GiB) write-back deactivated]    o- alua ....................................................................................................... [ALUA Groups: 1]      o- default_tg_pt_gp ........................................................................... [ALUA state: Active/optimized]/iscsi&gt; cd /iscsi//iscsi&gt; create iqn.2016-10.org.linuxplus.srv:target00Created target iqn.2016-10.org.linuxplus.srv:target00.Created TPG 1.Global pref auto_add_default_portal=trueCreated default portal listening on all IPs (0.0.0.0), port 3260./iscsi&gt; cd iqn.2016-10.org.linuxplus.srv:target00/tpg1/luns/iscsi/iqn.20...t00/tpg1/luns&gt;  create /backstores/fileio/disk0/backstores/fileio/disk01  /backstores/fileio/disk02  /iscsi/iqn.20...t00/tpg1/luns&gt;  create /backstores/fileio/disk01Created LUN 0./iscsi/iqn.20...t00/tpg1/luns&gt;  create /backstores/fileio/disk02Created LUN 1./iscsi/iqn.20...t00/tpg1/luns&gt; lso- luns .................................................................................................................. [LUNs: 2]  o- lun0 ................................................................... [fileio/disk01 (/stor1/disk01.img) (default_tg_pt_gp)]  o- lun1 ................................................................... [fileio/disk02 (/stor1/disk02.img) (default_tg_pt_gp)]/iscsi/iqn.20...t00/tpg1/luns&gt; cd ../acls [root@node1 ~]# yum install icssi-initiarot-utils #在node1和node2节点执行#在node1执行[root@node1 ~]# cat /etc/iscsi/initiatorname.iscsi InitiatorName=iqn.1994-05.com.redhat:50adb21c54[root@node1 ~]# vim /etc/iscsi/initiatorname.iscsi [root@node1 ~]# cat /etc/iscsi/initiatorname.iscsi InitiatorName=iqn.1994-05.com.redhat:node1#在node2执行[root@node2 ~]# cat /etc/iscsi/initiatorname.iscsi InitiatorName=iqn.1994-05.com.redhat:6b7ee6fb2748[root@node2 ~]# vim /etc/iscsi/initiatorname.iscsi [root@node2 ~]# cat /etc/iscsi/initiatorname.iscsiInitiatorName=iqn.1994-05.com.redhat:node2回到stor1服务器中执行：/iscsi/iqn.20...t00/tpg1/acls&gt; create iqn.1994-05.com.redhat:node1Created Node ACL for iqn.1994-05.com.redhat:node1Created mapped LUN 1.Created mapped LUN 0./iscsi/iqn.20...t00/tpg1/acls&gt; create iqn.1994-05.com.redhat:node2Created Node ACL for iqn.1994-05.com.redhat:node2Created mapped LUN 1.Created mapped LUN 0./iscsi/iqn.20...t00/tpg1/acls&gt; ls /o- / ......................................................................................................................... [...]  o- backstores .............................................................................................................. [...]  | o- block .................................................................................................. [Storage Objects: 0]  | o- fileio ................................................................................................. [Storage Objects: 2]  | | o- disk01 .................................................................. [/stor1/disk01.img (1.0GiB) write-back activated]  | | | o- alua ................................................................................................... [ALUA Groups: 1]  | | |   o- default_tg_pt_gp ....................................................................... [ALUA state: Active/optimized]  | | o- disk02 ................................................................. [/stor1/disk02.img (20.0GiB) write-back activated]  | |   o- alua ................................................................................................... [ALUA Groups: 1]  | |     o- default_tg_pt_gp ....................................................................... [ALUA state: Active/optimized]  | o- pscsi .................................................................................................. [Storage Objects: 0]  | o- ramdisk ................................................................................................ [Storage Objects: 0]  o- iscsi ............................................................................................................ [Targets: 1]  | o- iqn.2016-10.org.linuxplus.srv:target00 ............................................................................ [TPGs: 1]  |   o- tpg1 ............................................................................................... [no-gen-acls, no-auth]  |     o- acls .......................................................................................................... [ACLs: 2]  |     | o- iqn.1994-05.com.redhat:node1 ......................................................................... [Mapped LUNs: 2]  |     | | o- mapped_lun0 ............................................................................... [lun0 fileio/disk01 (rw)]  |     | | o- mapped_lun1 ............................................................................... [lun1 fileio/disk02 (rw)]  |     | o- iqn.1994-05.com.redhat:node2 ......................................................................... [Mapped LUNs: 2]  |     |   o- mapped_lun0 ............................................................................... [lun0 fileio/disk01 (rw)]  |     |   o- mapped_lun1 ............................................................................... [lun1 fileio/disk02 (rw)]  |     o- luns .......................................................................................................... [LUNs: 2]  |     | o- lun0 ........................................................... [fileio/disk01 (/stor1/disk01.img) (default_tg_pt_gp)]  |     | o- lun1 ........................................................... [fileio/disk02 (/stor1/disk02.img) (default_tg_pt_gp)]  |     o- portals .................................................................................................... [Portals: 1]  |       o- 0.0.0.0:3260 ..................................................................................................... [OK]  o- loopback ......................................................................................................... [Targets: 0]node1执行：[root@node1 ~]# iscsiadm --mode discovery --type sendtargets --portal 192.168.1.202 #扫描后端存储192.168.1.202:3260,1 iqn.2016-10.org.linuxplus.srv:target00[root@node1 ~]# iscsiadm -m node --login d 2 Logging in to [iface: default, target: iqn.2016-10.org.linuxplus.srv:target00, portal: 192.168.1.202,3260] (multiple)Login to [iface: default, target: iqn.2016-10.org.linuxplus.srv:target00, portal: 192.168.1.202,3260] successful.[root@node1 ~]# fdisk -l |grep sdDisk /dev/sda: 42.9 GB, 42949672960 bytes, 83886080 sectors/dev/sda1   *        2048     2099199     1048576   83  Linux/dev/sda2         2099200    83886079    40893440   8e  Linux LVMDisk /dev/sdb: 1073 MB, 1073741824 bytes, 2097152 sectorsDisk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectorsnode2执行：[root@node2 ~]#  iscsiadm --mode discovery --type sendtargets --portal 192.168.1.202192.168.1.202:3260,1 iqn.2016-10.org.linuxplus.srv:target00[root@node2 ~]# iscsiadm -m node --login d 2 Logging in to [iface: default, target: iqn.2016-10.org.linuxplus.srv:target00, portal: 192.168.1.202,3260] (multiple)Login to [iface: default, target: iqn.2016-10.org.linuxplus.srv:target00, portal: 192.168.1.202,3260] successful.[root@node2 ~]# fdisk -l | grep sdDisk /dev/sda: 42.9 GB, 42949672960 bytes, 83886080 sectors/dev/sda1   *        2048     2099199     1048576   83  Linux/dev/sda2         2099200    83886079    40893440   8e  Linux LVMDisk /dev/sdb: 1073 MB, 1073741824 bytes, 2097152 sectorsDisk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectors</code></pre><h1><span id="4-配置stonithdisk">4、配置STONITH（DISK）</span></h1><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/93.jpg"></p><p>node1执行：</p><pre><code>[root@node1 ~]# pcs stonith describe fence_scsifence_scsi - Fence agent for SCSI persistent reservationfence_scsi is an I/O fencing agent that uses SCSI-3 persistent reservations to control access to shared storage devices. These devices must support SCSI-3 persistent reservations (SPC-3 or greater) as well as the &quot;preempt-and-abort&quot; subcommand.The fence_scsi agent works by having each node in the cluster register a unique key with the SCSI device(s). Once registered, a single node will become the reservation holder by creating a &quot;write exclusive, registrants only&quot; reservation on the device(s). The result is that only registered nodes may write to the device(s). When a node failure occurs, the fence_scsi agent will remove the key belonging to the failed node from the device(s). The failed node will no longer be able to write to the device(s). A manual reboot is required.When used as a watchdog device you can define e.g. retry=1, retry-sleep=2 and verbose=yes parameters in /etc/sysconfigtonith if you have issues with it failing.Stonith options:  aptpl: Use the APTPL flag for registrations. This option is only used for the &#39;on&#39; action.  devices: List of devices to use for current operation. Devices can be comma-separated list of raw devices (eg. /dev/sdc). Each device must           support SCSI-3 persistent reservations.  key: Key to use for the current operation. This key should be unique to a node. For the &quot;on&quot; action, the key specifies the key use to       register the local node. For the &quot;off&quot; action, this key specifies the key to be removed from the device(s).  port: Name of the node to be fenced. The node name is used to generate the key value used for the current operation. This option will be        ignored when used with the -k option.  logfile: Log output (stdout and stderr) to file  quiet: Disable logging to stderr. Does not affect --verbose or --debug-file or logging to syslog.  verbose: Verbose mode  debug: Write debug information to given file  delay: Wait X seconds before fencing is started  login_timeout: Wait X seconds for cmd prompt after login  power_timeout: Test X seconds for status change after ON/OFF  power_wait: Wait X seconds after issuing ON/OFF  shell_timeout: Wait X seconds for cmd prompt after issuing command  retry_on: Count of attempts to retry power on  corosync_cmap_path: Path to corosync-cmapctl binary  sg_persist_path: Path to sg_persist binary  sg_turs_path: Path to sg_turs binary  vgs_path: Path to vgs binary  pcmk_host_map: A mapping of host names to ports numbers for devices that do not support host names. Eg. node1:1;node2:2,3 would tell the                 cluster to use port 1 for node1 and ports 2 and 3 for node2  pcmk_host_list: A list of machines controlled by this device (Optional unless pcmk_host_check=static-list).  pcmk_host_check: How to determine which machines are controlled by the device. Allowed values: dynamic-list (query the device via the                   &#39;list&#39; command), static-list (check the pcmk_host_list attribute), status (query the device via the &#39;status&#39; command),                   none (assume every device can fence every machine)  pcmk_delay_max: Enable a random delay for stonith actions and specify the maximum of random delay. This prevents double fencing when using                  slow devices such as sbd. Use this to enable a random delay for stonith actions. The overall delay is derived from this                  random delay value adding a static delay so that the sum is kept below the maximum delay.  pcmk_delay_base: Enable a base delay for stonith actions and specify base delay value. This prevents double fencing when different delays                   are configured on the nodes. Use this to enable a static delay for stonith actions. The overall delay is derived from a                   random delay value adding this static delay so that the sum is kept below the maximum delay.  pcmk_action_limit: The maximum number of actions can be performed in parallel on this device Pengine property concurrent-fencing=true needs                     to be configured first. Then use this to specify the maximum number of actions can be performed in parallel on this                     device. -1 is unlimited.Default operations:  monitor: interval=60s[root@node1 ~]# ll /dev/disk/by-id/|grep sdblrwxrwxrwx. 1 root root  9 Apr 15 19:36 scsi-36001405ac1f6f3fd0704cfab66c86851 -&gt; ../../sdblrwxrwxrwx. 1 root root  9 Apr 15 19:36 wwn-0x6001405ac1f6f3fd0704cfab66c86851 -&gt; ../../sdb[root@node1 ~]# pcs stonith create scsi-shooter fence_scsi \pcmk_host_list=&quot;node1-heartbeat node2-heartbeat&quot; \devices=&quot;/dev/disk/by-id/wwn-0x6001405ac1f6f3fd0704cfab66c86851&quot; \meta provides=unfencing[root@node1 ~]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node1-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 20:48:15 2023Last change: Sat Apr 15 20:46:56 2023 by root via cibadmin on node1-heartbeat2 nodes configured1 resource instance configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: scsi-shooter   (stonith:fence_scsi):   Started node1-heartbeatDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled</code></pre><h1><span id="5-配置dlm">5、配置DLM</span></h1><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/94.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# yum install -y gfs2-utils dlm[root@node1 ~]# rpm -qi gfs2-utilsName        : gfs2-utilsVersion     : 3.1.10Release     : 11.el7_9.1Architecture: x86_64Install Date: Sat 15 Apr 2023 08:52:22 PM CSTGroup       : System Environment/KernelSize        : 1006104License     : GPLv2+ and LGPLv2+Signature   : RSA/SHA256, Wed 18 Nov 2020 10:17:32 PM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : gfs2-utils-3.1.10-11.el7_9.1.src.rpmBuild Date  : Tue 17 Nov 2020 12:16:58 AM CSTBuild Host  : x86-01.bsys.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : https://pagure.io/gfs2-utilsSummary     : Utilities for managing the global file system (GFS2)Description :The gfs2-utils package contains a number of utilities for creating,checking, modifying, and correcting any inconsistencies in GFS2file systems.[root@node1 ~]# rpm -qi dlmName        : dlmVersion     : 4.0.7Release     : 1.el7Architecture: x86_64Install Date: Sat 15 Apr 2023 08:52:22 PM CSTGroup       : System Environment/KernelSize        : 176589License     : GPLv2 and GPLv2+ and LGPLv2+Signature   : RSA/SHA256, Thu 10 Aug 2017 11:37:37 PM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : dlm-4.0.7-1.el7.src.rpmBuild Date  : Fri 04 Aug 2017 01:33:37 AM CSTBuild Host  : c1bm.rdu2.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : https://fedorahosted.org/clusterSummary     : dlm control daemon and toolDescription :The kernel dlm requires a user daemon to control membership.</code></pre><p>node2:</p><pre><code>[root@node2 ~]# yum install -y gfs2-utils dlm</code></pre><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/95.jpg"></p><blockquote><p>clone-max&#x3D;2是指最多两个dlm，clone-host-max&#x3D;1 是指每个节点最多启动一个dlm</p></blockquote><p>node1:</p><pre><code>[root@node1 ~]# pcs resource create dlm ocf:pacemaker:controld \op monitor interval=30s on-fail=fence \clone interleave=true ordered=true[root@node1 ~]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node1-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 21:04:42 2023Last change: Sat Apr 15 21:02:15 2023 by root via cibadmin on node1-heartbeat2 nodes configured3 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: scsi-shooter   (stonith:fence_scsi):   Started node1-heartbeat Clone Set: dlm-clone [dlm]     Started: [ node1-heartbeat node2-heartbeat ]Daemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled</code></pre><h1><span id="6-配置clvm">6、配置CLVM</span></h1><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/96.jpg"><br><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/97.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# yum install -y lvm2-cluster.x86_64[root@node1 ~]# cat /etc/lvm/lvm.conf |grep -v &quot;#&quot; | grep -v ^$ | grep -A 25 globalglobal &#123;    umask = 077    test = 0    units = &quot;r&quot;    si_unit_consistency = 1    suffix = 1    activation = 1    proc = &quot;/proc&quot;    etc = &quot;/etc&quot;    locking_type = 1     wait_for_locks = 1    fallback_to_clustered_locking = 1    fallback_to_local_locking = 1    locking_dir = &quot;/run/lock/lvm&quot;    prioritise_write_locks = 1    abort_on_internal_errors = 0    metadata_read_only = 0    mirror_segtype_default = &quot;raid1&quot;    raid10_segtype_default = &quot;raid10&quot;    sparse_segtype_default = &quot;thin&quot;    use_lvmetad = 1    use_lvmlockd = 0    system_id_source = &quot;none&quot;    use_lvmpolld = 1    notify_dbus = 1&#125;[root@node1 ~]# whatis lvmconflvmconf (8)          - LVM configuration modifier[root@node1 ~]# lvmconf --help Usage: /usr/sbin/lvmconf &lt;command&gt;Commands:Enable clvm:  --enable-cluster [--lockinglibdir &lt;dir&gt;] [--lockinglib &lt;lib&gt;]Disable clvm: --disable-clusterEnable halvm: --enable-halvmDisable halvm: --disable-halvmSet locking library: --lockinglibdir &lt;dir&gt; [--lockinglib &lt;lib&gt;]Global options:Config file location: --file &lt;configfile&gt;Set services: --services [--mirrorservice] [--startstopservices]Use the separate command &#39;lvmconfig&#39; to display configuration information[root@node1 ~]# lvmconf --enable-clusterglobal &#123;    umask = 077    test = 0    units = &quot;r&quot;    si_unit_consistency = 1    suffix = 1    activation = 1    proc = &quot;/proc&quot;    etc = &quot;/etc&quot;    locking_type = 3 #通过上述命令，这个配置由1改成了3    wait_for_locks = 1    fallback_to_clustered_locking = 1    fallback_to_local_locking = 1    locking_dir = &quot;/run/lock/lvm&quot;    prioritise_write_locks = 1    abort_on_internal_errors = 0    metadata_read_only = 0    mirror_segtype_default = &quot;raid1&quot;    raid10_segtype_default = &quot;raid10&quot;    sparse_segtype_default = &quot;thin&quot;    use_lvmetad = 0    use_lvmlockd = 0    system_id_source = &quot;none&quot;    use_lvmpolld = 1    notify_dbus = 1&#125;[root@node1 ~]# reboot</code></pre><p>node2:</p><pre><code>[root@node2 ~]# yum install -y lvm2-cluster.x86_64[root@node2 ~]# lvmconf --enable-cluster[root@node2 ~]# reboot[root@node1 ~]# pcs cluster start --all node1-heartbeat: Starting Cluster (corosync)...node2-heartbeat: Starting Cluster (corosync)...node2-heartbeat: Starting Cluster (pacemaker)...node1-heartbeat: Starting Cluster (pacemaker)...[root@node1 ~]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 21:21:05 2023Last change: Sat Apr 15 21:02:15 2023 by root via cibadmin on node1-heartbeat2 nodes configured3 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: scsi-shooter   (stonith:fence_scsi):   Stopped Clone Set: dlm-clone [dlm]     Stopped: [ node1-heartbeat node2-heartbeat ]Daemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled</code></pre><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/98.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# pcs resource describe clvmAssumed agent name &#39;ocf:heartbeat:clvm&#39; (deduced from &#39;clvm&#39;)ocf:heartbeat:clvm - clvmdThis agent manages the clvmd daemon.Resource options:  with_cmirrord: Start with cmirrord (cluster mirror log daemon).  daemon_options: Options to clvmd. Refer to clvmd.8 for detailed descriptions.  activate_vgs: Whether or not to activate all cluster volume groups after starting the clvmd or not. Note that clustered volume groups will always be                deactivated before the clvmd stops regardless of what this option is set to.  exclusive: If set, only exclusive volume groups will be monitored.Default operations:  start: interval=0s timeout=90s  stop: interval=0s timeout=90s  monitor: interval=30s timeout=90s[root@node1 ~]# pcs resource create clvmd ocf:heartbeat:clvm op monitor interval=30s \on-fail=fence clone interleave=true ordered=true[root@node1 ~]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 21:25:06 2023Last change: Sat Apr 15 21:24:29 2023 by root via cibadmin on node1-heartbeat2 nodes configured5 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: scsi-shooter   (stonith:fence_scsi):   Started node1-heartbeat Clone Set: dlm-clone [dlm]     Started: [ node1-heartbeat node2-heartbeat ] Clone Set: clvmd-clone [clvmd]     Started: [ node1-heartbeat node2-heartbeat ]Daemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled[root@node1 ~]# systemctl status pacemaker.service ● pacemaker.service - Pacemaker High Availability Cluster Manager   Loaded: loaded (/usr/lib/systemd/system/pacemaker.service; disabled; vendor preset: disabled)   Active: active (running) since Sat 2023-04-15 21:20:42 CST; 6min ago     Docs: man:pacemakerd           https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/1.1/html-single/Pacemaker_Explained/index.html Main PID: 2074 (pacemakerd)   CGroup: /system.slice/pacemaker.service           ├─2074 /usr/sbin/pacemakerd -f           ├─2075 /usr/libexec/pacemaker/cib           ├─2076 /usr/libexec/pacemaker/stonithd           ├─2077 /usr/libexec/pacemaker/lrmd           ├─2078 /usr/libexec/pacemaker/attrd           ├─2079 /usr/libexec/pacemaker/pengine           ├─2080 /usr/libexec/pacemaker/crmd           ├─2231 dlm_controld -s 0           └─2563 /usr/sbin/clvmd -T90 -d0 #可以看到多了clvmd的线程，依赖于dlmApr 15 21:21:05 node1 stonith-ng[2076]:   notice: Operation &#39;on&#39; targeting node1-heartbeat on node1-heartbeat for crmd.2056@node2-heartbeat.f507774c: OKApr 15 21:21:05 node1 crmd[2080]:   notice: node1-heartbeat was successfully unfenced by node1-heartbeat (at the request of node2-heartbeat)Apr 15 21:21:06 node1 crmd[2080]:   notice: Result of probe operation for scsi-shooter on node1-heartbeat: 7 (not running)Apr 15 21:21:06 node1 crmd[2080]:   notice: Result of probe operation for dlm on node1-heartbeat: 7 (not running)Apr 15 21:21:06 node1 crmd[2080]:   notice: Result of start operation for scsi-shooter on node1-heartbeat: 0 (ok)Apr 15 21:21:07 node1 dlm_controld[2231]: 150 dlm_controld 4.0.7 startedApr 15 21:21:08 node1 crmd[2080]:   notice: Result of start operation for dlm on node1-heartbeat: 0 (ok)Apr 15 21:24:29 node1 crmd[2080]:   notice: Result of probe operation for clvmd on node1-heartbeat: 7 (not running)Apr 15 21:24:32 node1 clvmd[2563]: Cluster LVM daemon started - connected to CorosyncApr 15 21:24:33 node1 crmd[2080]:   notice: Result of start operation for clvmd on node1-heartbeat: 0 (ok)</code></pre><p>配置约束：Clmvd必须在dlm启动后启动，而且必须在同一节点运行</p><pre><code>[root@node1 ~]#  pcs constraint order start dlm-clone then clvmd-clone  #次序约束，先启动dlm，然后在启动clvmdAdding dlm-clone clvmd-clone (kind: Mandatory) (Options: first-action=start then-action=start)[root@node1 ~]# pcs constraint colocation add clvmd-clone with dlm-clone #位置约束，dlm和clvmd要同时在一个节点上运行[root@node1 ~]#   pcs constraint  #查看约束Location Constraints:Ordering Constraints:  start dlm-clone then start clvmd-clone (kind:Mandatory)Colocation Constraints:  clvmd-clone with dlm-clone (score:INFINITY)Ticket Constraints:</code></pre><p>创建lv</p><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/99.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# fdisk -l | grep sdcDisk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectors[root@node1 ~]# fdisk /dev/sdcWelcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Device does not contain a recognized partition tableBuilding a new DOS disklabel with disk identifier 0x760ad300.Command (m for help): pDisk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 4194304 bytesDisk label type: dosDisk identifier: 0x760ad300   Device Boot      Start         End      Blocks   Id  SystemCommand (m for help): nPartition type:   p   primary (0 primary, 0 extended, 4 free)   e   extendedSelect (default p): pPartition number (1-4, default 1): First sector (8192-41943039, default 8192): Using default value 8192Last sector, +sectors or +size&#123;K,M,G&#125; (8192-41943039, default 41943039): Using default value 41943039Partition 1 of type Linux and of size 20 GiB is setCommand (m for help): pDisk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 4194304 bytesDisk label type: dosDisk identifier: 0x760ad300   Device Boot      Start         End      Blocks   Id  System/dev/sdc1            8192    41943039    20967424   83  LinuxCommand (m for help): tSelected partition 1Hex code (type L to list all codes): 8eChanged type of partition &#39;Linux&#39; to &#39;Linux LVM&#39;Command (m for help): pDisk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 4194304 bytesDisk label type: dosDisk identifier: 0x760ad300   Device Boot      Start         End      Blocks   Id  System/dev/sdc1            8192    41943039    20967424   8e  Linux LVMCommand (m for help): wThe partition table has been altered!Calling ioctl() to re-read partition table.Syncing disks.[root@node1 ~]# vgscan   Reading all physical volumes.  This may take a while...  Found volume group &quot;centos&quot; using metadata type lvm2  [root@node1 ~]# vgcreate vmvg0 /dev/sdc1  Physical volume &quot;/dev/sdc1&quot; successfully created.  Clustered volume group &quot;vmvg0&quot; successfully created[root@node1 ~]# vgdisplay vmvg0  --- Volume group ---  VG Name               vmvg0  System ID               Format                lvm2  Metadata Areas        1  Metadata Sequence No  1  VG Access             read/write  VG Status             resizable  Clustered             yes #这是一个集群的lvm  Shared                no  MAX LV                0  Cur LV                0  Open LV               0  Max PV                0  Cur PV                1  Act PV                1  VG Size               19.99 GiB  PE Size               4.00 MiB  Total PE              5118  Alloc PE / Size       0 / 0     Free  PE / Size       5118 / 19.99 GiB  VG UUID               mbCxgE-6pfx-HKLT-5f4Z-vcRm-TKSo-3dEnNp[root@node1 ~]# vgs #Attr下的wz--nc中的c表示这是一个集群的  VG     #PV #LV #SN Attr   VSize   VFree   centos   1   2   0 wz--n- &lt;39.00g  4.00m  vmvg0    1   0   0 wz--nc  19.99g 19.99g[root@node1 ~]#  lvcreate -n  lvvm0 -l 100%FREE vmvg0  #创建lv  Logical volume &quot;lvvm0&quot; created.[root@node1 ~]# lvscan   ACTIVE            &#39;/dev/vmvg0/lvvm0&#39; [19.99 GiB] inherit  ACTIVE            &#39;/dev/centos/swap&#39; [&lt;3.88 GiB] inherit  ACTIVE            &#39;/dev/centos/root&#39; [&lt;35.12 GiB] inherit</code></pre><p>node2:</p><pre><code>[root@node2 ~]# partprobe Warning: Unable to open /dev/sr0 read-write (Read-only file system).  /dev/sr0 has been opened read-only.[root@node2 ~]# multipath -rApr 15 21:39:08 | DM multipath kernel driver not loadedApr 15 21:39:08 | /etc/multipath.conf does not exist, blacklisting all devices.Apr 15 21:39:08 | A default multipath.conf file is located atApr 15 21:39:08 | /usr/share/doc/device-mapper-multipath-0.4.9/multipath.confApr 15 21:39:08 | You can run /sbin/mpathconf --enable to createApr 15 21:39:08 | /etc/multipath.conf. See man mpathconf(8) for more detailsApr 15 21:39:08 | DM multipath kernel driver not loaded[root@node2 ~]# fdisk -l | grep sdcDisk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectors/dev/sdc1            8192    41943039    20967424   8e  Linux LVM[root@node2 ~]# lvscan  #可以看到/dev/vmvg0/lvvm0  ACTIVE            &#39;/dev/vmvg0/lvvm0&#39; [19.99 GiB] inherit  ACTIVE            &#39;/dev/centos/swap&#39; [&lt;3.88 GiB] inherit  ACTIVE            &#39;/dev/centos/root&#39; [&lt;35.12 GiB] inherit</code></pre><h1><span id="7-配置gfs2">7、配置GFS2</span></h1><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/100.jpg"><br><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/101.jpg"></p><pre><code>[root@node1 ~]# lvscan   ACTIVE            &#39;/dev/vmvg0/lvvm0&#39; [19.99 GiB] inherit  ACTIVE            &#39;/dev/centos/swap&#39; [&lt;3.88 GiB] inherit  ACTIVE            &#39;/dev/centos/root&#39; [&lt;35.12 GiB] inherit[root@node1 ~]# mkfs.gfs2 -p lock_dlm -j 2 -t cluster1:node1 /dev/vmvg0/lvvm0-p=锁定的协议-j=保存两个文件系统的日志，因为现在是双节点-t=dlm里面使用的表cluster1:node1=前面是集群的名称，后面是自定义名称/dev/vmvg0/lvvm0 is a symbolic link to /dev/dm-2This will destroy any data on /dev/dm-2Are you sure you want to proceed? [y/n] yDiscarding device contents (may take a while on large devices): DoneAdding journals: Done Building resource groups: Done   Creating quota file: DoneWriting superblock and syncing: DoneDevice:                    /dev/vmvg0/lvvm0Block size:                4096Device size:               19.99 GB (5240832 blocks)Filesystem size:           19.99 GB (5240829 blocks)Journals:                  2Journal size:              64MBResource groups:           82Locking protocol:          &quot;lock_dlm&quot;Lock table:                &quot;cluster1:node1&quot;UUID:                      4a42f87d-29ff-44c5-af0b-1d1576d7df46[root@node1 ~]# pcs resource create VMFS Filesystem \ #VMFS资源的名称device=&quot;/dev/vmvg0/lvvm0&quot; \ #存储路径directory=&quot;/vm&quot; \ #挂载点 fstype=&quot;gfs2&quot; \ #文件格式类型clone #克隆Assumed agent name &#39;ocf:heartbeat:Filesystem&#39; (deduced from &#39;Filesystem&#39;)[root@node1 vm]# df -HTFilesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  2.0G     0  2.0G   0% /devtmpfs                   tmpfs     2.0G   63M  2.0G   4% /dev/shmtmpfs                   tmpfs     2.0G   13M  2.0G   1% /runtmpfs                   tmpfs     2.0G     0  2.0G   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        38G  4.3G   34G  12% //dev/sda1               xfs       1.1G  206M  858M  20% /boottmpfs                   tmpfs     396M     0  396M   0% /run/user/0/dev/mapper/vmvg0-lvvm0 gfs2       22G  137M   22G   1% /vm #已经挂载上了</code></pre><p><strong>测试</strong></p><p>node1:</p><pre><code>[root@node1 ~]# df -HT Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  2.0G     0  2.0G   0% /devtmpfs                   tmpfs     2.0G   63M  2.0G   4% /dev/shmtmpfs                   tmpfs     2.0G   13M  2.0G   1% /runtmpfs                   tmpfs     2.0G     0  2.0G   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        38G  4.3G   34G  12% //dev/sda1               xfs       1.1G  206M  858M  20% /boottmpfs                   tmpfs     396M     0  396M   0% /run/user/0/dev/mapper/vmvg0-lvvm0 gfs2       22G  137M   22G   1% /vm[root@node1 ~]# cd /vm[root@node1 vm]# touch test.txt[root@node1 vm]# echo test &gt; test.txt [root@node1 vm]# ls -ltotal 8-rw-r--r--. 1 root root 5 Apr 16 20:57 test.txt</code></pre><p>node2:</p><pre><code>[root@node2 ~]# cd /vm[root@node2 vm]# lstest.txt[root@node2 vm]# cat test.txt test[root@node2 vm]# cp test.txt test2.txt  #测试节点2是否可以读写[root@node2 vm]# cat test2.txt test</code></pre><p><strong>配置约束</strong></p><pre><code>[root@node1 ~]# pcs constraint order clvmd-clone then VMFS-cloneAdding clvmd-clone VMFS-clone (kind: Mandatory) (Options: first-action=start then-action=start)[root@node1 ~]# pcs constraint colocation add VMFS-clone with clvmd-clone[root@node1 ~]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sun Apr 16 20:55:31 2023Last change: Sun Apr 16 20:51:30 2023 by root via cibadmin on node1-heartbeat2 nodes configured7 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: scsi-shooter   (stonith:fence_scsi):   Started node1-heartbeat Clone Set: dlm-clone [dlm]     Started: [ node1-heartbeat node2-heartbeat ] Clone Set: clvmd-clone [clvmd]     Started: [ node1-heartbeat node2-heartbeat ] Clone Set: VMFS-clone [VMFS]     Started: [ node1-heartbeat node2-heartbeat ]Daemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled  [root@node2 vm]# pcs constraint #查看约束Location Constraints:Ordering Constraints:  start dlm-clone then start clvmd-clone (kind:Mandatory)  start clvmd-clone then start VMFS-clone (kind:Mandatory)Colocation Constraints:  clvmd-clone with dlm-clone (score:INFINITY)  VMFS-clone with clvmd-clone (score:INFINITY)Ticket Constraints:</code></pre><p><strong>配置Selinux</strong></p><p>node1:</p><pre><code>[root@node1 vm]# yum install policycoreutils-python[root@node1 vm]# semanage fcontext -a -t virt_image_t &quot;/vm(/.*)?&quot;[root@node1 vm]# restorecon -R -v /vmrestorecon reset /vm context system_u:object_r:unlabeled_t:s0-&gt;system_u:object_r:virt_image_t:s0restorecon reset /vm/test.txt context unconfined_u:object_r:unlabeled_t:s0-&gt;unconfined_u:object_r:virt_image_t:s0restorecon reset /vm/test2.txt context unconfined_u:object_r:unlabeled_t:s0-&gt;unconfined_u:object_r:virt_image_t:s0</code></pre><p>node2:</p><pre><code>[root@node1 vm]# yum install policycoreutils-python[root@node2 vm]# semanage fcontext -a -t virt_image_t &quot;/vm(/.*)?&quot;[root@node2 vm]# restorecon -R -v /vm #很重要！</code></pre><h1><span id="8-向集群中添加虚拟机资源">8、向集群中添加虚拟机资源</span></h1><h2><span id="1-将虚拟机数据迁移到iscsi的共享目录">1、将虚拟机数据迁移到iSCSI的共享目录</span></h2><pre><code>[root@node1 vmdata]# virsh list --all  #查看关机的虚拟机 Id    Name                           State---------------------------------------------------- -     Centos7.9                      shut off[root@node1 vmdata]# virsh domblklist --domain Centos7.9  #获取硬盘路径Target     Source------------------------------------------------vda        /vmdata/Centos7.9.qcow2hda        -[root@node1 vmdata]# df -HT #查看Iscsi的挂载点在/vmFilesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  2.0G     0  2.0G   0% /devtmpfs                   tmpfs     2.0G   63M  2.0G   4% /dev/shmtmpfs                   tmpfs     2.0G   13M  2.0G   1% /runtmpfs                   tmpfs     2.0G     0  2.0G   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        38G   13G   25G  35% //dev/sda1               xfs       1.1G  206M  858M  20% /boottmpfs                   tmpfs     396M     0  396M   0% /run/user/0/dev/mapper/vmvg0-lvvm0 gfs2       22G  137M   22G   1% /vm[root@node1 vmdata]# mv /vmdata/Centos7.9.qcow2 /vm #将数据盘迁移到/vm[root@node1 vmdata]# sar -n DEV 1 4 #期间可以多开窗口查看网络流量[root@node1 vm]# mkdir -p /vm/qemu_config[root@node1 vm]# virsh dumpxml --domain Centos7.9 &gt; /vm/qemu_config/Centos7.9.xml #将配置文件输出到iSCSI的挂载点[root@node1 vm]# virsh undefine --domain Centos7.9  #取消原主机定义Domain Centos7.9 has been undefined[root@node1 vm]# vim /vm/qemu_config/Centos7.9.xml #修改硬盘路径为iSCSI的挂载点      &lt;source file=&#39;/vm/Centos7.9.qcow2&#39;/&gt;[root@node1 vm]# firewall-cmd --add-port=16509/tcp --permanent #别忘记添加防火墙规则success[root@node1 vm]# firewall-cmd --add-port=49152-49251/tcp --permanent success[root@node1 vm]# firewall-cmd --reload success</code></pre><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/102.jpg"></p><h2><span id="2-测试动态迁移">2、测试动态迁移</span></h2><p>node1:</p><pre><code>[root@node1 vm]# virsh define --file /vm/qemu_config/Centos7.9.xml Domain Centos7.9 defined from /vm/qemu_config/Centos7.9.xml[root@node1 vm]# virsh start --domain Centos7.9 Domain Centos7.9 started[root@node1 vm]# virsh migrate --domain Centos7.9 --desturi qemu+ssh://root@node2-storge/system --live  --persistent --verbose #热迁移到node2[100%][root@node1 vm]# virsh shutdown --domain Centos7.9 #关机[root@node1 ~]# virsh undefine --domain Centos7.9 #取消定义[root@node1 ~]# pcs resource create Centos7.9 VirtualDomain hypervisor=&quot;qemu:///system&quot; config=&quot;/vm/qemu_config/Centos7.9.xml&quot; migration_transport=ssh meta allow-migrate=&quot;true&quot; #在集群中定义主机[root@node1 ~]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Wed Apr 19 10:09:00 2023Last change: Wed Apr 19 10:08:55 2023 by root via crm_resource on node2-heartbeat2 nodes configured8 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: scsi-shooter   (stonith:fence_scsi):   Started node1-heartbeat Clone Set: dlm-clone [dlm]     Started: [ node1-heartbeat node2-heartbeat ] Clone Set: clvmd-clone [clvmd]     Started: [ node1-heartbeat node2-heartbeat ] Clone Set: VMFS-clone [VMFS]     Started: [ node1-heartbeat node2-heartbeat ] Centos7.9  (ocf::heartbeat:VirtualDomain): Started node2-heartbeatDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled[root@node1 ~]# pcs constraint order start VMFS-clone then Centos7.9 #配置顺序约束Adding VMFS-clone Centos7.9 (kind: Mandatory) (Options: first-action=start then-action=start)[root@node1 ~]# pcs constraint Location Constraints:Ordering Constraints:  start dlm-clone then start clvmd-clone (kind:Mandatory)  start clvmd-clone then start VMFS-clone (kind:Mandatory)  start VMFS-clone then start Centos7.9 (kind:Mandatory)Colocation Constraints:  clvmd-clone with dlm-clone (score:INFINITY)  VMFS-clone with clvmd-clone (score:INFINITY)Ticket Constraints:[root@node1 ~]# pcs resource move Centos7.9 node1-heartbeat #热迁移</code></pre><blockquote><p>经过多次测试，由于semanage fcontext -a -t virt_image_t “&#x2F;vm(&#x2F;.*)?”配置未生效，导致热迁移提示权限错误，原因就是因为selinux禁止了，关闭selinux后热迁移成功</p></blockquote><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KVM </tag>
            
            <tag> 基于iSCSI的KVM群集构建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RBD高级功能</title>
      <link href="/2023/04/19/Ceph/10.RBD%E9%AB%98%E7%BA%A7%E5%8A%9F%E8%83%BD/RBD%E9%AB%98%E7%BA%A7%E5%8A%9F%E8%83%BD/"/>
      <url>/2023/04/19/Ceph/10.RBD%E9%AB%98%E7%BA%A7%E5%8A%9F%E8%83%BD/RBD%E9%AB%98%E7%BA%A7%E5%8A%9F%E8%83%BD/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-rbd%E5%9B%9E%E6%94%B6%E7%AB%99%E6%9C%BA%E5%88%B6">一、RBD回收站机制</a><ul><li><a href="#1-%E7%A7%BB%E5%8A%A8%E5%88%B0%E5%9B%9E%E6%94%B6%E7%AB%99">1 移动到回收站</a></li><li><a href="#2-%E6%89%BE%E5%9B%9E%E5%88%A0%E9%99%A4%E7%9A%84%E9%95%9C%E5%83%8F">2 找回删除的镜像</a></li></ul></li><li><a href="#%E4%BA%8C-rbd%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C%E5%BF%AB%E7%85%A7">二、RBD镜像制作快照</a></li><li><a href="#%E4%B8%89-rbd%E5%BF%AB%E7%85%A7%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D">三、RBD快照数据恢复</a><ul><li><a href="#1-%E5%BF%AB%E7%85%A7%E6%81%A2%E5%A4%8D">1、快照恢复</a></li><li><a href="#2-%E5%88%A0%E9%99%A4%E5%BF%AB%E7%85%A7">2、删除快照</a></li></ul></li><li><a href="#%E5%9B%9B-rbd%E9%95%9C%E5%83%8F%E5%85%8B%E9%9A%86%E6%9C%BA%E5%88%B6">四、RBD镜像克隆机制</a></li><li><a href="#%E4%BA%94-rbd%E8%A7%A3%E9%99%A4%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB">五、RBD解除依赖关系</a><ul><li><a href="#1-%E5%AE%9E%E6%93%8D">1、实操</a></li><li><a href="#2-%E6%BC%94%E7%A4%BA%E6%8D%9F%E5%9D%8F%E8%BF%87%E7%A8%8B">2、演示损坏过程</a></li></ul></li><li><a href="#%E5%85%AD-rbd%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D">六、RBD备份与恢复</a><ul><li><a href="#1-%E5%A4%87%E4%BB%BD">1、备份</a></li><li><a href="#2-%E6%81%A2%E5%A4%8D">2、恢复</a></li></ul></li><li><a href="#%E4%B8%83-rbd%E5%A2%9E%E9%87%8F%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D">七、RBD增量备份与恢复</a><ul><li><a href="#1-%E5%A2%9E%E9%87%8F%E5%A4%87%E4%BB%BD">1、增量备份</a></li><li><a href="#2-%E5%A2%9E%E9%87%8F%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D">2、增量数据恢复</a></li></ul></li><li><a href="#%E5%85%AB-rbd%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA%E6%80%BB%E7%BB%93">八、RBD导入导出总结</a></li><li><a href="#%E4%B9%9D-rbd%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D%E4%B8%8E%E5%A2%9E%E9%87%8F%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D%E5%AE%9E%E6%93%8D">九、RBD备份恢复与增量备份恢复实操</a><ul><li><a href="#1-%E5%88%9B%E5%BB%BApool">1、创建pool</a></li><li><a href="#2-%E5%88%9B%E5%BB%BA%E5%9D%97%E8%AE%BE%E5%A4%871">2、创建块设备1</a></li><li><a href="#3-%E6%98%A0%E5%B0%84%E5%9D%97%E8%AE%BE%E5%A4%87%E5%88%B0%E6%9C%AC%E5%9C%B0">3、映射块设备到本地</a></li><li><a href="#4-%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%9D%97%E8%AE%BE%E5%A4%871">4、格式化块设备1</a></li><li><a href="#5-%E6%8C%82%E8%BD%BD%E5%9D%97%E8%AE%BE%E5%A4%871">5、挂载块设备1</a></li><li><a href="#6-%E5%86%99%E5%85%A5file1%E5%88%B0%E5%9D%97%E8%AE%BE%E5%A4%871">6、写入file1到块设备1</a></li><li><a href="#7-%E5%BF%AB%E7%85%A7%E5%9D%97%E8%AE%BE%E5%A4%871">7、快照块设备1</a></li><li><a href="#8-%E6%81%A2%E5%A4%8D%E5%9D%97%E8%AE%BE%E5%A4%871%E4%B8%BA%E5%9D%97%E8%AE%BE%E5%A4%872">8、恢复块设备1为块设备2</a></li><li><a href="#9-%E6%98%A0%E5%B0%84%E5%9D%97%E8%AE%BE%E5%A4%872">9、映射块设备2</a></li><li><a href="#10-%E6%8C%82%E8%BD%BD%E5%9D%97%E8%AE%BE%E5%A4%872">10、挂载块设备2</a></li><li><a href="#11-%E6%9F%A5%E7%9C%8B%E5%9D%97%E8%AE%BE%E5%A4%872%E7%9A%84%E6%95%B0%E6%8D%AE%E6%98%AF%E5%90%A6%E6%9C%89file1">11、查看块设备2的数据是否有file1</a></li><li><a href="#12-%E5%86%99%E5%85%A5file2%E5%88%B0%E5%9D%97%E8%AE%BE%E5%A4%871">12、写入file2到块设备1</a></li><li><a href="#13-%E5%A2%9E%E9%87%8F%E5%A4%87%E4%BB%BD%E5%9D%97%E8%AE%BE%E5%A4%871">13、增量备份块设备1</a></li><li><a href="#14-%E5%A2%9E%E9%87%8F%E6%81%A2%E5%A4%8D%E5%9D%97%E8%AE%BE%E5%A4%872">14、增量恢复块设备2</a></li><li><a href="#15-%E6%9F%A5%E7%9C%8B%E5%9D%97%E8%AE%BE%E5%A4%872%E6%98%AF%E5%90%A6%E6%9C%89file2%E6%96%87%E4%BB%B6">15 、查看块设备2是否有file2文件</a></li></ul></li></ul><!-- tocstop --><h1><span id="一-rbd回收站机制">一、RBD回收站机制</span></h1><h2><span id="1-移动到回收站">1 移动到回收站</span></h2><pre><code>[root@node-1 my-cluster]# rbd -p ceph-demo ls #查看poolcrush-demo.imgcrush-map.imgrbd-demo.img[root@node-1 my-cluster]# rbd create ceph-demo/ceph-trash.img --size 1G #在ceph-demo中创建ceph-trash.img[root@node-1 my-cluster]# rbd info ceph-demo/ceph-trash.img #查看RBD块设备信息rbd image &#39;ceph-trash.img&#39;:    size 1 GiB in 256 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 659d3aed954    block_name_prefix: rbd_data.659d3aed954    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Wed Mar  8 18:15:57 2023    access_timestamp: Wed Mar  8 18:15:57 2023    modify_timestamp: Wed Mar  8 18:15:57 2023[root@node-1 my-cluster]# rbd rm ceph-demo/ceph-trash.img #删除块设备Removing image: 100% complete...done.[root@node-1 my-cluster]# rbd -p ceph-demo ls #查看已经被删除crush-demo.imgcrush-map.imgrbd-demo.img[root@node-1 my-cluster]# rbd -p ceph-demo trash ls  #回收站中也被删除了[root@node-1 my-cluster]# rbd create ceph-demo/ceph-trash.img --size 1G #再次创建一个块设备[root@node-1 my-cluster]# rbd help trash move   #查看使用帮助usage: rbd trash move [--pool &lt;pool&gt;] [--namespace &lt;namespace&gt;]                       [--image &lt;image&gt;] [--expires-at &lt;expires-at&gt;]                       &lt;image-spec&gt; Move an image to the trash.Positional arguments  &lt;image-spec&gt;            image specification                          (example: [&lt;pool-name&gt;/[&lt;namespace&gt;/]]&lt;image-name&gt;)Optional arguments  -p [ --pool ] arg       pool name  --namespace arg         namespace name  --image arg             image name  --expires-at arg (=now) set the expiration time of an image so it can be                          purged when it is stale[root@node-1 my-cluster]# rbd trash move ceph-demo/ceph-trash.img --expires-at 20230310 #将ceph-demo pool下的ceph-trash.img 移动到回收站，设置过期日期为2023年03月10日[root@node-1 my-cluster]# rbd -p ceph-demo ls #查看ceph-demo的pool中已经没有 ceph-trash.imgcrush-demo.imgcrush-map.imgrbd-demo.img[root@node-1 my-cluster]# rbd -p ceph-demo trash ls #查看回收站66393ad8bed4 ceph-trash.img</code></pre><h2><span id="2-找回删除的镜像">2 找回删除的镜像</span></h2><pre><code>[root@node-1 my-cluster]# rbd help trash restore #查看使用帮助usage: rbd trash restore [--pool &lt;pool&gt;] [--namespace &lt;namespace&gt;]                          [--image-id &lt;image-id&gt;] [--image &lt;image&gt;]                          &lt;image-id&gt; Restore an image from trash.Positional arguments  &lt;image-id&gt;           image id                       (example: [&lt;pool-name&gt;/]&lt;image-id&gt;)Optional arguments  -p [ --pool ] arg    pool name  --namespace arg      namespace name  --image-id arg       image id  --image arg          image name[root@node-1 my-cluster]# rbd trash restore -p ceph-demo 66393ad8bed4 #将66393ad8bed4镜像恢复到ceph-demo的pool中，也可以指定到其他pool[root@node-1 my-cluster]# rbd -p ceph-demo ls  #查看已经恢复ceph-trash.img #这个crush-demo.imgcrush-map.img rbd-demo.img</code></pre><h1><span id="二-rbd镜像制作快照">二、RBD镜像制作快照</span></h1><p><img src="/images/Ceph/RBD%E9%AB%98%E7%BA%A7%E5%8A%9F%E8%83%BD/1.jpg"></p><pre><code>[root@node-1 my-cluster]# rbd create ceph-demo/rbd-test.img --image-feature layering --size 1G #创建一个1G可挂载的rbd块设备[root@node-1 my-cluster]# rbd info ceph-demo/rbd-test.img #查看块设备信息rbd image &#39;rbd-test.img&#39;:    size 1 GiB in 256 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 6678a4fda5fe    block_name_prefix: rbd_data.6678a4fda5fe    format: 2    features: layering #可挂载    op_features:     flags:     create_timestamp: Thu Mar  9 09:35:08 2023    access_timestamp: Thu Mar  9 09:35:08 2023    modify_timestamp: Thu Mar  9 09:35:08 2023[root@node-1 my-cluster]# rbd device map ceph-demo/rbd-test.img  #将rbd-test映射到本地硬盘/dev/rbd1[root@node-1 my-cluster]# mkfs.xfs /dev/rbd1 #初始化硬盘Discarding blocks...Done.meta-data=/dev/rbd1              isize=512    agcount=8, agsize=32768 blks         =                       sectsz=512   attr=2, projid32bit=1         =                       crc=1        finobt=0, sparse=0data     =                       bsize=4096   blocks=262144, imaxpct=25         =                       sunit=1024   swidth=1024 blksnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1log      =internal log           bsize=4096   blocks=2560, version=2         =                       sectsz=512   sunit=8 blks, lazy-count=1realtime =none                   extsz=4096   blocks=0, rtextents=0[root@node-1 my-cluster]# mkdir /data-snap #创建一个目录[root@node-1 my-cluster]# mount /dev/rbd1 /data-snap #将rbd1挂载到/data-snap[root@node-1 my-cluster]# df -HT  #查看挂载成功Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   36M  919M   4% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.3G   17G  13% //dev/sda1               xfs       1.1G  204M  860M  20% /boottmpfs                   tmpfs     191M     0  191M   0% /run/user/0tmpfs                   tmpfs     954M   54k  954M   1% /var/lib/ceph/osd/ceph-0tmpfs                   tmpfs     954M   54k  954M   1% /var/lib/ceph/osd/ceph-1/dev/rbd0               xfs       1.1G   34M  1.1G   4% /data/dev/rbd1               xfs       1.1G   34M  1.1G   4% /data-snap[root@node-1 data-snap]# cd /data-snap/ 进入到挂载目录[root@node-1 data-snap]# sync 刷新一下盘[root@node-1 data-snap]# echo &gt;&gt; 123 snap.txt #写入数据[root@node-1 data-snap]# rbd snap create ceph-demo/rbd-test.img@rbd-test.img_20230309 #将ceph-demo池子下的rbd-test.img块设备做快照，定义快照名称为：rbd-test.img_20230309[root@node-1 data-snap]# rbd snap ls ceph-demo/rbd-test.img #查看快照SNAPID NAME                  SIZE  PROTECTED TIMESTAMP                     4 rbd-test.img_20230309 1 GiB           Thu Mar  9 09:44:15 2023 </code></pre><h1><span id="三-rbd快照数据恢复">三、RBD快照数据恢复</span></h1><h2><span id="1-快照恢复">1、快照恢复</span></h2><pre><code>[root@node-1 data-snap]# ls #查看块设备文件123[root@node-1 data-snap]# rm -rf 123 #删除123文件 [root@node-1 ~]# umount /dev/rbd1 #取消挂载[root@node-1 data-snap]# rbd snap rollback ceph-demo/rbd-test.img@rbd-test.img_20230309 #恢复快照Rolling back to snapshot: 100% complete...done.[root@node-1 /]# mount /dev/rbd0 /data[root@node-1 data]# ls #文件恢复snap.txt</code></pre><h2><span id="2-删除快照">2、删除快照</span></h2><pre><code>[root@node-1 data]# rbd snap rm ceph-demo/rbd-test.img@rbd-test.img-snap-20230309 #删除快照[root@node-1 data]# rbd snap remove ceph-demo/rbd-test.img@rbd-test.img-snap-20230309 #删除快照[root@node-1 data]# rbd snap purge ceph-demo/rbd-test.img #删除当前所有快照</code></pre><h1><span id="四-rbd镜像克隆机制">四、RBD镜像克隆机制</span></h1><p><img src="/images/Ceph/RBD%E9%AB%98%E7%BA%A7%E5%8A%9F%E8%83%BD/2.jpg"></p><p>分层（copy-on-write）</p><pre><code>[root@node-1 data]# ceph osd pool ls #查看当前pool.rgw.rootdefault.rgw.controldefault.rgw.metadefault.rgw.logrbd-snap-demo[root@node-1 data]# rbd -p rbd-snap-demo ls #查看rbd-snap-demo下的块设备snap.img[root@node-1 data]# rbd snap create rbd-snap-demo/snap.img@template #创建一个template的快照[root@node-1 data]# rbd snap ls rbd-snap-demo/snap.img #查看快照创建成功SNAPID NAME              SIZE   PROTECTED TIMESTAMP                     4 snap.img-20230309 10 GiB           Thu Mar  9 11:11:07 2023      5 template          10 GiB           Thu Mar  9 11:28:50 2023                                [root@node-1 data]# rbd snap protect rbd-snap-demo/snap.img@template #保护快照[root@node-1 data]# rbd snap rm rbd-snap-demo/snap.img@template #测试无法删除Removing snap: 0% complete...failed.rbd: snapshot &#39;template&#39; is protected from removal.2023-03-09 11:49:40.689 7f92a1e8dc80 -1 librbd::Operations: snapshot is protected[root@node-1 data]# rbd clone rbd-snap-demo/snap.img@template rbd-snap-demo/vm1-clone.img #克隆一个vm1-clone.img的块设备，可以跨Pool[root@node-1 data]# rbd clone rbd-snap-demo/snap.img@template rbd-snap-demo/vm2-clone.img[root@node-1 data]# rbd clone rbd-snap-demo/snap.img@template rbd-snap-demo/vm3-clone.img[root@node-1 data]# rbd -p rbd-snap-demo ls #查看克隆的镜像snap.imgvm1-clone.imgvm2-clone.imgvm3-clone.img[root@node-1 data]# rbd -p rbd-snap-demo info vm1-clone.img  #查看克隆的镜像rbd image &#39;vm1-clone.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 69a2a3b7c9ab    block_name_prefix: rbd_data.69a2a3b7c9ab    format: 2    features: layering    op_features:     flags:     create_timestamp: Thu Mar  9 11:51:30 2023    access_timestamp: Thu Mar  9 11:51:30 2023    modify_timestamp: Thu Mar  9 11:51:30 2023    parent: rbd-snap-demo/snap.img@template #父镜像    overlap: 10 GiB</code></pre><h1><span id="五-rbd解除依赖关系">五、RBD解除依赖关系</span></h1><p>克隆的图像保留对父快照的引用。当您删除从子克隆到父快照的引用时，您通过将信息从快照复制到克隆来有效地“扁平化”图像。扁平化克隆所需的时间随着快照的大小而增加。要删除快照，您必须先展平子图像。</p><pre><code>rbd flatten &#123;pool-name&#125;/&#123;image-name&#125;</code></pre><p>例如：</p><pre><code>rbd flatten rbd/new-image</code></pre><blockquote><p>由于扁平化镜像包含快照的所有信息，扁平化镜像比分层克隆占用更多存储空间。</p></blockquote><h2><span id="1-实操">1、实操</span></h2><pre><code>[root@node-1 ~]# rbd info rbd-snap-demo/vm1-clone.img #查看克隆出来的镜像rbd image &#39;vm1-clone.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 69a2a3b7c9ab    block_name_prefix: rbd_data.69a2a3b7c9ab    format: 2    features: layering    op_features:     flags:     create_timestamp: Thu Mar  9 11:51:30 2023    access_timestamp: Thu Mar  9 11:51:30 2023    modify_timestamp: Thu Mar  9 11:51:30 2023    parent: rbd-snap-demo/snap.img@template #可以看到父级    overlap: 10 GiB[root@node-1 ~]# rbd children rbd-snap-demo/snap.img@template #查看这个快照克隆出来多少镜像rbd-snap-demo/vm1-clone.imgrbd-snap-demo/vm2-clone.imgrbd-snap-demo/vm3-clone.img[root@node-1 ~]# rbd flatten rbd-snap-demo/vm1-clone.img #取消依赖关系Image flatten: 100% complete...done.[root@node-1 ~]# rbd info rbd-snap-demo/vm1-clone.img #再次查看，已经取消了依赖关系rbd image &#39;vm1-clone.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 69a2a3b7c9ab    block_name_prefix: rbd_data.69a2a3b7c9ab    format: 2    features: layering    op_features:     flags:     create_timestamp: Thu Mar  9 11:51:30 2023    access_timestamp: Thu Mar  9 11:51:30 2023    modify_timestamp: Thu Mar  9 11:51:30 2023[root@node-1 ~]# rbd children rbd-snap-demo/snap.img@template #可以看到vm1的已经取消了rbd-snap-demo/vm2-clone.imgrbd-snap-demo/vm3-clone.img</code></pre><h2><span id="2-演示损坏过程">2、演示损坏过程</span></h2><pre><code>[root@node-1 ~]# rbd children rbd-snap-demo/snap.img@template #查看需要损坏的镜像rbd-snap-demo/vm2-clone.imgrbd-snap-demo/vm3-clone.img[root@node-1 ~]#  rbd flatten rbd-snap-demo/vm2-clone.img #取消依赖关系Image flatten: 100% complete...done.[root@node-1 ~]#  rbd flatten rbd-snap-demo/vm3-clone.img #取消依赖关系Image flatten: 100% complete...done.[root@node-1 ~]# rbd snap unprotect rbd-snap-demo/snap.img@template #取消标志位，达到破坏镜像的目的[root@node-1 ~]# rbd snap rm rbd-snap-demo/snap.img@template #删除快照Removing snap: 100% complete...done.[root@node-1 ~]# rbd device ls  #可以看到依旧时可以使用的，已经没有任何关系了。id pool          namespace image         snap device    0  rbd-snap-demo           snap.img      -    /dev/rbd0 1  rbd-snap-demo           vm3-clone.img -    /dev/rbd1</code></pre><h1><span id="六-rbd备份与恢复">六、RBD备份与恢复</span></h1><h2><span id="1-备份">1、备份</span></h2><pre><code>[root@node-1 ~]# rbd help export usage: rbd export [--pool &lt;pool&gt;] [--namespace &lt;namespace&gt;] [--image &lt;image&gt;]                   [--snap &lt;snap&gt;] [--path &lt;path&gt;] [--no-progress]                   [--export-format &lt;export-format&gt;]                   &lt;source-image-or-snap-spec&gt; &lt;path-name&gt; Export image to file.Positional arguments  &lt;source-image-or-snap-spec&gt;  source image or snapshot specification                               (example:                               [&lt;pool-name&gt;/[&lt;namespace&gt;/]]&lt;image-name&gt;[@&lt;snap-n                               ame&gt;])  &lt;path-name&gt;                  export file (or &#39;-&#39; for stdout)Optional arguments  -p [ --pool ] arg            source pool name  --namespace arg              source namespace name  --image arg                  source image name  --snap arg                   source snapshot name  --path arg                   export file (or &#39;-&#39; for stdout)  --no-progress                disable progress output  --export-format arg          format of image file  [root@node-1 ~]# ceph osd lspools #查看当前pool4 .rgw.root5 default.rgw.control6 default.rgw.meta7 default.rgw.log8 rbd-snap-demo[root@node-1 ~]# rbd -p rbd-snap-demo ls #查看rbd-snap-demo的块snap.imgvm1-clone.imgvm2-clone.imgvm3-clone.img[root@node-1 ~]# rbd snap create rbd-snap-demo/snap.img@snap-demo #创建快照[root@node-1 ~]# rbd snap ls rbd-snap-demo/snap.img #查看快照SNAPID NAME              SIZE   PROTECTED TIMESTAMP                     4 snap.img-20230309 10 GiB           Thu Mar  9 11:11:07 2023      7 snap-demo         10 GiB           Thu Mar  9 15:08:50 2023[root@node-1 ~]# rbd export rbd-snap-demo/snap.img@snap-demo /root/rbd-test.img #导出镜像Exporting image: 100% complete...done.</code></pre><h2><span id="2-恢复">2、恢复</span></h2><pre><code>[root@node-1 ~]# rbd help import usage: rbd import [--path &lt;path&gt;] [--dest-pool &lt;dest-pool&gt;]                   [--dest-namespace &lt;dest-namespace&gt;] [--dest &lt;dest&gt;]                   [--image-format &lt;image-format&gt;] [--new-format]                   [--order &lt;order&gt;] [--object-size &lt;object-size&gt;]                   [--image-feature &lt;image-feature&gt;] [--image-shared]                   [--stripe-unit &lt;stripe-unit&gt;]                   [--stripe-count &lt;stripe-count&gt;] [--data-pool &lt;data-pool&gt;]                   [--journal-splay-width &lt;journal-splay-width&gt;]                   [--journal-object-size &lt;journal-object-size&gt;]                   [--journal-pool &lt;journal-pool&gt;]                   [--sparse-size &lt;sparse-size&gt;] [--no-progress]                   [--export-format &lt;export-format&gt;] [--pool &lt;pool&gt;]                   [--image &lt;image&gt;]                   &lt;path-name&gt; &lt;dest-image-spec&gt; Import image from file.Positional arguments  &lt;path-name&gt;               import file (or &#39;-&#39; for stdin)  &lt;dest-image-spec&gt;         destination image specification                            (example: [&lt;pool-name&gt;/[&lt;namespace&gt;/]]&lt;image-name&gt;)Optional arguments  --path arg                import file (or &#39;-&#39; for stdin)  --dest-pool arg           destination pool name  --dest-namespace arg      destination namespace name  --dest arg                destination image name  --image-format arg        image format [1 (deprecated) or 2]  --new-format              use image format 2                            (deprecated)  --order arg               object order [12 &lt;= order &lt;= 25]  --object-size arg         object size in B/K/M [4K &lt;= object size &lt;= 32M]  --image-feature arg       image features                            [layering(+), exclusive-lock(+*), object-map(+*),                            deep-flatten(+-), journaling(*)]  --image-shared            shared image  --stripe-unit arg         stripe unit in B/K/M  --stripe-count arg        stripe count  --data-pool arg           data pool  --journal-splay-width arg number of active journal objects  --journal-object-size arg size of journal objects [4K &lt;= size &lt;= 64M]  --journal-pool arg        pool for journal objects  --sparse-size arg         sparse size in B/K/M [default: 4K]  --no-progress             disable progress output  --export-format arg       format of image file  -p [ --pool ] arg         pool name (deprecated)  --image arg               image name (deprecated)Image Features:  (*) supports enabling/disabling on existing images  (-) supports disabling-only on existing images  (+) enabled by default for new images if features not specified  [root@node-1 ~]# rbd import rbd-test.img rbd-snap-demo/snap-new.img #导入rbd-test.img的备份，恢复并重命名为snap-new.img，恢复到rbd-snap-demo的pool中Importing image: 100% complete...done.[root@node-1 ~]# rbd -p rbd-snap-demo ls  #查看已经成功导入snap-new.imgsnap.imgvm1-clone.imgvm2-clone.imgvm3-clone.img</code></pre><h1><span id="七-rbd增量备份与恢复">七、RBD增量备份与恢复</span></h1><h2><span id="1-增量备份">1、增量备份</span></h2><pre><code>usage: rbd export-diff [--pool &lt;pool&gt;] [--namespace &lt;namespace&gt;]                        [--image &lt;image&gt;] [--snap &lt;snap&gt;] [--path &lt;path&gt;]                        [--from-snap &lt;from-snap&gt;] [--whole-object]                        [--no-progress]                        &lt;source-image-or-snap-spec&gt; &lt;path-name&gt; Export incremental diff to file.Positional arguments  &lt;source-image-or-snap-spec&gt;  source image or snapshot specification                               (example:                               [&lt;pool-name&gt;/[&lt;namespace&gt;/]]&lt;image-name&gt;[@&lt;snap-n                               ame&gt;])  &lt;path-name&gt;                  export file (or &#39;-&#39; for stdout)Optional arguments  -p [ --pool ] arg            source pool name  --namespace arg              source namespace name  --image arg                  source image name  --snap arg                   source snapshot name  --path arg                   export file (or &#39;-&#39; for stdout)  --from-snap arg              snapshot starting point  --whole-object               compare whole object  --no-progress                disable progress output[root@node-1 ~]# rbd device ls #查看映射的块设备id pool          namespace image         snap device0  rbd-snap-demo           snap.img      -    /dev/rbd01  rbd-snap-demo           vm3-clone.img -    /dev/rbd1[root@node-1 ~]# df -HT  #rbd0映射到了/dataFilesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   44M  911M   5% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.4G   16G  13% //dev/sda1               xfs       1.1G  204M  860M  20% /boottmpfs                   tmpfs     954M   54k  954M   1% /var/lib/ceph/osd/ceph-0tmpfs                   tmpfs     954M   54k  954M   1% /var/lib/ceph/osd/ceph-1/dev/rbd0               xfs        11G   35M   11G   1% /datatmpfs                   tmpfs     191M     0  191M   0% /run/user/0[root@node-1 ~]# cd /data[root@node-1 data]# lssnap.txt[root@node-1 data]# echo &quot;增量备份测试&quot; &gt; test.txt[root@node-1 data]# rbd snap create rbd-snap-demo/snap.img@v1 #再次创建快照[root@node-1 data]# rbd snap ls rbd-snap-demo/snap.img #查看快照SNAPID NAME              SIZE   PROTECTED TIMESTAMP                     4 snap.img-20230309 10 GiB           Thu Mar  9 11:11:07 2023      7 snap-demo         10 GiB           Thu Mar  9 15:08:50 2023      8 v1                10 GiB           Thu Mar  9 15:42:52 2023[root@node-1 data]# rbd export-diff rbd-snap-demo/snap.img@v1 rbd-test.img@v1 #增量导出Exporting image: 100% complete...done.[root@node-1 ~]# ls -lha #查看大小相差很多-rw-r--r--   1 root root   10G Mar  9 15:12 rbd-test.img-rw-r--r--   1 root root  4.4M Mar  9 15:46 rbd-test.img@v1</code></pre><h2><span id="2-增量数据恢复">2、增量数据恢复</span></h2><pre><code>[root@node-1 ~]# rbd help import-diffusage: rbd import-diff [--path &lt;path&gt;] [--pool &lt;pool&gt;]                        [--namespace &lt;namespace&gt;] [--image &lt;image&gt;]                        [--sparse-size &lt;sparse-size&gt;] [--no-progress]                        &lt;path-name&gt; &lt;image-spec&gt; Import an incremental diff.Positional arguments  &lt;path-name&gt;          import file (or &#39;-&#39; for stdin)  &lt;image-spec&gt;         image specification                       (example: [&lt;pool-name&gt;/[&lt;namespace&gt;/]]&lt;image-name&gt;)Optional arguments  --path arg           import file (or &#39;-&#39; for stdin)  -p [ --pool ] arg    pool name  --namespace arg      namespace name  --image arg          image name  --sparse-size arg    sparse size in B/K/M [default: 4K]  --no-progress        disable progress outpu</code></pre><blockquote><p>如果有多个增量备份，一定要按照顺序去导入，比如v1 v2 v3 v4 这样的顺序去恢复</p></blockquote><pre><code>[root@node-1 ~]# rbd -p rbd-snap-demo ls  #确认导入的poolsnap-new.imgsnap.imgvm1-clone.imgvm2-clone.imgvm3-clone.img[root@node-1 ~]# rbd import-diff rbd-test.img@v1 rbd-snap-demo/snap-new.img #将rbd-test.img@v1的增量备份恢复到rbd-snap-demo池下的snap-new.img块设备Importing image diff: 100% complete...done.[root@node-1 ~]# rbd snap ls rbd-snap-demo/snap-new.img  #查看导入成功SNAPID NAME SIZE   PROTECTED TIMESTAMP                     9 v1   10 GiB           Thu Mar  9 15:52:14 2023</code></pre><h1><span id="八-rbd导入导出总结">八、RBD导入导出总结</span></h1><p><img src="/images/Ceph/RBD%E9%AB%98%E7%BA%A7%E5%8A%9F%E8%83%BD/3.jpg"></p><h1><span id="九-rbd备份恢复与增量备份恢复实操">九、RBD备份恢复与增量备份恢复实操</span></h1><blockquote><p>xfs重新挂载时有可能会出现UUID重复的情况，导致无法挂载，可以使用mount -o nouuid &#x2F;dev&#x2F;rbd1 &#x2F;data-diff&#x2F; 解决</p></blockquote><h2><span id="1-创建pool">1、创建pool</span></h2><pre><code>[root@node-1 /]# ceph osd pool create ceph-demo 16 16 pool &#39;ceph-demo&#39; created</code></pre><h2><span id="2-创建块设备1">2、创建块设备1</span></h2><pre><code>rbd create ceph-demo/rbd.img --image-feature layering --size 1G</code></pre><h2><span id="3-映射块设备到本地">3、映射块设备到本地</span></h2><pre><code>[root@node-1 /]# rbd device map ceph-demo/rbd.img/dev/rbd0</code></pre><h2><span id="4-格式化块设备1">4、格式化块设备1</span></h2><pre><code>[root@node-1 /]# mkfs.xfs /dev/rbd0 Discarding blocks...Done.meta-data=/dev/rbd0              isize=512    agcount=8, agsize=32768 blks         =                       sectsz=512   attr=2, projid32bit=1         =                       crc=1        finobt=0, sparse=0data     =                       bsize=4096   blocks=262144, imaxpct=25         =                       sunit=1024   swidth=1024 blksnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1log      =internal log           bsize=4096   blocks=2560, version=2         =                       sectsz=512   sunit=8 blks, lazy-count=1realtime =none                   extsz=4096   blocks=0, rtextents=0</code></pre><h2><span id="5-挂载块设备1">5、挂载块设备1</span></h2><pre><code>[root@node-1 /]# mount /dev/rbd0 /data[root@node-1 /]# mount -o nouuid /dev/rbd1 /data-diff/</code></pre><h2><span id="6-写入file1到块设备1">6、写入file1到块设备1</span></h2><pre><code>[root@node-1 /]# cd /data[root@node-1 data]# ls[root@node-1 data]# echo &quot;数据&quot; &gt; file1.txt[root@node-1 data]# cat file1.txt 数据</code></pre><h2><span id="7-快照块设备1">7、快照块设备1</span></h2><pre><code>[root@node-1 /]# rbd snap create ceph-demo/rbd.img@rbd-bakup.img[root@node-1 /]# rbd snap ls ceph-demo/rbd.imgSNAPID NAME          SIZE  PROTECTED TIMESTAMP                     4 rbd-bakup.img 1 GiB           Thu Mar  9 17:09:06 2023</code></pre><h2><span id="8-恢复块设备1为块设备2">8、恢复块设备1为块设备2</span></h2><pre><code>[root@node-1 ~]# rbd export ceph-demo/rbd.img@rbd-bakup.img rbd-bakup.imgExporting image: 100% complete...done.[root@node-1 ~]# rbd import rbd-bakup.img ceph-demo/rbd-bakup.imgImporting image: 100% complete...done.</code></pre><h2><span id="9-映射块设备2">9、映射块设备2</span></h2><pre><code>[root@node-1 ~]# rbd device map ceph-demo/rbd-bakup.img/dev/rbd1</code></pre><h2><span id="10-挂载块设备2">10、挂载块设备2</span></h2><pre><code>[root@node-1 ~]# mount /dev/rbd1 /data-diff/</code></pre><h2><span id="11-查看块设备2的数据是否有file1">11、查看块设备2的数据是否有file1</span></h2><pre><code>[root@node-1 ~]# cd /data-diff/[root@node-1 data-diff]# lsfile1.txt  lost+found</code></pre><h2><span id="12-写入file2到块设备1">12、写入file2到块设备1</span></h2><pre><code>[root@node-1 data]# echo &quot;增量数据&quot; &gt; file2.txt[root@node-1 data]# cat file2.txt 增量数据</code></pre><h2><span id="13-增量备份块设备1">13、增量备份块设备1</span></h2><pre><code>[root@node-1 /]# rbd snap create ceph-demo/rbd.img@rbd-bakup-diff.img[root@node-1 /]# rbd export-diff ceph-demo/rbd.img@rbd-bakup-diff.img rbd-bakup-diff.imgExporting image: 100% complete...done.</code></pre><h2><span id="14-增量恢复块设备2">14、增量恢复块设备2</span></h2><pre><code>[root@node-1 /]# rbd import-diff rbd-bakup-diff.img ceph-demo/rbd-bakup.imgImporting image diff: 100% complete...done.</code></pre><h2><span id="15-查看块设备2是否有file2文件">15 、查看块设备2是否有file2文件</span></h2><pre><code>[root@node-1 /]# mount /dev/rbd1 /data-diff/[root@node-1 /]# lsbin  boot  data  data-diff  dev  etc  home  lib  lib64  media  mnt  opt  proc  rbd-bakup-diff.img  root  run  sbin  srv  sys  tmp  usr  var[root@node-1 /]# cd /data-diff/[root@node-1 data-diff]# lsfile1.txt  file2.txt  lost+found[root@node-1 data-diff]# cat file2.txt增量数据</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> RBD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RGW高可用</title>
      <link href="/2023/04/19/Ceph/11.RGW%E9%AB%98%E5%8F%AF%E7%94%A8/RGW%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
      <url>/2023/04/19/Ceph/11.RGW%E9%AB%98%E5%8F%AF%E7%94%A8/RGW%E9%AB%98%E5%8F%AF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-%E6%89%A9%E5%B1%95rgw%E9%9B%86%E7%BE%A4">一、扩展RGW集群</a></li><li><a href="#%E4%BA%8C-%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E7%AB%AF%E5%8F%A3">二、修改默认端口</a></li><li><a href="#%E4%B8%89-%E4%BF%AE%E6%94%B9%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%8C%87%E5%90%91">三、修改客户端指向</a></li><li><a href="#%E5%9B%9B-haproxykeepalived%E6%9E%84%E5%BB%BArgw%E9%AB%98%E5%8F%AF%E7%94%A8">四、haproxy+keepalived构建RGW高可用</a><ul><li><a href="#1-%E7%8E%AF%E5%A2%83%E8%AF%B4%E6%98%8E">1. 环境说明</a></li><li><a href="#2-keepalived%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE">2. keepalived安装配置</a><ul><li><a href="#21-%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6">2.1 安装软件</a></li><li><a href="#22-%E9%85%8D%E7%BD%AEvip">2.2 配置VIP</a></li></ul></li><li><a href="#3%E5%AE%89%E8%A3%85haproxy">3.安装HAproxy</a></li><li><a href="#4%E6%B5%8B%E8%AF%95vip">4.测试VIP</a></li></ul></li></ul><!-- tocstop --><p><img src="/images/Ceph/RGW%E9%AB%98%E5%8F%AF%E7%94%A8/1.jpg"></p><h1><span id="一-扩展rgw集群">一、扩展RGW集群</span></h1><pre><code>[root@node-1 ~]# cd /opt/my-cluster/ #进入到ceph-deploy文件夹[root@node-1 my-cluster]# ceph-deploy rgw create node-2  #部署node-2 RGW网关[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy rgw create node-2[ceph_deploy.cli][INFO  ] ceph-deploy options:[ceph_deploy.cli][INFO  ]  username                      : None[ceph_deploy.cli][INFO  ]  verbose                       : False[ceph_deploy.cli][INFO  ]  rgw                           : [(&#39;node-2&#39;, &#39;rgw.node-2&#39;)][ceph_deploy.cli][INFO  ]  overwrite_conf                : False[ceph_deploy.cli][INFO  ]  subcommand                    : create[ceph_deploy.cli][INFO  ]  quiet                         : False[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f33196fcb00&gt;[ceph_deploy.cli][INFO  ]  cluster                       : ceph[ceph_deploy.cli][INFO  ]  func                          : &lt;function rgw at 0x7f33195adcf8&gt;[ceph_deploy.cli][INFO  ]  ceph_conf                     : None[ceph_deploy.cli][INFO  ]  default_release               : False[ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts node-2:rgw.node-2[node-2][DEBUG ] connected to host: node-2 [node-2][DEBUG ] detect platform information from remote host[node-2][DEBUG ] detect machine type[ceph_deploy.rgw][INFO  ] Distro info: CentOS Linux 7.9.2009 Core[ceph_deploy.rgw][DEBUG ] remote host will use systemd[ceph_deploy.rgw][DEBUG ] deploying rgw bootstrap to node-2[node-2][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[node-2][WARNIN] rgw keyring does not exist yet, creating one[node-2][DEBUG ] create a keyring file[node-2][DEBUG ] create path recursively if it doesn&#39;t exist[node-2][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-rgw --keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring auth get-or-create client.rgw.node-2 osd allow rwx mon allow rw -o /var/lib/ceph/radosgw/ceph-rgw.node-2/keyring[node-2][INFO  ] Running command: systemctl enable ceph-radosgw@rgw.node-2[node-2][WARNIN] Created symlink from /etc/systemd/system/ceph-radosgw.target.wants/ceph-radosgw@rgw.node-2.service to /usr/lib/systemd/system/ceph-radosgw@.service.[node-2][INFO  ] Running command: systemctl start ceph-radosgw@rgw.node-2[node-2][INFO  ] Running command: systemctl enable ceph.target[ceph_deploy.rgw][INFO  ] The Ceph Object Gateway (RGW) is now running on host node-2 and default port 7480[root@node-1 my-cluster]# vim ceph.conf  #修改node-2的默认端口为80  [global]fsid = b8e58b30-4568-4032-a9f4-837ed3fa9529public_network = 192.168.187.0/24cluster_network = 192.168.199.0/24mon_initial_members = node-1mon_host = 192.168.187.201auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxmon_allow_pool_delete = true #可手动删除pool[osd]osd crush update on start = false[client.rgw.node-1]rgw_frontends = &quot;civetweb port=80&quot;[client.rgw.node-2]rgw_frontends = &quot;civetweb port=80&quot;[root@node-1 my-cluster]# ceph-deploy --overwrite-conf config push node-1 node-2 node-3 #将修改好的文件推送到其他节点中[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy --overwrite-conf config push node-1 node-2 node-3[ceph_deploy.cli][INFO  ] ceph-deploy options:[ceph_deploy.cli][INFO  ]  username                      : None[ceph_deploy.cli][INFO  ]  verbose                       : False[ceph_deploy.cli][INFO  ]  overwrite_conf                : True[ceph_deploy.cli][INFO  ]  subcommand                    : push[ceph_deploy.cli][INFO  ]  quiet                         : False[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc2f33838c0&gt;[ceph_deploy.cli][INFO  ]  cluster                       : ceph[ceph_deploy.cli][INFO  ]  client                        : [&#39;node-1&#39;, &#39;node-2&#39;, &#39;node-3&#39;][ceph_deploy.cli][INFO  ]  func                          : &lt;function config at 0x7fc2f2ef4938&gt;[ceph_deploy.cli][INFO  ]  ceph_conf                     : None[ceph_deploy.cli][INFO  ]  default_release               : False[ceph_deploy.config][DEBUG ] Pushing config to node-1[node-1][DEBUG ] connected to host: node-1 [node-1][DEBUG ] detect platform information from remote host[node-1][DEBUG ] detect machine type[node-1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph_deploy.config][DEBUG ] Pushing config to node-2[node-2][DEBUG ] connected to host: node-2 [node-2][DEBUG ] detect platform information from remote host[node-2][DEBUG ] detect machine type[node-2][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph_deploy.config][DEBUG ] Pushing config to node-3[node-3][DEBUG ] connected to host: node-3 [node-3][DEBUG ] detect platform information from remote host[node-3][DEBUG ] detect machine type[node-3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[root@node-2 log]# systemctl restart ceph-radosgw@node-2 #在node-2中重启rgw服务[root@node-2 log]# systemctl restart ceph-radosgw.target #在node-2中重启rgw服务[root@node-2 log]# netstat -ntlp | grep 80tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      6626/radosgw       [root@node-1 my-cluster]# ceph -s   cluster:    id:     b8e58b30-4568-4032-a9f4-837ed3fa9529    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 8h)    mgr: node-1(active, since 2d), standbys: node-3, node-2    osd: 6 osds: 6 up (since 4h), 6 in (since 32h)    rgw: 2 daemons active (node-1, node-2)   task status:   data:    pools:   7 pools, 208 pgs    objects: 230 objects, 17 MiB    usage:   6.2 GiB used, 54 GiB / 60 GiB avail    pgs:     208 active+clean</code></pre><h1><span id="二-修改默认端口">二、修改默认端口</span></h1><pre><code>[root@node-1 my-cluster]# vim ceph.conf [global]fsid = b8e58b30-4568-4032-a9f4-837ed3fa9529public_network = 192.168.187.0/24cluster_network = 192.168.199.0/24mon_initial_members = node-1mon_host = 192.168.187.201auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxmon_allow_pool_delete = true #可手动删除pool[osd]osd crush update on start = false[client.rgw.node-1]rgw_frontends = &quot;civetweb port=81&quot;[client.rgw.node-2]rgw_frontends = &quot;civetweb port=81&quot;~                                                                                                                                          ~                                                                                                                                          ~                                                                                                                                          ~                                                                                                                                          ~                                                                                                                                          &quot;ceph.conf&quot; 20L, 472C written                                                                                            [root@node-1 my-cluster]# ceph-deploy --overwrite-conf config push node-1 node-2 node-3[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy --overwrite-conf config push node-1 node-2 node-3[ceph_deploy.cli][INFO  ] ceph-deploy options:[ceph_deploy.cli][INFO  ]  username                      : None[ceph_deploy.cli][INFO  ]  verbose                       : False[ceph_deploy.cli][INFO  ]  overwrite_conf                : True[ceph_deploy.cli][INFO  ]  subcommand                    : push[ceph_deploy.cli][INFO  ]  quiet                         : False[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2848b768c0&gt;[ceph_deploy.cli][INFO  ]  cluster                       : ceph[ceph_deploy.cli][INFO  ]  client                        : [&#39;node-1&#39;, &#39;node-2&#39;, &#39;node-3&#39;][ceph_deploy.cli][INFO  ]  func                          : &lt;function config at 0x7f28486e7938&gt;[ceph_deploy.cli][INFO  ]  ceph_conf                     : None[ceph_deploy.cli][INFO  ]  default_release               : False[ceph_deploy.config][DEBUG ] Pushing config to node-1[node-1][DEBUG ] connected to host: node-1 [node-1][DEBUG ] detect platform information from remote host[node-1][DEBUG ] detect machine type[node-1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph_deploy.config][DEBUG ] Pushing config to node-2[node-2][DEBUG ] connected to host: node-2 [node-2][DEBUG ] detect platform information from remote host[node-2][DEBUG ] detect machine type[node-2][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph_deploy.config][DEBUG ] Pushing config to node-3[node-3][DEBUG ] connected to host: node-3 [node-3][DEBUG ] detect platform information from remote host[node-3][DEBUG ] detect machine type[node-3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[root@node-1 my-cluster]# systemctl restart ceph-radosgw@node-1[root@node-1 my-cluster]#  systemctl restart ceph-radosgw.target[root@node-1 my-cluster]# netstat -ntlp | grep -w  81tcp        0      0 0.0.0.0:81              0.0.0.0:*               LISTEN      45628/radosgw       [root@node-2 my-cluster]# systemctl restart ceph-radosgw@node-2[root@node-2 my-cluster]#  systemctl restart ceph-radosgw.target[root@node-2 my-cluster]# netstat -ntlp | grep -w  81tcp        0      0 0.0.0.0:81              0.0.0.0:*               LISTEN      45628/radosgw  </code></pre><h1><span id="三-修改客户端指向">三、修改客户端指向</span></h1><p>node1:</p><pre><code>[root@node-1 my-cluster]# vim /root/.s3cfg  #修改s3的配置文件host_base = 192.168.187.200:80host_bucket = 192.168.187.200:80/%(bucket)s[root@node-1 my-cluster]# cat /root/swift-openrc.sh  #修改swift的配置文件export ST_AUTH=http://192.168.187.200:80/authexport ST_USER=ceph-s3-user:swift export ST_KEY=gltihmYN7FSyopcxY0lHh2N8FCqTayhXgG5cDEDn</code></pre><p>测试：</p><pre><code>[root@node-1 my-cluster]# s3cmd ls #查看bucket2023-03-09 10:33  s3://ceph-s3-bucket2023-03-09 10:36  s3://s3cmd-demo2023-03-09 10:39  s3://swift-demo[root@node-1 my-cluster]# s3cmd mb s3://test-1 #创建bucketBucket &#39;s3://test-1/&#39; created[root@node-1 my-cluster]# s3cmd ls #查看创建的bucket2023-03-09 10:33  s3://ceph-s3-bucket 2023-03-09 10:36  s3://s3cmd-demo2023-03-09 10:39  s3://swift-demo2023-03-09 12:25  s3://test-1[root@node-1 my-cluster]# . /root/swift-openrc.sh  #设置环境变量[root@node-1 my-cluster]# swift list  #swift查看bucketceph-s3-buckets3cmd-demoswift-demotest-1[root@node-1 my-cluster]# swift post test-2 #swift创建bucket[root@node-1 my-cluster]# swift list  #查看创建的bucketceph-s3-buckets3cmd-demoswift-demotest-1test-2[root@node-1 my-cluster]# s3cmd rb s3://test-1 #删除bucketBucket &#39;s3://test-1/&#39; removedYou have new mail in /var/spool/mail/root[root@node-1 my-cluster]# swift delete test-2 #删除buckettest-2</code></pre><h1><span id="四-haproxykeepalived构建rgw高可用">四、haproxy+keepalived构建RGW高可用</span></h1><h2><span id="1-环境说明">1. 环境说明</span></h2><pre><code>[root@node-1 my-cluster]# vim ceph.conf [global]fsid = b8e58b30-4568-4032-a9f4-837ed3fa9529public_network = 192.168.187.0/24cluster_network = 192.168.199.0/24mon_initial_members = node-1mon_host = 192.168.187.201auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxmon_allow_pool_delete = true #可手动删除pool[osd]osd crush update on start = false[client.rgw.node-1]rgw_frontends = &quot;civetweb port=81&quot;[client.rgw.node-2]rgw_frontends = &quot;civetweb port=81&quot;~                                                                                                                                          ~                                                                                                                                          ~                                                                                                                                          ~                                                                                                                                          ~                                                                                                                                          &quot;ceph.conf&quot; 20L, 472C written                                                                                            [root@node-1 my-cluster]# ceph-deploy --overwrite-conf config push node-1 node-2 node-3[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy --overwrite-conf config push node-1 node-2 node-3[ceph_deploy.cli][INFO  ] ceph-deploy options:[ceph_deploy.cli][INFO  ]  username                      : None[ceph_deploy.cli][INFO  ]  verbose                       : False[ceph_deploy.cli][INFO  ]  overwrite_conf                : True[ceph_deploy.cli][INFO  ]  subcommand                    : push[ceph_deploy.cli][INFO  ]  quiet                         : False[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f2848b768c0&gt;[ceph_deploy.cli][INFO  ]  cluster                       : ceph[ceph_deploy.cli][INFO  ]  client                        : [&#39;node-1&#39;, &#39;node-2&#39;, &#39;node-3&#39;][ceph_deploy.cli][INFO  ]  func                          : &lt;function config at 0x7f28486e7938&gt;[ceph_deploy.cli][INFO  ]  ceph_conf                     : None[ceph_deploy.cli][INFO  ]  default_release               : False[ceph_deploy.config][DEBUG ] Pushing config to node-1[node-1][DEBUG ] connected to host: node-1 [node-1][DEBUG ] detect platform information from remote host[node-1][DEBUG ] detect machine type[node-1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph_deploy.config][DEBUG ] Pushing config to node-2[node-2][DEBUG ] connected to host: node-2 [node-2][DEBUG ] detect platform information from remote host[node-2][DEBUG ] detect machine type[node-2][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph_deploy.config][DEBUG ] Pushing config to node-3[node-3][DEBUG ] connected to host: node-3 [node-3][DEBUG ] detect platform information from remote host[node-3][DEBUG ] detect machine type[node-3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[root@node-1 my-cluster]# systemctl restart ceph-radosgw@node-1[root@node-1 my-cluster]#  systemctl restart ceph-radosgw.target[root@node-1 my-cluster]# netstat -ntlp | grep -w  81tcp        0      0 0.0.0.0:81              0.0.0.0:*               LISTEN      45628/radosgw       [root@node-2 my-cluster]# systemctl restart ceph-radosgw@node-2[root@node-2 my-cluster]#  systemctl restart ceph-radosgw.target[root@node-2 my-cluster]# netstat -ntlp | grep -w  81tcp        0      0 0.0.0.0:81              0.0.0.0:*               LISTEN      45628/radosgw  </code></pre><h2><span id="2-keepalived安装配置">2. keepalived安装配置</span></h2><h3><span id="21-安装软件">2.1 安装软件</span></h3><pre><code>yum -y install keepalived 分别在两个节点node-1和node-2上执⾏安装即可。</code></pre><h3><span id="22-配置vip">2.2 配置VIP</span></h3><p>master节点配置(node-1)</p><pre><code>[root@node-1 my-cluster]#  cd /etc/keepalived/[root@node-1 keepalived]# lskeepalived.conf[root@node-1 keepalived]# cat keepalived.conf ! Configuration File for keepalivedglobal_defs &#123;   notification_email &#123;     acassen@firewall.loc     failover@firewall.loc     sysadmin@firewall.loc   &#125;   notification_email_from Alexandre.Cassen@firewall.loc   smtp_server 192.168.200.1   smtp_connect_timeout 30   router_id LVS_DEVEL   vrrp_skip_check_adv_addr   vrrp_garp_interval 0   vrrp_gna_interval 0&#125;vrrp_script chk_haproxy &#123; script &quot;killall -0 haproxy&quot; interval 2 weight -2&#125;vrrp_instance RGW &#123;    state MASTER    interface eth0  #有可能需要修改网卡    virtual_router_id 51    priority 100    advert_int 1    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        192.168.187.200/24 #需要修改VIP地址    &#125;    track_script &#123;    chk_haproxy    &#125;&#125;[root@node-1 keepalived]# scp keepalived.conf node-2:/etc/keeplivedkeepalived.conf                                                                                          100%  755   838.8KB/s   00:00    [root@node-1 keepalived]# systemctl restart keepalived #重启[root@node-1 keepalived]# systemctl enable keepalived#开机自启Created symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.[root@node-1 keepalived]# tail -f /var/log/messagesMar  9 19:32:35 node-1 Keepalived_vrrp[48310]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 19:32:37 node-1 Keepalived_vrrp[48310]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 19:32:39 node-1 Keepalived_vrrp[48310]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 19:32:41 node-1 Keepalived_vrrp[48310]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 19:32:43 node-1 Keepalived_vrrp[48310]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 19:32:45 node-1 Keepalived_vrrp[48310]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 19:32:47 node-1 Keepalived_vrrp[48310]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 19:32:49 node-1 Keepalived_vrrp[48310]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 19:32:51 node-1 Keepalived_vrrp[48310]: /usr/bin/killall -0 haproxy exited with status 1[root@node-1 my-cluster]# iptables -t filter -L -n Chain INPUT (policy ACCEPT)target     prot opt source               destination         Chain FORWARD (policy ACCEPT)target     prot opt source               destination         Chain OUTPUT (policy ACCEPT)target     prot opt source               destination</code></pre><p>BAKUP节点配置（node-2）</p><pre><code>[root@node-2 my-cluster]#  cd /etc/keepalived/[root@node-2 keepalived]# lskeepalived.conf[root@node-2 keepalived]# cat keepalived.conf ! Configuration File for keepalivedglobal_defs &#123;   notification_email &#123;     acassen@firewall.loc     failover@firewall.loc     sysadmin@firewall.loc   &#125;   notification_email_from Alexandre.Cassen@firewall.loc   smtp_server 192.168.200.1   smtp_connect_timeout 30   router_id LVS_DEVEL   vrrp_skip_check_adv_addr      vrrp_garp_interval 0   vrrp_gna_interval 0&#125;vrrp_script chk_haproxy &#123; script &quot;killall -0 haproxy&quot; interval 2 weight -2&#125;vrrp_instance RGW &#123;    state BACKUP    interface ens33 #有可能需要修改网卡    virtual_router_id 51    priority 99    advert_int 1    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        192.168.187.200/24 #需要修改VIP地址    &#125;    track_script &#123;    chk_haproxy    &#125;&#125;[root@node-2 keepalived]# systemctl restart keepalived[root@node-2 keepalived]# systemctl enable keepalivedCreated symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.</code></pre><h2><span id="3安装haproxy">3.安装HAproxy</span></h2><p>node-1</p><pre><code>[root@node-1 keepalived]# yum install haproxy -y #节点1安装[root@node-1 keepalived]# cd /etc/haproxy/[root@node-1 haproxy]# lshaproxy.cfg[root@node-1 haproxy]# vim haproxy.cfg global  log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/statsdefaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000frontend http_web *:80 mode http default_backend rgwbackend rgw balance roundrobin mode http server node-1 192.168.187.201:81 check #需要修改IP地址 server node-2 192.168.187.202:81 check #需要修改IP地址[root@node-1 haproxy]# systemctl restart haproxy[root@node-1 haproxy]# systemctl enable haproxyCreated symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.[root@node-1 haproxy]# ps aux | grep haproxy root       49724  0.0  0.0  44744  1756 ?        Ss   19:46   0:00 /usr/sbin/haproxy-systemd-wrapper -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pidhaproxy    49725  0.0  0.1  48460  3128 ?        S    19:46   0:00 /usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -Dshaproxy    49726  0.0  0.0  48460  1308 ?        Ss   19:46   0:00 /usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -Dsroot       49865  0.0  0.0 112812   980 pts/0    R+   19:48   0:00 grep --color=auto haproxy</code></pre><p>node-2</p><pre><code>[root@node-2 keepalived]# yum install haproxy -y #节点2安装[root@node-2 keepalived]# cd /etc/haproxy/[root@node-2 haproxy]# lshaproxy.cfg[root@node-2 haproxy]# vim haproxy.cfgglobal  log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/statsdefaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000frontend http_web *:80 mode http default_backend rgwbackend rgw balance roundrobin mode http server node-1 192.168.187.201:81 check  #需要修改IP地址 server node-2 192.168.187.202:81 check  #需要修改IP地址[root@node-2 haproxy]# systemctl enable haproxyCreated symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.[root@node-1 haproxy]# ps aux | grep haproxy root       49724  0.0  0.0  44744  1756 ?        Ss   19:46   0:00 /usr/sbin/haproxy-systemd-wrapper -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pidhaproxy    49725  0.0  0.1  48460  3128 ?        S    19:46   0:00 /usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -Dshaproxy    49726  0.0  0.0  48460  1308 ?        Ss   19:46   0:00 /usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -Dsroot       49865  0.0  0.0 112812   980 pts/0    R+   19:48   0:00 grep --color=auto haproxy</code></pre><h2><span id="4测试vip">4.测试VIP</span></h2><p>node-1</p><pre><code>[root@node-1 my-cluster]# ip addr show ens33 #查看VIP2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:58:07:8c brd ff:ff:ff:ff:ff:ff    inet 192.168.187.201/24 brd 192.168.187.255 scope global noprefixroute ens33       valid_lft forever preferred_lft forever    inet 192.168.187.200/24 scope global secondary ens33       valid_lft forever preferred_lft forever    inet6 fe80::853:dac8:cf9c:a472/64 scope link tentative noprefixroute dadfailed        valid_lft forever preferred_lft forever    inet6 fe80::6f94:11d4:e167:3e5a/64 scope link noprefixroute        valid_lft forever preferred_lft forever[root@node-1 my-cluster]# systemctl stop haproxy  #停止HA服务[root@node-1 my-cluster]# ip addr show ens33  #查看IP完成漂移2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:58:07:8c brd ff:ff:ff:ff:ff:ff    inet 192.168.187.201/24 brd 192.168.187.255 scope global noprefixroute ens33       valid_lft forever preferred_lft forever    inet6 fe80::853:dac8:cf9c:a472/64 scope link tentative noprefixroute dadfailed        valid_lft forever preferred_lft forever    inet6 fe80::6f94:11d4:e167:3e5a/64 scope link noprefixroute        valid_lft forever preferred_lft forever[root@node-1 my-cluster]# tail -f /var/log/messages #查看日志报错Mar  9 20:35:19 node-1 Keepalived_vrrp[4651]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 20:35:21 node-1 Keepalived_vrrp[4651]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 20:35:23 node-1 Keepalived_vrrp[4651]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 20:35:25 node-1 Keepalived_vrrp[4651]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 20:35:27 node-1 Keepalived_vrrp[4651]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 20:35:29 node-1 Keepalived_vrrp[4651]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 20:35:31 node-1 Keepalived_vrrp[4651]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 20:35:33 node-1 Keepalived_vrrp[4651]: /usr/bin/killall -0 haproxy exited with status 1Mar  9 20:35:35 node-1 Keepalived_vrrp[4651]: /usr/bin/killall -0 haproxy exited with status 1[root@node-1 my-cluster]# systemctl restart haproxy [root@node-1 my-cluster]# ip a show ens33 #VIP回归2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:58:07:8c brd ff:ff:ff:ff:ff:ff    inet 192.168.187.201/24 brd 192.168.187.255 scope global noprefixroute ens33       valid_lft forever preferred_lft forever    inet 192.168.187.200/24 scope global secondary ens33       valid_lft forever preferred_lft forever    inet6 fe80::853:dac8:cf9c:a472/64 scope link tentative noprefixroute dadfailed        valid_lft forever preferred_lft forever    inet6 fe80::6f94:11d4:e167:3e5a/64 scope link noprefixroute        valid_lft forever preferred_lft forever[root@node-1 my-cluster]# cat /var/log/messages| grep VRRP_Mar  9 19:30:21 node-1 Keepalived_vrrp[48310]: VRRP_Instance(RGW) removing protocol VIPs.Mar  9 19:30:21 node-1 Keepalived_vrrp[48310]: VRRP_Instance(RGW) removing protocol iptable drop ruleMar  9 19:30:21 node-1 Keepalived_vrrp[48310]: VRRP_Instance(RGW) Entering BACKUP STATEMar  9 19:30:22 node-1 Keepalived_vrrp[48310]: VRRP_Instance(RGW) Changing effective priority from 100 to 98 #将权重修改成98</code></pre><p>node-2</p><pre><code>[root@node-2 ~]# ip a show ens33 #查看IP完成漂移2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:e6:b6:33 brd ff:ff:ff:ff:ff:ff    inet 192.168.187.202/24 brd 192.168.187.255 scope global noprefixroute ens33       valid_lft forever preferred_lft forever    inet 192.168.187.200/24 scope global secondary ens33       valid_lft forever preferred_lft forever    inet6 fe80::853:dac8:cf9c:a472/64 scope link noprefixroute        valid_lft forever preferred_lft forever</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> RGW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph集群测试</title>
      <link href="/2023/04/19/Ceph/12.Ceph%E9%9B%86%E7%BE%A4%E6%B5%8B%E8%AF%95/Ceph%E9%9B%86%E7%BE%A4%E6%B5%8B%E8%AF%95/"/>
      <url>/2023/04/19/Ceph/12.Ceph%E9%9B%86%E7%BE%A4%E6%B5%8B%E8%AF%95/Ceph%E9%9B%86%E7%BE%A4%E6%B5%8B%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-mon%E9%AB%98%E5%8F%AF%E7%94%A8">一、Mon高可用</a><ul><li><a href="#1-%E6%A8%A1%E6%8B%9Fnode-2-mon%E6%95%85%E9%9A%9C">1、模拟Node-2 Mon故障</a></li><li><a href="#2-%E6%A8%A1%E6%8B%9Fnode-2-%E5%92%8Cnode-3-mon%E5%90%8C%E6%97%B6%E6%95%85%E9%9A%9C">2、模拟Node-2 和Node-3 mon同时故障</a></li></ul></li><li><a href="#%E4%BA%8C-mds%E4%B8%BB%E4%BB%8E%E5%88%87%E6%8D%A2">二、MDS主从切换</a><ul><li><a href="#1-%E6%9F%A5%E7%9C%8B%E7%8A%B6%E6%80%81">1、查看状态</a></li><li><a href="#2-%E6%A8%A1%E6%8B%9Fnode-1-mds%E6%95%85%E9%9A%9C">2、模拟node-1 MDS故障</a></li><li><a href="#3-%E6%A8%A1%E6%8B%9Fnode-1-%E5%92%8Cnode-2-mds%E6%95%85%E9%9A%9C">3、模拟node-1 和node-2 MDS故障</a></li></ul></li><li><a href="#%E4%B8%89-rgw%E9%AB%98%E5%8F%AF%E7%94%A8%E6%B5%8B%E8%AF%95">三、RGW高可用测试</a><ul><li><a href="#1-%E4%B8%AD%E6%96%ADnode1%E7%9A%84rgw%E6%9C%8D%E5%8A%A1">1、中断node1的RGW服务</a></li><li><a href="#2-%E6%B5%8B%E8%AF%95s3-cmd%E5%92%8Cswift">2、测试s3-cmd和Swift</a></li></ul></li><li><a href="#%E5%9B%9B-osd%E5%9D%8F%E7%9B%98%E6%B5%8B%E8%AF%95">四、OSD坏盘测试</a><ul><li><a href="#1-%E6%A8%A1%E6%8B%9Fnode-1%E6%95%85%E9%9A%9C">1、模拟node-1故障</a></li><li><a href="#2-%E6%A8%A1%E6%8B%9Fnode-1%E5%92%8Cnode-2%E5%90%8C%E6%97%B6%E6%95%85%E9%9A%9C">2、模拟node-1和node-2同时故障</a></li></ul></li><li><a href="#%E4%BA%94-fio%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95">五、fio性能测试</a><ul><li><a href="#1-4k%E9%9A%8F%E6%9C%BA%E5%86%99-iops">1. 4K随机写-iops</a></li><li><a href="#2-4k%E9%9A%8F%E6%9C%BA%E8%AF%BB-iops">2. 4k随机读-iops</a></li><li><a href="#3-4k%E9%9A%8F%E6%9C%BA%E8%AF%BB%E5%86%99-iops">3. 4k随机读写-iops</a></li><li><a href="#4-1m%E9%A1%BA%E5%BA%8F%E5%86%99-%E5%90%9E%E5%90%90">4. 1M顺序写-吞吐</a></li></ul></li><li><a href="#%E5%85%AD-rbd-bench%E6%B5%8B%E8%AF%95">六、rbd bench测试</a><ul><li><a href="#14k%E9%9A%8F%E6%9C%BA%E5%86%99">1.4K随机写</a></li><li><a href="#24k%E9%9A%8F%E6%9C%BA%E8%AF%BB">2.4K随机读</a></li><li><a href="#34k%E9%9A%8F%E6%9C%BA%E6%B7%B7%E5%90%88%E8%AF%BB%E5%86%99">3.4K随机混合读写</a></li><li><a href="#41m%E9%A1%BA%E5%BA%8F%E5%86%99">4.1M顺序写</a></li></ul></li></ul><!-- tocstop --><h1><span id="一-mon高可用">一、Mon高可用</span></h1><h2><span id="1-模拟node-2-mon故障">1、模拟Node-2 Mon故障</span></h2><pre><code>[root@node-1 my-cluster]# ssh node-2 #远程到节点2Last login: Thu Mar  9 20:10:10 2023 from 192.168.187.1[root@node-2 ~]# [root@node-2 ~]# systemctl stop ceph-mon@node-2 #停止Mon服务    [root@node-2 ~]# logout[root@node-1 my-cluster]# ceph -s 查看集群状态，可以看到一个mon故障  cluster:    id:     b8e58b30-4568-4032-a9f4-837ed3fa9529    health: HEALTH_WARN            1/3 mons down, quorum node-1,node-3   services:    mon: 3 daemons, quorum node-1,node-3 (age 2s), out of quorum: node-2    mgr: node-1(active, since 18h), standbys: node-3, node-2    osd: 6 osds: 6 up (since 18h), 6 in (since 2d)    rgw: 2 daemons active (node-1, node-2)   task status:   data:    pools:   7 pools, 208 pgs    objects: 262 objects, 17 MiB    usage:   6.2 GiB used, 54 GiB / 60 GiB avail    pgs:     208 active+clean[root@node-1 my-cluster]# rbd create --size 1G ceph-demo/test-demo #尝试创建一个RBD块设备[root@node-1 my-cluster]# rbd -p ceph-demo ls  #创建成功，可以判定当前故障一个MON节点对集群没有影响rbd-bakup.imgrbd.imgtest-demo</code></pre><h2><span id="2-模拟node-2-和node-3-mon同时故障">2、模拟Node-2 和Node-3 mon同时故障</span></h2><pre><code>[root@node-1 my-cluster]# ssh node-3 #远程到node-3Last login: Tue Mar  7 13:38:26 2023 from node-1[root@node-3 ~]# systemctl stop ceph-mon@node-3 #停止node-3的Mon服务 [root@node-3 ~]#logout[root@node-1 my-cluster]# ceph -s #查看集群状态卡死，无法读取[root@node-1 my-cluster]# tail -f /var/log/ceph/ceph-mon.node-1.log #查看集群一直报错，由于整个集群一共有3个Mon，导致只剩一个Mon的时候没办法选举，只能一直报错，Paxos选举要求就是要超过半数以上的节点是在线且正常的2023-03-10 15:08:51.130 7f8c75d13700  0 mon.node-1@0(probing) e3 handle_command mon_command(&#123;&quot;prefix&quot;: &quot;mon metadata&quot;, &quot;id&quot;: &quot;node-3&quot;&#125; v 0) v12023-03-10 15:08:51.130 7f8c75d13700  0 log_channel(audit) log [DBG] : from=&#39;mgr.44223 192.168.187.201:0/1094&#39; entity=&#39;mgr.node-1&#39; cmd=[&#123;&quot;prefix&quot;: &quot;mon metadata&quot;, &quot;id&quot;: &quot;node-3&quot;&#125;]: dispatch[root@node-1 my-cluster]# ssh node-2 systemctl restart ceph-mon@node-2 #恢复集群[root@node-1 my-cluster]# ssh node-3 systemctl restart ceph-mon@node-3 #恢复集群[root@node-1 my-cluster]# ceph -s  #集群恢复成功  cluster:    id:     b8e58b30-4568-4032-a9f4-837ed3fa9529    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 33s)    mgr: node-1(active, since 19h), standbys: node-3, node-2    osd: 6 osds: 6 up (since 19h), 6 in (since 2d)    rgw: 2 daemons active (node-1, node-2)   task status:   data:    pools:   7 pools, 208 pgs    objects: 265 objects, 17 MiB    usage:   6.2 GiB used, 54 GiB / 60 GiB avail    pgs:     208 active+clean</code></pre><h1><span id="二-mds主从切换">二、MDS主从切换</span></h1><h2><span id="1-查看状态">1、查看状态</span></h2><pre><code>[root@node-1 my-cluster]# ceph -s #查看集群状态   cluster:    id:     b8e58b30-4568-4032-a9f4-837ed3fa9529    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 6m)    mgr: node-1(active, since 19h), standbys: node-3, node-2    mds: cephfs-demo:1 &#123;0=node-1=up:active&#125; 2 up:standby    osd: 6 osds: 6 up (since 19h), 6 in (since 2d)    rgw: 2 daemons active (node-1, node-2)   task status:   data:    pools:   9 pools, 240 pgs    objects: 287 objects, 17 MiB    usage:   6.2 GiB used, 54 GiB / 60 GiB avail    pgs:     240 active+clean[root@node-1 my-cluster]#  ceph mds stat  #查看MDS状态cephfs-demo:1 &#123;0=node-1=up:active&#125; 2 up:standby[root@node-1 my-cluster]# mount -t ceph 192.168.187.201:6789:/ /mnt/cephfs/ -o name=admin #挂载Ceph FS文件系统，如之前挂载过，此命令可不执行[root@node-1 my-cluster]# df -HT #查看挂载情况Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  2.0G     0  2.0G   0% /devtmpfs                   tmpfs     2.0G     0  2.0G   0% /dev/shmtmpfs                   tmpfs     2.0G   21M  2.0G   2% /runtmpfs                   tmpfs     2.0G     0  2.0G   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.4G   16G  13% //dev/sda1               xfs       1.1G  204M  860M  20% /boottmpfs                   tmpfs     2.0G   25k  2.0G   1% /var/lib/ceph/osd/ceph-1tmpfs                   tmpfs     2.0G   25k  2.0G   1% /var/lib/ceph/osd/ceph-0tmpfs                   tmpfs     396M     0  396M   0% /run/user/0192.168.187.201:6789:/  ceph      9.1G     0  9.1G   0% /mnt/cephfs</code></pre><h2><span id="2-模拟node-1-mds故障">2、模拟node-1 MDS故障</span></h2><pre><code>Node-1：[root@node-1 my-cluster]# systemctl stop ceph-mds@node-1  #停止MDS服务[root@node-1 my-cluster]# ceph -s #查看当前MDS主节点切换到node-2了  cluster:    id:     b8e58b30-4568-4032-a9f4-837ed3fa9529    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 11m)    mgr: node-1(active, since 19h), standbys: node-3, node-2    mds: cephfs-demo:1 &#123;0=node-2=up:active&#125; 1 up:standby    osd: 6 osds: 6 up (since 19h), 6 in (since 2d)    rgw: 2 daemons active (node-1, node-2)   task status:   data:    pools:   9 pools, 240 pgs    objects: 287 objects, 17 MiB    usage:   6.2 GiB used, 54 GiB / 60 GiB avail    pgs:     240 active+clean[root@node-1 my-cluster]# ceph mds stat  #查看MDS状态cephfs-demo:1 &#123;0=node-2=up:active&#125; 1 up:standby[root@node-1 my-cluster]# df -HT #查看挂载情况Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  2.0G     0  2.0G   0% /devtmpfs                   tmpfs     2.0G     0  2.0G   0% /dev/shmtmpfs                   tmpfs     2.0G   21M  2.0G   2% /runtmpfs                   tmpfs     2.0G     0  2.0G   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.4G   16G  13% //dev/sda1               xfs       1.1G  204M  860M  20% /boottmpfs                   tmpfs     2.0G   25k  2.0G   1% /var/lib/ceph/osd/ceph-1tmpfs                   tmpfs     2.0G   25k  2.0G   1% /var/lib/ceph/osd/ceph-0tmpfs                   tmpfs     396M     0  396M   0% /run/user/0192.168.187.201:6789:/  ceph      9.1G     0  9.1G   0% /mnt/cephfs[root@node-1 my-cluster]# cd /mnt/cephfs/ #进入到Ceph Fs挂载目录[root@node-1 cephfs]# ls #查看当前目录为空[root@node-1 cephfs]# echo aaa &gt; 1.txt #尝试写入[root@node-1 cephfs]# cat 1.txt  #尝试读取aaanode-2：[root@node-2 ~]# tail -f /var/log/ceph/ceph-mds.node-2.log  #在node-2中查看MDS选举过程2023-03-10 15:24:58.344 7fa4bcf9a700  1 mds.0.9 handle_mds_map state change up:reconnect --&gt; up:rejoin2023-03-10 15:24:58.344 7fa4bcf9a700  1 mds.0.9 rejoin_start2023-03-10 15:24:58.344 7fa4bcf9a700  1 mds.0.9 rejoin_joint_start2023-03-10 15:24:58.348 7fa4b678d700  1 mds.0.9 rejoin_done2023-03-10 15:24:59.357 7fa4bcf9a700  1 mds.node-2 Updating MDS map to version 12 from mon.12023-03-10 15:24:59.357 7fa4bcf9a700  1 mds.0.9 handle_mds_map i am now mds.0.92023-03-10 15:24:59.357 7fa4bcf9a700  1 mds.0.9 handle_mds_map state change up:rejoin --&gt; up:active2023-03-10 15:24:59.357 7fa4bcf9a700  1 mds.0.9 recovery_done -- successful recovery!2023-03-10 15:24:59.358 7fa4bcf9a700  1 mds.0.9 active_start2023-03-10 15:24:59.358 7fa4bcf9a700  1 mds.0.9 cluster recovered.</code></pre><h2><span id="3-模拟node-1-和node-2-mds故障">3、模拟node-1 和node-2 MDS故障</span></h2><pre><code>node-1：[root@node-1 cephfs]# ceph -s   cluster:    id:     b8e58b30-4568-4032-a9f4-837ed3fa9529    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 17m)    mgr: node-1(active, since 19h), standbys: node-3, node-2    mds: cephfs-demo:1 &#123;0=node-2=up:active&#125; 1 up:standby    osd: 6 osds: 6 up (since 19h), 6 in (since 2d)    rgw: 2 daemons active (node-1, node-2)   task status:   data:    pools:   9 pools, 240 pgs    objects: 288 objects, 17 MiB    usage:   6.2 GiB used, 54 GiB / 60 GiB avail    pgs:     240 active+clean   io:    client:   32 KiB/s rd, 0 B/s wr, 31 op/s rd, 21 op/s wr [root@node-1 cephfs]# ceph mds stat cephfs-demo:1 &#123;0=node-2=up:active&#125; 1 up:standbynode-2：</code></pre><h1><span id="三-rgw高可用测试">三、RGW高可用测试</span></h1><h2><span id="1-中断node1的rgw服务">1、中断node1的RGW服务</span></h2><blockquote><p>只要Node-1的HA服务不中断，VIP不会漂移。</p></blockquote><pre><code>[root@node-1 ~]# ceph -s  #查看当前状态  cluster:    id:     b8e58b30-4568-4032-a9f4-837ed3fa9529    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 5m)    mgr: node-1(active, since 5m), standbys: node-2, node-3    mds: cephfs-demo:1 &#123;0=node-3=up:active&#125; 2 up:standby    osd: 6 osds: 6 up (since 5m), 6 in (since 3d)    rgw: 2 daemons active (node-1, node-2)   task status:   data:    pools:   9 pools, 240 pgs    objects: 288 objects, 17 MiB    usage:   6.2 GiB used, 54 GiB / 60 GiB avail    pgs:     240 active+clean[root@node-1 ~]# systemctl stop ceph-radosgw.target #中断节点一的RGW服务[root@node-1 ~]# ceph -s  #查看服务，RGW服务只剩下node-2了  cluster:    id:     b8e58b30-4568-4032-a9f4-837ed3fa9529    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 7m)    mgr: node-1(active, since 6m), standbys: node-2, node-3    mds: cephfs-demo:1 &#123;0=node-3=up:active&#125; 2 up:standby    osd: 6 osds: 6 up (since 6m), 6 in (since 3d)    rgw: 1 daemon active (node-2)   task status:   data:    pools:   9 pools, 240 pgs    objects: 288 objects, 17 MiB    usage:   6.2 GiB used, 54 GiB / 60 GiB avail    pgs:     240 active+clean[root@node-2 ~]# curl http://192.168.187.201:81 #查看node-1的rgw服务，已经中断curl: (7) Failed connect to 192.168.187.201:81; Connection refused[root@node-2 ~]# curl http://192.168.187.202:81 #查看node-2的rgw服务，正常&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ListAllMyBucketsResult xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;[root@node-2 ~]# [root@node-2 ~]# curl http://192.168.187.200 #查看VIP的RGW服务，正常&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ListAllMyBucketsResult xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;[root@node-2 ~]# </code></pre><h2><span id="2-测试s3-cmd和swift">2、测试s3-cmd和Swift</span></h2><pre><code>[root@node-1 ~]# cat /root/.s3cfg |grep 192. #查看当前RGW的指向是VIP地址，进行测试host_base = 192.168.187.200:80host_bucket = 192.168.187.200:80/%(bucket)s[root@node-1 ~]# s3cmd ls  #测试访问无问题2023-03-09 10:33  s3://ceph-s3-bucket2023-03-09 10:36  s3://s3cmd-demo2023-03-09 10:39  s3://swift-demo[root@node-1 ~]# swift list #测试swift无问题，记得要查看VIP地址是否切换ceph-s3-buckets3cmd-demoswift-demo</code></pre><h1><span id="四-osd坏盘测试">四、OSD坏盘测试</span></h1><blockquote><p>3台机器时，两个节点同时down掉不影响集群使用，但是会非常慢，读写正常</p></blockquote><h2><span id="1-模拟node-1故障">1、模拟node-1故障</span></h2><pre><code>[root@node-1 ~]# [root@node-1 ~]# systemctl stop ceph-osd.target #停止node-1所有OSD守护进程[root@node-1 ~]# ceph -s #查看集群状态  cluster:    id:     b8e58b30-4568-4032-a9f4-837ed3fa9529    health: HEALTH_WARN            2 osds down            2 hosts (2 osds) down            Degraded data redundancy: 288/864 objects degraded (33.333%), 80 pgs degraded   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 43m)    mgr: node-1(active, since 5h), standbys: node-2, node-3    mds: cephfs-demo:1 &#123;0=node-2=up:active&#125; 2 up:standby    osd: 6 osds: 4 up (since 20s), 6 in (since 43m)    rgw: 2 daemons active (node-1, node-2)   task status:   data:    pools:   9 pools, 240 pgs    objects: 288 objects, 17 MiB    usage:   6.2 GiB used, 54 GiB / 60 GiB avail    pgs:     288/864 objects degraded (33.333%)             160 active+undersized             80  active+undersized+degraded [root@node-1 ~]# ceph osd tree #查看OSD状态ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF  -9       0.02939 root ssd                                    -10       0.00980     host node-1-ssd                           1   ssd 0.00980         osd.1         down  1.00000 1.00000 -11       0.00980     host node-2-ssd                           3   ssd 0.00980         osd.3           up  1.00000 1.00000 -12       0.00980     host node-3-ssd                           5   ssd 0.00980         osd.5           up  1.00000 1.00000  -1       0.02939 root default                                 -3       0.00980     host node-1                               0   hdd 0.00980         osd.0         down  1.00000 1.00000  -5       0.00980     host node-2                               2   hdd 0.00980         osd.2           up  1.00000 1.00000  -7       0.00980     host node-3                               4   hdd 0.00980         osd.4           up  1.00000 1.00000 [root@node-1 ~]# rbd create --size 1G ceph-demo/test-demo-1.img #尝试创建RDB块[root@node-1 ~]# rbd -p ceph-demo ls #查看创建成功rbd-bakup.imgrbd.imgtest-demotest-demo-1.img</code></pre><h2><span id="2-模拟node-1和node-2同时故障">2、模拟node-1和node-2同时故障</span></h2><pre><code>[root@node-2 ~]# systemctl stop ceph-osd.target #停止node-2的OSD守护进程[root@node-2 ~]# ceph -s   cluster:    id:     b8e58b30-4568-4032-a9f4-837ed3fa9529    health: HEALTH_WARN            4 osds down            4 hosts (4 osds) down            Reduced data availability: 109 pgs inactive            Degraded data redundancy: 582/873 objects degraded (66.667%), 80 pgs degraded   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 47m)    mgr: node-1(active, since 5h), standbys: node-2, node-3    mds: cephfs-demo:1 &#123;0=node-2=up:active&#125; 2 up:standby    osd: 6 osds: 2 up (since 31s), 6 in (since 47m)    rgw: 2 daemons active (node-1, node-2)   task status:   data:    pools:   9 pools, 240 pgs    objects: 291 objects, 17 MiB    usage:   6.2 GiB used, 54 GiB / 60 GiB avail    pgs:     100.000% pgs not active             582/873 objects degraded (66.667%)             160 undersized+peered             80  undersized+degraded+peered [root@node-2 ~]# ceph -s #查看集群状态  cluster:    id:     b8e58b30-4568-4032-a9f4-837ed3fa9529    health: HEALTH_WARN            4 osds down            4 hosts (4 osds) down            Reduced data availability: 109 pgs inactive            Degraded data redundancy: 582/873 objects degraded (66.667%), 80 pgs degraded   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 47m)    mgr: node-1(active, since 5h), standbys: node-2, node-3    mds: cephfs-demo:1 &#123;0=node-2=up:active&#125; 2 up:standby    osd: 6 osds: 2 up (since 31s), 6 in (since 47m)    rgw: 2 daemons active (node-1, node-2)   task status:   data:    pools:   9 pools, 240 pgs    objects: 291 objects, 17 MiB    usage:   6.2 GiB used, 54 GiB / 60 GiB avail    pgs:     100.000% pgs not active             582/873 objects degraded (66.667%)             160 undersized+peered             80  undersized+degraded+peered [root@node-2 ~]# ceph osd tree #查看OSD状态ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF  -9       0.02939 root ssd                                    -10       0.00980     host node-1-ssd                           1   ssd 0.00980         osd.1         down  1.00000 1.00000 -11       0.00980     host node-2-ssd                           3   ssd 0.00980         osd.3         down  1.00000 1.00000 -12       0.00980     host node-3-ssd                           5   ssd 0.00980         osd.5           up  1.00000 1.00000  -1       0.02939 root default                                 -3       0.00980     host node-1                               0   hdd 0.00980         osd.0         down  1.00000 1.00000  -5       0.00980     host node-2                               2   hdd 0.00980         osd.2         down  1.00000 1.00000  -7       0.00980     host node-3                               4   hdd 0.00980         osd.4           up  1.00000 1.00000 [root@node-1 ~]# rbd create --size 1G ceph-demo/test-demo-2.img #创建一直不成功，一直卡住</code></pre><h1><span id="五-fio性能测试">五、fio性能测试</span></h1><h2><span id="1-4k随机写-iops">1. 4K随机写-iops</span></h2><pre><code>fio -filename=/mnt/rbd-test/fio.img -direct=1 -iodepth 32 -thread -rw=randwrite -ioengine=libaio -bs=4k -size=200m -numjobs=8 -runtime=60 -group_reporting -name=mytest</code></pre><h2><span id="2-4k随机读-iops">2. 4k随机读-iops</span></h2><pre><code>fio -filename=/mnt/rbd-test/fio.img -direct=1 -iodepth 32 -thread -rw=randread -ioengine=libaio -bs=4k -size=200m -numjobs=8 -runtime=60 -group_reporting -name=mytest</code></pre><h2><span id="3-4k随机读写-iops">3. 4k随机读写-iops</span></h2><pre><code>fio -filename=/mnt/rbd-test/fio.img -direct=1 -iodepth 32 -thread -rw=randrw -rwmixread=70 -ioengine=libaio -bs=4k -size=200m -numjobs=8 -runtime=60 -group_reporting -name=mytest</code></pre><h2><span id="4-1m顺序写-吞吐">4. 1M顺序写-吞吐</span></h2><pre><code>fio -filename=/mnt/rbd-test/fio.img -direct=1 -iodepth 32 -thread -rw=write -ioengine=libaio -bs=1M -size=200m -numjobs=8 -runtime=60 -group_reporting -name=mytest</code></pre><h1><span id="六-rbd-bench测试">六、rbd bench测试</span></h1><h2><span id="14k随机写">1.4K随机写</span></h2><pre><code>rbd bench --io-size 4K --io-threads 16 --io-pattern rand --io-type write ceph-demo/test.img</code></pre><h2><span id="24k随机读">2.4K随机读</span></h2><pre><code>rbd bench --io-size 4K --io-threads 16 --io-pattern rand --io-type read ceph-demo/test.img</code></pre><h2><span id="34k随机混合读写">3.4K随机混合读写</span></h2><pre><code>rbd bench --io-size 4K --io-threads 16 --io-pattern rand --io-type readwrite --rw-mix-read 70 ceph-demo/test.img</code></pre><h2><span id="41m顺序写">4.1M顺序写</span></h2><pre><code>rbd bench --io-size 1M --io-threads 16 --io-pattern rand --io-type write ceph-demo/test.img</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> 集群测试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RBD块存储</title>
      <link href="/2023/04/19/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%9D%97%E5%AD%98%E5%82%A8/"/>
      <url>/2023/04/19/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%9D%97%E5%AD%98%E5%82%A8/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-rbd%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B">一、RBD写入流程</a></li><li><a href="#%E4%BA%8C-rbd%E5%9D%97%E5%88%9B%E5%BB%BA%E5%8F%8A%E6%98%A0%E5%B0%84">二、RBD块创建及映射</a><ul><li><a href="#%E4%B8%80-%E5%88%9B%E5%BB%BA%E5%9D%97%E8%AE%BE%E5%A4%87">一、创建块设备</a></li><li><a href="#%E4%BA%8C-%E4%BD%BF%E7%94%A8%E5%9D%97%E8%AE%BE%E5%A4%87">二、使用块设备</a></li></ul></li></ul><!-- tocstop --><h1><span id="一-rbd写入流程">一、RBD写入流程</span></h1><p><img src="/images/Ceph/RBD%E5%9D%97%E5%AD%98%E5%82%A8/1.jpg"></p><blockquote><p>块设备都是瘦分配的，意味着使用的越多，分配的空间越多</p></blockquote><pre><code>[root@node-1 ~]# rbd -p ceph-demo info rbd-demo.img #查看镜像信息rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023[root@node-1 ~]# rados  -p ceph-demo ls | grep rbd_data.11e24b04cf01 #查看objectrbd_data.11e24b04cf01.0000000000000960rbd_data.11e24b04cf01.0000000000000500rbd_data.11e24b04cf01.0000000000000780rbd_data.11e24b04cf01.0000000000000320rbd_data.11e24b04cf01.00000000000008c0rbd_data.11e24b04cf01.00000000000006e0rbd_data.11e24b04cf01.00000000000009ffrbd_data.11e24b04cf01.0000000000000640rbd_data.11e24b04cf01.0000000000000000rbd_data.11e24b04cf01.0000000000000280rbd_data.11e24b04cf01.0000000000000501rbd_data.11e24b04cf01.0000000000000001rbd_data.11e24b04cf01.00000000000003c0rbd_data.11e24b04cf01.0000000000000502rbd_data.11e24b04cf01.00000000000005a0rbd_data.11e24b04cf01.0000000000000460rbd_data.11e24b04cf01.0000000000000140rbd_data.11e24b04cf01.00000000000001e0rbd_data.11e24b04cf01.0000000000000820rbd_data.11e24b04cf01.00000000000000a0[root@node-1 data]# rados -p ceph-demo stat rbd_data.11e24b04cf01.000000000000007a#查看object 大小 size=比特，需要除以两次1024得出Mceph-demo/rbd_data.11e24b04cf01.000000000000007a mtime 2023-03-04 15:08:07.000000, size 4194304[root@node-1 ~]#  ceph osd map ceph-demo rbd_data.11e24b04cf01.000000000000007a #查看object 落到那个OSD和PG上 PG=1.20 最终落在0，1，2三个OSD上osdmap e339 pool &#39;ceph-demo&#39; (1) object &#39;rbd_data.11e24b04cf01.0000000000000960&#39; -&gt; pg 1.85b57b60 (1.20) -&gt; up ([2,1,0], p2) acting ([2,1,0], p2)[root@node-1 ~]# rados  -p ceph-demo ls |wc -l 查看当前ceph-demo的pool中一共存在25个object 每个object等于4 M，一共有25个，所以总共分配空间为100M，随着数据增加，会动态扩容分配空间。25[root@node-1 data]# dd if=/dev/zero of=test.img bs=1M count=1024  #尝试写入1G的文件到集群中1024+0 records in1024+0 records out1073741824 bytes (1.1 GB) copied, 5.97513 s, 180 MB/sYou have new mail in /var/spool/mail/root[root@node-1 data]# ls -lha total 1.1Gdrwxr-xr-x.  2 root root   34 Mar  4 15:08 .dr-xr-xr-x. 18 root root  236 Mar  4 02:27 ..-rw-r--r--.  1 root root    5 Mar  4 02:30 test-rw-r--r--.  1 root root 1.0G Mar  4 15:08 test.img[root@node-1 data]# rados  -p ceph-demo ls | grep rbd_data.11e24b04cf01 |wc -l  #可以看到object的数量由25个增加到了276个 每个object等于4 M，所以总计分配1,104M276[root@node-1 data]# df -HT #查看实际使用情况发现基本相当Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   14M  941M   2% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.7G   16G  15% //dev/sda1               xfs       1.1G  190M  874M  18% /boottmpfs                   tmpfs     191M     0  191M   0% /run/user/0tmpfs                   tmpfs     954M   54k  954M   1% /var/lib/ceph/osd/ceph-0/dev/rbd0               xfs        11G  1.2G  9.7G  11% /data</code></pre><h1><span id="二-rbd块创建及映射">二、RBD块创建及映射</span></h1><h2><span id="一-创建块设备">一、创建块设备</span></h2><p><strong>任意节点执行即可：</strong></p><pre><code>[root@node-1 my-cluster]#  rbd help create #查看create的帮助文档[root@node-1 my-cluster]#  rbd create -p ceph-demo --image rbd-demo.img --size 10G #在ceph-demo的pool中创建一个叫做rbd-demo.img的块设备，大小为10G[root@node-1 my-cluster]# rbd ls ceph-demo #查看创建结果rbd-demo.img[root@node-1 my-cluster]#  rbd create  ceph-demo/rbd-demo-1.img --size 10G #在ceph-demo的pool中创建一个叫做rbd-demo.img的块设备，大小为10G[root@node-1 my-cluster]# rbd ls ceph-demo  #查看创建结果rbd-demo-1.imgrbd-demo.img[root@node-1 my-cluster]# rbd -p ceph-demo ls  #与rbd ls ceph-demo 作用一致rbd-demo-1.imgrbd-demo.img[root@node-1 my-cluster]#  rbd info ceph-demo/rbd-demo.img #查看块设备的具体信息，与rbd info-p ceph-demo --image rbd-demo.img 一致rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Fri Mar  3 13:03:28 2023    access_timestamp: Fri Mar  3 13:03:28 2023    modify_timestamp: Fri Mar  3 13:03:28 2023[root@node-1 my-cluster]# rbd rm  ceph-demo/rbd-demo-1.img #删除块设备，与rbd rm -p ceph-demo --image rbd-demo-1.img用法一致Removing image: 100% complete...done.</code></pre><h2><span id="二-使用块设备">二、使用块设备</span></h2><pre><code>[root@node-1 my-cluster]#  rbd map ceph-demo/rbd-demo.img #映射块设备到本地rbd: sysfs write failedRBD image feature set mismatch. You can disable features unsupported by the kernel with &quot;rbd feature disable ceph-demo/rbd-demo.img object-map fast-diff deep-flatten&quot;.In some cases useful info is found in syslog - try &quot;dmesg | tail&quot;.rbd: map failed: (6) No such device or address</code></pre><blockquote><p>上述报错主要是由于Centos 7 不支持某些特性，需要手动禁用，具体可使用rbd info ceph-demo&#x2F;rbd-demo.img 进行查看，特性主要记录在features中</p></blockquote><p><strong>禁用相关特性：</strong></p><pre><code>[root@node-1 my-cluster]#  rbd info ceph-demo/rbd-demo.img #查看当前特性rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Fri Mar  3 13:03:28 2023    access_timestamp: Fri Mar  3 13:03:28 2023    modify_timestamp: Fri Mar  3 13:03:28 2023[root@node-1 my-cluster]# rbd help feature disable #查看使用文档usage: rbd feature disable [--pool &lt;pool&gt;] [--namespace &lt;namespace&gt;]                            [--image &lt;image&gt;]                            &lt;image-spec&gt; &lt;features&gt; [&lt;features&gt; ...]Disable the specified image feature.Positional arguments  &lt;image-spec&gt;         image specification                       (example: [&lt;pool-name&gt;/[&lt;namespace&gt;/]]&lt;image-name&gt;)  &lt;features&gt;           image features                       [exclusive-lock, object-map, journaling]Optional arguments  -p [ --pool ] arg    pool name  --namespace arg      namespace name  --image arg          image name[root@node-1 my-cluster]# rbd feature disable ceph-demo/rbd-demo.img deep-flatten #禁用rbd-demo.img块设备的deep-flatten特性[root@node-1 my-cluster]# rbd feature disable ceph-demo/rbd-demo.img fast-diff #禁用rbd-demo.img块设备的fast-diff特性[root@node-1 my-cluster]# rbd feature disable ceph-demo/rbd-demo.img exclusive-lock #禁用rbd-demo.img块设备的exclusive-lock特性[root@node-1 my-cluster]# rbd info ceph-demo/rbd-demo.img #查看相关特性已经被禁用rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering #layering不影响使用，可不禁用    op_features:     flags:     create_timestamp: Fri Mar  3 13:03:28 2023    access_timestamp: Fri Mar  3 13:03:28 2023    modify_timestamp: Fri Mar  3 13:03:28 2023 [root@node-1 my-cluster]#  rbd map ceph-demo/rbd-demo.img #挂载到本地/dev/rbd0[root@node-1 my-cluster]# lsblkNAME                                                                                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsda                                                                                                     8:0    0   20G  0 disk ├─sda1                                                                                                  8:1    0    1G  0 part /boot└─sda2                                                                                                  8:2    0   19G  0 part   ├─centos-root                                                                                       253:0    0   17G  0 lvm  /  └─centos-swap                                                                                       253:1    0    2G  0 lvm  [SWAP]sdb                                                                                                     8:16   0   10G  0 disk └─ceph--53398982--af97--4b05--9a0b--bf91741b7f6a-osd--block--68c09ae0--7d72--4984--b5c0--6f1476698c2b 253:2    0   10G  0 lvm  sdc                                                                                                     8:32   0   10G  0 disk sr0                                                                                                    11:0    1 1024M  0 rom  rbd0                                                                                                  252:0    0   10G  0 disk [root@node-1 my-cluster]# rbd device list #使用此命令进行查看id pool      namespace image        snap device    0  ceph-demo           rbd-demo.img -    /dev/rbd0[root@node-1 my-cluster]#  mkfs.xfs /dev/rbd0 #格式化为xfs格式Discarding blocks...Done.meta-data=/dev/rbd0              isize=512    agcount=16, agsize=163840 blks         =                       sectsz=512   attr=2, projid32bit=1         =                       crc=1        finobt=0, sparse=0data     =                       bsize=4096   blocks=2621440, imaxpct=25         =                       sunit=1024   swidth=1024 blksnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1log      =internal log           bsize=4096   blocks=2560, version=2         =                       sectsz=512   sunit=8 blks, lazy-count=1realtime =none                   extsz=4096   blocks=0, rtextents=0[root@node-1 my-cluster]# mkdir /data[root@node-1 my-cluster]# mount /dev/rbd0 /data #进行挂载[root@node-1 my-cluster]# df -HT #查看挂载情况Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   11M  944M   2% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.6G   16G  15% //dev/sda1               xfs       1.1G  190M  874M  18% /boottmpfs                   tmpfs     191M     0  191M   0% /run/user/0tmpfs                   tmpfs     954M   54k  954M   1% /var/lib/ceph/osd/ceph-0/dev/rbd0               xfs        11G   35M   11G   1% /data</code></pre><p>BD存储扩容</p><p><strong>查看需要扩容的镜像</strong></p><pre><code>[root@node-1 data]# rbd ls ceph-demo rbd-demo.img</code></pre><p><strong>查看需要扩容的镜像</strong></p><pre><code>[root@node-1 data]# rbd -p ceph-demo info rbd-demo.img rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023</code></pre><p><strong>使用rbd resize进行扩容帮助</strong></p><pre><code>[root@node-1 data]# rbd help resize  #使用rbd resize进行扩容usage: rbd resize [--pool &lt;pool&gt;] [--namespace &lt;namespace&gt;]                   [--image &lt;image&gt;] --size &lt;size&gt; [--allow-shrink]                   [--no-progress]                   &lt;image-spec&gt; Resize (expand or shrink) image.Positional arguments  &lt;image-spec&gt;         image specification                       (example: [&lt;pool-name&gt;/[&lt;namespace&gt;/]]&lt;image-name&gt;)Optional arguments  -p [ --pool ] arg    pool name  --namespace arg      namespace name  --image arg          image name  -s [ --size ] arg    image size (in M/G/T) [default: M]  --allow-shrink       permit shrinking  #缩容操作，不建议，可能会导致数据丢失  --no-progress        disable progress output</code></pre><p><strong>将ceph-demo池下的rbd-demo.img镜像扩容到20G</strong></p><pre><code>[root@node-1 data]#  rbd resize ceph-demo/rbd-demo.img --size 20G Resizing image: 100% complete...done.[root@node-1 data]# rbd -p ceph-demo info rbd-demo.img #查看已经扩容到20G了rbd image &#39;rbd-demo.img&#39;:    size 20 GiB in 5120 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023</code></pre><p><strong>#将上述扩容到20G的硬盘缩容到15G</strong></p><pre><code>[root@node-1 data]#  rbd resize ceph-demo/rbd-demo.img --allow-shrink --size 15GResizing image: 100% complete...done.[root@node-1 data]# rbd -p ceph-demo info rbd-demo.imgrbd image &#39;rbd-demo.img&#39;:    size 15 GiB in 3840 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> RBD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OSD扩容和换盘</title>
      <link href="/2023/04/19/Ceph/7.OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/"/>
      <url>/2023/04/19/Ceph/7.OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-osd%E6%89%A9%E5%AE%B9">一、OSD扩容</a><ul><li><a href="#1-%E7%BA%B5%E5%90%91%E6%A8%AA%E5%90%91%E6%89%A9%E5%AE%B9">1、纵向&amp;横向扩容</a><ul><li><a href="#11-%E7%BA%B5%E5%90%91%E6%89%A9%E5%AE%B9">1.1、纵向扩容</a></li></ul></li></ul></li><li><a href="#%E4%BA%8C-%E6%95%B0%E6%8D%AE%E9%87%8D%E5%88%86%E5%B8%83">二、数据重分布</a></li><li><a href="#%E4%B8%89-%E5%9D%8F%E7%9B%98%E6%9B%B4%E6%8D%A2">三、坏盘更换</a><ul><li><a href="#%E6%A8%A1%E6%8B%9F%E5%88%A0%E9%99%A4osd">模拟删除OSD</a></li></ul></li><li><a href="#%E5%9B%9B-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7">四、数据一致性</a></li></ul><!-- tocstop --><h1><span id="一-osd扩容">一、OSD扩容</span></h1><p>OSD扩容大体上可分为两种：</p><ol><li>横向扩容: scale out 通过增加节点来完成扩容。</li><li>纵向扩容: csale up 通过在现有节点下增加硬盘来完成扩容。</li></ol><p>详情查看<a href="https://docs.ceph.com/en/nautilus/rados/deployment/ceph-deploy-osd/#zap-disks">官方文档</a>。</p><h2><span id="1-纵向amp横向扩容">1、纵向&amp;横向扩容</span></h2><p><strong>横向扩容时请先安装ceph的软件包，ntp，防火墙等基础功能</strong></p><h3><span id="11-纵向扩容">1.1、纵向扩容</span></h3><pre><code>ceph-deploy osd create --data &#123;data-disk&#125; &#123;node-name&#125; ceph-deploy disk zap &lt;node-name&gt; &lt;data-disk&gt; #清除要增加硬盘的分区    </code></pre><h1><span id="二-数据重分布">二、数据重分布</span></h1><p>当您将 Ceph OSD 守护进程添加到 Ceph 存储集群时，集群映射会使用新的 OSD 进行更新。回头参考 <a href="https://docs.ceph.com/en/nautilus/architecture/#calculating-pg-ids">计算PG ID</a>，这会更改集群映射。因此，它会更改对象放置，因为它会更改计算的输入。下图描述了重新平衡过程（虽然相当粗略，因为它对大型集群的影响要小得多），其中一些但不是所有 PG 从现有 OSD（OSD 1 和 OSD 2）迁移到新的 OSD（OSD 3） ). 即使在重新平衡时，CR​​USH 也很稳定。许多归置组仍保留其原始配置，并且每个 OSD 都获得了一些额外的容量，因此在重新平衡完成后新 OSD 上不会出现负载峰值。</p><p><img src="/images/Ceph/OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/1.jpg"></p><p><strong>写入一个2G的文件</strong></p><p><code>[root@node-1 /]#dd if=/dev/zero of=rebalancing-file.img bs=1M count=2048</code></p><p><strong>上传到data中</strong></p><p><code>[root@node-1 /]# mv rebalancing-file.img /data</code></p><p><strong>data是ceph中的块设备</strong></p><pre><code>[root@node-1 /]# df -HTFilesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   11M  944M   2% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  4.8G   14G  26% //dev/sda1               xfs       1.1G  204M  860M  20% /boottmpfs                   tmpfs     954M   25k  954M   1% /var/lib/ceph/osd/ceph-0tmpfs                   tmpfs     954M   25k  954M   1% /var/lib/ceph/osd/ceph-3tmpfs                   tmpfs     191M     0  191M   0% /run/user/0/dev/rbd0               xfs        11G  2.2G  8.6G  21% /data</code></pre><p><strong>可以看到ceph-demo中有6G的数据，一共写入了2G的数据，3副本，所以共使用6G</strong></p><pre><code>[root@node-1 /]# ceph df RAW STORAGE:    CLASS     SIZE       AVAIL      USED       RAW USED     %RAW USED     hdd       60 GiB     24 GiB     30 GiB       36 GiB         60.75     TOTAL     60 GiB     24 GiB     30 GiB       36 GiB         60.75  POOLS:    POOL                          ID     PGS     STORED      OBJECTS     USED        %USED     MAX AVAIL     ceph-demo                      1     512     2.0 GiB         537     6.0 GiB     25.56       5.9 GiB     .rgw.root                      2      32     1.2 KiB           4     768 KiB         0       5.9 GiB     default.rgw.control            3      32         0 B           8         0 B         0       5.9 GiB     default.rgw.meta               4      32     1.5 KiB           8     1.3 MiB         0       5.9 GiB     default.rgw.log                5      32         0 B         207         0 B         0       5.9 GiB     default.rgw.buckets.index      6      32         0 B           2         0 B         0       5.9 GiB     default.rgw.buckets.data       7      32         0 B           0         0 B         0       5.9 GiB     cephfs_data                    8      16       8 GiB       2.05k      24 GiB     57.73       5.9 GiB     cephfs_metadata                9      16     361 KiB          23     2.6 MiB      0.01       5.9 GiB </code></pre><p><strong>PG数据重分布状态</strong></p><p><img src="/images/Ceph/OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/2.jpg"></p><p><strong>查看OSD线程</strong></p><pre><code>[root@node-1 /]# ceph --admin-daemon /var/run/ceph/ceph-mon.node-1.asok config show | grep max_b    &quot;bluestore_compression_max_blob_size&quot;: &quot;0&quot;,    &quot;bluestore_compression_max_blob_size_hdd&quot;: &quot;524288&quot;,    &quot;bluestore_compression_max_blob_size_ssd&quot;: &quot;65536&quot;,    &quot;bluestore_max_blob_size&quot;: &quot;0&quot;,    &quot;bluestore_max_blob_size_hdd&quot;: &quot;524288&quot;,    &quot;bluestore_max_blob_size_ssd&quot;: &quot;65536&quot;,    &quot;bluestore_rocksdb_options&quot;: &quot;compression=kNoCompression,max_write_buffer_number=4,min_write_buffer_number_to_merge=1,recycle_log_file_num=4,write_buffer_size=268435456,writable_file_max_buffer_size=0,compaction_readahead_size=2097152,max_background_compactions=2&quot;,    &quot;client_readahead_max_bytes&quot;: &quot;0&quot;,    &quot;filestore_queue_max_bytes&quot;: &quot;104857600&quot;,    &quot;filestore_rocksdb_options&quot;: &quot;max_background_jobs=10,compaction_readahead_size=2097152,compression=kNoCompression&quot;,    &quot;kstore_max_bytes&quot;: &quot;67108864&quot;,    &quot;ms_max_backoff&quot;: &quot;15.000000&quot;,    &quot;osd_bench_max_block_size&quot;: &quot;67108864&quot;,    &quot;osd_map_message_max_bytes&quot;: &quot;10485760&quot;,    &quot;osd_max_backfills&quot;: &quot;1&quot;, #每个OSD最多有一个线程    &quot;osd_tier_promote_max_bytes_sec&quot;: &quot;5242880&quot;,    &quot;rbd_readahead_max_bytes&quot;: &quot;524288&quot;,    &quot;rgw_sync_log_trim_max_buckets&quot;: &quot;16&quot;,    &quot;rgw_user_max_buckets&quot;: &quot;1000&quot;,</code></pre><p><strong>数据同步的时候使用cluster_network网络，客户端连接的时候使用pubic_network网络 生产环境中最好分开</strong></p><pre><code>[root@node-1 /]# cat /etc/ceph/ceph.conf #数据同步的时候使用cluster_network网络，客户端连接的时候使用pubic_network网络 生产环境中最好分开[global]fsid = e9a90625-4707-4b6b-b52f-661512ea831dpublic_network = 192.168.187.0/24cluster_network = 192.168.199.0/24mon_initial_members = node-1mon_host = 192.168.187.201auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxosd pool default pg num = 32osd pool default pgp num = 32mon_pg_warn_max_per_osd = 512mon_max_pg_per_osd=3000[client.rgw.node-1]rgw_frontends = &quot;civetweb port=80&quot;</code></pre><p><strong>如果是生产环境中需要扩容OSD，但是业务收到影响后可以使用 norebalance 停止</strong><br><strong>查看帮助</strong></p><pre><code>[root@node-1 /]# ceph --help |grep reba osd set full|pause|noup|nodown|noout|noin|nobackfill|norebalance|     set &lt;key&gt;osd unset full|pause|noup|nodown|noout|noin|nobackfill|norebalance|   unset &lt;key&gt;</code></pre><p><strong>暂停同步</strong></p><pre><code>[root@node-1 /]# ceph osd set norebalance  #暂停同步norebalance is set[root@node-1 /]# ceph osd set nobackfill #同时需要暂停这个，这个也会进行数据填充的nobackfill is set</code></pre><p><strong>查看集群状态</strong></p><pre><code>[root@node-1 /]# ceph -s   cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            norebalance flag(s) set #可以看到已经暂停同步了            1 pools have too few placement groups            1 pools have too many placement groups            clock skew detected on mon.node-2, mon.node-3   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 2m)    mgr: node-3(active, since 56m), standbys: node-2, node-1    mds: cephfs-demo:1 &#123;0=node-3=up:active&#125; 2 up:standby    osd: 6 osds: 6 up (since 20m), 6 in (since 19h)         flags norebalance    rgw: 1 daemon active (node-1)   task status:   data:    pools:   9 pools, 736 pgs    objects: 2.84k objects, 10 GiB    usage:   36 GiB used, 24 GiB / 60 GiB avail    pgs:     736 active+clean</code></pre><p><strong>启动同步</strong></p><pre><code>[root@node-1 /]# ceph osd unset norebalance  #启动同步norebalance is unset[root@node-1 /]# ceph osd unset nobackfill #启动同步</code></pre><h1><span id="三-坏盘更换">三、坏盘更换</span></h1><p>当你想减少集群的大小或更换硬件时，你可以在运行时删除一个 OSD。对于 Ceph，一个 OSD 通常是ceph-osd 主机中一个存储驱动器的一个 Ceph 守护进程。如果您的主机有多个存储驱动器，您可能需要ceph-osd为每个驱动器删除一个守护进程。通常，最好检查集群的容量以查看是否已达到其容量的上限。确保当您删除 OSD 时，您的集群未达到其比率。near full</p><p><strong>可通过这个命令查看到坏盘的信息</strong></p><p><code>[root@node-3 ~]# dmesg</code></p><p><strong>可以通过这个命令看到那个OSD的延迟比较大， 来判断是否存在坏道情况</strong></p><pre><code>[root@node-1 /]# ceph osd perf #可以通过这个命令看到那个OSD的延迟比较大， 来判断是否存在坏道情况osd commit_latency(ms) apply_latency(ms)   5                  0                 0   4                  0                 0   0                  0                 0   1                  0                 0   2                  0                 0   3                  0                 0 </code></pre><h2><span id="模拟删除osd">模拟删除OSD</span></h2><pre><code>[root@node-1 /]# ceph osd tree  #模拟删除osd.5ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.05878 root default                            -3       0.01959     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000  3   hdd 0.00980         osd.3       up  1.00000 1.00000 -5       0.01959     host node-2                          1   hdd 0.00980         osd.1       up  1.00000 1.00000  4   hdd 0.00980         osd.4       up  1.00000 1.00000 -7       0.01959     host node-3                          2   hdd 0.00980         osd.2       up  1.00000 1.00000  5   hdd 0.00980         osd.5       up  1.00000 1.00000  [root@node-1 /]# ssh node-3 #远程到node-3服务器Last login: Sun Mar  5 19:50:53 2023 from 192.168.187.1[root@node-3 ~]# systemctl stop ceph-osd@5 #停止osd5服务[root@node-3 ~]# systemctl status ceph-osd@5 #查看状态● ceph-osd@5.service - Ceph object storage daemon osd.5   Loaded: loaded (/usr/lib/systemd/system/ceph-osd@.service; enabled-runtime; vendor preset: disabled)   Active: inactive (dead) since Sun 2023-03-05 20:45:15 CST; 8s ago  Process: 1729 ExecStart=/usr/bin/ceph-osd -f --cluster $&#123;CLUSTER&#125; --id %i --setuser ceph --setgroup ceph (code=exited, status=0/SUCCESS)  Process: 1724 ExecStartPre=/usr/lib/ceph/ceph-osd-prestart.sh --cluster $&#123;CLUSTER&#125; --id %i (code=exited, status=0/SUCCESS) Main PID: 1729 (code=exited, status=0/SUCCESS)Mar 05 19:49:50 node-3 systemd[1]: Starting Ceph object storage daemon osd.5...Mar 05 19:49:50 node-3 systemd[1]: Started Ceph object storage daemon osd.5.Mar 05 19:49:56 node-3 ceph-osd[1729]: 2023-03-05 19:49:56.053 7f3fa59a6a80 -1 osd.5 1454 log_to_monitors &#123;default=true&#125;Mar 05 19:50:11 node-3 ceph-osd[1729]: 2023-03-05 19:50:11.748 7f3f97fc5700 -1 osd.5 1454 set_numa_affinity unable to identify...rectoryMar 05 20:45:15 node-3 systemd[1]: Stopping Ceph object storage daemon osd.5...Mar 05 20:45:15 node-3 ceph-osd[1729]: 2023-03-05 20:45:15.800 7f3f9b84b700 -1 received  signal: Terminated from /usr/lib/syst... UID: 0Mar 05 20:45:15 node-3 ceph-osd[1729]: 2023-03-05 20:45:15.800 7f3f9b84b700 -1 osd.5 1479 *** Got signal Terminated ***Mar 05 20:45:15 node-3 ceph-osd[1729]: 2023-03-05 20:45:15.800 7f3f9b84b700 -1 osd.5 1479 *** Immediate shutdown (osd_fast_shu...ue) ***Mar 05 20:45:15 node-3 systemd[1]: Stopped Ceph object storage daemon osd.5.Hint: Some lines were ellipsized, use -l to show in full.[root@node-3 ~]# ceph osd tree #在集群中查看状态ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.05878 root default                            -3       0.01959     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000  3   hdd 0.00980         osd.3       up  1.00000 1.00000 -5       0.01959     host node-2                          1   hdd 0.00980         osd.1       up  1.00000 1.00000  4   hdd 0.00980         osd.4       up  1.00000 1.00000 -7       0.01959     host node-3                          2   hdd 0.00980         osd.2       up  1.00000 1.00000  5   hdd 0.00980         osd.5     down  1.00000 1.00000 [root@node-3 ~]# ceph -s  #大概需要10分钟的时候才会触发rebalance的操作  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            1 osds down            Degraded data redundancy: 1272/8514 objects degraded (14.940%), 199 pgs degraded            1 pools have too few placement groups            1 pools have too many placement groups            clock skew detected on mon.node-2, mon.node-3   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 25m)    mgr: node-3(active, since 53m), standbys: node-2, node-1    mds: cephfs-demo:1 &#123;0=node-3=up:active&#125; 2 up:standby    osd: 6 osds: 5 up (since 136y), 6 in (since 19h)    rgw: 1 daemon active (node-1)   task status:   data:    pools:   9 pools, 736 pgs    objects: 2.84k objects, 10 GiB    usage:   36 GiB used, 24 GiB / 60 GiB avail    pgs:     1272/8514 objects degraded (14.940%)             365 active+clean             199 active+undersized+degraded             172 active+undersized[root@node-3 ~]# ceph osd out osd.5 #踢出集群marked out osd.5. [root@node-3 ~]# ceph osd tree  #可以看到权重变小，从1变成了0ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.05878 root default                            -3       0.01959     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000  3   hdd 0.00980         osd.3       up  1.00000 1.00000 -5       0.01959     host node-2                          1   hdd 0.00980         osd.1       up  1.00000 1.00000  4   hdd 0.00980         osd.4       up  1.00000 1.00000 -7       0.01959     host node-3                          2   hdd 0.00980         osd.2       up  1.00000 1.00000  5   hdd 0.00980         osd.5     down        0 1.00000 [root@node-3 ~]# ceph osd crush dump |grep osd.5 #查看crush            &quot;name&quot;: &quot;osd.5&quot;,[root@node-3 ~]# ceph osd crush rm osd.5  #删除crushremoved item id 5 name &#39;osd.5&#39; from crush map[root@node-3 ~]# ceph osd tree #查看已经删除crushID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.04898 root default                            -3       0.01959     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000  3   hdd 0.00980         osd.3       up  1.00000 1.00000 -5       0.01959     host node-2                          1   hdd 0.00980         osd.1       up  1.00000 1.00000  4   hdd 0.00980         osd.4       up  1.00000 1.00000 -7       0.00980     host node-3                          2   hdd 0.00980         osd.2       up  1.00000 1.00000  5             0 osd.5             down        0 1.00000 [root@node-3 ~]# ceph osd rm osd.5 #删除osd.5removed osd.5[root@node-3 ~]# ceph osd tree #查看已经删除ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.04898 root default                            -3       0.01959     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000  3   hdd 0.00980         osd.3       up  1.00000 1.00000 -5       0.01959     host node-2                          1   hdd 0.00980         osd.1       up  1.00000 1.00000  4   hdd 0.00980         osd.4       up  1.00000 1.00000 -7       0.00980     host node-3                          2   hdd 0.00980         osd.2       up  1.00000 1.00000 [root@node-3 ~]#  ceph auth list #查看认证信息mds.node-1    key: AQBLPwRkgcRTARAAJD+hzTIC+jSlAqsrLqsd4w==    caps: [mds] allow    caps: [mon] allow profile mds    caps: [osd] allow rwxmds.node-2    key: AQBLPwRk6Z6QOBAAbcoYG/A7WVQ6Xq26QngXcw==    caps: [mds] allow    caps: [mon] allow profile mds    caps: [osd] allow rwxmds.node-3    key: AQBMPwRk2fIcNhAAtP/K0mVMYRJfN8h5Jlbyig==    caps: [mds] allow    caps: [mon] allow profile mds    caps: [osd] allow rwxosd.0    key: AQCSKgJkOuSDBBAAN3OVzTiJK9rNWEePf/cHZQ==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *osd.1    key: AQAMKwJkXYOpChAA8Q/pNsKr9BVz+LG5qKLpGA==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *osd.2    key: AQAbKwJkIEoeFRAAVns6sv+UQc85fP4MjoB89w==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *osd.3    key: AQBCgwNkuu0HOhAAz/pnHg4xHpusO6/exoee9Q==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *osd.4    key: AQBSgwNkQp6uJRAAtS5oUJjJsnyWwtruAy9q4Q==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *osd.5    key: AQBigwNkaWJAJRAAUNcvokMMax4UGTgVXSRB4w==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *client.admin    key: AQCJJwJk4RpHLRAAQWMtAAO/4EXn2qXm35LzKw==    caps: [mds] allow *    caps: [mgr] allow *    caps: [mon] allow *    caps: [osd] allow *client.bootstrap-mds    key: AQCJJwJkHStHLRAAj1Nc65WqUG2R+YNC/ewQVQ==    caps: [mon] allow profile bootstrap-mdsclient.bootstrap-mgr    key: AQCJJwJkyTVHLRAA9eMnrbNwGYgbmietKQ33Kw==    caps: [mon] allow profile bootstrap-mgrclient.bootstrap-osd    key: AQCJJwJkrj5HLRAA3n8qXNp0Z7k4zPFtb2UXQg==    caps: [mon] allow profile bootstrap-osdclient.bootstrap-rbd    key: AQCJJwJk4UhHLRAAb0x5K5jM/YqcJNpP0aDBQg==    caps: [mon] allow profile bootstrap-rbdclient.bootstrap-rbd-mirror    key: AQCJJwJkfVFHLRAA/vh27nUJAeA2aSquy6oBug==    caps: [mon] allow profile bootstrap-rbd-mirrorclient.bootstrap-rgw    key: AQCJJwJk1VlHLRAALUO430jjgsp+TZKY151KDw==    caps: [mon] allow profile bootstrap-rgwclient.rgw.node-1    key: AQDKMANkby1/GxAAfeOIDDwPxjrwS0pibixXtg==    caps: [mon] allow rw    caps: [osd] allow rwxmgr.node-1    key: AQDIKAJkngmTBhAAGSBO+ELb4O93uqGQyhtfRA==    caps: [mds] allow *    caps: [mon] allow profile mgr    caps: [osd] allow *mgr.node-2    key: AQBvLwJkDDPYAhAAkmvWkFb/rdiggG/f1n18dA==    caps: [mds] allow *    caps: [mon] allow profile mgr    caps: [osd] allow *mgr.node-3    key: AQBwLwJkrN9IJRAA9VAsfNeDV14AOf7uaiqRTQ==    caps: [mds] allow *    caps: [mon] allow profile mgr    caps: [osd] allow *installed auth entries:[root@node-3 ~]# ceph auth rm osd.5 #删除关于osd5的认证updated需要重新添加上述硬盘时需要格式化硬盘，在node-3执行  清除逻辑卷的DM映射，操作如下：dmsetup info -Cdmsetup remove [dm_map_name]格式化磁盘mkfs.xfs -f /dev/vdbceph-deploy disk zap node-3 /dev/sdcceph-deploy osd create node-3 --data /dev/sdc --journal /dev/sdc</code></pre><h1><span id="四-数据一致性">四、数据一致性</span></h1><p>作为维护数据一致性和清洁度的一部分，Ceph OSD 还可以清理归置组中的对象。也就是说，Ceph OSD 可以将一个归置组中的对象元数据与其存储在其他 OSD 中的归置组中的副本进行比较。清理（通常每天执行）会捕获 OSD 错误或文件系统错误。OSD 还可以通过逐位比较对象中的数据来执行更深入的清理。深度清理（通常每周执行一次）会发现磁盘上的坏扇区，而这些坏扇区在轻度清理中并不明显。<br>有关配置清理的详细信息，请参阅<a href="https://docs.ceph.com/en/nautilus/rados/configuration/osd-config-ref#scrubbing">数据清理</a>。</p><p>除了制作对象的多个副本外，Ceph 还通过清理归置组来确保数据完整性。Ceph 清理类似于fsck对象存储层。对于每个归置组，Ceph 生成所有对象的目录并比较每个主对象及其副本以确保没有对象丢失或不匹配。轻度擦洗（每天）检查对象大小和属性。深度清理（每周一次）读取数据并使用校验和确保数据完整性。</p><p><strong>查看帮助</strong></p><pre><code>[root@node-3 ~]# ceph --help | grep scrub   --block               block until completion (scrub and deep-scrub only)mon scrub                                                             scrub the monitor storesosd deep-scrub &lt;who&gt;                                                  initiate deep scrub on osd &lt;who&gt;, or use &lt;all|any&gt; to deep scrub allosd pool deep-scrub &lt;poolname&gt; [&lt;poolname&gt;...]                        initiate deep-scrub on pool &lt;who&gt; noscrub|nodeep-scrub|hit_set_type|hit_set_period|hit_set_count|hit_   read|hit_set_grade_decay_rate|hit_set_search_last_n|scrub_min_        interval|scrub_max_interval|deep_scrub_interval|recovery_priority|    recovery_op_priority|scrub_priority|compression_mode|compression_    osd pool scrub &lt;poolname&gt; [&lt;poolname&gt;...]                             initiate scrub on pool &lt;who&gt; fadvise_dontneed|noscrub|nodeep-scrub|hit_set_type|hit_set_period|    hit_set_grade_decay_rate|hit_set_search_last_n|scrub_min_interval|    scrub_max_interval|deep_scrub_interval|recovery_priority|recovery_    op_priority|scrub_priority|compression_mode|compression_algorithm|   osd scrub &lt;who&gt;                                                       initiate scrub on osd &lt;who&gt;, or use &lt;all|any&gt; to scrub all norecover|noscrub|nodeep-scrub|notieragent|nosnaptrim|pglog_          norecover|noscrub|nodeep-scrub|notieragent|nosnaptrim                pg deep-scrub &lt;pgid&gt;                                                  start deep-scrub on &lt;pgid&gt; #深度清理pg scrub &lt;pgid&gt;                                                       start scrub on &lt;pgid&gt; #轻量清理</code></pre><p><strong>清理PG</strong></p><pre><code>[root@node-3 ~]# ceph pg dump #先拿到PG的ID[root@node-3 ~]# ceph pg scrub 1.197 #进行对比instructing pg 1.197 on osd.1 to scrub</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> OSD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph集群运维</title>
      <link href="/2023/04/19/Ceph/8.Ceph%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/Ceph%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/"/>
      <url>/2023/04/19/Ceph/8.Ceph%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/Ceph%E9%9B%86%E7%BE%A4%E8%BF%90%E7%BB%B4/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-ceph%E5%AE%88%E6%8A%A4%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86">一、Ceph守护服务管理</a><ul><li><a href="#1-%E7%90%86%E8%AE%BA">1、理论</a><ul><li><a href="#11-%E5%90%AF%E5%8A%A8%E6%89%80%E6%9C%89%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B">1.1、启动所有守护进程</a></li><li><a href="#12-%E5%81%9C%E6%AD%A2%E6%89%80%E6%9C%89%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B">1.2 、停止所有守护进程</a></li><li><a href="#13-%E6%8C%89%E7%B1%BB%E5%9E%8B%E5%90%AF%E5%8A%A8%E6%89%80%E6%9C%89%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B">1.3、按类型启动所有守护进程</a></li><li><a href="#14-%E6%8C%89%E7%B1%BB%E5%9E%8B%E5%81%9C%E6%AD%A2%E6%89%80%E6%9C%89%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B">1.4 按类型停止所有守护进程</a></li><li><a href="#15-%E5%90%AF%E5%8A%A8%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B">1.5、启动守护进程</a></li><li><a href="#16-%E5%81%9C%E6%AD%A2%E6%89%80%E6%9C%89%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B">1.6、停止所有守护进程</a></li><li><a href="#17-%E6%8C%89%E7%B1%BB%E5%9E%8B%E5%90%AF%E5%8A%A8%E6%89%80%E6%9C%89%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B">1.7、按类型启动所有守护进程</a></li><li><a href="#18-%E6%8C%89%E7%B1%BB%E5%9E%8B%E5%81%9C%E6%AD%A2%E6%89%80%E6%9C%89%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B">1.8、按类型停止所有守护进程</a></li><li><a href="#19-%E5%90%AF%E5%8A%A8%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B">1.9、启动守护进程</a></li><li><a href="#2-%E5%81%9C%E6%AD%A2%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B">2、停止守护进程</a></li></ul></li><li><a href="#2-%E8%BF%90%E8%A1%8C-ceph">2、运行 CEPH</a></li><li><a href="#3-%E6%9F%A5%E7%9C%8B%E6%89%80%E6%9C%89%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B">3、查看所有守护进程</a></li></ul></li><li><a href="#%E4%BA%8C-ceph%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90">二、Ceph日志分析</a><ul><li><a href="#1%E8%BF%90%E8%A1%8C%E6%97%B6">1.运行时</a></li><li><a href="#2%E5%90%AF%E5%8A%A8%E6%97%B6%E9%97%B4">2.启动时间</a></li><li><a href="#3%E5%8A%A0%E9%80%9F%E6%97%A5%E5%BF%97%E8%BD%AE%E6%8D%A2">3.加速日志轮换</a></li></ul></li><li><a href="#%E4%B8%89-%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81%E7%9B%91%E6%8E%A7">三、集群状态监控</a><ul><li><a href="#1%E4%BD%BF%E7%94%A8%E5%91%BD%E4%BB%A4%E8%A1%8C">1.使用命令行</a><ul><li><a href="#11-%E4%BA%A4%E4%BA%92%E6%A8%A1%E5%BC%8F">1.1 交互模式</a></li><li><a href="#12-%E9%9D%9E%E9%BB%98%E8%AE%A4%E8%B7%AF%E5%BE%84">1.2 非默认路径</a></li></ul></li><li><a href="#2%E6%A3%80%E6%9F%A5%E9%9B%86%E7%BE%A4%E7%8A%B6%E6%80%81">2.检查集群状态</a></li><li><a href="#3%E8%A7%82%E5%AF%9F%E9%9B%86%E7%BE%A4">3.观察集群</a></li><li><a href="#4%E7%9B%91%E6%8E%A7%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5">4.监控健康检查</a></li><li><a href="#5%E7%BD%91%E7%BB%9C%E6%80%A7%E8%83%BD%E6%A3%80%E6%9F%A5">5.网络性能检查</a></li><li><a href="#6%E9%9D%99%E9%9F%B3%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5">6.静音健康检查</a></li><li><a href="#7%E6%A3%80%E6%9F%A5%E9%9B%86%E7%BE%A4%E7%9A%84%E4%BD%BF%E7%94%A8%E7%BB%9F%E8%AE%A1">7.检查集群的使用统计</a></li><li><a href="#8%E6%A3%80%E6%9F%A5-osd-%E7%8A%B6%E6%80%81">8.检查 OSD 状态</a></li><li><a href="#9%E6%A3%80%E6%9F%A5%E7%9B%91%E8%A7%86%E5%99%A8%E7%8A%B6%E6%80%81">9.检查监视器状态</a></li><li><a href="#10%E6%A3%80%E6%9F%A5-mds-%E7%8A%B6%E6%80%81">10.检查 MDS 状态</a></li><li><a href="#11%E6%A3%80%E6%9F%A5%E5%BD%92%E7%BD%AE%E7%BB%84%E7%8A%B6%E6%80%81">11.检查归置组状态</a></li></ul></li><li><a href="#%E5%9B%9B-pg%E7%BB%84">四、PG组</a><ul><li><a href="#1%E8%87%AA%E5%8A%A8%E7%BC%A9%E6%94%BEpg%E7%BB%84">1.自动缩放PG组</a><ul><li><a href="#1%E6%9F%A5%E7%9C%8Bpg%E7%BC%A9%E6%94%BE%E5%BB%BA%E8%AE%AE">1.查看PG缩放建议</a></li><li><a href="#2%E8%87%AA%E5%8A%A8%E7%BC%A9%E6%94%BE">2.自动缩放</a></li><li><a href="#3%E6%8C%87%E5%AE%9A%E9%A2%84%E6%9C%9F%E6%B1%A0%E5%A4%A7%E5%B0%8F">3.指定预期池大小</a></li><li><a href="#4%E6%8C%87%E5%AE%9A%E6%B1%A0%E7%9A%84pg%E8%BE%B9%E7%95%8C">4.指定池的PG边界</a></li></ul></li><li><a href="#2pg_num%E7%9A%84%E9%A2%84%E9%80%89">2.PG_NUM的预选</a></li><li><a href="#3%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8pg%E7%BB%84">3.如何使用PG组</a></li><li><a href="#4pg%E7%BB%84%E6%9D%83%E8%A1%A1">4.PG组权衡</a><ul><li><a href="#1%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96">1.数据持久化</a></li><li><a href="#2%E6%B1%A0%E4%B8%AD%E7%9A%84%E5%AF%B9%E8%B1%A1%E5%88%86%E5%B8%83">2.池中的对象分布</a></li><li><a href="#3%E5%86%85%E5%AD%98-cpu%E5%92%8C%E7%BD%91%E7%BB%9C%E4%BD%BF%E7%94%A8">3.内存、CPU和网络使用</a></li></ul></li><li><a href="#5%E9%80%89%E6%8B%A9pg%E7%BB%84%E7%9A%84%E6%95%B0%E9%87%8F">5.选择PG组的数量</a></li><li><a href="#6%E8%AE%BE%E7%BD%AEpg%E7%BB%84%E7%9A%84%E6%95%B0%E9%87%8F">6.设置PG组的数量</a></li><li><a href="#7%E8%8E%B7%E5%8F%96pg%E7%BB%84%E7%9A%84%E6%95%B0%E9%87%8F">7.获取PG组的数量</a></li><li><a href="#8%E8%8E%B7%E5%8F%96%E9%9B%86%E7%BE%A4%E7%9A%84pg%E7%BB%9F%E8%AE%A1%E4%BF%A1%E6%81%AF">8.获取集群的PG统计信息</a></li><li><a href="#9%E8%8E%B7%E5%8F%96%E5%8D%A1%E4%BD%8Fpg%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE">9.获取卡住PG的统计数据</a></li><li><a href="#10%E8%8E%B7%E5%8F%96pg%E5%9C%B0%E5%9B%BE">10.获取PG地图</a></li><li><a href="#11%E8%8E%B7%E5%8F%96pg%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE">11.获取PG统计数据</a></li><li><a href="#12%E6%B8%85%E7%90%86pg%E7%BB%84">12.清理PG组</a></li><li><a href="#13%E4%BC%98%E5%85%88%E8%80%83%E8%99%91pg%E7%BB%84%E7%9A%84%E5%9B%9E%E5%A1%AB%E6%81%A2%E5%A4%8D">13.优先考虑PG组的回填&#x2F;恢复</a></li><li><a href="#14%E6%81%A2%E5%A4%8D%E4%B8%A2%E5%A4%B1">14.恢复丢失</a></li><li><a href="#15%E5%AE%9E%E6%93%8D">15.实操</a></li></ul></li></ul><!-- tocstop --><h1><span id="一-ceph守护服务管理">一、Ceph守护服务管理</span></h1><h2><span id="1-理论">1、理论</span></h2><p>对于支持 systemd 的所有发行版（CentOS 7、Fedora、Debian Jessie 8 及更高版本、SUSE），ceph 守护进程现在使用本机 systemd 文件而不是旧的 sysvinit 脚本进行管理。例如：</p><pre><code>sudo systemctl start ceph.target       # start all daemonssudo systemctl status ceph-osd@12      # check status of osd.12</code></pre><p>要列出节点上的 Ceph systemd 单元，请执行：</p><p><code>sudo systemctl status ceph\*.service ceph\*.target</code></p><h3><span id="11-启动所有守护进程">1.1、启动所有守护进程</span></h3><p>要在 Ceph 节点（无论类型如何）上启动所有守护进程，请执行以下命令：</p><p><code>sudo systemctl start ceph.target</code></p><h3><span id="12-停止所有守护进程">1.2 、停止所有守护进程</span></h3><p>要停止 Ceph 节点上的所有守护进程（无论类型如何），请执行以下命令：</p><p><code>sudo systemctl stop ceph\*.service ceph\*.target</code></p><h3><span id="13-按类型启动所有守护进程">1.3、按类型启动所有守护进程</span></h3><p>要在 Ceph 节点上启动特定类型的所有守护进程，请执行以下操作之一：</p><pre><code>sudo systemctl start ceph-osd.targetsudo systemctl start ceph-mon.targetsudo systemctl start ceph-mds.target</code></pre><h3><span id="14-按类型停止所有守护进程">1.4 按类型停止所有守护进程</span></h3><p>要停止 Ceph 节点上特定类型的所有守护进程，请执行以下操作之一：</p><pre><code>sudo systemctl stop ceph-mon\*.service ceph-mon.targetsudo systemctl stop ceph-osd\*.service ceph-osd.targetsudo systemctl stop ceph-mds\*.service ceph-mds.target</code></pre><h3><span id="15-启动守护进程">1.5、启动守护进程</span></h3><p>要在 Ceph 节点上启动特定的守护进程实例，请执行以下操作之一：</p><pre><code>sudo systemctl start ceph-osd@&#123;id&#125;sudo systemctl start ceph-mon@&#123;hostname&#125;sudo systemctl start ceph-mds@&#123;hostname&#125;</code></pre><p>例如：</p><pre><code>systemctl start ceph-osd@1sudo systemctl start ceph-mon@ceph-serversudo systemctl start ceph-mds@ceph-server</code></pre><h3><span id="16-停止所有守护进程">1.6、停止所有守护进程</span></h3><p>要停止 Ceph 节点上的所有守护进程（无论类型如何），请执行以下命令：</p><p><code>sudo stop ceph-all</code></p><h3><span id="17-按类型启动所有守护进程">1.7、按类型启动所有守护进程</span></h3><p>要在 Ceph 节点上启动特定类型的所有守护进程，请执行以下操作之一：</p><pre><code>sudo start ceph-osd-allsudo start ceph-mon-allsudo start ceph-mds-all</code></pre><h3><span id="18-按类型停止所有守护进程">1.8、按类型停止所有守护进程</span></h3><p>要停止 Ceph 节点上特定类型的所有守护进程，请执行以下操作之一：</p><pre><code>sudo stop ceph-osd-allsudo stop ceph-mon-allsudo stop ceph-mds-all</code></pre><h3><span id="19-启动守护进程">1.9、启动守护进程</span></h3><p>要在 Ceph 节点上启动特定的守护进程实例，请执行以下操作之一：</p><pre><code>sudo start ceph-osd id=&#123;id&#125;sudo start ceph-mon id=&#123;hostname&#125;sudo start ceph-mds id=&#123;hostname&#125;</code></pre><p>例如：</p><pre><code>sudo start ceph-osd id=1sudo start ceph-mon id=ceph-serversudo start ceph-mds id=ceph-server</code></pre><h3><span id="2-停止守护进程">2、停止守护进程</span></h3><p>要停止 Ceph 节点上的特定守护进程实例，请执行以下操作之一：</p><pre><code>sudo stop ceph-osd id=&#123;id&#125;sudo stop ceph-mon id=&#123;hostname&#125;sudo stop ceph-mds id=&#123;hostname&#125;</code></pre><p>例如：</p><pre><code>sudo stop ceph-osd id=1sudo start ceph-mon id=ceph-serversudo start ceph-mds id=ceph-server</code></pre><h2><span id="2-运行-ceph">2、运行 CEPH</span></h2><p>每次启动、重新启动和 停止Ceph 守护进程（或整个集群）时，您必须至少指定一个选项和一个命令。您还可以指定守护进程类型或守护进程实例。</p><p><code>&#123;commandline&#125; [options] [commands] [daemons]</code></p><p>选项ceph包括：</p><table><thead><tr><th>选项</th><th>捷径</th><th>描述</th></tr></thead><tbody><tr><td>–verbose</td><td>-v</td><td>使用详细日志记录。</td></tr><tr><td>–valgrind</td><td>N&#x2F;A</td><td>（仅限开发和 QA）使用Valgrind调试。</td></tr><tr><td>–allhosts</td><td>-a</td><td>在 中的所有节点上执行ceph.conf. 否则，它只在 上执行localhost。</td></tr><tr><td>–restart</td><td>N&#x2F;A</td><td>如果核心转储，自动重启守护进程。</td></tr><tr><td>–norestart</td><td>N&#x2F;A</td><td>如果核心转储，请不要重新启动守护进程。</td></tr><tr><td>–conf</td><td>-c</td><td>使用备用配置文件。</td></tr></tbody></table><p>命令ceph包括：</p><table><thead><tr><th>命令</th><th>描述</th></tr></thead><tbody><tr><td>start</td><td>启动守护进程。</td></tr><tr><td>stop</td><td>停止守护进程。</td></tr><tr><td>forcestop</td><td>强制守护进程停止。与…一样kill -9</td></tr><tr><td>killall</td><td>杀死特定类型的所有守护进程。</td></tr><tr><td>cleanlogs</td><td>清除日志目录。</td></tr><tr><td>cleanalllogs</td><td>清除日志目录中的所有内容。</td></tr></tbody></table><p>对于子系统操作，ceph服务可以通过为选项添加特定的守护程序类型来针对特定的守护程序类型[daemons]。守护进程类型包括：</p><ul><li>mon</li><li>osd</li><li>mds</li></ul><h2><span id="3-查看所有守护进程">3、查看所有守护进程</span></h2><pre><code>[root@node-1 cephfs]# cd /usr/lib/systemd/system[root@node-1 system]# ls | grep ceph ceph-crash.serviceceph-fuse@.serviceceph-fuse.targetceph-mds@.serviceceph-mds.targetceph-mgr@.serviceceph-mgr.targetceph-mon@.serviceceph-mon.targetceph-osd@.serviceceph-osd.targetceph-radosgw@.serviceceph-radosgw.targetceph.targetceph-volume@.service</code></pre><h1><span id="二-ceph日志分析">二、Ceph日志分析</span></h1><p>通常，当您向 Ceph 配置添加调试时，您会在运行时进行。如果您在启动集群时遇到问题，您还可以将 Ceph 调试日志记录添加到您的 Ceph 配置文件中。&#x2F;var&#x2F;log&#x2F;ceph 您可以在（默认位置）下查看 Ceph 日志文件。</p><p>当调试输出减慢您的系统时，延迟可以隐藏竞争条件。<br>日志记录是资源密集型的。如果您在集群的特定区域遇到问题，请为该集群区域启用日志记录。例如，如果您的 OSD 运行良好，但您的元数据服务器运行不正常，您应该首先为给您带来麻烦的特定元数据服务器实例启用调试日志记录。根据需要为每个子系统启用日志记录。</p><p>详细日志记录每小时可以生成超过 1GB的数据。如果您的操作系统磁盘达到其容量，节点将停止工作。<br>如果您启用或增加 Ceph日志记录的速率，请确保您的操作系统磁盘上有足够的磁盘空间。有关旋转日志文件的详细信息，请参阅加速日志旋转。当您的系统运行良好时，删除不必要的调试设置以确保您的集群以最佳状态运行。记录调试输出消息相对较慢，并且在操作集群时浪费资源。</p><p>有关可用设置的详细信息，请参阅子系统、日志和调试设置。</p><h2><span id="1运行时">1.运行时</span></h2><p>如果您想在运行时查看配置设置，您必须登录到运行守护进程的主机并执行以下命令：</p><pre><code>ceph daemon &#123;daemon-name&#125; config show | less</code></pre><p>例如，：</p><pre><code>ceph daemon osd.0 config show | less</code></pre><p>要在运行时激活 Ceph 的调试输出（即, dout()），请使用以下 命令将参数注入运行时配置：ceph tell</p><pre><code>ceph tell &#123;daemon-type&#125;.&#123;daemon id or *&#125; config set &#123;name&#125; &#123;value&#125;</code></pre><p>替换{daemon-type}为osd,mon或 之一mds。您可以使用 将运行时设置应用于特定类型的所有守护进程*，或指定特定守护进程的 ID。例如，要增加ceph-osd名为 的守护程序的调试日志记录osd.0，请执行以下命令：</p><pre><code>ceph tell osd.0 config set debug_osd 0/5</code></pre><p>命令通过监视器。如果您无法绑定到监视器，您仍然可以通过登录到您想要更改其配置的守护程序的主机来使用 进行更改。例如：ceph tellceph daemon</p><pre><code>sudo ceph daemon osd.0 config set debug_osd 0/5</code></pre><p>有关可用设置的详细信息，请参阅子系统、日志和调试设置。</p><h2><span id="2启动时间">2.启动时间</span></h2><p>要在引导时激活 Ceph 的调试输出（即, ），您必须将设置添加到 Ceph 配置文件中。dout()每个守护进程共有的子系统可以在您的配置文件中设置[global]。特定守护进程的子系统在配置文件的守护进程部分下设置（例如，，，）。例如：[mon][osd][mds]</p><pre><code>[global]        debug ms = 1/5[mon]        debug mon = 20        debug paxos = 1/5        debug auth = 2[osd]        debug osd = 1/5        debug filestore = 1/5        debug journal = 1        debug monc = 5/20[mds]        debug mds = 1        debug mds balancer = 1</code></pre><h2><span id="3加速日志轮换">3.加速日志轮换</span></h2><p>如果你的 OS 磁盘比较满，你可以通过修改位于&#x2F;etc&#x2F;logrotate.d&#x2F;ceph. 如果您的日志超过大小设置，则在轮换频率后添加大小设置以加速日志轮换（通过 cronjob）。例如，默认设置如下所示：</p><pre><code>rotate 7weeklycompresssharedscripts</code></pre><p>通过添加size设置对其进行修改。</p><pre><code>rotate 7weeklysize 500Mcompresssharedscripts</code></pre><p>然后，为您的用户空间启动 crontab 编辑器。<br>crontab -e</p><p>最后，添加一个条目来检查etc&#x2F;logrotate.d&#x2F;ceph文件。</p><pre><code>30 * * * * /usr/sbin/logrotate /etc/logrotate.d/ceph &gt;/dev/null 2&gt;&amp;1</code></pre><p>前面的示例etc&#x2F;logrotate.d&#x2F;ceph每 30 分钟检查一次文件。</p><h1><span id="三-集群状态监控">三、集群状态监控</span></h1><p>一旦您有一个正在运行的集群，您就可以使用该ceph工具来监控您的集群。监控集群通常包括检查 OSD 状态、监视器状态、归置组状态和元数据服务器状态。</p><h2><span id="1使用命令行">1.使用命令行</span></h2><h3><span id="11-交互模式">1.1 交互模式</span></h3><p>ceph要以交互模式运行该工具，请ceph在命令行中键入不带参数的内容。例如：</p><pre><code>cephceph&gt; healthceph&gt; statusceph&gt; quorum_statusceph&gt; mon stat</code></pre><h3><span id="12-非默认路径">1.2 非默认路径</span></h3><p>如果您为配置或密钥环指定了非默认位置，则可以指定它们的位置：</p><pre><code>ceph -c /path/to/conf -k /path/to/keyring health</code></pre><h2><span id="2检查集群状态">2.检查集群状态</span></h2><p>启动集群后，在开始读取和&#x2F;或写入数据之前，请先检查集群的状态。<br>要检查集群的状态，请执行以下命令：</p><pre><code>ceph status</code></pre><p>或者：</p><pre><code>ceph -s</code></pre><p>在交互模式下，键入status并按Enter 键。<br><code>ceph&gt; status</code></p><p>Ceph 将打印集群状态。例如，一个带有每个服务的小型 Ceph 演示集群可能会打印以下内容：</p><pre><code>cluster:  id:     477e46f1-ae41-4e43-9c8f-72c918ab0a20  health: HEALTH_OKservices:  mon: 3 daemons, quorum a,b,c  mgr: x(active)  mds: cephfs_a-1/1/1 up  &#123;0=a=up:active&#125;, 2 up:standby  osd: 3 osds: 3 up, 3 indata:  pools:   2 pools, 16 pgs  objects: 21 objects, 2.19K  usage:   546 GB used, 384 GB / 931 GB avail  pgs:     16 active+clean</code></pre><p>Ceph 如何计算数据使用量<br>该usage值反映了实际使用的原始存储量。该 值表示集群整体存储容量的可用量（较小的数字）。名义数量反映了存储数据在被复制、克隆或快照之前的大小。因此，实际存储的数据量通常会超过名义上存储的数据量，因为 Ceph 会创建数据的副本，并且还可能使用存储容量进行克隆和快照。xxx GB &#x2F; xxx GB</p><h2><span id="3观察集群">3.观察集群</span></h2><p>除了每个守护进程的本地日志记录外，Ceph 集群还维护一个集群日志，记录有关整个系统的高级事件。这被记录到监视器服务器上的磁盘（&#x2F;var&#x2F;log&#x2F;ceph&#x2F;ceph.log默认情况下），但也可以通过命令行进行监视。<br>要跟踪集群日志，请使用以下命令</p><pre><code>ceph -w</code></pre><p>Ceph 将打印系统状态，然后是发出的每条日志消息。例如：</p><pre><code>cluster:  id:     477e46f1-ae41-4e43-9c8f-72c918ab0a20  health: HEALTH_OKservices:  mon: 3 daemons, quorum a,b,c  mgr: x(active)  mds: cephfs_a-1/1/1 up  &#123;0=a=up:active&#125;, 2 up:standby  osd: 3 osds: 3 up, 3 indata:  pools:   2 pools, 16 pgs  objects: 21 objects, 2.19K  usage:   546 GB used, 384 GB / 931 GB avail  pgs:     16 active+clean2017-07-24 08:15:11.329298 mon.a mon.0 172.21.9.34:6789/0 23 : cluster [INF] osd.0 172.21.9.34:6806/20527 boot2017-07-24 08:15:14.258143 mon.a mon.0 172.21.9.34:6789/0 39 : cluster [INF] Activating manager daemon x2017-07-24 08:15:15.446025 mon.a mon.0 172.21.9.34:6789/0 47 : cluster [INF] Manager daemon x is now available</code></pre><p>除了用于在发出日志行时打印它们之外，还用于查看集群日志中的最新行。<code>ceph -wceph log last [n]n</code></p><h2><span id="4监控健康检查">4.监控健康检查</span></h2><p>Ceph 不断地针对自身状态运行各种健康检查。当健康检查失败时，这会反映在(or )的输出中。此外，消息会发送到集群日志以指示检查何时失败以及集群何时恢复。ceph statusceph health<br>例如，当 OSD 关闭时，health状态输出部分可能会更新如下：</p><pre><code>health: HEALTH_WARN        1 osds down        Degraded data redundancy: 21/63 objects degraded (33.333%), 16 pgs unclean, 16 pgs degraded</code></pre><p>这个时候也会发出集群日志消息来记录健康检查的失败：</p><pre><code>2017-07-25 10:08:58.265945 mon.a mon.0 172.21.9.34:6789/0 91 : cluster [WRN] Health check failed: 1 osds down (OSD_DOWN)2017-07-25 10:09:01.302624 mon.a mon.0 172.21.9.34:6789/0 94 : cluster [WRN] Health check failed: Degraded data redundancy: 21/63 objects degraded (33.333%), 16 pgs unclean, 16 pgs degraded (PG_DEGRADED)</code></pre><p>当 OSD 重新上线时，集群日志记录集群恢复到健康状态：</p><pre><code>2017-07-25 10:11:11.526841 mon.a mon.0 172.21.9.34:6789/0 109 : cluster [WRN] Health check update: Degraded data redundancy: 2 pgs unclean, 2 pgs degraded, 2 pgs undersized (PG_DEGRADED)2017-07-25 10:11:13.535493 mon.a mon.0 172.21.9.34:6789/0 110 : cluster [INF] Health check cleared: PG_DEGRADED (was: Degraded data redundancy: 2 pgs unclean, 2 pgs degraded, 2 pgs undersized)2017-07-25 10:11:13.535577 mon.a mon.0 172.21.9.34:6789/0 111 : cluster [INF] Cluster is now healthy</code></pre><h2><span id="5网络性能检查">5.网络性能检查</span></h2><p>Ceph OSD 在它们之间发送心跳 ping 消息来监控守护进程的可用性。我们还使用响应时间来监控网络性能。虽然繁忙的 OSD 可能会延迟 ping 响应，但我们可以假设，如果网络切换失败，将在不同的 OSD 对之间检测到多个延迟。<br>默认情况下，我们会警告超过 1 秒（1000 毫秒）的 ping 时间。</p><pre><code>HEALTH_WARN Slow OSD heartbeats on back (longest 1118.001ms)</code></pre><p>运行状况详细信息将添加 OSD 的组合看到的延迟以及延迟的程度。有 10 个明细行项目的限制。</p><pre><code>[WRN] OSD_SLOW_PING_TIME_BACK: Slow OSD heartbeats on back (longest 1118.001ms)    Slow OSD heartbeats on back from osd.0 [dc1,rack1] to osd.1 [dc1,rack1] 1118.001 msec possibly improving    Slow OSD heartbeats on back from osd.0 [dc1,rack1] to osd.2 [dc1,rack2] 1030.123 msec    Slow OSD heartbeats on back from osd.2 [dc1,rack2] to osd.1 [dc1,rack1] 1015.321 msec    Slow OSD heartbeats on back from osd.1 [dc1,rack1] to osd.0 [dc1,rack1] 1010.456 msec</code></pre><p>要查看更多详细信息和网络性能信息的完整转储，dump_osd_network可以使用该命令。通常，这将被发送到一个 mgr，但它可以通过将它发布到任何 OSD 来限制特定 OSD 的交互。默认为 1 秒（1000 毫秒）的当前阈值可以作为以毫秒为单位的参数被覆盖。<br>以下命令将通过指定阈值 0 并发送到 mgr 来显示所有收集的网络性能数据。</p><pre><code>$ ceph daemon /var/run/ceph/ceph-mgr.x.asok dump_osd_network 0&#123;    &quot;threshold&quot;: 0,    &quot;entries&quot;: [        &#123;            &quot;last update&quot;: &quot;Wed Sep  4 17:04:49 2019&quot;,            &quot;stale&quot;: false,            &quot;from osd&quot;: 2,            &quot;to osd&quot;: 0,            &quot;interface&quot;: &quot;front&quot;,            &quot;average&quot;: &#123;                &quot;1min&quot;: 1.023,                &quot;5min&quot;: 0.860,                &quot;15min&quot;: 0.883            &#125;,            &quot;min&quot;: &#123;                &quot;1min&quot;: 0.818,                &quot;5min&quot;: 0.607,                &quot;15min&quot;: 0.607            &#125;,            &quot;max&quot;: &#123;                &quot;1min&quot;: 1.164,                &quot;5min&quot;: 1.173,                &quot;15min&quot;: 1.544            &#125;,            &quot;last&quot;: 0.924        &#125;,        &#123;            &quot;last update&quot;: &quot;Wed Sep  4 17:04:49 2019&quot;,            &quot;stale&quot;: false,            &quot;from osd&quot;: 2,            &quot;to osd&quot;: 0,            &quot;interface&quot;: &quot;back&quot;,            &quot;average&quot;: &#123;                &quot;1min&quot;: 0.968,                &quot;5min&quot;: 0.897,                &quot;15min&quot;: 0.830            &#125;,            &quot;min&quot;: &#123;                &quot;1min&quot;: 0.860,                &quot;5min&quot;: 0.563,                &quot;15min&quot;: 0.502            &#125;,            &quot;max&quot;: &#123;                &quot;1min&quot;: 1.171,                &quot;5min&quot;: 1.216,                &quot;15min&quot;: 1.456            &#125;,            &quot;last&quot;: 0.845        &#125;,        &#123;            &quot;last update&quot;: &quot;Wed Sep  4 17:04:48 2019&quot;,            &quot;stale&quot;: false,            &quot;from osd&quot;: 0,            &quot;to osd&quot;: 1,            &quot;interface&quot;: &quot;front&quot;,            &quot;average&quot;: &#123;                &quot;1min&quot;: 0.965,                &quot;5min&quot;: 0.811,                &quot;15min&quot;: 0.850            &#125;,            &quot;min&quot;: &#123;                &quot;1min&quot;: 0.650,                &quot;5min&quot;: 0.488,                &quot;15min&quot;: 0.466            &#125;,            &quot;max&quot;: &#123;                &quot;1min&quot;: 1.252,                &quot;5min&quot;: 1.252,                &quot;15min&quot;: 1.362            &#125;,        &quot;last&quot;: 0.791    &#125;,    ...</code></pre><h2><span id="6静音健康检查">6.静音健康检查</span></h2><p>健康检查可以静音，这样它们就不会影响集群的整体报告状态。使用健康检查代码指定警报（请参阅健康检查）：</p><pre><code>ceph health mute &lt;code&gt;</code></pre><p>例如，如果有健康警告，将其静音将使集群报告整体状态为HEALTH_OK. 例如，要使OSD_DOWN警报静音：</p><pre><code>ceph health mute OSD_DOWN</code></pre><p>静音被报告为命令的短格式和长格式的一部分。例如，在上面的场景中，集群会报告：ceph health</p><pre><code>$ ceph healthHEALTH_OK (muted: OSD_DOWN)$ ceph health detailHEALTH_OK (muted: OSD_DOWN)(MUTED) OSD_DOWN 1 osds down    osd.1 is down</code></pre><p>可以通过以下方式显式删除静音：</p><pre><code>ceph health unmute &lt;code&gt;</code></pre><p>例如，：</p><pre><code>ceph health unmute OSD_DOWN</code></pre><p>健康检查静音可以选择关联一个 TTL（生存时间），这样静音将在指定的时间段过去后自动过期。TTL 被指定为可选的持续时间参数，例如：</p><pre><code>ceph health mute OSD_DOWN 4h    # mute for 4 hoursceph health mute MON_DOWN 15m   # mute for 15  minutes</code></pre><p>通常，如果解决了静音健康警报（例如，在上面的示例中，OSD 重新启动），静音就会消失。如果警报稍后返回，将以通常的方式报告。<br>可以将静音设置为“粘性”，这样即使警报清除，静音也会保留。例如，：</p><pre><code>ceph health mute OSD_DOWN 1h --sticky   # ignore any/all down OSDs for next hour</code></pre><p>如果警报的程度变得更糟，大多数健康静音也会消失。例如，如果有一个 OSD 宕机，并且警报被静音，如果一个或多个其他 OSD 宕机，则静音将消失。对于任何涉及指示有多少触发警告或错误的事物的计数的健康警报都是如此。<br>检测配置问题<br>除了 Ceph 根据自身状态持续运行的健康检查之外，还有一些配置问题可能只能由外部工具检测到。<br>使用ceph-medic工具对 Ceph 集群的配置运行这些附加检查。</p><h2><span id="7检查集群的使用统计">7.检查集群的使用统计</span></h2><p>要检查集群的数据使用情况和池之间的数据分布，您可以使用该df选项。它类似于 Linux df。执行以下操作：</p><pre><code>ceph df</code></pre><p>输出的RAW STORAGE部分概述了集群管理的存储量。</p><ul><li>CLASS： OSD 设备的类别（或集群的总类别）</li><li>SIZE：集群管理的存储容量。</li><li>AVAIL：集群中可用的空闲空间量。</li><li>USED：用户数据消耗的原始存储量。</li><li>RAW USED：用户数据、内部开销或保留容量消耗的原始存储量。</li><li>%RAW USED：使用的原始存储的百分比。将此数字与和结合使用，以确保您没有达到集群的容量。有关更多详细信息，请参阅存储容量。full rationear full ratio</li><li>输出的POOLS部分提供了一个池列表和每个池的名义用途。此部分的输出不反映副本、克隆或快照。例如，如果您存储一个包含 1MB 数据的对象，则名义使用量将为 1MB，但实际使用量可能为 2MB 或更多，具体取决于副本、克隆和快照的数量。</li><li>名称：池的名称。</li><li>ID：池 ID。</li><li>USED​​：以千字节为单位存储的名义数据量，除非数字附加M表示兆字节或G表示千兆字节。</li><li>%USED：每个池使用的存储的名义百分比。</li><li>MAX AVAIL：对可以写入此池的名义数据量的估计。</li><li>OBJECTS：每个池存储的对象的名义数量。</li><li>QUOTA OBJECTS：配额对象的数量。</li><li>QUOTA BYTES：配额对象中的字节数。</li><li>DIRTY：缓存池中已写入缓存池但尚未刷新到基础池的对象数。此字段仅在使用缓存分层时可用。</li><li>USED​​ COMPR：为压缩数据分配的空间量（即这包括压缩数据加上所有分配、复制和纠删码开销）。</li><li>UNDER COMPR：通过压缩传递的数据量（对所有副本求和）并且足以以压缩形式存储。</li></ul><p>POOLS部分中的数字是名义上的。它们不包括副本、快照或克隆的数量。因此，USED和%USED金额的总和不会加到 输出的RAW部分中的USED和%USED金额。</p><p>MAX AVAIL值是使用的复制或擦除代码、将存储映射到设备的 CRUSH 规则、这些设备的利用率以及配置的 mon_osd_full_ratio 的复杂函数。</p><h2><span id="8检查-osd-状态">8.检查 OSD 状态</span></h2><p>您可以通过执行以下命令来检查 OSD 以确保它们是up正确的in：</p><pre><code>ceph osd stat</code></pre><p>或者：</p><pre><code>ceph osd dump</code></pre><p>您还可以根据视图 OSD 在 CRUSH 图中的位置来检查它们。</p><pre><code>ceph osd tree</code></pre><p>Ceph 将打印出一个 CRUSH 树，其中包含一个主机、它的 OSD、它们是否启动以及它们的权重。</p><pre><code>#ID CLASS WEIGHT  TYPE NAME             STATUS REWEIGHT PRI-AFF -1       3.00000 pool default -3       3.00000 rack mainrack -2       3.00000 host osd-host  0   ssd 1.00000         osd.0             up  1.00000 1.00000  1   ssd 1.00000         osd.1             up  1.00000 1.00000  2   ssd 1.00000         osd.2             up  1.00000 1.00000</code></pre><h2><span id="9检查监视器状态">9.检查监视器状态</span></h2><p>如果您的集群有多个监视器（可能），您应该在启动集群之后和读取和&#x2F;或写入数据之前检查监视器仲裁状态。当多个监视器正在运行时，必须存在法定人数。您还应该定期检查监视器状态以确保它们正在运行。<br>要查看显示监视器图，请执行以下命令：</p><pre><code>ceph mon stat</code></pre><p>或者：</p><pre><code>ceph mon dump</code></pre><p>要检查监视器集群的仲裁状态，请执行以下命令：</p><pre><code>ceph quorum_status</code></pre><p>Ceph 将返回仲裁状态。例如，由三个监视器组成的 Ceph 集群可能会返回以下内容：</p><pre><code>&#123; &quot;election_epoch&quot;: 10,  &quot;quorum&quot;: [        0,        1,        2],  &quot;quorum_names&quot;: [        &quot;a&quot;,        &quot;b&quot;,        &quot;c&quot;],  &quot;quorum_leader_name&quot;: &quot;a&quot;,  &quot;monmap&quot;: &#123; &quot;epoch&quot;: 1,      &quot;fsid&quot;: &quot;444b489c-4f16-4b75-83f0-cb8097468898&quot;,      &quot;modified&quot;: &quot;2011-12-12 13:28:27.505520&quot;,      &quot;created&quot;: &quot;2011-12-12 13:28:27.505520&quot;,      &quot;features&quot;: &#123;&quot;persistent&quot;: [                        &quot;kraken&quot;,                        &quot;luminous&quot;,                        &quot;mimic&quot;],        &quot;optional&quot;: []      &#125;,      &quot;mons&quot;: [            &#123; &quot;rank&quot;: 0,              &quot;name&quot;: &quot;a&quot;,              &quot;addr&quot;: &quot;127.0.0.1:6789/0&quot;,              &quot;public_addr&quot;: &quot;127.0.0.1:6789/0&quot;&#125;,            &#123; &quot;rank&quot;: 1,              &quot;name&quot;: &quot;b&quot;,              &quot;addr&quot;: &quot;127.0.0.1:6790/0&quot;,              &quot;public_addr&quot;: &quot;127.0.0.1:6790/0&quot;&#125;,            &#123; &quot;rank&quot;: 2,              &quot;name&quot;: &quot;c&quot;,              &quot;addr&quot;: &quot;127.0.0.1:6791/0&quot;,              &quot;public_addr&quot;: &quot;127.0.0.1:6791/0&quot;&#125;           ]  &#125;&#125;</code></pre><h2><span id="10检查-mds-状态">10.检查 MDS 状态</span></h2><p>元数据服务器为 CephFS 提供元数据服务。元数据服务器有两组状态：和。为确保您的元数据服务器是和，请执行以下操作：up | downactive | inactiveupactive</p><pre><code>ceph mds stat</code></pre><p>要显示元数据集群的详细信息，请执行以下命令：</p><pre><code>ceph fs dump</code></pre><h2><span id="11检查归置组状态">11.检查归置组状态</span></h2><p>归置组将对象映射到 OSD。当您监控归置组时，您会希望它们是active和clean。有关详细讨论，请参阅监控 OSD 和归置组。<br>使用管理套接字<br>Ceph 管理套接字允许您通过套接字接口查询守护进程。默认情况下，Ceph 套接字位于&#x2F;var&#x2F;run&#x2F;ceph. 要通过管理套接字访问守护进程，登录到运行守护进程的主机并使用以下命令：</p><pre><code>ceph daemon &#123;daemon-name&#125;ceph daemon &#123;path-to-socket-file&#125;</code></pre><p>例如，以下是等效的：</p><pre><code>ceph daemon osd.0 fooceph daemon /var/run/ceph/ceph-osd.0.asok foo</code></pre><p>要查看可用的管理套接字命令，请执行以下命令：</p><pre><code>ceph daemon &#123;daemon-name&#125; help</code></pre><p>admin socket 命令使您能够在运行时显示和设置您的配置。有关详细信息，请参阅在运行时查看配置。<br>此外，您可以在运行时直接设置配置值（即，管理套接字绕过监视器，不像，它依赖于监视器但不需要您直接登录到有问题的主机）。ceph tell {daemon-type}.{id} config set</p><h1><span id="四-pg组">四、PG组</span></h1><h2><span id="1自动缩放pg组">1.自动缩放PG组</span></h2><p>归置组 (PG) 是 Ceph 如何分发数据的内部实现细节。您可以通过启用pg-autoscaling允许集群根据集群的使用方式提出建议或自动调整 PG 。<br>系统中的每个池都有一个pg_autoscale_mode可以设置为off、on或 的属性warn。</p><ul><li>off：禁用此池的自动缩放。管理员可以为每个池选择合适的 PG 编号。有关详细信息，请参阅选择归置组的数量。</li><li>on：启用给定池的 PG 计数的自动调整。</li><li>warn: 当应该调整 PG 计数时发出健康警报</li></ul><p>要为现有池设置自动缩放模式，请：</p><pre><code>ceph osd pool set &lt;pool-name&gt; pg_autoscale_mode &lt;mode&gt;</code></pre><p>例如，要在 pool 上启用自动缩放foo，：</p><pre><code>ceph osd pool set foo pg_autoscale_mode on</code></pre><p>pg_autoscale_mode您还可以配置应用于将来创建的任何池的默认值：</p><pre><code>ceph config set global osd_pool_default_pg_autoscale_mode &lt;mode&gt;</code></pre><h3><span id="1查看pg缩放建议">1.查看PG缩放建议</span></h3><p>您可以使用以下命令查看每个池、其相对利用率以及对 PG 计数的任何建议更改：</p><pre><code>ceph osd pool autoscale-status</code></pre><p>输出将类似于：</p><pre><code>POOL    SIZE  TARGET SIZE  RATE  RAW CAPACITY   RATIO  TARGET RATIO  EFFECTIVE RATIO PG_NUM  NEW PG_NUM  AUTOSCALEa     12900M                3.0        82431M  0.4695                                     8         128  warnc         0                 3.0        82431M  0.0000        0.2000           0.9884      1          64  warnb         0        953.6M   3.0        82431M  0.0347                                     8              warn</code></pre><ul><li>SIZE是池中存储的数据量。</li><li>TARGET SIZE（如果存在）是管理员指定的他们希望最终存储在此池中的数据量。系统使用两个值中较大的一个进行计算。</li><li>RATE是池的乘数，决定消耗了多少原始存储容量。例如，一个 3 副本池的比率为 3.0，而 ak&#x3D;4,m&#x3D;2 纠删码池的比率为 1.5。</li><li>RAW CAPACITY是 OSD 上负责存储该池（可能还有其他池）数据的原始存储容量总量。 RATIO是该池消耗的总容量的比率（即，比率 &#x3D; 大小 * 速率 &#x2F; 原始容量）。</li><li>TARGET RATIO（如果存在）是管理员指定的存储比率，他们希望此池相对于设置了目标比率的其他池消耗。如果指定了目标大小字节和比率，则比率优先。</li><li>EFFECTIVE RATIO是经过两种方式调整后的目标比率：<ul><li>减去设置了目标大小的池预期使用的任何容量</li><li>使用目标比率集对池之间的目标比率进行标准化，以便它们共同瞄准其余空间。</li><li>例如，target_ratio 为 1.0 的 4 个池的有效比率为 0.25。</li><li>系统使用实际比率和有效比率中的较大者进行计算。</li></ul></li><li>PG_NUM是池的当前 PG 数量（或者如果正在进行更改，则池正在努力的 PG 的当前数量pg_num ）。 NEW PG_NUM（如果存在）是系统认为池pg_num应该更改为的值。它始终是 2 的幂，只有当“理想”值与当前值的差异超过 3 倍时才会出现。</li><li>AUTOSCALE是 pool pg_autoscale_mode，将是on、off或warn。</li></ul><h3><span id="2自动缩放">2.自动缩放</span></h3><p>允许集群根据使用情况自动扩展 PG 是最简单的方法。Ceph 会查看整个系统的可用存储总量和目标 PG 数量，查看每个池中存储了多少数据，并尝试相应地分配 PG。该系统的方法相对保守，仅当当前 PG 数量 ( pg_num) 与其认为应有的数量相差 3 倍以上时才对池进行更改。<br>每个 OSD 的目标 PG 数量基于 mon_target_pg_per_osd可配置的（默认值：100），可以通过以下方式进行调整：</p><pre><code>ceph config set global mon_target_pg_per_osd 100</code></pre><p>自动缩放器分析池并在每个子树的基础上进行调整。因为每个池可能映射到不同的 CRUSH 规则，并且每个规则可能将数据分布在不同的设备上，所以 Ceph 将独立地考虑层次结构中每个子树的利用率。例如，一个映射到ssd类 OSD 的池和一个映射到hdd类 OSD 的池将各自具有最佳的 PG 计数，这取决于各自设备类型的数量。</p><h3><span id="3指定预期池大小">3.指定预期池大小</span></h3><p>首次创建集群或池时，它将消耗集群总容量的一小部分，并且在系统看来似乎只需要少量归置组。但是，在大多数情况下，集群管理员很清楚哪些池预计会随着时间的推移消耗大部分系统容量。pg_num通过向 Ceph 提供此信息，可以从一开始就使用更合适数量的 PG，从而防止在进行这些调整时进行后续更改 以及与移动数据相关的开销。<br>池的目标大小可以通过两种方式指定：根据池的绝对大小（即字节），或者作为相对于具有集合的其他池的权重target_size_ratio。<br>例如，：</p><pre><code>ceph osd pool set mypool target_size_bytes 100T</code></pre><p>将告诉系统mypool预计会消耗 100 TiB 的空间。或者，：</p><pre><code>ceph osd pool set mypool target_size_ratio 1.0</code></pre><p>将告诉系统mypool预计将消耗 1.0 相对于target_size_ratio设置的其他池。如果mypool是集群中唯一的池，这意味着预期使用总容量的 100%。如果有第二个池为target_size_ratio 1.0，则两个池都将使用 50% 的集群容量。</p><p>您还可以在创建时使用命令的可选参数或参数设置池的目标大小。–target-size-bytes <bytes>–target-size-ratio <ratio>ceph osd pool create</ratio></bytes></p><p>请注意，如果指定了不可能的目标大小值（例如，容量大于整个集群），则会POOL_TARGET_SIZE_BYTES_OVERCOMMITTED发出健康警告 ( )。</p><p>如果同时为池指定了target_size_ratio和target_size_bytes，则只考虑比率，并POOL_HAS_TARGET_SIZE_BYTES_AND_RATIO发出健康警告 ( )。</p><h3><span id="4指定池的pg边界">4.指定池的PG边界</span></h3><p>也可以为池指定最小 PG 数。这对于建立客户端在执行 IO 时将看到的并行度数量的下限很有用，即使池几乎是空的。设置下限可防止 Ceph 将 PG 数量减少（或建议您减少）配置数量以下。<br>你可以为一个池设置 PG 的最小数量：</p><pre><code>ceph osd pool set &lt;pool-name&gt; pg_num_min &lt;num&gt;</code></pre><p>您还可以使用命令的可选参数指定池创建时的最小 PG 计数。–pg-num-min <num>ceph osd pool create</num></p><h2><span id="2pg_num的预选">2.PG_NUM的预选</span></h2><p>使用以下命令创建新池时：</p><pre><code>ceph osd pool create &#123;pool-name&#125; pg_num</code></pre><p>必须选择 的值，pg_num因为它不能（当前）自动计算。下面是一些常用的值：<br>少于 5 个 OSD 设置pg_num为 128<br>5 到 10 个 OSD 设置pg_num为 512<br>10 到 50 个 OSD 设置pg_num为 1024</p><p>如果你有超过 50 个 OSD，你需要了解权衡以及如何pg_num自己计算价值</p><p>pg_num自己计算值，请借助pgcalc工具<br>随着 OSD 数量的增加，为 pg_num 选择正确的值变得更加重要，因为它对集群的行为以及出现问题时数据的持久性有重大影响（即灾难性事件导致的概率）数据丢失）。</p><h2><span id="3如何使用pg组">3.如何使用PG组</span></h2><p>归置组 (PG) 聚合池中的对象，因为在每个对象的基础上跟踪对象放置和对象元数据在计算上是昂贵的——即，具有数百万个对象的系统实际上无法在每个对象的基础上跟踪放置。</p><p>Ceph 客户端将计算一个对象应该在哪个归置组中。它通过散列对象 ID 并根据定义的池中 PG 的数量和池的 ID 应用操作来实现这一点。有关详细信息，请参阅将 PG 映射到 OSD。</p><p>归置组中对象的内容存储在一组 OSD 中。例如，在一个大小为 2 的复制池中，每个归置组将对象存储在两个 OSD 上，如下所示。</p><p>如果 OSD #2 失败，另一个将被分配到 Placement Group #1 并填充 OSD #1 中所有对象的副本。如果池大小从 2 变为 3，一个额外的 OSD将被分配给归置组，并将接收归置组中所有对象的副本。</p><p>归置组不拥有 OSD；他们与来自同一池甚至其他池的其他归置组共享它。如果 OSD #2 失败，归置组 #2 也必须使用 OSD #3 恢复对象的副本。</p><p>当归置组数量增加时，新的归置组将被分配 OSD。CRUSH 函数的结果也会发生变化，一些来自以前的归置组的对象将被复制到新的归置组并从旧的归置组中删除。</p><h2><span id="4pg组权衡">4.PG组权衡</span></h2><p>数据持久化和在所有 OSD 之间的均匀分布需要更多的归置组，但它们的数量应该减少到最少以节省 CPU 和内存。</p><h3><span id="1数据持久化">1.数据持久化</span></h3><p>OSD 发生故障后，数据丢失的风险会增加，直到它包含的数据被完全恢复。让我们想象一个在单个归置组中导致永久数据丢失的场景：</p><p>OSD 失败，它包含的对象的所有副本都丢失了。对于归置组中的所有对象，副本的数量突然从三个减少到两个。</p><p>Ceph 通过选择一个新的 OSD 重新创建所有对象的第三个副本来开始恢复这个归置组。</p><p>同一归置组中的另一个 OSD 在新 OSD 完全填充第三个副本之前发生故障。一些对象将只有一个幸存的副本。</p><p>Ceph 选择另一个 OSD 并不断复制对象以恢复所需的副本数。</p><p>同一归置组中的第三个 OSD 在恢复完成之前发生故障。如果此 OSD 包含对象的唯一剩余副本，则它会永久丢失。</p><p>在一个包含 10 个 OSD 的集群中，在一个三副本池中有 512 个归置组，CRUSH 会给每个归置组三个 OSD。最后，每个 OSD 将托管 (512 * 3) &#x2F; 10 &#x3D; ~150 个归置组。当第一个 OSD 发生故障时，上述场景将因此同时开始对所有 150 个归置组进行恢复。</p><p>正在恢复的 150 个归置组很可能均匀分布在剩余的 9 个 OSD 上。因此，每个剩余的 OSD 可能会向所有其他 OSD 发送对象的副本，并且还会接收一些要存储的新对象，因为它们成为新归置组的一部分。</p><p>完成此恢复所需的时间完全取决于 Ceph 集群的架构。假设每个 OSD 都由一台机器上的 1TB SSD 托管，并且所有 OSD 都连接到一个 10Gb&#x2F;s 交换机，并且单个 OSD 的恢复在 M 分钟内完成。如果每台机器有两个 OSD 使用没有 SSD 日志和 1Gb&#x2F;s 交换机的旋转器，它至少会慢一个数量级。</p><p>在这种规模的集群中，归置组的数量对数据持久性几乎没有影响。它可能是 128 或 8192，恢复不会更慢或更快。</p><p>然而，将同一个 Ceph 集群增加到 20 个 OSD 而不是 10 个 OSD 可能会加快恢复速度，从而显着提高数据持久性。每个 OSD 现在只参与大约 75 个归置组，而不是当时只有 10 个 OSD 时的大约 150 个，并且它仍然需要所有 19 个剩余的 OSD 执行相同数量的对象副本才能恢复。但是 10 个 OSD 每个必须复制大约 100GB，现在它们每个必须复制 50GB。如果网络是瓶颈，恢复速度将提高一倍。换句话说，当 OSD 的数量增加时，恢复会更快。</p><p>如果这个集群增长到 40 个 OSD，每个 OSD 将只托管大约 35 个归置组。如果一个 OSD 死了，恢复将继续进行得更快，除非它被另一个瓶颈阻塞。然而，如果这个集群增长到 200 个 OSD，每个 OSD 将只托管 ~7 个归置组。如果一个 OSD 挂掉，恢复将发生在这些归置组中最多约 21 (7 * 3) 个 OSD 之间：恢复将比有 40 个 OSD 时花费更长的时间，这意味着应该增加归置组的数量。</p><p>无论恢复时间有多短，第二个 OSD 在执行过程中都有可能失败。在上面描述的 10 个 OSD 集群中，如果其中任何一个发生故障，那么大约 17 个归置组（即大约 150 &#x2F; 9 个归置组正在恢复）将只有一个幸存的副本。如果剩下的 8 个 OSD 中的任何一个失败，两个归置组的最后一个对象很可能会丢失（即大约 17 &#x2F; 8 个归置组，只有一个剩余副本被恢复）。</p><p>当集群规模增长到 20 个 OSD 时，由于丢失三个 OSD 而损坏的归置组数量下降。第二个 OSD 丢失将降级 ~4（即 ~75 &#x2F; 19 个归置组被恢复）而不是 ~17 并且第三个丢失的 OSD 仅当它是包含幸存副本的四个 OSD 之一时才会丢失数据。换句话说，如果在恢复时间范围内丢失一个 OSD 的概率为 0.0001%，则它从具有 10 个 OSD 的集群中的 17 * 10 * 0.0001% 变为具有 20 个 OSD 的集群中的 4 * 20 * 0.0001%。</p><p>简而言之，更多的 OSD 意味着更快的恢复和更低的级联故障导致归置组永久丢失的风险。就数据持久性而言，拥有 512 或 4096 个归置组大致相当于少于 50 个 OSD 的集群。</p><p>注意：添加到集群的新 OSD 可能需要很长时间才能填充分配给它的归置组。但是，任何对象都不会降级，并且不会影响集群中包含的数据的持久性。</p><h3><span id="2池中的对象分布">2.池中的对象分布</span></h3><p>理想情况下，对象均匀分布在每个归置组中。由于 CRUSH 会为每个对象计算归置组，但实际上并不知道该归置组内每个 OSD 中存储了多少数据，归置组数量与 OSD 数量之间的比例可能会显着影响数据的分布。</p><p>例如，如果在一个三副本池中有一个用于十个 OSD 的归置组，则只会使用三个 OSD，因为 CRUSH 别无选择。当有更多归置组可用时，对象更有可能在其中均匀分布。CRUSH 还尽一切努力在所有现有的归置组中均匀分布 OSD。</p><p>只要归置组比 OSD 多一到两个数量级，分布就应该是均匀的。例如，3 个 OSD 有 256 个归置组，10 个 OSD 有 512 或 1024 个归置组等。</p><p>数据分布不均可能是由 OSD 和归置组之间的比例以外的因素引起的。由于 CRUSH 不考虑对象的大小，一些非常大的对象可能会造成不平衡。</p><p>假设一百万个 4K 对象（总计 4GB）平均分布在 10 个 OSD 上的 1024 个归置组中。他们将在每个 OSD 上使用 4GB &#x2F; 10 &#x3D; 400MB。如果将一个 400MB 的对象添加到池中，则支持放置该对象的归置组的三个 OSD 将被 400MB + 400MB &#x3D; 800MB 填充，而其他七个 OSD 将仅占用 400MB。</p><h3><span id="3内存-cpu和网络使用">3.内存、CPU和网络使用</span></h3><p>对于每个归置组，OSD 和 MON 始终需要内存、网络和 CPU，在恢复期间甚至需要更多。通过在归置组中集群对象来分担这种开销是它们存在的主要原因之一。<br>最大限度地减少归置组的数量可以节省大量资源。</p><h2><span id="5选择pg组的数量">5.选择PG组的数量</span></h2><p>如果您有超过 50 个 OSD，我们建议每个 OSD 大约有 50-100 个归置组，以平衡资源使用、数据持久性和分布。如果您的 OSD 少于 50 个，最好在上面的预选中进行选择。对于单个对象池，您可以使用以下公式来获取基线：</p><pre><code>             (OSDs * 100)Total PGs =  ------------              pool size</code></pre><p>其中池大小是复制池的副本数或纠删码池的 K+M 总和（由ceph osd erasure-code-profile get返回）。 </p><p>然后，您应该检查结果是否符合您设计 Ceph 集群的方式，以最大化数据持久性、 对象分布和最小化资源使用。</p><p>结果应始终四舍五入为最接近的 2 的幂。</p><p>只有 2 的幂才能平均平衡放置组中的对象数量。其他值将导致 OSD 之间的数据分布不均匀。它们的使用应仅限于从 2 的一个幂递增到另一个。<br>例如，对于具有 200 个 OSD 和池大小为 3 个副本的集群，您可以按如下方式估算 PG 的数量：</p><pre><code>(200 * 100)----------- = 6667. Nearest power of 2: 8192     3</code></pre><p>当使用多个数据池存储对象时，您需要确保平衡每个池的归置组数量与每个 OSD 的归置组数量，以便获得合理的归置组总数，从而为每个 OSD 提供合理的低方差无需占用系统资源或使对等过程太慢。</p><p>例如，一个由 10 个池组成的集群，每个池在 10 个 OSD 上有 512 个归置组，总共有 5,120 个归置组分布在 10 个 OSD 上，即每个 OSD 有 512 个归置组。那不会使用太多资源。但是，如果创建了 1,000 个池，每个池有 512 个归置组，每个 OSD 将处理大约 50,000 个归置组，这将需要更多的资源和时间来进行对等。</p><h2><span id="6设置pg组的数量">6.设置PG组的数量</span></h2><p>要设置池中归置组的数量，您必须在创建池时指定归置组的数量。有关详细信息，请参阅创建池。即使在创建池之后，您也可以通过以下方式更改归置组的数量：</p><pre><code>ceph osd pool set &#123;pool-name&#125; pg_num &#123;pg_num&#125;</code></pre><p>增加归置组数量后，您还必须增加归置组 ( ) 的数量，pgp_num然后集群才会重新平衡。这pgp_num将是 CRUSH 算法将考虑放置的放置组的数量。增加pg_num拆分放置组，但数据不会迁移到较新的放置组，直到放置组用于放置，即。pgp_num增加。应该pgp_num 等于pg_num。要增加放置组的数量，请执行以下操作：</p><pre><code>ceph osd pool set &#123;pool-name&#125; pgp_num &#123;pgp_num&#125;</code></pre><p>当减少 PG 的数量时，pgp_num会自动为您调整。</p><h2><span id="7获取pg组的数量">7.获取PG组的数量</span></h2><p>要获取池中归置组的数量，请执行以下命令：</p><pre><code>ceph osd pool get &#123;pool-name&#125; pg_num</code></pre><h2><span id="8获取集群的pg统计信息">8.获取集群的PG统计信息</span></h2><p>要获取集群中归置组的统计信息，请执行以下命令：</p><pre><code>ceph pg dump [--format &#123;format&#125;]</code></pre><p>有效格式为plain（默认）和json.</p><h2><span id="9获取卡住pg的统计数据">9.获取卡住PG的统计数据</span></h2><p>要获取所有卡在指定状态的归置组的统计信息，请执行以下命令：</p><pre><code>mt &lt;format&gt;] [-t|--threshold &lt;seconds&gt;]</code></pre><p>Inactive归置组无法处理读取或写入，因为它们正在等待具有最新数据的 OSD 出现和进入。</p><p>Unclean Placement 组包含未复制所需次数的对象。他们应该正在康复。</p><p>Stale归置组处于未知状态 - 托管它们的 OSD 有一段时间没有向监视器集群报告（由配置mon_osd_report_timeout）。</p><p>有效格式为plain（默认）和json. 阈值定义归置组在包含在返回的统计信息之前被卡住的最小秒数（默认 300 秒）。</p><h2><span id="10获取pg地图">10.获取PG地图</span></h2><p>要获取特定归置组的归置组映射，请执行以下命令：</p><pre><code>ceph pg map &#123;pg-id&#125;</code></pre><p>例如：</p><pre><code>ceph pg map 1.6c</code></pre><p>Ceph 将返回归置组图、归置组和 OSD 状态：</p><pre><code>osdmap e13 pg 1.6c (1.6c) -&gt; up [1,0] acting [1,0]</code></pre><h2><span id="11获取pg统计数据">11.获取PG统计数据</span></h2><p>要检索特定归置组的统计信息，请执行以下命令：</p><pre><code>ceph pg &#123;pg-id&#125; query</code></pre><h2><span id="12清理pg组">12.清理PG组</span></h2><p>要清理归置组，请执行以下命令：</p><pre><code>ceph pg scrub &#123;pg-id&#125;</code></pre><p>Ceph 检查主节点和任何副本节点，生成归置组中所有对象的目录并进行比较以确保没有对象丢失或不匹配，并且它们的内容是一致的。假设副本全部匹配，最后的语义扫描确保所有与快照相关的对象元数据是一致的。通过日志报告错误。<br>要清除特定池中的所有归置组，请执行以下操作：</p><pre><code>ceph osd pool scrub &#123;pool-name&#125;</code></pre><h2><span id="13优先考虑pg组的回填x2f恢复">13.优先考虑PG组的回填&#x2F;恢复</span></h2><p>您可能会遇到这样一种情况，即一堆归置组需要恢复和&#x2F;或回填，并且某些特定组保存的数据比其他组更重要（例如，那些 PG 可能保存正在运行的机器使用的图像的数据，而其他 PG 可能是由不活动的机器&#x2F;不太相关的数据使用）。在这种情况下，您可能希望优先恢复这些组，以便更早地恢复存储在这些组上的数据的性能和&#x2F;或可用性。为此（在回填或恢复期间将特定归置组标记为优先级），执行以下命令：</p><pre><code>ceph pg force-recovery &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...]ceph pg force-backfill &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...]</code></pre><p>这将导致 Ceph 在其他归置组之前先对指定的归置组执行恢复或回填。这不会中断当前正在进行的回填或恢复，但会导致尽快处理指定的 PG。如果您改变主意或优先考虑错误的群体，请使用：</p><pre><code>ceph pg cancel-force-recovery &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...]ceph pg cancel-force-backfill &#123;pg-id&#125; [&#123;pg-id #2&#125;] [&#123;pg-id #3&#125; ...]</code></pre><p>这将从这些 PG 中删除“force”标志，它们将按默认顺序处理。同样，这不会影响当前处理的归置组，只会影响仍在排队的归置组。<br>“强制”标志在恢复或组回填完成后自动清除。<br>同样，你可以使用以下命令强制 Ceph 先从指定池中对所有归置组执行恢复或回填：</p><pre><code>ceph osd pool force-recovery &#123;pool-name&#125;ceph osd pool force-backfill &#123;pool-name&#125;</code></pre><p>或者：</p><pre><code>ceph osd pool cancel-force-recovery &#123;pool-name&#125;ceph osd pool cancel-force-backfill &#123;pool-name&#125;</code></pre><p>如果您改变主意，可以恢复到默认的恢复或回填优先级。<br>请注意，这些命令可能会破坏 Ceph 内部优先级计算的顺序，因此请谨慎使用！特别是，如果您有多个当前共享相同底层 OSD 的池，并且某些特定池持有比其他池更重要的数据，我们建议您使用以下命令以更好的顺序重新安排所有池的恢复&#x2F;回填优先级：</p><pre><code>ceph osd pool set &#123;pool-name&#125; recovery_priority &#123;value&#125;</code></pre><p>例如，如果您有 10 个池，您可以将最重要的一个设置为 10，下一个为 9，等等。或者您可以不理会大多数池，并假设 3 个重要的池的优先级分别为 1 或优先级 3、2、1。</p><h2><span id="14恢复丢失">14.恢复丢失</span></h2><p>如果集群丢失了一个或多个对象，并且您决定放弃搜索丢失的数据，则必须将未找到的对象标记为lost。<br>如果已经查询了所有可能的位置并且对象仍然丢失，您可能不得不放弃丢失的对象。这是可能的，因为异常的故障组合允许集群了解在写入本身被恢复之前执行的写入。<br>目前唯一支持的选项是“还原”，它将回滚到对象的先前版本或者（如果它是一个新对象）完全忘记它。要将“未找到”对象标记为“丢失”，请执行以下命令：</p><pre><code>ceph pg &#123;pg-id&#125; mark_unfound_lost revert|delete</code></pre><blockquote><p>请谨慎使用此功能，因为它可能会混淆期望对象存在的应用程序。</p></blockquote><h2><span id="15实操">15.实操</span></h2><pre><code>删除pool ，重启mon后失效[root@node-1 ceph-1]# ceph osd pool rm ceph-demo-3 ceph-demo-3 --yes-i-really-really-mean-it #尝试删除poolError EPERM: pool deletion is disabled; you must first set the mon_allow_pool_delete config option to true before you can destroy a pool[root@node-1 ceph-1]# ceph --admin-daemon /var/run/ceph/ceph-mon.node-1.asok config show | grep mon_allow_pool_delete #查看mon_allow_pool_delete参数    &quot;mon_allow_pool_delete&quot;: &quot;false&quot;,You have new mail in /var/spool/mail/root[root@node-1 ceph-1]# ceph --admin-daemon /var/run/ceph/ceph-mon.node-1.asok config set  mon_allow_pool_delete true #修改mon_allow_pool_delete参数&#123;    &quot;success&quot;: &quot;mon_allow_pool_delete = &#39;true&#39; &quot;&#125;[root@node-1 ceph-1]# ceph osd pool rm ceph-demo-3 ceph-demo-3 --yes-i-really-really-mean-it #删除成功pool &#39;ceph-demo-3&#39; removed永久生效[root@node-1 my-cluster]# cat /opt/my-cluster/ceph.conf #查看配置文件[global]fsid = b8e58b30-4568-4032-a9f4-837ed3fa9529public_network = 192.168.187.0/24cluster_network = 192.168.199.0/24mon_initial_members = node-1mon_host = 192.168.187.201auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxmon_allow_pool_delete = true #可手动删除pool[root@node-1 my-cluster]# ceph-deploy --overwrite-conf config push node-1 node-2 node-3  #将配置文件推送到所有mon节点中[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy --overwrite-conf config push node-1 node-2 node-3[ceph_deploy.cli][INFO  ] ceph-deploy options:[ceph_deploy.cli][INFO  ]  username                      : None[ceph_deploy.cli][INFO  ]  verbose                       : False[ceph_deploy.cli][INFO  ]  overwrite_conf                : True[ceph_deploy.cli][INFO  ]  subcommand                    : push[ceph_deploy.cli][INFO  ]  quiet                         : False[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f048853b878&gt;[ceph_deploy.cli][INFO  ]  cluster                       : ceph[ceph_deploy.cli][INFO  ]  client                        : [&#39;node-1&#39;, &#39;node-2&#39;, &#39;node-3&#39;][ceph_deploy.cli][INFO  ]  func                          : &lt;function config at 0x7f04880aa938&gt;[ceph_deploy.cli][INFO  ]  ceph_conf                     : None[ceph_deploy.cli][INFO  ]  default_release               : False[ceph_deploy.config][DEBUG ] Pushing config to node-1[node-1][DEBUG ] connected to host: node-1 [node-1][DEBUG ] detect platform information from remote host[node-1][DEBUG ] detect machine type[node-1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph_deploy.config][DEBUG ] Pushing config to node-2[node-2][DEBUG ] connected to host: node-2 [node-2][DEBUG ] detect platform information from remote host[node-2][DEBUG ] detect machine type[node-2][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph_deploy.config][DEBUG ] Pushing config to node-3[node-3][DEBUG ] connected to host: node-3 [node-3][DEBUG ] detect platform information from remote host[node-3][DEBUG ] detect machine type[node-3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[root@node-1 my-cluster]# for i in &#123;1..3&#125;;do ssh node-$&#123;i&#125; systemctl restart ceph-mon.target;done #重启所有节点mon服务[root@node-1 my-cluster]# ceph --admin-daemon /var/run/ceph/ceph-mon.node-1.asok config show | grep mon_allow_pool_delete  #查看生效    &quot;mon_allow_pool_delete&quot;: &quot;true&quot;,</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Ceph守护进程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>定制Crush Map规则</title>
      <link href="/2023/04/19/Ceph/9.%E5%AE%9A%E5%88%B6Crush_map%E8%A7%84%E5%88%99/%E5%AE%9A%E5%88%B6Crush_map%E8%A7%84%E5%88%99/"/>
      <url>/2023/04/19/Ceph/9.%E5%AE%9A%E5%88%B6Crush_map%E8%A7%84%E5%88%99/%E5%AE%9A%E5%88%B6Crush_map%E8%A7%84%E5%88%99/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E9%9B%B6-%E6%9F%A5%E7%9C%8Bcrush-map-%E8%A7%84%E5%88%99">零、查看Crush Map 规则</a></li><li><a href="#%E4%B8%80-%E6%89%8B%E5%8A%A8%E7%BC%96%E8%BE%91crush-map%E8%A7%84%E5%88%99">一、手动编辑Crush MAP规则</a><ul><li><a href="#%E4%B8%80-%E8%8E%B7%E5%8F%96crush-map%E8%A7%84%E5%88%99">一、获取Crush map规则</a></li><li><a href="#%E4%BA%8C-%E7%BC%96%E8%AF%91%E6%88%90txt%E6%96%87%E4%BB%B6">二、编译成Txt文件</a></li><li><a href="#%E4%B8%89-%E4%BF%AE%E6%94%B9crush-map%E6%96%87%E4%BB%B6">三、修改Crush Map文件</a></li><li><a href="#%E5%9B%9B-%E9%87%8D%E6%96%B0%E7%BC%96%E8%AF%91%E6%88%90%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6">四、重新编译成二进制文件</a></li><li><a href="#%E4%BA%94-%E5%BA%94%E7%94%A8%E4%B8%8A%E4%BC%A0%E7%9A%84%E6%96%B0%E8%A7%84%E5%88%99">五、应用上传的新规则</a></li><li><a href="#%E5%85%AD-%E5%88%9B%E5%BB%BApool%E8%B0%83%E7%94%A8%E6%96%B0%E7%9A%84crush-map">六、创建pool调用新的Crush Map</a></li><li><a href="#%E4%B8%83-%E6%B5%8B%E8%AF%95crush-map%E8%A7%84%E5%88%99">七、测试Crush Map规则</a></li><li><a href="#%E5%85%AB-%E6%B8%85%E7%90%86crush%E8%A7%84%E5%88%99%E6%81%A2%E5%A4%8D%E9%BB%98%E8%AE%A4">八、清理Crush规则，恢复默认</a></li></ul></li><li><a href="#%E4%BA%8C-%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%B0%83%E6%95%B4crush-map">二、命令行调整Crush Map</a><ul><li><a href="#%E4%B8%80-%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8Dcrush-map">一、查看当前Crush Map</a></li><li><a href="#%E4%BA%8C-%E5%88%9B%E5%BB%BA%E6%A0%B9">二、创建根</a></li><li><a href="#%E5%9B%9B-%E5%B0%86%E8%8A%82%E7%82%B9%E5%8A%A0%E5%85%A5%E5%88%B0ssd%E7%9A%84%E6%A0%B9%E4%B8%8B">四、将节点加入到ssd的根下</a></li><li><a href="#%E5%9B%9B-%E5%B0%86osd%E5%8A%A0%E5%85%A5%E5%88%B0host%E4%B8%AD">四、将OSD加入到HOST中</a></li><li><a href="#%E4%BA%94-%E4%BF%AE%E6%94%B9class">五、修改CLASS</a></li><li><a href="#%E5%85%AD-%E5%88%9B%E5%BB%BA%E8%A7%84%E5%88%99%E7%BB%91%E5%AE%9Aroot">六、创建规则绑定root</a></li><li><a href="#%E4%B8%83-%E5%BA%94%E7%94%A8%E8%A7%84%E5%88%99">七、应用规则</a></li></ul></li><li><a href="#%E4%B8%89-%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9">三、注意事项</a></li></ul><!-- tocstop --><p><img src="/images/Ceph/%E5%AE%9A%E5%88%B6CrushMap%E8%A7%84%E5%88%99/1.jpg"></p><p>CRUSH算法通过计算数据存储位置来决定如何存储和检索数据。 CRUSH使 Ceph 客户端能够直接与 OSD通信，而不是通过集中式服务器或代理。通过算法确定的存储和检索数据的方法，Ceph 避免了单点故障、性能瓶颈和可扩展性的物理限制。</p><p>CRUSH需要集群的地图，并使用 CRUSH 地图伪随机地在 OSD 中存储和检索数据，并在整个集群中均匀分布数据。有关 CRUSH 的详细讨论，请参阅 CRUSH-受控的、可扩展的、分散的复制数据放置</p><p>CRUSH 图包含OSD列表，用于将设备聚合到物理位置的“桶”列表，以及告诉 CRUSH 应如何在 Ceph集群的池中复制数据的规则列表。通过反映安装的底层物理组织，CRUSH可以建模——从而解决——相关设备故障的潜在来源。典型的来源包括物理距离、共享电源和共享网络。通过将此信息编码到集群映射中，CRUSH 放置策略可以在不同的故障域中分离对象副本，同时仍保持所需的分布。例如，为了解决并发故障的可能性，可能需要确保数据副本位于使用不同架子、机架、电源、控制器和&#x2F;或物理位置的设备上。</p><p>当您部署 OSD 时，它们会自动放置在CRUSH映射中的一个 host节点下，该节点以运行它们的主机的主机名命名。这与默认的 CRUSH 故障域相结合，可确保副本或纠删码分片在主机之间分离，并且单个主机故障不会影响可用性。然而，对于较大的集群，管理员应该仔细考虑他们对故障域的选择。例如，跨机架分离副本对于大中型集群来说很常见。</p><h1><span id="零-查看crush-map-规则">零、查看Crush Map 规则</span></h1><pre><code>[root@node-1 /]# ceph osd crush tree ID CLASS WEIGHT  TYPE NAME       -1       0.05878 root default    -3       0.01959     host node-1  0   hdd 0.00980         osd.0    1   hdd 0.00980         osd.1   -5       0.01959     host node-2  2   hdd 0.00980         osd.2    3   hdd 0.00980         osd.3   -7       0.01959     host node-3  4   hdd 0.00980         osd.4    5   hdd 0.00980         osd.5   [root@node-1 /]# ceph osd crush dump #查看详细信息&#123;    &quot;devices&quot;: [#设备信息        &#123;            &quot;id&quot;: 0, #OSD的ID号            &quot;name&quot;: &quot;osd.0&quot;, #OSD名称            &quot;class&quot;: &quot;hdd&quot;#归类        &#125;,        &#123;            &quot;id&quot;: 1,            &quot;name&quot;: &quot;osd.1&quot;,            &quot;class&quot;: &quot;hdd&quot;        &#125;,        &#123;            &quot;id&quot;: 2,            &quot;name&quot;: &quot;osd.2&quot;,            &quot;class&quot;: &quot;hdd&quot;        &#125;,        &#123;            &quot;id&quot;: 3,            &quot;name&quot;: &quot;osd.3&quot;,            &quot;class&quot;: &quot;hdd&quot;        &#125;,        &#123;            &quot;id&quot;: 4,            &quot;name&quot;: &quot;osd.4&quot;,            &quot;class&quot;: &quot;hdd&quot;        &#125;,        &#123;            &quot;id&quot;: 5,            &quot;name&quot;: &quot;osd.5&quot;,            &quot;class&quot;: &quot;hdd&quot;        &#125;    ],    &quot;types&quot;: [ #定义的类型        &#123;            &quot;type_id&quot;: 0,            &quot;name&quot;: &quot;osd&quot; #硬盘        &#125;,        &#123;            &quot;type_id&quot;: 1,            &quot;name&quot;: &quot;host&quot;#主机        &#125;,        &#123;            &quot;type_id&quot;: 2,            &quot;name&quot;: &quot;chassis&quot; #主板/机箱        &#125;,        &#123;            &quot;type_id&quot;: 3,            &quot;name&quot;: &quot;rack&quot; #机架        &#125;,        &#123;            &quot;type_id&quot;: 4,            &quot;name&quot;: &quot;row&quot;        &#125;,        &#123;            &quot;type_id&quot;: 5,            &quot;name&quot;: &quot;pdu&quot; #PDU电源        &#125;,        &#123;            &quot;type_id&quot;: 6,            &quot;name&quot;: &quot;pod&quot;        &#125;,        &#123;            &quot;type_id&quot;: 7,             &quot;name&quot;: &quot;room&quot; #机房        &#125;,        &#123;            &quot;type_id&quot;: 8,            &quot;name&quot;: &quot;datacenter&quot; #数据中心        &#125;,        &#123;            &quot;type_id&quot;: 9,            &quot;name&quot;: &quot;zone&quot; #区域        &#125;,        &#123;            &quot;type_id&quot;: 10,            &quot;name&quot;: &quot;region&quot;        &#125;,        &#123;            &quot;type_id&quot;: 11,            &quot;name&quot;: &quot;root&quot;        &#125;    ],    &quot;buckets&quot;: [        &#123;            &quot;id&quot;: -1,            &quot;name&quot;: &quot;default&quot;,            &quot;type_id&quot;: 11,            &quot;type_name&quot;: &quot;root&quot;,            &quot;weight&quot;: 3852,            &quot;alg&quot;: &quot;straw2&quot;,            &quot;hash&quot;: &quot;rjenkins1&quot;,            &quot;items&quot;: [                &#123;                    &quot;id&quot;: -3,                    &quot;weight&quot;: 1284,                    &quot;pos&quot;: 0                &#125;,                &#123;                    &quot;id&quot;: -5,                    &quot;weight&quot;: 1284,                    &quot;pos&quot;: 1                &#125;,                &#123;                    &quot;id&quot;: -7,                    &quot;weight&quot;: 1284,                    &quot;pos&quot;: 2                &#125;            ]        &#125;,        &#123;            &quot;id&quot;: -2,            &quot;name&quot;: &quot;default~hdd&quot;,            &quot;type_id&quot;: 11,            &quot;type_name&quot;: &quot;root&quot;,            &quot;weight&quot;: 3852,            &quot;alg&quot;: &quot;straw2&quot;,            &quot;hash&quot;: &quot;rjenkins1&quot;,            &quot;items&quot;: [                &#123;                    &quot;id&quot;: -4,                    &quot;weight&quot;: 1284,                    &quot;pos&quot;: 0                &#125;,                &#123;                    &quot;id&quot;: -6,                    &quot;weight&quot;: 1284,                    &quot;pos&quot;: 1                &#125;,                &#123;                    &quot;id&quot;: -8,                    &quot;weight&quot;: 1284,                    &quot;pos&quot;: 2                &#125;            ]        &#125;,        &#123;            &quot;id&quot;: -3,            &quot;name&quot;: &quot;node-1&quot;,            &quot;type_id&quot;: 1,            &quot;type_name&quot;: &quot;host&quot;,            &quot;weight&quot;: 1284,            &quot;alg&quot;: &quot;straw2&quot;,            &quot;hash&quot;: &quot;rjenkins1&quot;,            &quot;items&quot;: [                &#123;                    &quot;id&quot;: 0,                    &quot;weight&quot;: 642,                    &quot;pos&quot;: 0                &#125;,                &#123;                    &quot;id&quot;: 1,                    &quot;weight&quot;: 642,                    &quot;pos&quot;: 1                &#125;            ]        &#125;,        &#123;            &quot;id&quot;: -4,            &quot;name&quot;: &quot;node-1~hdd&quot;,            &quot;type_id&quot;: 1,            &quot;type_name&quot;: &quot;host&quot;,            &quot;weight&quot;: 1284,            &quot;alg&quot;: &quot;straw2&quot;,            &quot;hash&quot;: &quot;rjenkins1&quot;,            &quot;items&quot;: [                &#123;                    &quot;id&quot;: 0,                    &quot;weight&quot;: 642,                    &quot;pos&quot;: 0                &#125;,                &#123;                    &quot;id&quot;: 1,                    &quot;weight&quot;: 642,                    &quot;pos&quot;: 1                &#125;            ]        &#125;,        &#123;            &quot;id&quot;: -5,            &quot;name&quot;: &quot;node-2&quot;,            &quot;type_id&quot;: 1,            &quot;type_name&quot;: &quot;host&quot;,            &quot;weight&quot;: 1284,            &quot;alg&quot;: &quot;straw2&quot;,            &quot;hash&quot;: &quot;rjenkins1&quot;,            &quot;items&quot;: [                &#123;                    &quot;id&quot;: 2,                    &quot;weight&quot;: 642,                    &quot;pos&quot;: 0                &#125;,                &#123;                    &quot;id&quot;: 3,                    &quot;weight&quot;: 642,                    &quot;pos&quot;: 1                &#125;            ]        &#125;,        &#123;            &quot;id&quot;: -6,            &quot;name&quot;: &quot;node-2~hdd&quot;,            &quot;type_id&quot;: 1,            &quot;type_name&quot;: &quot;host&quot;,            &quot;weight&quot;: 1284,            &quot;alg&quot;: &quot;straw2&quot;,            &quot;hash&quot;: &quot;rjenkins1&quot;,            &quot;items&quot;: [                &#123;                    &quot;id&quot;: 2,                    &quot;weight&quot;: 642,                    &quot;pos&quot;: 0                &#125;,                &#123;                    &quot;id&quot;: 3,                    &quot;weight&quot;: 642,                    &quot;pos&quot;: 1                &#125;            ]        &#125;,        &#123;            &quot;id&quot;: -7,            &quot;name&quot;: &quot;node-3&quot;,            &quot;type_id&quot;: 1,            &quot;type_name&quot;: &quot;host&quot;,            &quot;weight&quot;: 1284,            &quot;alg&quot;: &quot;straw2&quot;,            &quot;hash&quot;: &quot;rjenkins1&quot;,            &quot;items&quot;: [                &#123;                    &quot;id&quot;: 4,                    &quot;weight&quot;: 642,                    &quot;pos&quot;: 0                &#125;,                &#123;                    &quot;id&quot;: 5,                    &quot;weight&quot;: 642,                    &quot;pos&quot;: 1                &#125;            ]        &#125;,        &#123;            &quot;id&quot;: -8,            &quot;name&quot;: &quot;node-3~hdd&quot;,            &quot;type_id&quot;: 1,            &quot;type_name&quot;: &quot;host&quot;,            &quot;weight&quot;: 1284,            &quot;alg&quot;: &quot;straw2&quot;,            &quot;hash&quot;: &quot;rjenkins1&quot;,            &quot;items&quot;: [                &#123;                    &quot;id&quot;: 4,                    &quot;weight&quot;: 642,                    &quot;pos&quot;: 0                &#125;,                &#123;                    &quot;id&quot;: 5,                    &quot;weight&quot;: 642,                    &quot;pos&quot;: 1                &#125;            ]        &#125;    ],    &quot;rules&quot;: [        &#123;            &quot;rule_id&quot;: 0,            &quot;rule_name&quot;: &quot;replicated_rule&quot;,            &quot;ruleset&quot;: 0,            &quot;type&quot;: 1,            &quot;min_size&quot;: 1,            &quot;max_size&quot;: 10,            &quot;steps&quot;: [                &#123;                    &quot;op&quot;: &quot;take&quot;,                    &quot;item&quot;: -1,                    &quot;item_name&quot;: &quot;default&quot;                &#125;,                &#123;                    &quot;op&quot;: &quot;chooseleaf_firstn&quot;,                    &quot;num&quot;: 0,                    &quot;type&quot;: &quot;host&quot;                &#125;,                &#123;                    &quot;op&quot;: &quot;emit&quot;                &#125;            ]        &#125;    ],    &quot;tunables&quot;: &#123;        &quot;choose_local_tries&quot;: 0,        &quot;choose_local_fallback_tries&quot;: 0,        &quot;choose_total_tries&quot;: 50,        &quot;chooseleaf_descend_once&quot;: 1,        &quot;chooseleaf_vary_r&quot;: 1,        &quot;chooseleaf_stable&quot;: 1,        &quot;straw_calc_version&quot;: 1,        &quot;allowed_bucket_algs&quot;: 54,        &quot;profile&quot;: &quot;jewel&quot;,        &quot;optimal_tunables&quot;: 1,        &quot;legacy_tunables&quot;: 0,        &quot;minimum_required_version&quot;: &quot;jewel&quot;,        &quot;require_feature_tunables&quot;: 1,        &quot;require_feature_tunables2&quot;: 1,        &quot;has_v2_rules&quot;: 0,        &quot;require_feature_tunables3&quot;: 1,        &quot;has_v3_rules&quot;: 0,        &quot;has_v4_buckets&quot;: 1,        &quot;require_feature_tunables5&quot;: 1,        &quot;has_v5_rules&quot;: 0    &#125;,    &quot;choose_args&quot;: &#123;&#125;&#125;[root@node-3 ~]#  ceph osd crush rule ls  #默认规则replicated_rule[root@node-3 ~]# ceph osd pool get ceph-demo crush_rule #查看pool关联的规则crush_rule: replicated_rule</code></pre><h1><span id="一-手动编辑crush-map规则">一、手动编辑Crush MAP规则</span></h1><h2><span id="一-获取crush-map规则">一、获取Crush map规则</span></h2><p>请将Crush Map文件进行保存，恢复时会需要，尽量不要在生产环境修改，风险太大，最好在规划阶段就完成</p><pre><code>[root@node-1 /]# ceph osd getcrushmap -o crushmap.bin 13[root@node-1 /]# file crushmap.bin  #查看文件是个二进制文件crushmap.bin: MS Windows icon resource - 8 icons, 1-colors</code></pre><h2><span id="二-编译成txt文件">二、编译成Txt文件</span></h2><pre><code>[root@node-1 my-cluster]# crushtool -d crushmap.bin -o crushmap.txt #将二进制文件编译成Txt文件</code></pre><h2><span id="三-修改crush-map文件">三、修改Crush Map文件</span></h2><p>查看原文件：</p><pre><code>[root@node-1 my-cluster]# cat crushmap.txt # begin crush maptunable choose_local_tries 0tunable choose_local_fallback_tries 0tunable choose_total_tries 50tunable chooseleaf_descend_once 1tunable chooseleaf_vary_r 1tunable chooseleaf_stable 1tunable straw_calc_version 1tunable allowed_bucket_algs 54# devicesdevice 0 osd.0 class hdddevice 1 osd.1 class hdddevice 2 osd.2 class hdddevice 3 osd.3 class hdddevice 4 osd.4 class hdddevice 5 osd.5 class hdd# typestype 0 osdtype 1 hosttype 2 chassistype 3 racktype 4 rowtype 5 pdutype 6 podtype 7 roomtype 8 datacentertype 9 zonetype 10 regiontype 11 root# bucketshost node-1 &#123;    id -3        # do not change unnecessarily    id -4 class hdd        # do not change unnecessarily    # weight 0.020    alg straw2    hash 0    # rjenkins1    item osd.0 weight 0.010    item osd.1 weight 0.010&#125;host node-2 &#123;    id -5        # do not change unnecessarily    id -6 class hdd        # do not change unnecessarily    # weight 0.020    alg straw2    hash 0    # rjenkins1    item osd.2 weight 0.010    item osd.3 weight 0.010&#125;host node-3 &#123;    id -7        # do not change unnecessarily    id -8 class hdd        # do not change unnecessarily    # weight 0.020    alg straw2    hash 0    # rjenkins1    item osd.4 weight 0.010    item osd.5 weight 0.010&#125;root default &#123;    id -1        # do not change unnecessarily    id -2 class hdd        # do not change unnecessarily    # weight 0.059    alg straw2    hash 0    # rjenkins1    item node-1 weight 0.020    item node-2 weight 0.020    item node-3 weight 0.020&#125;# rulesrule replicated_rule &#123;    id 0    type replicated    min_size 1    max_size 10    step take default    step chooseleaf firstn 0 type host    step emit&#125;# end crush map</code></pre><p>查看修改后的文件</p><pre><code>[root@node-1 my-cluster]# cat crushmap.txt # begin crush maptunable choose_local_tries 0tunable choose_local_fallback_tries 0tunable choose_total_tries 50tunable chooseleaf_descend_once 1tunable chooseleaf_vary_r 1tunable chooseleaf_stable 1tunable straw_calc_version 1tunable allowed_bucket_algs 54# devicesdevice 0 osd.0 class hdddevice 1 osd.1 class ssd #设置此硬盘为ssddevice 2 osd.2 class hdddevice 3 osd.3 class ssd #设置此硬盘为ssddevice 4 osd.4 class hdddevice 5 osd.5 class ssd #设置此硬盘为ssd# typestype 0 osdtype 1 hosttype 2 chassistype 3 racktype 4 rowtype 5 pdutype 6 podtype 7 roomtype 8 datacentertype 9 zonetype 10 regiontype 11 root# bucketshost node-1 &#123;    id -3        # do not change unnecessarily    id -4 class hdd        # do not change unnecessarily    # weight 0.020    alg straw2    hash 0    # rjenkins1    item osd.0 weight 0.010 #删除OSD.1&#125;host node-2 &#123;    id -5        # do not change unnecessarily    id -6 class hdd        # do not change unnecessarily    # weight 0.020    alg straw2    hash 0    # rjenkins1    item osd.2 weight 0.010 #删除OSD.3&#125;host node-3 &#123;    id -7        # do not change unnecessarily    id -8 class hdd        # do not change unnecessarily    # weight 0.020    alg straw2    hash 0    # rjenkins1    item osd.4 weight 0.010 #删除OSD.5&#125;#新增SSD buckets#----------host node-1-ssd &#123;    id -9        # do not change unnecessarily    id -10 class ssd        # do not change unnecessarily    # weight 0.020    alg straw2    hash 0    # rjenkins1    item osd.1 weight 0.010&#125;host node-2-ssd &#123;    id -11        # do not change unnecessarily    id -12 class ssd        # do not change unnecessarily    # weight 0.020    alg straw2    hash 0    # rjenkins1    item osd.3 weight 0.010&#125;host node-3-ssd &#123;    id -13        # do not change unnecessarily    id -14 class ssd        # do not change unnecessarily    # weight 0.020    alg straw2    hash 0    # rjenkins1    item osd.5 weight 0.010&#125;#----------root default &#123;    id -1        # do not change unnecessarily    id -2 class hdd        # do not change unnecessarily    # weight 0.059    alg straw2    hash 0    # rjenkins1    item node-1 weight 0.010    item node-2 weight 0.010    item node-3 weight 0.010&#125;#新增根节点#----------root ssd &#123;    id -15        # do not change unnecessarily    id -16 class ssd        # do not change unnecessarily    # weight 0.059    alg straw2    hash 0    # rjenkins1    item node-1-ssd weight 0.010    item node-2-ssd weight 0.010    item node-3-ssd weight 0.010&#125;#----------# rulesrule replicated_rule &#123;    id 0    type replicated    min_size 1    max_size 10    step take default    step chooseleaf firstn 0 type host    step emit&#125;#新增Demo_rule#----------rule Demo_rule &#123;    id 1    type replicated    min_size 1    max_size 10    step take ssd    step chooseleaf firstn 0 type host    step emit&#125;#----------# end crush map</code></pre><h2><span id="四-重新编译成二进制文件">四、重新编译成二进制文件</span></h2><pre><code>[root@node-1 my-cluster]# crushtool -c crushmap.txt -o crushmap-new.bin</code></pre><h2><span id="五-应用上传的新规则">五、应用上传的新规则</span></h2><p>查看当前结构</p><pre><code>[root@node-2 log]# ceph osd tree ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.05878 root default                            -3       0.01959     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000  1   hdd 0.00980         osd.1       up  1.00000 1.00000 -5       0.01959     host node-2                          2   hdd 0.00980         osd.2       up  1.00000 1.00000  3   hdd 0.00980         osd.3       up  1.00000 1.00000 -7       0.01959     host node-3                          4   hdd 0.00980         osd.4       up  1.00000 1.00000  5   hdd 0.00980         osd.5       up  1.00000 1.00000 [root@node-1 my-cluster]# ceph -h |grep setcrush  #查看使用帮助osd setcrushmap &#123;&lt;int&gt;&#125;                                               set crush map from input file[root@node-1 my-cluster]# ceph osd setcrushmap -i crushmap-new.bin #应用规则14[root@node-2 log]# ceph osd tree #再次查看规则变化ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF -15       0.02998 root ssd                                     -9       0.00999     host node-1-ssd                           1   ssd 0.00999         osd.1           up  1.00000 1.00000 -11       0.00999     host node-2-ssd                           3   ssd 0.00999         osd.3           up  1.00000 1.00000 -13       0.00999     host node-3-ssd                           5   ssd 0.00999         osd.5           up  1.00000 1.00000  -1       0.02998 root default                                 -3       0.00999     host node-1                               0   hdd 0.00999         osd.0           up  1.00000 1.00000  -5       0.00999     host node-2                               2   hdd 0.00999         osd.2           up  1.00000 1.00000  -7       0.00999     host node-3                               4   hdd 0.00999         osd.4           up  1.00000 1.00000</code></pre><h2><span id="六-创建pool调用新的crush-map">六、创建pool调用新的Crush Map</span></h2><pre><code>[root@node-1 my-cluster]# ceph osd lspools1 ceph-demo2 ceph-demo-24 .rgw.root5 default.rgw.control6 default.rgw.meta7 default.rgw.log[root@node-1 my-cluster]# ceph osd pool get ceph-demo crush_rule  #查看ceph-demo调用的规则crush_rule: replicated_rule[root@node-1 my-cluster]# ceph osd crush rule  ls #查看当前ceph集群中的规则replicated_ruleDemo_rule[root@node-1 my-cluster]# ceph osd pool set ceph-demo crush_rule Demo_rule  #修改规则set pool 1 crush_rule to Demo_rule[root@node-1 my-cluster]# ceph osd pool get ceph-demo crush_rule #查看修改成功crush_rule: Demo_rule</code></pre><h2><span id="七-测试crush-map规则">七、测试Crush Map规则</span></h2><pre><code>[root@node-1 my-cluster]# rbd create ceph-demo/crush-demo.img --size 10G #创建一个块设备[root@node-1 my-cluster]# rbd ls ceph-demo #查看创建成功crush-demo.imgrbd-demo.img[root@node-1 my-cluster]# ceph osd map ceph-demo crush-demo.img #查看OSD分布情况osdmap e59 pool &#39;ceph-demo&#39; (1) object &#39;crush-demo.img&#39; -&gt; pg 1.d267742c (1.2c) -&gt; up ([1,5,3], p1) acting ([1,5,3], p1)[root@node-1 my-cluster]# ceph osd tree  #看到此快设备落在了osd1.3.5上ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF -15       0.02998 root ssd                                     -9       0.00999     host node-1-ssd                           1   ssd 0.00999         osd.1           up  1.00000 1.00000 -11       0.00999     host node-2-ssd                           3   ssd 0.00999         osd.3           up  1.00000 1.00000 -13       0.00999     host node-3-ssd                           5   ssd 0.00999         osd.5           up  1.00000 1.00000  -1       0.02998 root default                                 -3       0.00999     host node-1                               0   hdd 0.00999         osd.0           up  1.00000 1.00000  -5       0.00999     host node-2                               2   hdd 0.00999         osd.2           up  1.00000 1.00000  -7       0.00999     host node-3                               4   hdd 0.00999         osd.4           up  1.00000 1.00000 [root@node-1 my-cluster]# ceph osd lspools #查看pool1 ceph-demo2 ceph-demo-24 .rgw.root5 default.rgw.control6 default.rgw.meta7 default.rgw.log[root@node-1 my-cluster]# ceph osd pool get ceph-demo-2 crush_rule #查看此pool属于默认crush 规则crush_rule: replicated_rule[root@node-1 my-cluster]# rbd create ceph-demo-2/crush-map.img --size 3G #在ceph-demo-2的pool中创建一个快[root@node-1 my-cluster]# ceph osd map ceph-demo-2 crush-map.img  #查看此块设备的数据存放在OSD0.2.4中osdmap e62 pool &#39;ceph-demo-2&#39; (2) object &#39;crush-map.img&#39; -&gt; pg 2.e23c3129 (2.29) -&gt; up ([2,0,4], p2) acting ([2,0,4], p2)</code></pre><h2><span id="八-清理crush规则恢复默认">八、清理Crush规则，恢复默认</span></h2><pre><code>[root@node-1 my-cluster]# ceph osd crush rule ls  @查看默认规则名称replicated_rule #默认规则Demo_rule[root@node-1 my-cluster]# ceph osd pool set ceph-demo crush_rule replicated_rule #修改set pool 1 crush_rule to replicated_rule[root@node-1 my-cluster]# ceph osd setcrushmap -i crushmap.bin  #重新导入原有配置文件15[root@node-1 my-cluster]# ceph osd tree  #查看已经恢复ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.05878 root default                            -3       0.01959     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000  1   hdd 0.00980         osd.1       up  1.00000 1.00000 -5       0.01959     host node-2                          2   hdd 0.00980         osd.2       up  1.00000 1.00000  3   hdd 0.00980         osd.3       up  1.00000 1.00000 -7       0.01959     host node-3                          4   hdd 0.00980         osd.4       up  1.00000 1.00000  5   hdd 0.00980         osd.5       up  1.00000 1.00000</code></pre><h1><span id="二-命令行调整crush-map">二、命令行调整Crush Map</span></h1><h2><span id="一-查看当前crush-map">一、查看当前Crush Map</span></h2><pre><code>[root@node-1 my-cluster]# ceph osd tree ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.05878 root default                            -3       0.01959     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000  1   hdd 0.00980         osd.1       up  1.00000 1.00000 -5       0.01959     host node-2                          2   hdd 0.00980         osd.2       up  1.00000 1.00000  3   hdd 0.00980         osd.3       up  1.00000 1.00000 -7       0.01959     host node-3                          4   hdd 0.00980         osd.4       up  1.00000 1.00000  5   hdd 0.00980         osd.5       up  1.00000 1.00000</code></pre><h2><span id="二-创建根">二、创建根</span></h2><pre><code>[root@node-1 my-cluster]# ceph osd crush add-bucket ssd root #ssd是名称 root是类型added bucket ssd type root to crush map[root@node-1 my-cluster]# ceph osd tree #查看SSD的bucketID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -9             0 root ssd                                -1       0.05878 root default                            -3       0.01959     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000  1   hdd 0.00980         osd.1       up  1.00000 1.00000 -5       0.01959     host node-2                          2   hdd 0.00980         osd.2       up  1.00000 1.00000  3   hdd 0.00980         osd.3       up  1.00000 1.00000 -7       0.01959     host node-3                          4   hdd 0.00980         osd.4       up  1.00000 1.00000  5   hdd 0.00980         osd.5       up  1.00000 1.00000 三、将节点加入到bucket[root@node-1 my-cluster]# ceph osd crush add-bucket node-1-ssd host #node-1-ssd是名称 host是类型added bucket node-1-ssd type host to crush map[root@node-1 my-cluster]# ceph osd crush add-bucket node-2-ssd host added bucket node-2-ssd type host to crush map[root@node-1 my-cluster]# ceph osd crush add-bucket node-3-ssd host added bucket node-3-ssd type host to crush map[root@node-1 my-cluster]# ceph osd tree ID  CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -12             0 host node-3-ssd                         -11             0 host node-2-ssd                         -10             0 host node-1-ssd                          -9             0 root ssd                                 -1       0.05878 root default                             -3       0.01959     host node-1                           0   hdd 0.00980         osd.0       up  1.00000 1.00000   1   hdd 0.00980         osd.1       up  1.00000 1.00000  -5       0.01959     host node-2                           2   hdd 0.00980         osd.2       up  1.00000 1.00000   3   hdd 0.00980         osd.3       up  1.00000 1.00000  -7       0.01959     host node-3                           4   hdd 0.00980         osd.4       up  1.00000 1.00000   5   hdd 0.00980         osd.5       up  1.00000 1.00000</code></pre><h2><span id="四-将节点加入到ssd的根下">四、将节点加入到ssd的根下</span></h2><pre><code>[root@node-1 my-cluster]# ceph osd crush move node-1-ssd root=ssd #将node-1-ssd加入到ssd的根下moved item id -10 name &#39;node-1-ssd&#39; to location &#123;root=ssd&#125; in crush map[root@node-1 my-cluster]# ceph osd crush move node-2-ssd root=ssdmoved item id -11 name &#39;node-2-ssd&#39; to location &#123;root=ssd&#125; in crush map #将node-2-ssd加入到ssd的根下[root@node-1 my-cluster]# ceph osd crush move node-3-ssd root=ssdmoved item id -12 name &#39;node-3-ssd&#39; to location &#123;root=ssd&#125; in crush map #将node-3-ssd加入到ssd的根下[root@node-1 my-cluster]# ceph osd tree  #查看加入情况ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF  -9             0 root ssd                                    -10             0     host node-1-ssd                         -11             0     host node-2-ssd                         -12             0     host node-3-ssd                          -1       0.05878 root default                                 -3       0.01959     host node-1                               0   hdd 0.00980         osd.0           up  1.00000 1.00000   1   hdd 0.00980         osd.1           up  1.00000 1.00000  -5       0.01959     host node-2                               2   hdd 0.00980         osd.2           up  1.00000 1.00000   3   hdd 0.00980         osd.3           up  1.00000 1.00000  -7       0.01959     host node-3                               4   hdd 0.00980         osd.4           up  1.00000 1.00000   5   hdd 0.00980         osd.5           up  1.00000 1.00000</code></pre><h2><span id="四-将osd加入到host中">四、将OSD加入到HOST中</span></h2><pre><code>[root@node-1 my-cluster]# ceph osd crush move osd.1 host=node-1-ssd root=ssdmoved item id 1 name &#39;osd.1&#39; to location &#123;host=node-1-ssd&#125; in crush map[root@node-1 my-cluster]# ceph osd crush move osd.3 host=node-2-ssd root=ssdmoved item id 3 name &#39;osd.3&#39; to location &#123;host=node-2-ssd&#125; in crush map[root@node-1 my-cluster]# ceph osd crush move osd.5 host=node-3-ssd root=ssdmoved item id 5 name &#39;osd.5&#39; to location &#123;host=node-3-ssd&#125; in crush map[root@node-1 my-cluster]# ceph osd tree ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF  -9       0.02939 root ssd                                    -10       0.00980     host node-1-ssd                           1   hdd 0.00980         osd.1           up  1.00000 1.00000 -11       0.00980     host node-2-ssd                           3   hdd 0.00980         osd.3           up  1.00000 1.00000 -12       0.00980     host node-3-ssd                           5   hdd 0.00980         osd.5           up  1.00000 1.00000  -1       0.02939 root default                                 -3       0.00980     host node-1                               0   hdd 0.00980         osd.0           up  1.00000 1.00000  -5       0.00980     host node-2                               2   hdd 0.00980         osd.2           up  1.00000 1.00000  -7       0.00980     host node-3                               4   hdd 0.00980         osd.4           up  1.00000 1.00000</code></pre><h2><span id="五-修改class">五、修改CLASS</span></h2><pre><code>[root@node-1 my-cluster]# ceph -h | grep class #查看帮助文档osd crush class create &lt;class&gt;                                        create crush device class &lt;class&gt;osd crush class ls                                                    list all crush device classesosd crush class ls-osd &lt;class&gt;                                        list all osds belonging to the specific &lt;class&gt;osd crush class rename &lt;srcname&gt; &lt;dstname&gt;                            rename crush device class &lt;srcname&gt; to &lt;dstname&gt;osd crush class rm &lt;class&gt;                                            remove crush device class &lt;class&gt;osd crush get-device-class &lt;ids&gt; [&lt;ids&gt;...]                           get classes of specified osd(s) &lt;id&gt; [&lt;id&gt;...]osd crush rm-device-class &lt;ids&gt; [&lt;ids&gt;...]                            remove class of the osd(s) &lt;id&gt; [&lt;id&gt;...],or use &lt;all|any&gt; to remove osd crush rule create-replicated &lt;name&gt; &lt;root&gt; &lt;type&gt; &#123;&lt;class&gt;&#125;       create crush rule &lt;name&gt; for replicated pool to start from &lt;root&gt;,                                                                        &lt;class&gt; (ssd or hdd)osd crush rule ls-by-class &lt;class&gt;                                    list all crush rules that reference the same &lt;class&gt;osd crush set-device-class &lt;class&gt; &lt;ids&gt; [&lt;ids&gt;...]                   set the &lt;class&gt; of the osd(s) &lt;id&gt; [&lt;id&gt;...],or use &lt;all|any&gt; to set osd df &#123;plain|tree&#125; &#123;class|name&#125; &#123;&lt;filter&gt;&#125;                           show OSD utilization[root@node-1 my-cluster]# ceph osd crush set-device-class hdd osd.1 #提示需要删除原来的ssd类型才可以重新赋值osd.0 already set to class hdd. set-device-class item id 0 name &#39;osd.1&#39; device_class &#39;hdd&#39;: no change. [root@node-1 my-cluster]# ceph osd tree ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF  -9       0.02939 root ssd                                    -10       0.00980     host node-1-ssd                           1   hdd 0.00980         osd.1           up  1.00000 1.00000 -11       0.00980     host node-2-ssd                           3   hdd 0.00980         osd.3           up  1.00000 1.00000 -12       0.00980     host node-3-ssd                           5   hdd 0.00980         osd.5           up  1.00000 1.00000  -1       0.02939 root default                                 -3       0.00980     host node-1                               0   hdd 0.00980         osd.0           up  1.00000 1.00000  -5       0.00980     host node-2                               2   hdd 0.00980         osd.2           up  1.00000 1.00000  -7       0.00980     host node-3                               4   hdd 0.00980         osd.4           up  1.00000 1.00000 [root@node-1 my-cluster]# ceph osd crush rm-device-class osd.1 osd.3 osd.5done removing class of osd(s): 1,3,5[root@node-1 my-cluster]# ceph osd tree ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF  -9       0.02939 root ssd                                    -10       0.00980     host node-1-ssd                           1       0.00980         osd.1           up  1.00000 1.00000 -11       0.00980     host node-2-ssd                           3       0.00980         osd.3           up  1.00000 1.00000 -12       0.00980     host node-3-ssd                           5       0.00980         osd.5           up  1.00000 1.00000  -1       0.02939 root default                                 -3       0.00980     host node-1                               0   hdd 0.00980         osd.0           up  1.00000 1.00000  -5       0.00980     host node-2                               2   hdd 0.00980         osd.2           up  1.00000 1.00000  -7       0.00980     host node-3                               4   hdd 0.00980         osd.4           up  1.00000 1.00000 [root@node-1 my-cluster]# ceph osd crush set-device-class ssd osd.1set osd(s) 1 to class &#39;ssd&#39;[root@node-1 my-cluster]# ceph osd crush set-device-class ssd osd.3set osd(s) 3 to class &#39;ssd&#39;[root@node-1 my-cluster]# ceph osd crush set-device-class ssd osd.5set osd(s) 5 to class &#39;ssd&#39;[root@node-1 my-cluster]# ceph osd tree ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF  -9       0.02939 root ssd                                    -10       0.00980     host node-1-ssd                           1   ssd 0.00980         osd.1           up  1.00000 1.00000 -11       0.00980     host node-2-ssd                           3   ssd 0.00980         osd.3           up  1.00000 1.00000 -12       0.00980     host node-3-ssd                           5   ssd 0.00980         osd.5           up  1.00000 1.00000  -1       0.02939 root default                                 -3       0.00980     host node-1                               0   hdd 0.00980         osd.0           up  1.00000 1.00000  -5       0.00980     host node-2                               2   hdd 0.00980         osd.2           up  1.00000 1.00000  -7       0.00980     host node-3                               4   hdd 0.00980         osd.4           up  1.00000 1.00000</code></pre><h2><span id="六-创建规则绑定root">六、创建规则绑定root</span></h2><pre><code>[root@node-1 my-cluster]# ceph -h |grep rule osd crush rule create-erasure &lt;name&gt; &#123;&lt;profile&gt;&#125;                      create crush rule &lt;name&gt; for erasure coded pool created with osd crush rule create-replicated &lt;name&gt; &lt;root&gt; &lt;type&gt; &#123;&lt;class&gt;&#125;       create crush rule &lt;name&gt; for replicated pool to start from &lt;root&gt;, osd crush rule create-simple &lt;name&gt; &lt;root&gt; &lt;type&gt; &#123;firstn|indep&#125;      create crush rule &lt;name&gt; to start from &lt;root&gt;, replicate across osd crush rule dump &#123;&lt;name&gt;&#125;                                          dump crush rule &lt;name&gt; (default all)osd crush rule ls                                                     list crush rulesosd crush rule ls-by-class &lt;class&gt;                                    list all crush rules that reference the same &lt;class&gt;osd crush rule rename &lt;srcname&gt; &lt;dstname&gt;                             rename crush rule &lt;srcname&gt; to &lt;dstname&gt;osd crush rule rm &lt;name&gt;                                              remove crush rule &lt;name&gt; erasure&#125; &#123;&lt;erasure_code_profile&gt;&#125; &#123;&lt;rule&gt;&#125; &#123;&lt;int&gt;&#125; &#123;&lt;int&gt;&#125; &#123;&lt;int[0-  osd pool get &lt;poolname&gt; size|min_size|pg_num|pgp_num|crush_rule|      get pool parameter &lt;var&gt; crush_rule|hashpspool|nodelete|nopgchange|nosizechange|write_        [root@node-1 my-cluster]# ceph osd crush rule create-replicated #查看使用帮助Invalid command: missing required parameter name(&lt;string(goodchars [A-Za-z0-9-_.])&gt;)osd crush rule create-replicated &lt;name&gt; &lt;root&gt; &lt;type&gt; &#123;&lt;class&gt;&#125; :  create crush rule &lt;name&gt; for replicated pool to start from &lt;root&gt;, replicate across buckets of type &lt;type&gt;, use devices of type &lt;class&gt; (ssd or hdd)[root@node-1 my-cluster]# ceph osd crush rule create-replicated ssd-demo ssd host ssd #查看报错为没有ssd的类型[root@node-1 my-cluster]# ceph osd crush class ls #查看类型[    &quot;hdd&quot;][root@node-1 my-cluster]# ceph osd crush class create ssd #添加ssd的类型created class ssd with id 1 to crush map[root@node-1 my-cluster]# ceph osd crush class ls #添加成功[    &quot;hdd&quot;,    &quot;ssd&quot;][root@node-1 my-cluster]# ceph osd crush rule create-replicated ssd-demo ssd host ssd #查看报错信息，未找到原因，等待一段时间后重新执行就没问题了Error EINVAL: root ssd has no devices with class ssd[root@node-1 my-cluster]# ceph osd crush rule ls #查看创建成功replicated_rulessd-demo[root@node-1 my-cluster]# ceph osd crush rule dump #查看创建规则的详细信息[    &#123;        &quot;rule_id&quot;: 0,        &quot;rule_name&quot;: &quot;replicated_rule&quot;,        &quot;ruleset&quot;: 0,        &quot;type&quot;: 1,        &quot;min_size&quot;: 1,        &quot;max_size&quot;: 10,        &quot;steps&quot;: [            &#123;                &quot;op&quot;: &quot;take&quot;,                &quot;item&quot;: -1,                &quot;item_name&quot;: &quot;default&quot;            &#125;,            &#123;                &quot;op&quot;: &quot;chooseleaf_firstn&quot;,                &quot;num&quot;: 0,                &quot;type&quot;: &quot;host&quot;            &#125;,            &#123;                &quot;op&quot;: &quot;emit&quot;            &#125;        ]    &#125;,    &#123;        &quot;rule_id&quot;: 1,        &quot;rule_name&quot;: &quot;ssd-demo&quot;,        &quot;ruleset&quot;: 1,        &quot;type&quot;: 1,        &quot;min_size&quot;: 1,        &quot;max_size&quot;: 10,        &quot;steps&quot;: [            &#123;                &quot;op&quot;: &quot;take&quot;,                &quot;item&quot;: -20,                &quot;item_name&quot;: &quot;ssd~ssd&quot;            &#125;,            &#123;                &quot;op&quot;: &quot;chooseleaf_firstn&quot;,                &quot;num&quot;: 0,                &quot;type&quot;: &quot;host&quot;            &#125;,            &#123;                &quot;op&quot;: &quot;emit&quot;            &#125;        ]    &#125;]</code></pre><h2><span id="七-应用规则">七、应用规则</span></h2><pre><code>[root@node-1 my-cluster]# ceph osd pool set ceph-demo crush_rule ssd-demo  #应用到ceph-demo的pool中set pool 1 crush_rule to ssd-demo[root@node-1 my-cluster]# ceph osd pool get ceph-demo crush_rule #查看应用成功crush_rule: ssd-demo[root@node-1 my-cluster]# rbd -p ceph-demo ls 查看ceph-demo的块crush-demo.imgrbd-demo.img[root@node-1 my-cluster]# rbd create ceph-demo/crush-map.img --size 3G  #创建一个块[root@node-1 my-cluster]# ceph osd map ceph-demo crush-demo.img#查看块在OSD中的分布情况osdmap e95 pool &#39;ceph-demo&#39; (1) object &#39;crush-demo.img&#39; -&gt; pg 1.d267742c (1.2c) -&gt; up ([5,3,1], p5) acting ([5,3,1], p5)</code></pre><h1><span id="三-注意事项">三、注意事项</span></h1><ol><li>做扩容或删除OSD的时候尽可能保存Crush_map的bin文件</li><li>编辑Crush_map的bin文件之前也要备份</li><li>做Crush_map的时候尽量要在初始化的时候去做，不要在已经运行的Ceph集群中去做</li><li>做了Crush_map后一定要配置osd crush update on start &#x3D; false ，但是新增主机和OSD时需要手动配置Crush map</li></ol><blockquote><p>重启OSD后，Crush Map状态会发生转变，由于下面配置，导致OSD添加的时候方便，但是写了Crush Map后此功能需注意，如果集群中有OSD重启或者故障后，会导致大量PG进行迁移</p></blockquote><p><code>osd crush update on start = false </code></p><pre><code>[root@node-1 my-cluster]# ceph osd tree ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF  -9       0.02939 root ssd                                    -10       0.00980     host node-1-ssd                           1   ssd 0.00980         osd.1           up  1.00000 1.00000 -11       0.00980     host node-2-ssd                           3   ssd 0.00980         osd.3           up  1.00000 1.00000 -12       0.00980     host node-3-ssd                           5   ssd 0.00980         osd.5           up  1.00000 1.00000  -1       0.02939 root default                                 -3       0.00980     host node-1                               0   hdd 0.00980         osd.0           up  1.00000 1.00000  -5       0.00980     host node-2                               2   hdd 0.00980         osd.2           up  1.00000 1.00000  -7       0.00980     host node-3                               4   hdd 0.00980         osd.4           up  1.00000 1.00000 [root@node-1 my-cluster]# systemctl restart ceph-osd@1[root@node-1 my-cluster]# ceph osd tree ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF  -9       0.01959 root ssd                                    -10             0     host node-1-ssd                         -11       0.00980     host node-2-ssd                           3   ssd 0.00980         osd.3           up  1.00000 1.00000 -12       0.00980     host node-3-ssd                           5   ssd 0.00980         osd.5           up  1.00000 1.00000  -1       0.03918 root default                                 -3       0.01959     host node-1                               0   hdd 0.00980         osd.0           up  1.00000 1.00000   1   ssd 0.00980         osd.1           up  1.00000 1.00000  -5       0.00980     host node-2                               2   hdd 0.00980         osd.2           up  1.00000 1.00000  -7       0.00980     host node-3                               4   hdd 0.00980         osd.4           up  1.00000 1.00000 [root@node-1 my-cluster]# pwd/opt/my-cluster[root@node-1 my-cluster]# vim ceph.conf [global]fsid = b8e58b30-4568-4032-a9f4-837ed3fa9529public_network = 192.168.187.0/24cluster_network = 192.168.199.0/24mon_initial_members = node-1mon_host = 192.168.187.201auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxmon_allow_pool_delete = true #可手动删除pool[osd]osd crush update on start = false #加入这个参数                                                                                                                           &quot;ceph.conf&quot; 14L, 360C written                                                                                            [root@node-1 my-cluster]# ceph-deploy --overwrite-conf  config push node-1 node-2 node-3 #推送到其他节点[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy --overwrite-conf config push node-1 node-2 node-3[ceph_deploy.cli][INFO  ] ceph-deploy options:[ceph_deploy.cli][INFO  ]  username                      : None[ceph_deploy.cli][INFO  ]  verbose                       : False[ceph_deploy.cli][INFO  ]  overwrite_conf                : True[ceph_deploy.cli][INFO  ]  subcommand                    : push[ceph_deploy.cli][INFO  ]  quiet                         : False[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f5ee8345878&gt;[ceph_deploy.cli][INFO  ]  cluster                       : ceph[ceph_deploy.cli][INFO  ]  client                        : [&#39;node-1&#39;, &#39;node-2&#39;, &#39;node-3&#39;][ceph_deploy.cli][INFO  ]  func                          : &lt;function config at 0x7f5ee7eb5938&gt;[ceph_deploy.cli][INFO  ]  ceph_conf                     : None[ceph_deploy.cli][INFO  ]  default_release               : False[ceph_deploy.config][DEBUG ] Pushing config to node-1[node-1][DEBUG ] connected to host: node-1 [node-1][DEBUG ] detect platform information from remote host[node-1][DEBUG ] detect machine type[node-1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph_deploy.config][DEBUG ] Pushing config to node-2[node-2][DEBUG ] connected to host: node-2 [node-2][DEBUG ] detect platform information from remote host[node-2][DEBUG ] detect machine type[node-2][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph_deploy.config][DEBUG ] Pushing config to node-3[node-3][DEBUG ] connected to host: node-3 [node-3][DEBUG ] detect platform information from remote host[node-3][DEBUG ] detect machine type[node-3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[root@node-1 my-cluster]# systemctl restart ceph-osd.target  #重启node1的所有osd[root@node-1 my-cluster]# ssh node-2 systemctl restart ceph-osd.target #重启node2的所有osd[root@node-1 my-cluster]# ssh node-3 systemctl restart ceph-osd.target #重启node3的所有osd[root@node-1 my-cluster]# ceph osd crush move osd.1 host=node-1-ssd #需要将刚刚测试的osd.1手动放到node-1-ssd下moved item id 1 name &#39;osd.1&#39; to location &#123;host=node-1-ssd&#125; in crush map[root@node-1 my-cluster]# ceph osd tree ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF  -9       0.02939 root ssd                                    -10       0.00980     host node-1-ssd                           1   ssd 0.00980         osd.1           up  1.00000 1.00000 -11       0.00980     host node-2-ssd                           3   ssd 0.00980         osd.3           up  1.00000 1.00000 -12       0.00980     host node-3-ssd                           5   ssd 0.00980         osd.5           up  1.00000 1.00000  -1       0.02939 root default                                 -3       0.00980     host node-1                               0   hdd 0.00980         osd.0           up  1.00000 1.00000  -5       0.00980     host node-2                               2   hdd 0.00980         osd.2           up  1.00000 1.00000  -7       0.00980     host node-3                               4   hdd 0.00980         osd.4           up  1.00000 1.00000 [root@node-1 my-cluster]# ceph osd tree #查看当前状态ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF  -9       0.02939 root ssd                                    -10       0.00980     host node-1-ssd                           1   ssd 0.00980         osd.1           up  1.00000 1.00000 -11       0.00980     host node-2-ssd                           3   ssd 0.00980         osd.3           up  1.00000 1.00000 -12       0.00980     host node-3-ssd                           5   ssd 0.00980         osd.5           up  1.00000 1.00000  -1       0.02939 root default                                 -3       0.00980     host node-1                               0   hdd 0.00980         osd.0           up  1.00000 1.00000  -5       0.00980     host node-2                               2   hdd 0.00980         osd.2           up  1.00000 1.00000  -7       0.00980     host node-3                               4   hdd 0.00980         osd.4           up  1.00000 1.00000 [root@node-1 my-cluster]# systemctl restart ceph-osd@1 #再次重启，校验配置文件是否生效Job for ceph-osd@1.service failed because start of the service was attempted too often. See &quot;systemctl status ceph-osd@1.service&quot; and &quot;journalctl -xe&quot; for details.To force a start use &quot;systemctl reset-failed ceph-osd@1.service&quot; followed by &quot;systemctl start ceph-osd@1.service&quot; again.[root@node-1 my-cluster]# systemctl reset-failed ceph-osd@1.service #执行reset命令[root@node-1 my-cluster]# systemctl restart ceph-osd@1 #再次重启[root@node-1 my-cluster]# ceph osd tree #重启后osd没有出现漂移的问题ID  CLASS WEIGHT  TYPE NAME           STATUS REWEIGHT PRI-AFF  -9       0.02939 root ssd                                    -10       0.00980     host node-1-ssd                           1   ssd 0.00980         osd.1           up  1.00000 1.00000 -11       0.00980     host node-2-ssd                           3   ssd 0.00980         osd.3           up  1.00000 1.00000 -12       0.00980     host node-3-ssd                           5   ssd 0.00980         osd.5           up  1.00000 1.00000  -1       0.02939 root default                                 -3       0.00980     host node-1                               0   hdd 0.00980         osd.0           up  1.00000 1.00000  -5       0.00980     host node-2                               2   hdd 0.00980         osd.2           up  1.00000 1.00000  -7       0.00980     host node-3                               4   hdd 0.00980         osd.4           up  1.00000 1.00000 [root@node-1 my-cluster]# ceph daemon /var/run/ceph/ceph-osd.1.asok config show | grep &quot;osd_crush_update_on_start&quot; #可以看到配置已经生效    &quot;osd_crush_update_on_start&quot;: &quot;false&quot;,如果重启OSD报错可尝试下列命令systemctl reset-failed ceph-osd@1.servicesystemctl restart ceph-osd@1.service</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> CrushMap </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph与OpenStack对接</title>
      <link href="/2023/04/16/Ceph/15,Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/Ceph%E4%B8%8EOpenStack/"/>
      <url>/2023/04/16/Ceph/15,Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/Ceph%E4%B8%8EOpenStack/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#1-openstack-%E7%9A%84%E4%B8%89%E4%B8%AA%E9%83%A8%E5%88%86%E4%B8%8E-ceph-%E7%9A%84%E5%9D%97%E8%AE%BE%E5%A4%87%E9%9B%86%E6%88%90">1、OpenStack 的三个部分与 Ceph 的块设备集成：</a></li><li><a href="#2-%E5%88%9B%E5%BB%BA%E6%B1%A0">2、创建池</a></li></ul><ul><li><a href="#%E4%BA%8C-%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2openstack">二、一键部署Openstack</a><ul><li><a href="#1-%E6%9F%A5%E7%9C%8Boepnstack%E7%89%88%E6%9C%AC">1、查看Oepnstack版本</a></li><li><a href="#2-%E5%AE%89%E8%A3%85">2、安装</a></li><li><a href="#3-%E8%A7%A3%E5%86%B3%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98">3、解决安装过程中遇到的问题</a></li></ul></li><li><a href="#%E4%B8%89-openstack%E5%AF%B9%E6%8E%A5%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87">三、Openstack对接环境准备</a><ul><li><a href="#1-%E5%88%9B%E5%BB%BA%E6%B1%A0">1、创建池</a></li><li><a href="#2-%E9%85%8D%E7%BD%AE-openstack-ceph-%E5%AE%A2%E6%88%B7%E7%AB%AF">2、配置 OPENSTACK CEPH 客户端</a><ul><li><a href="#21%E5%AE%89%E8%A3%85-ceph-%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8C%85">2.1安装 CEPH 客户端包</a></li><li><a href="#22%E8%AE%BE%E7%BD%AE-ceph-%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BA%AB%E4%BB%BD%E9%AA%8C%E8%AF%81">2.2设置 CEPH 客户端身份验证</a></li></ul></li></ul></li><li><a href="#%E5%9B%9B-glance%E5%92%8Cceph%E5%AF%B9%E6%8E%A5">四、Glance和Ceph对接</a><ul><li><a href="#1-%E9%85%8D%E7%BD%AE%E6%A6%82%E8%A7%88">1、配置概览</a></li><li><a href="#2-junoprior-to-juno">2、JUNOPRIOR TO JUNO</a></li><li><a href="#3-juno">3、JUNO</a></li><li><a href="#4-kilo-and-after">4、KILO AND AFTER</a></li><li><a href="#5-%E5%90%AF%E7%94%A8%E5%9B%BE%E5%83%8F%E7%9A%84%E5%86%99%E6%97%B6%E5%A4%8D%E5%88%B6%E5%85%8B%E9%9A%86">5、启用图像的写时复制克隆</a></li><li><a href="#6-%E7%A6%81%E7%94%A8%E7%BC%93%E5%AD%98%E7%AE%A1%E7%90%86%E4%BB%BB%E4%BD%95-openstack-%E7%89%88%E6%9C%AC">6、禁用缓存管理（任何 OPENSTACK 版本）</a></li><li><a href="#7-%E5%9B%BE%E5%83%8F%E5%B1%9E%E6%80%A7">7、图像属性</a></li><li><a href="#8-%E5%AE%9E%E8%B7%B5">8、实践</a></li></ul></li><li><a href="#%E4%BA%94-glance%E5%AF%B9%E6%8E%A5%E5%8A%9F%E8%83%BD%E6%B5%8B%E8%AF%95">五、Glance对接功能测试</a></li><li><a href="#%E5%85%AD-cinder%E4%B8%8Eceph%E5%AF%B9%E6%8E%A5">六、Cinder与Ceph对接</a></li><li><a href="#%E4%B8%83-cinder%E5%AF%B9%E6%8E%A5%E5%8A%9F%E8%83%BD%E6%B5%8B%E8%AF%95">七、Cinder对接功能测试</a><ul><li><a href="#1-%E6%89%BE%E5%88%B0%E5%8D%B7">1、找到卷</a></li><li><a href="#2-%E5%88%9B%E5%BB%BA%E5%8D%B7">2、创建卷</a></li><li><a href="#3-%E6%89%A9%E5%B1%95%E5%8D%B7">3、扩展卷</a></li></ul></li><li><a href="#%E5%85%AB-cinder-bakup%E5%AF%B9%E6%8E%A5%E5%92%8C%E6%B5%8B%E8%AF%95">八、Cinder-bakup对接和测试</a><ul><li><a href="#1-%E9%85%8D%E7%BD%AE-cinder-%E5%A4%87%E4%BB%BD">1、配置 CINDER 备份</a></li><li><a href="#2-%E5%AE%9E%E8%B7%B5">2、实践</a></li><li><a href="#3-%E6%B5%8B%E8%AF%95">3、测试</a></li></ul></li><li><a href="#%E4%B9%9D-nova%E5%92%8Cceph%E5%AF%B9%E6%8E%A5">九、Nova和Ceph对接</a><ul><li><a href="#1-%E9%85%8D%E7%BD%AE-nova-%E9%99%84%E5%8A%A0-ceph-rbd-%E5%9D%97%E8%AE%BE%E5%A4%87">1、配置 NOVA 附加 CEPH RBD 块设备</a></li></ul></li><li><a href="#2-%E9%85%8D%E7%BD%AE-nova">2、配置 NOVA</a><ul><li><a href="#21-havana-and-icehouse">2.1、HAVANA AND ICEHOUSE</a></li><li><a href="#22-juno">2.2、JUNO</a></li><li><a href="#33-kilo">3.3、KILO</a></li></ul><ul><li><a href="#3-%E5%AE%9E%E8%B7%B5">3、实践</a></li></ul></li><li><a href="#%E5%8D%81-nova%E5%92%8Cceph%E5%AF%B9%E6%8E%A5%E6%B5%8B%E8%AF%95">十、Nova和Ceph对接测试</a></li><li><a href="#%E5%8D%81%E4%B8%80-%E9%87%8D%E5%90%AF-openstack">十一、重启 OPENSTACK</a></li><li><a href="#%E5%8D%81%E4%BA%8C-%E4%BB%8E%E5%9D%97%E8%AE%BE%E5%A4%87%E5%90%AF%E5%8A%A8">十二、从块设备启动</a></li></ul><!-- tocstop --><p>一、Ceph与Openstack对接概述</p><p>您可以通过 OpenStack 使用 Ceph 块设备映像libvirt，它将 QEMU 接口配置为librbd. Ceph 将块设备映像作为对象跨集群进行条带化，这意味着大型 Ceph 块设备映像比独立服务器具有更好的性能！</p><p>要将 Ceph 块设备与 OpenStack 一起使用，您必须先安装 QEMU、libvirt和 OpenStack。我们建议为您的 OpenStack 安装使用单独的物理节点。</p><p>OpenStack 推荐至少 8GB 内存和四核处理器。下图描述了 OpenStack&#x2F;Ceph 技术栈。</p><p><img src="/images/Ceph/Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/1.jpg"></p><h2><span id="1-openstack-的三个部分与-ceph-的块设备集成">1、OpenStack 的三个部分与 Ceph 的块设备集成：</span></h2><ul><li>图像：OpenStack Glance 管理 VM 的图像。图像是不可变的。OpenStack 将图像视为二进制 blob 并相应地下载它们。</li><li>卷：卷是块设备。OpenStack 使用卷来引导 VM，或将卷附加到正在运行的 VM。OpenStack 使用 Cinder 服务管理卷。</li><li>来宾磁盘：来宾磁盘是来宾操作系统磁盘。默认情况下，当您启动虚拟机时，其磁盘在管理程序的文件系统中显示为一个文件（通常在 下&#x2F;var&#x2F;lib&#x2F;nova&#x2F;instances&#x2F;<uuid>&#x2F;）。在 OpenStack Havana 之前，在 Ceph 中启动 VM 的唯一方法是使用 Cinder 的从卷启动功能。但是，现在可以在不使用 Cinder 的情况下直接启动 Ceph 中的每个虚拟机，这是有利的，因为它允许您通过实时迁移过程轻松执行维护操作。此外，如果您的管理程序死机，也可以方便地几乎无缝地在其他地方触发和运行虚拟机。这样做时， 独占锁会阻止多个计算节点同时访问来宾磁盘。nova evacuate</uuid></li></ul><blockquote><p>您可以使用 OpenStack Glance 将映像存储在 Ceph 块设备中，并且可以使用 Cinder 通过映像的写时复制克隆来启动 VM。</p></blockquote><blockquote><p>下面的说明详细介绍了 Glance、Cinder 和 Nova 的设置，尽管它们不必一起使用。您可以在使用本地磁盘运行 VM 时将图像存储在 Ceph 块设备中，反之亦然。</p></blockquote><blockquote><p>不建议使用 QCOW2 托管虚拟机磁盘。如果你想在 Ceph 中启动虚拟机（临时后端或从卷启动），请使用rawGlance 中的图像格式。</p></blockquote><h2><span id="2-创建池">2、创建池</span></h2><p>** 默认情况下，Ceph 块设备使用rbd池。您可以使用任何可用的池。我们建议为 Cinder 创建一个池，为 Glance 创建一个池。确保您的 Ceph 集群正在运行，然后创建池。**</p><pre><code>[root@node-1 ~]#  ceph osd pool create volumes   16 16  #存放卷[root@node-1 ~]#  ceph osd pool create images    16 16  #存放镜像[root@node-1 ~]#  ceph osd pool create backups   16 16  #存放卷的备份[root@node-1 ~]#  ceph osd pool create vms       16 16  #存放虚拟机</code></pre><p><strong>新创建的池必须在使用前进行初始化。使用该rbd工具初始化池：</strong></p><pre><code>[root@node-1 ~]#  rbd pool init volumes[root@node-1 ~]#  rbd pool init images[root@node-1 ~]#  rbd pool init backups[root@node-1 ~]#  rbd pool init vms</code></pre><h1><span id="二-一键部署openstack">二、一键部署Openstack</span></h1><blockquote><p>使用Packstack一键部署：<a href="https://wiki.openstack.org/wiki/Packstack">https://wiki.openstack.org/wiki/Packstack</a><br>新主机需要初始化及安装ceph-common的软件包</p></blockquote><h2><span id="1-查看oepnstack版本">1、查看Oepnstack版本</span></h2><pre><code>[root@node-1 ~]# yum list | grep openstackRepository docker-ce-stable is listed more than once in the configurationansible-openstack-modules.noarch         0-20140902git79d751a.el7      epel     centos-release-openstack-queens.noarch   1-2.el7.centos                extras   centos-release-openstack-rocky.noarch    1-1.el7.centos                extras   centos-release-openstack-stein.noarch    1-1.el7.centos                extras   centos-release-openstack-train.noarch    1-1.el7.centos                extras   resalloc-openstack.noarch                9.5-1.el7                     epel     [root@node-1 ~]# yum info centos-release-openstack-train.noarch已加载插件：fastestmirror, langpacksRepository docker-ce-stable is listed more than once in the configurationLoading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.aliyun.com可安装的软件包名称    ：centos-release-openstack-train架构    ：noarch版本    ：1发布    ：1.el7.centos大小    ：5.3 k源    ：extras/7/x86_64简介    ： OpenStack from the CentOS Cloud SIG repo configs网址    ：http://wiki.centos.org/SpecialInterestGroup/Cloud协议    ： GPL描述    ： yum Configs and basic docs for OpenStack as delivered via the CentOS Cloud SIG.</code></pre><h2><span id="2-安装">2、安装</span></h2><pre><code>[root@node-1 ~]# yum install  centos-release-openstack-train.noarch -y #安装Openstack[root@node-1 yum.repos.d]# ls -lha /etc/yum.repos.d/*OpenStack*-rw-r--r-- 1 root root 1.3K 10月 23 2019 /etc/yum.repos.d/CentOS-OpenStack-train.repo[root@node-1 yum.repos.d]# rpm -ql centos-release-openstack-train /etc/pki/rpm-gpg/etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-SIG-Cloud/etc/yum.repos.d/CentOS-OpenStack-train.repo[root@node-1 yum.repos.d]# yum install openstack-packstack -y #安装自动化部署工具[root@node-1 yum.repos.d]# packstack --gen-answer-file train-for-ceph.txt #生成应答文件Additional information: * Parameter CONFIG_NEUTRON_L2_AGENT: You have chosen OVN Neutron backend. Note that this backend does not support the VPNaaS or FWaaS services. Geneve will be used as the encapsulation method for tenant networks[root@node-1 yum.repos.d]# vim train-for-ceph.txt #修改应答文件中的密码，可不修改，后续在此文档查看即可；CONFIG_KEYSTONE_ADMIN_PW=password[root@node-1 yum.repos.d]# packstack --answer-file train-for-ceph.txt #开始安装[root@localhost ~]# cat train-for-ceph.txt | grep ADMIN_PW #获取登录的密码，用户名为adminCONFIG_KEYSTONE_ADMIN_PW=78d34f707f0745ad</code></pre><p><strong>测试成功访问</strong></p><p><img src="/images/Ceph/Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/2.jpg"></p><h2><span id="3-解决安装过程中遇到的问题">3、解决安装过程中遇到的问题</span></h2><pre><code>[root@node-1 yum.repos.d]# yum list |grep leathermaRepository docker-ce-stable is listed more than once in the configurationleatherman.x86_64                        1.10.0-1.el7                  @epel    leatherman-devel.x86_64                  1.10.0-1.el7                  epel     [root@node-1 yum.repos.d]#  yum downgrade leatherman #回退版本[root@node-1 yum.repos.d]# facter -p|more #测试正常2023-03-19 01:51:32.656090 WARN  puppetlabs.facter - skipping external facts for &quot;/var/lib/puppet/facts.d&quot;: No such file or directorydisks =&gt; &#123;  sda =&gt; &#123;    model =&gt; &quot;VMware Virtual S&quot;,    size =&gt; &quot;20.00 GiB&quot;,    size_bytes =&gt; 21474836480,    vendor =&gt; &quot;VMware,&quot;  &#125;,  sdb =&gt; &#123;    model =&gt; &quot;VMware Virtual S&quot;,    size =&gt; &quot;10.00 GiB&quot;,    size_bytes =&gt; 10737418240,    vendor =&gt; &quot;VMware,&quot;  &#125;,</code></pre><p><img src="/images/Ceph/Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/3.jpg"></p><h1><span id="三-openstack对接环境准备">三、Openstack对接环境准备</span></h1><p><a href="https://docs.ceph.com/en/nautilus/rbd/rbd-openstack/">官方文档</a></p><p>您可以通过 OpenStack 使用 Ceph 块设备映像libvirt，它将 QEMU 接口配置为librbd. Ceph 将块设备映像作为对象跨集群进行条带化，这意味着大型 Ceph 块设备映像比独立服务器具有更好的性能！</p><p>要将 Ceph 块设备与 OpenStack 一起使用，您必须先安装 QEMU、libvirt和 OpenStack。我们建议为您的 OpenStack 安装使用单独的物理节点。OpenStack 推荐至少 8GB 内存和四核处理器。下图描述了 OpenStack&#x2F;Ceph 技术栈。</p><blockquote><p>要将 Ceph 块设备与 OpenStack 一起使用，您必须有权访问正在运行的 Ceph 存储集群。</p></blockquote><p><img src="/images/Ceph/Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/4.jpg"></p><p>OpenStack 的三个部分与 Ceph 的块设备集成：</p><ul><li>图像：OpenStack Glance 管理 VM 的图像。图像是不可变的。OpenStack 将图像视为二进制 blob 并相应地下载它们。</li><li>卷：卷是块设备。OpenStack 使用卷来引导 VM，或将卷附加到正在运行的 VM。OpenStack 使用 Cinder 服务管理卷。</li><li>来宾磁盘：来宾磁盘是来宾操作系统磁盘。默认情况下，当您启动虚拟机时，其磁盘在管理程序的文件系统中显示为一个文件（通常在 下&#x2F;var&#x2F;lib&#x2F;nova&#x2F;instances&#x2F;<uuid>&#x2F;）。在 OpenStack Havana 之前，在 Ceph 中启动 VM 的唯一方法是使用 Cinder 的从卷启动功能。但是，现在可以在不使用 Cinder 的情况下直接启动 Ceph 中的每个虚拟机，这是有利的，因为它允许您通过实时迁移过程轻松执行维护操作。此外，如果您的管理程序死机，也可以方便地几乎无缝地在其他地方触发和运行虚拟机。nova evacuate</uuid></li></ul><p><strong>您可以使用 OpenStack Glance 将映像存储在 Ceph 块设备中，并且可以使用 Cinder 通过映像的写时复制克隆来启动 VM。</strong></p><p><strong>下面的说明详细介绍了 Glance、Cinder 和 Nova 的设置，尽管它们不必一起使用。您可以在使用本地磁盘运行 VM 时将图像存储在 Ceph 块设备中，反之亦然。</strong></p><blockquote><p>Ceph 不支持 QCOW2 来托管虚拟机磁盘。因此，如果你想在 Ceph 中启动虚拟机（临时后端或从卷启动），Glance 映像格式必须是RAW.</p></blockquote><h2><span id="1-创建池">1、创建池</span></h2><pre><code>ceph节点：[root@node-1 ~]# ceph osd pool create volumes 16 #创建volumes的pool，后期存放卷[root@node-1 ~]# ceph osd pool create images 16 #创建images的pool,后期存放镜像[root@node-1 ~]# ceph osd pool create backups 16 #创建bakups的pool，后期存放卷的备份[root@node-1 ~]# ceph osd pool create vms 16 #创建vms的pool，后期存放虚拟机系统[root@node-1 ~]# rbd pool init volumes #初始化池[root@node-1 ~]# rbd pool init images #初始化池[root@node-1 ~]# rbd pool init backups #初始化池[root@node-1 ~]# rbd pool init vms #初始化池</code></pre><h2><span id="2-配置-openstack-ceph-客户端">2、配置 OPENSTACK CEPH 客户端</span></h2><p>glance-api运行、cinder-volume和nova-compute的 节点cinder-backup充当 Ceph 客户端。每个都需要ceph.conf文件：</p><pre><code>ceph节点：[root@node-1 ~]# ssh &#123;your-openstack-server&#125; sudo tee /etc/ceph/ceph.conf &lt;/etc/ceph/ceph.conf #将Ceph的配置文件复制到Openstack节点中</code></pre><h3><span id="21安装-ceph-客户端包">2.1安装 CEPH 客户端包</span></h3><p>在glance-api节点上，您将需要 Python 绑定librbd：</p><pre><code>Openstack节点：[root@controller ~]# sudo apt-get install python-rbd #Ubuntu在Glance服务器中安装必要的软件包[root@controller ~]# sudo yum install python-rbd #Centos在Glance服务器中安装必要的软件包</code></pre><p>在nova-compute和cinder-backup节点上cinder-volume，同时使用 Python 绑定和客户端命令行工具：</p><pre><code>[root@controller ~]# sudo apt-get install ceph-common #Ubuntu在Nova和Cinder服务器中安装必要的软件包[root@controller ~]# sudo yum install ceph-common #Centos在Nova和Cinder服务器中安装必要的软件包</code></pre><h3><span id="22设置-ceph-客户端身份验证">2.2设置 CEPH 客户端身份验证</span></h3><p>如果启用了cephx 身份验证，请为 Nova&#x2F;Cinder 和 Glance 创建一个新用户。执行以下操作：</p><pre><code>Ceph节点：[root@node-1 ~]# ceph auth get-or-create client.glance mon &#39;profile rbd&#39; osd &#39;profile rbd pool=images&#39; mgr &#39;profile rbd pool=images&#39; #在Ceph集群中创建glance的用户，后期与OpenStack对接使用，并授权访问images的池[root@node-1 ~]# ceph auth get-or-create client.cinder mon &#39;profile rbd&#39; osd &#39;profile rbd pool=volumes, profile rbd pool=vms, profile rbd-read-only pool=images&#39; mgr &#39;profile rbd pool=volumes, profile rbd pool=vms&#39; #在Ceph集群中创建cinder的用户，后期与OpenStack对接使用，并授权访问images、vms、volumes的池[root@node-1 ~]# ceph auth get-or-create client.cinder-backup mon &#39;profile rbd&#39; osd &#39;profile rbd pool=backups&#39; mgr &#39;profile rbd pool=backups&#39; #在Ceph集群中创建cinder=backup的用户，后期与OpenStack对接使用，并授权访问backups的池</code></pre><p>client.cinder将、client.glance和 的密钥环添加client.cinder-backup到适当的节点并更改其所有权：</p><pre><code>Ceph节点：[root@node-1 ~]# ceph auth get-or-create client.glance | ssh &#123;your-glance-api-server&#125; sudo tee /etc/ceph/ceph.client.glance.keyring #将创建的glance用户的密钥传到Openstack的Glance服务器中[root@node-1 ~]# ssh &#123;your-glance-api-server&#125; sudo chown glance:glance /etc/ceph/ceph.client.glance.keyring #修改权限[root@node-1 ~]# ceph auth get-or-create client.cinder | ssh &#123;your-volume-server&#125; sudo tee /etc/ceph/ceph.client.cinder.keyring #将创建的cinnder用户的密钥传到Openstack的Cinder服务器中[root@node-1 ~]# ssh &#123;your-cinder-volume-server&#125; sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring #修改权限[root@node-1 ~]# ceph auth get-or-create client.cinder-backup | ssh &#123;your-cinder-backup-server&#125; sudo tee /etc/ceph/ceph.client.cinder-backup.keyring #将创建的backup用户的密钥传到Openstack的Cinder-backup服务器中[root@node-1 ~]# ssh &#123;your-cinder-backup-server&#125; sudo chown cinder:cinder /etc/ceph/ceph.client.cinder-backup.keyring #修改权限Openstack节点：[root@controller ceph]# pwd #在Openstack中查看文件是否接受，并查看相关权限/etc/ceph[root@controller ceph]# ls -lha total 32Kdrwxr-xr-x    2 root   root    146 Mar 19 23:28 .drwxr-xr-x. 118 root   root   8.0K Mar 19 22:23 ..-rw-r--r--    1 cinder cinder   71 Mar 19 23:28 ceph.client.cinder-backup.keyring-rw-r--r--    1 cinder cinder   64 Mar 19 22:23 ceph.client.cinder.keyring-rw-r--r--    1 glance glance   64 Mar 19 23:27 ceph.client.glance.keyring-rw-r--r--    1 root   root    472 Mar 19 22:18 ceph.conf-rw-r--r--    1 root   root     92 Apr 21  2021 rbdmap</code></pre><p>运行的节点nova-compute需要进程的密钥环文件nova-compute ：</p><pre><code>Ceph节点：[root@node-1 ~]# ceph auth get-or-create client.cinder | ssh &#123;your-nova-compute-server&#125; sudo tee /etc/ceph/ceph.client.cinder.keyring #将创建的cinder用户的密钥传到Openstack的Nova-Server服务器中</code></pre><p>他们还需要将用户的密钥存储client.cinder在 libvirt. libvirt 进程需要它来访问集群，同时从 Cinder 附加块设备。<br>在运行的节点上创建密钥的临时副本 nova-compute：</p><pre><code>Ceph节点：[root@node-1 ~]# ceph auth get-key client.cinder | ssh &#123;your-compute-node&#125; tee client.cinder.key #将创建的cinder用户的密钥传到Openstack的Nova-node服务器中</code></pre><p>然后，在计算节点上，将密钥添加到libvirt并删除密钥的临时副本：</p><pre><code>Openstack:[root@controller ceph]# uuidgen #随机生成UUID617ac927-1a98-444f-b3b4-1f682d7bfd7a[root@controller ceph]# cat &gt; secret.xml &lt;&lt;EOF #将UUID写入到这个文件中&lt;secret ephemeral=&#39;no&#39; private=&#39;no&#39;&gt;  &lt;uuid&gt;617ac927-1a98-444f-b3b4-1f682d7bfd7a&lt;/uuid&gt;  &lt;usage type=&#39;ceph&#39;&gt;    &lt;name&gt;client.cinder secret&lt;/name&gt;  &lt;/usage&gt;&lt;/secret&gt;EOF[root@controller ceph]# sudo virsh secret-define --file secret.xml #导入密钥Secret 617ac927-1a98-444f-b3b4-1f682d7bfd7a created[root@controller ceph]# cat ceph.client.cinder.keyring  #查看cinder用户的密钥[client.cinder]    key = AQCY4Bdkt7hdBRAAJNz56pz79U+PoTHueqMofA==[root@controller ceph]# cat client.cinder.key #创建文件，并将Cinder的密钥放在里面AQCY4Bdkt7hdBRAAJNz56pz79U+PoTHueqMofA==[root@controller ceph]# sudo virsh secret-set-value --secret 617ac927-1a98-444f-b3b4-1f682d7bfd7a --base64 $(cat client.cinder.key) &amp;&amp; rm client.cinder.key secret.xml #配置认证[root@controller ceph]# virsh secret-list  #查看认证 UUID                                  Usage-------------------------------------------------------------------------------- 617ac927-1a98-444f-b3b4-1f682d7bfd7a  ceph client.cinder secret[root@controller ceph]# virsh secret-get-value 617ac927-1a98-444f-b3b4-1f682d7bfd7a  #查看认证AQCY4Bdkt7hdBRAAJNz56pz79U+PoTHueqMofA==</code></pre><p>保存 secret 的 uuid 以备nova-compute后用。</p><h1><span id="四-glance和ceph对接">四、Glance和Ceph对接</span></h1><h2><span id="1-配置概览">1、配置概览</span></h2><p>Glance 可以使用多个后端来存储图像。要默认使用 Ceph 块设备，请像下面这样配置 Glance。</p><h2><span id="2-junoprior-to-juno">2、JUNOPRIOR TO JUNO</span></h2><p>编辑&#x2F;etc&#x2F;glance&#x2F;glance-api.conf并在该部分下添加[DEFAULT]：</p><pre><code>default_store = rbdrbd_store_user = glancerbd_store_pool = imagesrbd_store_chunk_size = 8</code></pre><h2><span id="3-juno">3、JUNO</span></h2><p>编辑&#x2F;etc&#x2F;glance&#x2F;glance-api.conf并在该部分下添加[glance_store]：</p><pre><code>[DEFAULT]...default_store = rbd...[glance_store]stores = rbdrbd_store_pool = imagesrbd_store_user = glancerbd_store_ceph_conf = /etc/ceph/ceph.confrbd_store_chunk_size = 8</code></pre><p>Glance 还没有完全移动到“存储”。所以我们仍然需要在 DEFAULT 部分配置存储，直到 Kilo。</p><h2><span id="4-kilo-and-after">4、KILO AND AFTER</span></h2><p>编辑&#x2F;etc&#x2F;glance&#x2F;glance-api.conf并在该部分下添加[glance_store]：</p><pre><code>[glance_store]stores = rbddefault_store = rbdrbd_store_pool = imagesrbd_store_user = glancerbd_store_ceph_conf = /etc/ceph/ceph.confrbd_store_chunk_size = 8</code></pre><p>有关 Glance 中可用配置选项的更多信息，请参阅 OpenStack 配置参考： http: &#x2F;&#x2F;docs.openstack.org&#x2F;。</p><h2><span id="5-启用图像的写时复制克隆">5、启用图像的写时复制克隆</span></h2><p>请注意，这会通过 Glance 的 API 公开后端位置，因此启用此选项的端点不应公开访问。<br>除 MITAKA 之外的任何 OPENSTACK 版本<br>如果要启用图像的写时复制克隆，还要在该[DEFAULT]部分下添加：</p><pre><code>show_image_direct_url = True</code></pre><p>仅限三鹰<br>要启用图像位置并利用图像的写时复制克隆，请在该[DEFAULT]部分下添加：</p><pre><code>show_multiple_locations = Trueshow_image_direct_url = True</code></pre><h2><span id="6-禁用缓存管理任何-openstack-版本">6、禁用缓存管理（任何 OPENSTACK 版本）</span></h2><p>禁用 Glance 缓存管理以避免图像缓存在 下&#x2F;var&#x2F;lib&#x2F;glance&#x2F;image-cache&#x2F;，假设您的配置文件具有：flavor &#x3D; keystone+cachemanagement</p><pre><code>[paste_deploy]flavor = keystone</code></pre><h2><span id="7-图像属性">7、图像属性</span></h2><p>我们建议为您的图像使用以下属性：</p><ul><li>hw_scsi_model&#x3D;virtio-scsi: 添加 virtio-scsi 控制器并获得更好的性能和对丢弃操作的支持</li><li>hw_disk_bus&#x3D;scsi：将每个煤渣块设备连接到该控制器</li><li>hw_qemu_guest_agent&#x3D;yes: 启用 QEMU 来宾代理</li><li>os_require_quiesce&#x3D;yes: 通过 QEMU 来宾代理发送 fs-freeze&#x2F;thaw 调用</li></ul><h2><span id="8-实践">8、实践</span></h2><pre><code>[root@controller ceph]# cat  /etc/glance/glance-api.conf | grep -v  &quot;#&quot; | grep -v &quot;^$&quot;[DEFAULT]bind_host=0.0.0.0bind_port=9292workers=4image_cache_dir=/var/lib/glance/image-cacheregistry_host=0.0.0.0debug=Falselog_file=/var/log/glance/api.loglog_dir=/var/log/glancetransport_url=rabbit://guest:guest@192.168.187.204:5672/enable_v1_api=Falseshow_multiple_locations = True #启用copy-on-write show_image_direct_url = True  #启用copy-on-write [cinder][cors][database]connection=mysql+pymysql://glance:6588a16556eb4abf@192.168.187.204/glance[file][glance.store.http.store][glance.store.rbd.store][glance.store.sheepdog.store][glance.store.swift.store][glance.store.vmware_datastore.store][glance_store]stores = rbddefault_store = rbdrbd_store_pool = imagesrbd_store_user = glancerbd_store_ceph_conf = /etc/ceph/ceph.confrbd_store_chunk_size = 8stores=file,http,swift,rbdos_region_name=RegionOne[image_format][keystone_authtoken]www_authenticate_uri=http://192.168.187.204:5000/v3auth_type=passwordauth_url=http://192.168.187.204:5000username=glancepassword=f20a4d2094ab4018user_domain_name=Defaultproject_name=servicesproject_domain_name=Default[oslo_concurrency][oslo_messaging_amqp][oslo_messaging_kafka][oslo_messaging_notifications]driver=messagingv2topics=notifications[oslo_messaging_rabbit]ssl=Falsedefault_notification_exchange=glance[oslo_middleware][oslo_policy]policy_file=/etc/glance/policy.json[paste_deploy]flavor=keystone[profiler][root@controller ceph]# systemctl restart openstack-glance-api.service [root@controller ceph]# systemctl status openstack-glance-api.service ● openstack-glance-api.service - OpenStack Image Service (code-named Glance) API server   Loaded: loaded (/usr/lib/systemd/system/openstack-glance-api.service; enabled; vendor preset: disabled)   Active: active (running) since Mon 2023-03-20 00:36:19 EDT; 10s ago Main PID: 10583 (glance-api)    Tasks: 5   CGroup: /system.slice/openstack-glance-api.service           ├─10583 /usr/bin/python2 /usr/bin/glance-api           ├─10599 /usr/bin/python2 /usr/bin/glance-api           ├─10600 /usr/bin/python2 /usr/bin/glance-api           ├─10601 /usr/bin/python2 /usr/bin/glance-api           └─10602 /usr/bin/python2 /usr/bin/glance-apiMar 20 00:36:19 controller systemd[1]: Started OpenStack Image Service (code-named Glance) API server.Mar 20 00:36:21 controller glance-api[10583]: /usr/lib/python2.7/site-packages/paste/deploy/loadwsgi.py:22: PkgResourcesDeprecati...rately.Mar 20 00:36:21 controller glance-api[10583]: return pkg_resources.EntryPoint.parse(&quot;x=&quot; + s).load(False)Hint: Some lines were ellipsized, use -l to show in full.[root@controller ceph]# tail -f /var/log/glance/api.log 2023-03-20 00:36:22.933 10583 WARNING glance_store.driver [-] Failed to configure store correctly: A value for swift_store_auth_address is required. Disabling add method.: BadStoreConfiguration: A value for swift_store_auth_address is required.2023-03-20 00:36:22.935 10583 INFO glance.common.wsgi [-] Starting 4 workers2023-03-20 00:36:22.954 10599 INFO eventlet.wsgi.server [-] (10599) wsgi starting up on http://0.0.0.0:92922023-03-20 00:36:22.941 10583 INFO glance.common.wsgi [-] Started child 105992023-03-20 00:36:22.979 10583 INFO glance.common.wsgi [-] Started child 106002023-03-20 00:36:22.982 10600 INFO eventlet.wsgi.server [-] (10600) wsgi starting up on http://0.0.0.0:92922023-03-20 00:36:22.988 10583 INFO glance.common.wsgi [-] Started child 106012023-03-20 00:36:23.013 10601 INFO eventlet.wsgi.server [-] (10601) wsgi starting up on http://0.0.0.0:92922023-03-20 00:36:23.034 10602 INFO eventlet.wsgi.server [-] (10602) wsgi starting up on http://0.0.0.0:92922023-03-20 00:36:23.005 10583 INFO glance.common.wsgi [-] Started child 10602</code></pre><h1><span id="五-glance对接功能测试">五、Glance对接功能测试</span></h1><pre><code>[root@controller ~]# lsanaconda-ks.cfg  client.cinder.key  keystonerc_admin  keystonerc_demo  train-for-ceph.txt[root@controller ~]# source keystonerc_admin [root@controller ~(keystone_admin)]# pwd/root[root@controller ~(keystone_admin)]# ls -lha /var/lib/glance/images/total 4.0Kdrwxr-x--- 2 glance glance  50 Mar 19 11:38 .drwxr-xr-x 3 glance nobody  20 Mar 19 11:37 ..-rw-r----- 1 glance glance 273 Mar 19 11:38 38c7585e-23ec-449d-8026-ca5a87442154[root@controller ~(keystone_admin)]# glance help image-createusage: glance image-create [--architecture &lt;ARCHITECTURE&gt;]                           [--protected [True|False]] [--name &lt;NAME&gt;]                           [--instance-uuid &lt;INSTANCE_UUID&gt;]                           [--min-disk &lt;MIN_DISK&gt;] [--visibility &lt;VISIBILITY&gt;]                           [--kernel-id &lt;KERNEL_ID&gt;]                           [--tags &lt;TAGS&gt; [&lt;TAGS&gt; ...]]                           [--os-version &lt;OS_VERSION&gt;]                           [--disk-format &lt;DISK_FORMAT&gt;]                           [--os-distro &lt;OS_DISTRO&gt;] [--id &lt;ID&gt;]                           [--owner &lt;OWNER&gt;] [--ramdisk-id &lt;RAMDISK_ID&gt;]                           [--min-ram &lt;MIN_RAM&gt;]                           [--container-format &lt;CONTAINER_FORMAT&gt;]                           [--hidden [True|False]] [--property &lt;key=value&gt;]                           [--file &lt;FILE&gt;] [--progress] [--store &lt;STORE&gt;]Create a new image.Optional arguments:  --architecture &lt;ARCHITECTURE&gt;                        Operating system architecture as specified in                        https://docs.openstack.org/python-                        glanceclient/latest/cli/property-keys.html  --protected [True|False]                        If true, image will not be deletable.  --name &lt;NAME&gt;         Descriptive name for the image  --instance-uuid &lt;INSTANCE_UUID&gt;                        Metadata which can be used to record which instance                        this image is associated with. (Informational only,                        does not create an instance snapshot.)  --min-disk &lt;MIN_DISK&gt;                        Amount of disk space (in GB) required to boot image.  --visibility &lt;VISIBILITY&gt;                        Scope of image accessibility Valid values: public,                        private, community, shared  --kernel-id &lt;KERNEL_ID&gt;                        ID of image stored in Glance that should be used as                        the kernel when booting an AMI-style image.  --tags &lt;TAGS&gt; [&lt;TAGS&gt; ...]                        List of strings related to the image  --os-version &lt;OS_VERSION&gt;                        Operating system version as specified by the                        distributor  --disk-format &lt;DISK_FORMAT&gt;                        Format of the disk Valid values: None, ami, ari, aki,                        vhd, vhdx, vmdk, raw, qcow2, vdi, iso, ploop  --os-distro &lt;OS_DISTRO&gt;                        Common name of operating system distribution as                        specified in https://docs.openstack.org/python-                        glanceclient/latest/cli/property-keys.html  --id &lt;ID&gt;             An identifier for the image  --owner &lt;OWNER&gt;       Owner of the image  --ramdisk-id &lt;RAMDISK_ID&gt;                        ID of image stored in Glance that should be used as                        the ramdisk when booting an AMI-style image.  --min-ram &lt;MIN_RAM&gt;   Amount of ram (in MB) required to boot image.  --container-format &lt;CONTAINER_FORMAT&gt;                        Format of the container Valid values: None, ami, ari,                        aki, bare, ovf, ova, docker  --hidden [True|False]                        If true, image will not appear in default image list                        response.  --property &lt;key=value&gt;                        Arbitrary property to associate with image. May be                        used multiple times.  --file &lt;FILE&gt;         Local file that contains disk image to be uploaded                        during creation. Alternatively, the image data can be                        passed to the client via stdin.  --progress            Show upload progress bar.  --store &lt;STORE&gt;       Backend store to upload image to.Run `glance --os-image-api-version 1 help image-create` for v1 help[root@controller ~(keystone_admin)]# glance image-create --name ceph-test --disk-format raw --container-format bare --file /var/lib/glance/images/38c7585e-23ec-449d-8026-ca5a87442154 --progress --public  #上传镜像[=============================&gt;] 100%+------------------+----------------------------------------------------------------------------------+| Property         | Value                                                                            |+------------------+----------------------------------------------------------------------------------+| checksum         | 52ba1c45042aa3688c09f303da283524                                                 || container_format | bare                                                                             || created_at       | 2023-03-20T04:42:52Z                                                             || disk_format      | raw                                                                              || id               | a3af191a-70c5-467e-a1f4-1c7991fff838                                             || min_disk         | 0                                                                                || min_ram          | 0                                                                                || name             | ceph-test                                                                        || os_hash_algo     | sha512                                                                           || os_hash_value    | 507ca3ef53a49145ede39d6cb7394bb40b5f419aec5abac496880112825cf177c15f49e35ac7c8c9 ||                  | a45d659598b71d0c59b93d85c79c2d54c69748bdeda2b0a5                                 || os_hidden        | False                                                                            || owner            | 536255bf80df451ebcdf8277d214661d                                                 || protected        | False                                                                            || size             | 273                                                                              || status           | active                                                                           || tags             | []                                                                               || updated_at       | 2023-03-20T04:42:53Z                                                             || virtual_size     | Not available                                                                    || visibility       | shared                                                                           |+------------------+----------------------------------------------------------------------------------+[root@controller ~(keystone_admin)]# glance image-list #查看创建的镜像 +--------------------------------------+-----------+| ID                                   | Name      |+--------------------------------------+-----------+| a3af191a-70c5-467e-a1f4-1c7991fff838 | ceph-test || 38c7585e-23ec-449d-8026-ca5a87442154 | cirros    |+--------------------------------------+-----------+[root@node-1 ~]# rbd ls -p images #在Ceph中查看是否创建镜像a3af191a-70c5-467e-a1f4-1c7991fff838</code></pre><h1><span id="六-cinder与ceph对接">六、Cinder与Ceph对接</span></h1><pre><code>cat /etc/cinder/cinder.conf |grep -v &quot;#&quot; |grep -v &quot;^$&quot; #修改配置文件[DEFAULT]backup_swift_url=http://192.168.187.204:8080/v1/AUTH_backup_swift_container=volumebackupsbackup_driver=cinder.backup.drivers.swift.SwiftBackupDriverenable_v3_api=Trueauth_strategy=keystonestorage_availability_zone=novadefault_availability_zone=novadefault_volume_type=iscsienabled_backends=ceph #修改成Cephglance_api_version = 2 #设置API的版本，需要新增，原来没有配置osapi_volume_listen=0.0.0.0osapi_volume_workers=4debug=Falselog_dir=/var/log/cindertransport_url=rabbit://guest:guest@192.168.187.204:5672/control_exchange=openstackapi_paste_config=/etc/cinder/api-paste.iniglance_host=192.168.187.204[backend][backend_defaults][barbican][brcd_fabric_example][cisco_fabric_example][coordination][cors][database]connection=mysql+pymysql://cinder:57095b3dc17b45b3@192.168.187.204/cinder[fc-zone-manager][healthcheck][key_manager][keystone_authtoken]www_authenticate_uri=http://192.168.187.204:5000/auth_type=passwordauth_url=http://192.168.187.204:5000username=cinderpassword=0b71eb2ae1d84aebuser_domain_name=Defaultproject_name=servicesproject_domain_name=Default[nova][oslo_concurrency]lock_path=/var/lib/cinder/tmp[oslo_messaging_amqp][oslo_messaging_kafka][oslo_messaging_notifications]driver=messagingv2[oslo_messaging_rabbit]ssl=False[oslo_middleware][oslo_policy]policy_file=/etc/cinder/policy.json[oslo_reports][oslo_versionedobjects][privsep][profiler][sample_castellan_source][sample_remote_file_source][service_user][ssl][vault][lvm]volume_backend_name=lvmvolume_driver=cinder.volume.drivers.lvm.LVMVolumeDrivertarget_ip_address=192.168.187.204target_helper=lioadmvolume_group=cinder-volumesvolumes_dir=/var/lib/cinder/volumes[ceph] #新增如下配置volume_driver = cinder.volume.drivers.rbd.RBDDriver #RBD的驱动volume_backend_name = ceph #需要前后一致rbd_pool = volumes #连接的pool的名称rbd_ceph_conf = /etc/ceph/ceph.conf #Ceph的配置文件rbd_flatten_volume_from_snapshot = false  #超过设置的克隆最大深度就会执行flatten的操作rbd_max_clone_depth = 5 #克隆的最大深度rbd_store_chunk_size = 4rados_connect_timeout = -1rbd_user = cinder #RBD使用的用户rbd_secret_uuid = 617ac927-1a98-444f-b3b4-1f682d7bfd7a #RBD的认证信息，可通过virsh secret-list 查看[root@controller ~(keystone_admin)]# systemctl restart openstack-cinder-volume.service #重启Cinder-volumes服务[root@controller ~(keystone_admin)]# systemctl status openstack-cinder-volume.service #查看状态● openstack-cinder-volume.service - OpenStack Cinder Volume Server   Loaded: loaded (/usr/lib/systemd/system/openstack-cinder-volume.service; enabled; vendor preset: disabled)   Active: active (running) since Mon 2023-03-20 08:27:27 EDT; 26s ago Main PID: 22309 (cinder-volume)    Tasks: 22   CGroup: /system.slice/openstack-cinder-volume.service           ├─22309 /usr/bin/python2 /usr/bin/cinder-volume --config-file /usr/share/cinder/cinder-dist.conf --config-file /etc/cinder/cinder.conf --logfile /var/log/cinder/volume.log           └─22321 /usr/bin/python2 /usr/bin/cinder-volume --config-file /usr/share/cinder/cinder-dist.conf --config-file /etc/cinder/cinder.conf --logfile /var/log/cinder/volume.logMar 20 08:27:27 controller systemd[1]: Stopped OpenStack Cinder Volume Server.Mar 20 08:27:27 controller systemd[1]: Started OpenStack Cinder Volume Server.Mar 20 08:27:28 controller cinder-volume[22309]: Deprecated: Option &quot;logdir&quot; from group &quot;DEFAULT&quot; is deprecated. Use option &quot;log-dir&quot; from group &quot;DEFAULT&quot;.[root@controller ~]# . keystonerc_admin  #执行变量[root@controller ~(keystone_admin)]# cinder type-create ceph  #创建Cinder类型+--------------------------------------+------+-------------+-----------+| ID                                   | Name | Description | Is_Public |+--------------------------------------+------+-------------+-----------+| cc001eeb-cb9a-4f7a-a399-645564cfd1b9 | ceph | -           | True      |+--------------------------------------+------+-------------+-----------+[root@controller ~(keystone_admin)]# cinder type-key cc001eeb-cb9a-4f7a-a399-645564cfd1b9 set volume_backend_name=ceph #在/etc/cinder/cinder.conf中设置的volume_backend_name=ceph参数[root@controller ~(keystone_admin)]# cinder type-list #查看Cinder类型+--------------------------------------+-------------+---------------------+-----------+| ID                                   | Name        | Description         | Is_Public |+--------------------------------------+-------------+---------------------+-----------+| 1a116c34-beb5-472a-9bdb-1233e1ac7ecc | iscsi       | -                   | True      || c1b4d434-b521-4818-86f1-be2cb7a8c771 | __DEFAULT__ | Default Volume Type | True      || cc001eeb-cb9a-4f7a-a399-645564cfd1b9 | ceph        | -                   | True      |+--------------------------------------+-------------+---------------------+-----------+[root@controller ~(keystone_admin)]# tail -f /var/log/cinder/cinder-manage.log #查看Cinder日志2023-03-20 00:01:12.745 5965 INFO cinder.db.sqlalchemy.api [req-07bd8d2a-a84e-47f6-9d5b-e41f53b58321 - - - - -] Purging deleted rows older than age=1 days from table=quota_usages2023-03-20 00:01:12.749 5965 INFO cinder.db.sqlalchemy.api [req-07bd8d2a-a84e-47f6-9d5b-e41f53b58321 - - - - -] Purging deleted rows older than age=1 days from table=quota_classes2023-03-20 00:01:12.754 5965 INFO cinder.db.sqlalchemy.api [req-07bd8d2a-a84e-47f6-9d5b-e41f53b58321 - - - - -] Purging deleted rows older than age=1 days from table=quality_of_service_specs2023-03-20 00:01:13.012 5965 INFO cinder.db.sqlalchemy.api [req-07bd8d2a-a84e-47f6-9d5b-e41f53b58321 - - - - -] Purging deleted rows older than age=1 days from table=messages2023-03-20 00:01:13.019 5965 INFO cinder.db.sqlalchemy.api [req-07bd8d2a-a84e-47f6-9d5b-e41f53b58321 - - - - -] Purging deleted rows older than age=1 days from table=groups</code></pre><h1><span id="七-cinder对接功能测试">七、Cinder对接功能测试</span></h1><h2><span id="1-找到卷">1、找到卷</span></h2><p><img src="/images/Ceph/Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/5.jpg"></p><h2><span id="2-创建卷">2、创建卷</span></h2><p><img src="/images/Ceph/Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/6.jpg"></p><p>创建成功：</p><p><img src="/images/Ceph/Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/7.jpg"></p><h2><span id="3-扩展卷">3、扩展卷</span></h2><p><img src="/images/Ceph/Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/8.jpg"></p><p>扩展成功：</p><p><img src="/images/Ceph/Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/9.jpg"></p><p>Ceph服务器查看状态</p><pre><code>[root@node-1 ~]# rbd info -p volumes volume-7fbedcfa-0b83-4986-bbd8-25c3f17c8497rbd image &#39;volume-7fbedcfa-0b83-4986-bbd8-25c3f17c8497&#39;:    size 3 GiB in 768 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 69ff51af94c05    block_name_prefix: rbd_data.69ff51af94c05    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Mon Mar 20 20:29:18 2023    access_timestamp: Mon Mar 20 20:29:18 2023    modify_timestamp: Mon Mar 20 20:29:18 2023</code></pre><p><img src="/images/Ceph/Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/10.jpg"><br><img src="/images/Ceph/Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/11.jpg"><br><img src="/images/Ceph/Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/12.jpg"></p><pre><code>[root@node-1 ~]# rbd snap ls volumes/volume-7fbedcfa-0b83-4986-bbd8-25c3f17c8497SNAPID NAME                                          SIZE  PROTECTED TIMESTAMP                     4 snapshot-92cfcfe4-0d79-4134-bbd3-b5450590723a 3 GiB yes       Mon Mar 20 20:33:15 2023</code></pre><h1><span id="八-cinder-bakup对接和测试">八、Cinder-bakup对接和测试</span></h1><h2><span id="1-配置-cinder-备份">1、配置 CINDER 备份</span></h2><p>OpenStack Cinder Backup 需要一个特定的守护进程，所以不要忘记安装它。在您的 Cinder Backup 节点上，编辑&#x2F;etc&#x2F;cinder&#x2F;cinder.conf并添加：</p><pre><code>backup_driver = cinder.backup.drivers.cephbackup_ceph_conf = /etc/ceph/ceph.confbackup_ceph_user = cinder-backupbackup_ceph_chunk_size = 134217728backup_ceph_pool = backupsbackup_ceph_stripe_unit = 0backup_ceph_stripe_count = 0restore_discard_excess_bytes = true</code></pre><h2><span id="2-实践">2、实践</span></h2><pre><code>[root@controller ~(keystone_admin)]# cat /etc/cinder/cinder.conf | grep -v &quot;#&quot; |grep -v &quot;^$&quot; #在DEFAULT下增加即可，需要注释原来的backup_driver=cinder.backup.drivers.swift.SwiftBackupDriver[DEFAULT]backup_driver = cinder.backup.drivers.cephbackup_ceph_conf = /etc/ceph/ceph.confbackup_ceph_user = cinder-backupbackup_ceph_chunk_size = 134217728backup_ceph_pool = backupsbackup_ceph_stripe_unit = 0backup_ceph_stripe_count = 0restore_discard_excess_bytes = truebackup_swift_url=http://192.168.187.204:8080/v1/AUTH_backup_swift_container=volumebackupsenable_v3_api=Trueauth_strategy=keystonestorage_availability_zone=novadefault_availability_zone=novadefault_volume_type=iscsienabled_backends = cephglance_api_version = 2osapi_volume_listen=0.0.0.0osapi_volume_workers=4debug=Falselog_dir=/var/log/cindertransport_url=rabbit://guest:guest@192.168.187.204:5672/control_exchange=openstackapi_paste_config=/etc/cinder/api-paste.iniglance_host=192.168.187.204[backend][backend_defaults][barbican][brcd_fabric_example][cisco_fabric_example][coordination][cors][database]connection=mysql+pymysql://cinder:57095b3dc17b45b3@192.168.187.204/cinder[fc-zone-manager][healthcheck][key_manager][keystone_authtoken]www_authenticate_uri=http://192.168.187.204:5000/auth_type=passwordauth_url=http://192.168.187.204:5000username=cinderpassword=0b71eb2ae1d84aebuser_domain_name=Defaultproject_name=servicesproject_domain_name=Default[nova][oslo_concurrency]lock_path=/var/lib/cinder/tmp[oslo_messaging_amqp][oslo_messaging_kafka][oslo_messaging_notifications]driver=messagingv2[oslo_messaging_rabbit]ssl=False[oslo_middleware][oslo_policy]policy_file=/etc/cinder/policy.json[oslo_reports][oslo_versionedobjects][privsep][profiler][sample_castellan_source][sample_remote_file_source][service_user][ssl][vault][lvm]volume_backend_name=lvmvolume_driver=cinder.volume.drivers.lvm.LVMVolumeDrivertarget_ip_address=192.168.187.204target_helper=lioadmvolume_group=cinder-volumesvolumes_dir=/var/lib/cinder/volumes[ceph]volume_driver = cinder.volume.drivers.rbd.RBDDrivervolume_backend_name = cephrbd_pool = volumesrbd_ceph_conf = /etc/ceph/ceph.confrbd_flatten_volume_from_snapshot = falserbd_max_clone_depth = 5rbd_store_chunk_size = 4rados_connect_timeout = -1rbd_user = cinderrbd_secret_uuid = 617ac927-1a98-444f-b3b4-1f682d7bfd7a[root@controller ~(keystone_admin)]# systemctl restart openstack-cinder-backup.service[root@controller ~(keystone_admin)]# systemctl status openstack-cinder-backup.service● openstack-cinder-backup.service - OpenStack Cinder Backup Server   Loaded: loaded (/usr/lib/systemd/system/openstack-cinder-backup.service; enabled; vendor preset: disabled)   Active: active (running) since Mon 2023-03-20 09:10:25 EDT; 2s ago Main PID: 27495 (cinder-backup)    Tasks: 1   CGroup: /system.slice/openstack-cinder-backup.service           └─27495 /usr/bin/python2 /usr/bin/cinder-backup --config-file /usr/share/cinder/cinder-dist.conf --config-file /etc/cinder/cinder.conf --logfile /var/log/cinder/backup.logMar 20 09:10:25 controller systemd[1]: Started OpenStack Cinder Backup Server.[root@controller ~(keystone_admin)]# tail -f /var/log/cinder/backup.log  #目前报错内容是版本问题导致的，查找原因是因为驱动错误，新的T版本的驱动名称产生变化2023-03-20 09:10:48.420 27634 ERROR cinder.cmd.backup   File &quot;/usr/lib/python2.7/site-packages/cinder/service.py&quot;, line 400, in create2023-03-20 09:10:48.420 27634 ERROR cinder.cmd.backup     cluster=cluster, **kwargs)2023-03-20 09:10:48.420 27634 ERROR cinder.cmd.backup   File &quot;/usr/lib/python2.7/site-packages/cinder/service.py&quot;, line 155, in __init__2023-03-20 09:10:48.420 27634 ERROR cinder.cmd.backup     *args, **kwargs)2023-03-20 09:10:48.420 27634 ERROR cinder.cmd.backup   File &quot;/usr/lib/python2.7/site-packages/cinder/backup/manager.py&quot;, line 136, in __init__2023-03-20 09:10:48.420 27634 ERROR cinder.cmd.backup     self.service = importutils.import_class(self.driver_name)2023-03-20 09:10:48.420 27634 ERROR cinder.cmd.backup   File &quot;/usr/lib/python2.7/site-packages/oslo_utils/importutils.py&quot;, line 36, in import_class2023-03-20 09:10:48.420 27634 ERROR cinder.cmd.backup     traceback.format_exception(*sys.exc_info())))2023-03-20 09:10:48.420 27634 ERROR cinder.cmd.backup ImportError: Class ceph cannot be found ([&#39;Traceback (most recent call last):\n&#39;, &#39;  File &quot;/usr/lib/python2.7/site-packages/oslo_utils/importutils.py&quot;, line 32, in import_class\n    return getattr(sys.modules[mod_str], class_str)\n&#39;, &quot;AttributeError: &#39;module&#39; object has no attribute &#39;ceph&#39;\n&quot;])2023-03-20 09:10:48.420 27634 ERROR cinder.cmd.backup 2023-03-20 09:10:51.071 27648 INFO cinder.cmd.backup [-] Backup running with 1 processes.2023-03-20 09:10:51.402 27648 INFO cinder.rpc [req-5dd92b53-f481-46b5-8a2d-c7578b77cd4c - - - - -] Automatically selected cinder-backup objects version 1.38 as minimum service version.2023-03-20 09:10:51.409 27648 INFO cinder.rpc [req-5dd92b53-f481-46b5-8a2d-c7578b77cd4c - - - - -] Automatically selected cinder-backup RPC version 2.1 as minimum service version.2023-03-20 09:10:51.416 27648 INFO cinder.rpc [req-5dd92b53-f481-46b5-8a2d-c7578b77cd4c - - - - -] Automatically selected cinder-volume objects version 1.38 as minimum service version.2023-03-20 09:10:51.428 27648 INFO cinder.rpc [req-5dd92b53-f481-46b5-8a2d-c7578b77cd4c - - - - -] Automatically selected cinder-volume RPC version 3.16 as minimum service version.2023-03-20 09:10:51.430 27648 ERROR cinder.cmd.backup [req-5dd92b53-f481-46b5-8a2d-c7578b77cd4c - - - - -] Backup service controller failed to start.: ImportError: Class ceph cannot be found ([&#39;Traceback (most recent call last):\n&#39;, &#39;  File &quot;/usr/lib/python2.7/site-packages/oslo_utils/importutils.py&quot;, line 32, in import_class\n    return getattr(sys.modules[mod_str], class_str)\n&#39;, &quot;AttributeError: &#39;module&#39; object has no attribute &#39;ceph&#39;\n&quot;])2023-03-20 09:10:51.430 27648 ERROR cinder.cmd.backup Traceback (most recent call last):2023-03-20 09:10:51.430 27648 ERROR cinder.cmd.backup   File &quot;/usr/lib/python2.7/site-packages/cinder/cmd/backup.py&quot;, line 71, in _launch_backup_process2023-03-20 09:10:51.430 27648 ERROR cinder.cmd.backup     process_number=num_process)2023-03-20 09:10:51.430 27648 ERROR cinder.cmd.backup   File &quot;/usr/lib/python2.7/site-packages/cinder/service.py&quot;, line 400, in create2023-03-20 09:10:51.430 27648 ERROR cinder.cmd.backup     cluster=cluster, **kwargs)2023-03-20 09:10:51.430 27648 ERROR cinder.cmd.backup   File &quot;/usr/lib/python2.7/site-packages/cinder/service.py&quot;, line 155, in __init__2023-03-20 09:10:51.430 27648 ERROR cinder.cmd.backup     *args, **kwargs)2023-03-20 09:10:51.430 27648 ERROR cinder.cmd.backup   File &quot;/usr/lib/python2.7/site-packages/cinder/backup/manager.py&quot;, line 136, in __init__2023-03-20 09:10:51.430 27648 ERROR cinder.cmd.backup     self.service = importutils.import_class(self.driver_name)2023-03-20 09:10:51.430 27648 ERROR cinder.cmd.backup   File &quot;/usr/lib/python2.7/site-packages/oslo_utils/importutils.py&quot;, line 36, in import_class2023-03-20 09:10:51.430 27648 ERROR cinder.cmd.backup     traceback.format_exception(*sys.exc_info())))2023-03-20 09:10:51.430 27648 ERROR cinder.cmd.backup ImportError: Class ceph cannot be found ([&#39;Traceback (most recent call last):\n&#39;, &#39;  File &quot;/usr/lib/python2.7/site-packages/oslo_utils/importutils.py&quot;, line 32, in import_class\n    return getattr(sys.modules[mod_str], class_str)\n&#39;, &quot;AttributeError: &#39;module&#39; object has no attribute &#39;ceph&#39;\n&quot;])2023-03-20 09:10:51.430 27648 ERROR cinder.cmd.backup 2023-03-20 09:10:53.965 27671 INFO cinder.cmd.backup [-] Backup running with 1 processes.2023-03-20 09:10:54.380 27671 INFO cinder.rpc [req-a36ac363-171c-4ebf-9a56-189a2cb47116 - - - - -] Automatically selected cinder-backup objects version 1.38 as minimum service version.2023-03-20 09:10:54.388 27671 INFO cinder.rpc [req-a36ac363-171c-4ebf-9a56-189a2cb47116 - - - - -] Automatically selected cinder-backup RPC version 2.1 as minimum service version.2023-03-20 09:10:54.396 27671 INFO cinder.rpc [req-a36ac363-171c-4ebf-9a56-189a2cb47116 - - - - -] Automatically selected cinder-volume objects version 1.38 as minimum service version.2023-03-20 09:10:54.408 27671 INFO cinder.rpc [req-a36ac363-171c-4ebf-9a56-189a2cb47116 - - - - -] Automatically selected cinder-volume RPC version 3.16 as minimum service version.2023-03-20 09:10:54.411 27671 ERROR cinder.cmd.backup [req-a36ac363-171c-4ebf-9a56-189a2cb47116 - - - - -] Backup service controller failed to start.: ImportError: Class ceph cannot be found ([&#39;Traceback (most recent call last):\n&#39;, &#39;  File &quot;/usr/lib/python2.7/site-packages/oslo_utils/importutils.py&quot;, line 32, in import_class\n    return getattr(sys.modules[mod_str], class_str)\n&#39;, &quot;AttributeError: &#39;module&#39; object has no attribute &#39;ceph&#39;\n&quot;])2023-03-20 09:10:54.411 27671 ERROR cinder.cmd.backup Traceback (most recent call last):2023-03-20 09:10:54.411 27671 ERROR cinder.cmd.backup   File &quot;/usr/lib/python2.7/site-packages/cinder/cmd/backup.py&quot;, line 71, in _launch_backup_process2023-03-20 09:10:54.411 27671 ERROR cinder.cmd.backup     process_number=num_process)2023-03-20 09:10:54.411 27671 ERROR cinder.cmd.backup   File &quot;/usr/lib/python2.7/site-packages/cinder/service.py&quot;, line 400, in create2023-03-20 09:10:54.411 27671 ERROR cinder.cmd.backup     cluster=cluster, **kwargs)2023-03-20 09:10:54.411 27671 ERROR cinder.cmd.backup   File &quot;/usr/lib/python2.7/site-packages/cinder/service.py&quot;, line 155, in __init__2023-03-20 09:10:54.411 27671 ERROR cinder.cmd.backup     *args, **kwargs)2023-03-20 09:10:54.411 27671 ERROR cinder.cmd.backup   File &quot;/usr/lib/python2.7/site-packages/cinder/backup/manager.py&quot;, line 136, in __init__2023-03-20 09:10:54.411 27671 ERROR cinder.cmd.backup     self.service = importutils.import_class(self.driver_name)2023-03-20 09:10:54.411 27671 ERROR cinder.cmd.backup   File &quot;/usr/lib/python2.7/site-packages/oslo_utils/importutils.py&quot;, line 36, in import_class2023-03-20 09:10:54.411 27671 ERROR cinder.cmd.backup     traceback.format_exception(*sys.exc_info())))2023-03-20 09:10:54.411 27671 ERROR cinder.cmd.backup ImportError: Class ceph cannot be found ([&#39;Traceback (most recent call last):\n&#39;, &#39;  File &quot;/usr/lib/python2.7/site-packages/oslo_utils/importutils.py&quot;, line 32, in import_class\n    return getattr(sys.modules[mod_str], class_str)\n&#39;, &quot;AttributeError: &#39;module&#39; object has no attribute &#39;ceph&#39;\n&quot;])2023-03-20 09:10:54.411 27671 ERROR cinder.cmd.backup </code></pre><p><a href="https://bugs.launchpad.net/kolla-ansible/+bug/1859889"><strong>参考文档</strong></a></p><pre><code>[root@controller ~(keystone_admin)]# cat /etc/cinder/cinder.conf #修改如下配置backup_driver = cinder.backup.drivers.ceph.CephBackupDriver[root@controller ~(keystone_admin)]# systemctl restart openstack-cinder-backup.service #重启服务[root@controller ~(keystone_admin)]# tail -f /var/log/cinder/backup.log  #查看日志，没有报错了，就是因为驱动的名称变动引起的故障；2023-03-20 09:15:56.547 29397 INFO cinder.rpc [req-ed7ee319-2a97-468e-8e17-a0e08a45221c - - - - -] Automatically selected cinder-backup RPC version 2.1 as minimum service version.2023-03-20 09:15:56.555 29397 INFO cinder.rpc [req-ed7ee319-2a97-468e-8e17-a0e08a45221c - - - - -] Automatically selected cinder-volume objects version 1.38 as minimum service version.2023-03-20 09:15:56.560 29397 INFO cinder.rpc [req-ed7ee319-2a97-468e-8e17-a0e08a45221c - - - - -] Automatically selected cinder-volume RPC version 3.16 as minimum service version.2023-03-20 09:15:56.588 29397 INFO oslo_service.service [req-ed7ee319-2a97-468e-8e17-a0e08a45221c - - - - -] Starting 1 workers2023-03-20 09:15:56.614 29409 INFO cinder.service [-] Starting cinder-backup node (version 15.6.0)2023-03-20 09:15:56.777 29409 INFO cinder.backup.manager [req-ea25f46d-2c7b-4057-84be-f1150b60fc2b - - - - -] Cleaning up incomplete backup operations.2023-03-20 09:15:56.875 29409 INFO cinder.keymgr.migration [req-8ab38b7c-ed98-470b-b698-10a35f399b13 - - - - -] Not migrating encryption keys because the ConfKeyManager&#39;s fixed_key is not in use.2023-03-20 09:16:56.032 29397 INFO oslo_service.service [req-ed7ee319-2a97-468e-8e17-a0e08a45221c - - - - -] Caught SIGTERM, stopping children2023-03-20 09:16:56.034 29397 INFO oslo_service.service [req-ed7ee319-2a97-468e-8e17-a0e08a45221c - - - - -] Waiting on 1 children to exit2023-03-20 09:16:56.051 29397 INFO oslo_service.service [req-ed7ee319-2a97-468e-8e17-a0e08a45221c - - - - -] Child 29409 killed by signal 152023-03-20 09:16:59.092 29603 INFO cinder.cmd.backup [-] Backup running with 1 processes.2023-03-20 09:16:59.611 29603 INFO cinder.rpc [req-9235ef09-7845-42f9-ba72-acda5526d958 - - - - -] Automatically selected cinder-backup objects version 1.38 as minimum service version.2023-03-20 09:16:59.627 29603 INFO cinder.rpc [req-9235ef09-7845-42f9-ba72-acda5526d958 - - - - -] Automatically selected cinder-backup RPC version 2.1 as minimum service version.2023-03-20 09:16:59.639 29603 INFO cinder.rpc [req-9235ef09-7845-42f9-ba72-acda5526d958 - - - - -] Automatically selected cinder-volume objects version 1.38 as minimum service version.2023-03-20 09:16:59.651 29603 INFO cinder.rpc [req-9235ef09-7845-42f9-ba72-acda5526d958 - - - - -] Automatically selected cinder-volume RPC version 3.16 as minimum service version.2023-03-20 09:16:59.690 29603 INFO oslo_service.service [req-9235ef09-7845-42f9-ba72-acda5526d958 - - - - -] Starting 1 workers2023-03-20 09:16:59.702 29618 INFO cinder.service [-] Starting cinder-backup node (version 15.6.0)2023-03-20 09:16:59.745 29618 INFO cinder.backup.manager [req-bdfe6eb4-35a8-4374-a04b-90a9983f21fe - - - - -] Cleaning up incomplete backup operations.2023-03-20 09:16:59.858 29618 INFO cinder.keymgr.migration [req-76156fe7-6a74-4155-a040-f534cb90a9e0 - - - - -] Not migrating encryption keys because the ConfKeyManager&#39;s fixed_key is not in use.</code></pre><h2><span id="3-测试">3、测试</span></h2><pre><code>[root@controller ~(keystone_admin)]# cinder -h | grep backup #搜索帮助文档    backup-create       Creates a volume backup.    backup-delete       Removes one or more backups.    backup-export       Export backup metadata record.    backup-import       Import backup metadata record.    backup-list         Lists all backups.    backup-reset-state  Explicitly updates the backup state.    backup-restore      Restores a backup.    backup-show         Shows backup details.    backup-update       Updates a backup. (Supported by API versions 3.9 -[root@controller ~(keystone_admin)]# cinder help backup-create  #查看帮助文档usage: cinder backup-create [--container &lt;container&gt;] [--name &lt;name&gt;]                            [--description &lt;description&gt;] [--incremental]                            [--force] [--snapshot-id &lt;snapshot-id&gt;]                            [--metadata [&lt;key=value&gt; [&lt;key=value&gt; ...]]]                            [--availability-zone AVAILABILITY_ZONE]                            &lt;volume&gt;Creates a volume backup.Positional Arguments:  &lt;volume&gt;              Name or ID of volume to backup.Optional Arguments:  --container &lt;container&gt;                        Backup container name. Default=None.  --name &lt;name&gt;         Backup name. Default=None.  --description &lt;description&gt;                        Backup description. Default=None.  --incremental         Incremental backup. Default=False.  --force               Allows or disallows backup of a volume when the volume                        is attached to an instance. If set to True, backs up                        the volume whether its status is &quot;available&quot; or &quot;in-                        use&quot;. The backup of an &quot;in-use&quot; volume means your data                        is crash consistent. Default=False.  --snapshot-id &lt;snapshot-id&gt;                        ID of snapshot to backup. Default=None.  --metadata [&lt;key=value&gt; [&lt;key=value&gt; ...]]                        Metadata key and value pairs. Default=None. (Supported                        by API version 3.43 and later)  --availability-zone AVAILABILITY_ZONE                        AZ where the backup should be stored, by default it                        will be the same as the source. (Supported by API                        version 3.51 and later)[root@controller ~(keystone_admin)]# cinder list +--------------------------------------+-----------+-------------+------+-------------+----------+-------------+| ID                                   | Status    | Name        | Size | Volume Type | Bootable | Attached to |+--------------------------------------+-----------+-------------+------+-------------+----------+-------------+| 4b50e431-e1b2-4e4a-8a78-e224a4112a0b | available | Cinder-ceph | 10   | ceph        | false    |             |+--------------------------------------+-----------+-------------+------+-------------+----------+-------------+[root@controller ~(keystone_admin)]# cinder backup-create 4b50e431-e1b2-4e4a-8a78-e224a4112a0b --name Cinder-ceph-BACKUP #创建备份+-----------+--------------------------------------+| Property  | Value                                |+-----------+--------------------------------------+| id        | 5a4dce08-3205-4c4e-add6-fda12ff2bffb || name      | Cinder-ceph-BACKUP                   || volume_id | 4b50e431-e1b2-4e4a-8a78-e224a4112a0b |+-----------+--------------------------------------+[root@controller ~(keystone_admin)]# cinder backup-list #查看备份列表+--------------------------------------+--------------------------------------+-----------+--------------------+------+--------------+-----------+----------------------------------+| ID                                   | Volume ID                            | Status    | Name               | Size | Object Count | Container | User ID                          |+--------------------------------------+--------------------------------------+-----------+--------------------+------+--------------+-----------+----------------------------------+| 5a4dce08-3205-4c4e-add6-fda12ff2bffb | 4b50e431-e1b2-4e4a-8a78-e224a4112a0b | available | Cinder-ceph-BACKUP | 10   | 0            | backups   | d4e27e0e22864b6caaa6c9b7f9252f19 |+--------------------------------------+--------------------------------------+-----------+--------------------+------+--------------+-----------+----------------------------------+[root@node-1 ~]# rbd ls -p backups #ceph节点中查看已经能看到这个备份的卷了volume-4b50e431-e1b2-4e4a-8a78-e224a4112a0b.backup.5a4dce08-3205-4c4e-add6-fda12ff2bffb</code></pre><h1><span id="九-nova和ceph对接">九、Nova和Ceph对接</span></h1><h2><span id="1-配置-nova-附加-ceph-rbd-块设备">1、配置 NOVA 附加 CEPH RBD 块设备</span></h2><p>为了附加 Cinder 设备（普通块或从卷启动），您必须告诉 Nova（和 libvirt）在附加设备时要引用哪个用户和 UUID。在与 Ceph 集群连接和验证时，libvirt 将引用此用户。</p><pre><code>[libvirt]...rbd_user = cinderrbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337</code></pre><p>Nova 临时后端也使用这两个标志。</p><h1><span id="2-配置-nova">2、配置 NOVA</span></h1><p>为了将所有虚拟机直接引导到 Ceph 中，您必须为 Nova 配置临时后端。<br>建议在 Ceph 配置文件中启用 RBD 缓存（自 Giant 以来默认启用）。此外，启用管理套接字在故障排除时带来了很多好处。每个使用 Ceph 块设备的虚拟机都有一个套接字将有助于调查性能和&#x2F;或错误行为。<br>这个套接字可以像这样访问：</p><pre><code>ceph daemon /var/run/ceph/ceph-client.cinder.19195.32310016.asok help</code></pre><p>现在在每个计算节点上编辑 Ceph 配置文件：</p><pre><code>[client]    rbd cache = true    rbd cache writethrough until flush = true    admin socket = /var/run/ceph/guests/$cluster-$type.$id.$pid.$cctid.asok    log file = /var/log/qemu/qemu-guest-$pid.log    rbd concurrent management ops = 20</code></pre><p>配置这些路径的权限：</p><pre><code>mkdir -p /var/run/ceph/guests/ /var/log/qemu/chown qemu:libvirtd /var/run/ceph/guests /var/log/qemu/</code></pre><p>请注意，用户qemu和组libvirtd可能因您的系统而异。提供的示例适用于基于 RedHat 的系统。</p><blockquote><p>如果您的虚拟机已经在运行，您只需重新启动它即可获取套接字</p></blockquote><h3><span id="21-havana-and-icehouse">2.1、HAVANA AND ICEHOUSE</span></h3><p>Havana 和 Icehouse 需要补丁来实现写时复制克隆并修复图像大小和 rbd 上临时磁盘实时迁移的错误。这些在基于上游 Nova s ​​table&#x2F;havana 和 stable&#x2F;icehouse 的分支中可用。使用它们不是强制性的，但强烈建议使用它们以利用写时复制克隆功能。<br>在每个计算节点上，编辑&#x2F;etc&#x2F;nova&#x2F;nova.conf并添加：</p><pre><code>libvirt_images_type = rbdlibvirt_images_rbd_pool = vmslibvirt_images_rbd_ceph_conf = /etc/ceph/ceph.confdisk_cachemodes=&quot;network=writeback&quot;rbd_user = cinderrbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337</code></pre><p>禁用文件注入也是一个好习惯。在启动实例时，Nova 通常会尝试打开虚拟机的 rootfs。然后，Nova 将密码、ssh 密钥等值直接注入文件系统。但是，最好依靠元数据服务和 cloud-init.<br>在每个计算节点上，编辑&#x2F;etc&#x2F;nova&#x2F;nova.conf并添加：</p><pre><code>libvirt_inject_password = falselibvirt_inject_key = falselibvirt_inject_partition = -2</code></pre><p>为确保正确的实时迁移，请使用以下标志：</p><pre><code>libvirt_live_migration_flag=&quot;VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED&quot;</code></pre><h3><span id="22-juno">2.2、JUNO</span></h3><p>在 Juno 中，Ceph 块设备被移到该[libvirt]部分下。在每个计算节点上，&#x2F;etc&#x2F;nova&#x2F;nova.conf在该[libvirt] 部分下进行编辑并添加：</p><pre><code>[libvirt]images_type = rbdimages_rbd_pool = vmsimages_rbd_ceph_conf = /etc/ceph/ceph.confrbd_user = cinderrbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337disk_cachemodes=&quot;network=writeback&quot;</code></pre><p>禁用文件注入也是一个好习惯。在启动实例时，Nova 通常会尝试打开虚拟机的 rootfs。然后，Nova 将密码、ssh 密钥等值直接注入文件系统。但是，最好依靠元数据服务和 cloud-init.<br>在每个计算节点上，编辑&#x2F;etc&#x2F;nova&#x2F;nova.conf并在部分下添加以下内容[libvirt]：</p><pre><code>inject_password = falseinject_key = falseinject_partition = -2</code></pre><p>为确保正确的实时迁移，请使用以下标志（在[libvirt]部分下）：</p><pre><code>live_migration_flag=&quot;VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED&quot;</code></pre><h3><span id="33-kilo">3.3、KILO</span></h3><p>启用对虚拟机临时根磁盘的丢弃支持：</p><pre><code>[libvirt]......hw_disk_discard = unmap # enable discard support (be careful of performance)</code></pre><h2><span id="3-实践">3、实践</span></h2><pre><code>[root@controller ~(keystone_admin)]# openstack compute service list #查看nova服务+----+----------------+-----------------------+----------+---------+-------+----------------------------+| ID | Binary         | Host                  | Zone     | Status  | State | Updated At                 |+----+----------------+-----------------------+----------+---------+-------+----------------------------+|  2 | nova-conductor | localhost.localdomain | internal | enabled | up    | 2023-03-20T13:37:01.000000 ||  3 | nova-scheduler | localhost.localdomain | internal | enabled | up    | 2023-03-20T13:36:58.000000 ||  5 | nova-compute   | localhost.localdomain | nova     | enabled | up    | 2023-03-20T13:37:03.000000 |+----+----------------+-----------------------+----------+---------+-------+----------------------------+[root@controller ~(keystone_admin)]# cat /etc/nova/nova.conf  | grep -v &quot;#&quot; | grep -v &quot;^$&quot; #修改配置文件[DEFAULT]instance_usage_audit_period=hourrootwrap_config=/etc/nova/rootwrap.confcompute_driver=libvirt.LibvirtDriverallow_resize_to_same_host=Truevif_plugging_is_fatal=Truevif_plugging_timeout=300force_raw_images=Truereserved_host_memory_mb=512cpu_allocation_ratio=16.0ram_allocation_ratio=1.5instance_usage_audit=Trueheal_instance_info_cache_interval=60host=localhost.localdomainmetadata_host=192.168.187.204ssl_only=Falsestate_path=/var/lib/novareport_interval=10service_down_time=60enabled_apis=osapi_compute_listen=0.0.0.0osapi_compute_listen_port=8774osapi_compute_workers=4debug=Falselog_dir=/var/log/novatransport_url=rabbit://guest:guest@192.168.187.204:5672/volume_api_class=nova.volume.cinder.API[api]auth_strategy=keystoneuse_forwarded_for=False[api_database]connection=mysql+pymysql://nova_api:58a6947c5d08492f@192.168.187.204/nova_api[barbican][cache][cinder][compute][conductor]workers=4[console][consoleauth][cors][database]connection=mysql+pymysql://nova:58a6947c5d08492f@192.168.187.204/nova[devices][ephemeral_storage_encryption][filter_scheduler]host_subset_size=1max_io_ops_per_host=8max_instances_per_host=50available_filters=nova.scheduler.filters.all_filtersenabled_filters=RetryFilter,AvailabilityZoneFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilterweight_classes=nova.scheduler.weights.all_weighers[glance]api_servers=http://192.168.187.204:9292[guestfs][healthcheck][hyperv][ironic][key_manager]backend=nova.keymgr.conf_key_mgr.ConfKeyManager[keystone][keystone_authtoken]www_authenticate_uri=http://192.168.187.204:5000/auth_type=passwordauth_url=http://192.168.187.204:5000username=novapassword=1ccc1cc150394fc5user_domain_name=Defaultproject_name=servicesproject_domain_name=Default[libvirt]virt_type=kvm #虚拟化类型，可写qemuinject_password=False #注入相关inject_key=False #注入相关inject_partition=-2live_migration_uri=qemu+ssh://nova_migration@%s/system?keyfile=/etc/nova/migration/identity #热迁移相关cpu_mode=none #CPU模式images_type=rbd #镜像类型images_rbd_pool=vms #ceph的存储池images_rbd_ceph_conf = /etc/ceph/ceph.conf #ceph的配置文件rbd_user=cinder #使用的用户rbd_secret_uuid=617ac927-1a98-444f-b3b4-1f682d7bfd7a #密钥[metrics][mks][neutron]ovs_bridge=br-intdefault_floating_pool=publicextension_sync_interval=600service_metadata_proxy=Truemetadata_proxy_shared_secret=7c8ad972562c4094timeout=30auth_type=v3passwordauth_url=http://192.168.187.204:5000/v3project_name=servicesproject_domain_name=Defaultusername=neutronuser_domain_name=Defaultpassword=84633da1829243efregion_name=RegionOne[notifications]notify_on_state_change=vm_and_task_state[osapi_v21][oslo_concurrency]lock_path=/var/lib/nova/tmp[oslo_messaging_amqp][oslo_messaging_kafka][oslo_messaging_notifications]driver=messagingv2[oslo_messaging_rabbit]ssl=False[oslo_middleware][oslo_policy]policy_file=/etc/nova/policy.json[pci][placement]auth_type=passwordauth_url=http://192.168.187.204:5000/v3project_name=servicesproject_domain_name=Defaultusername=placementuser_domain_name=Defaultpassword=1ccc1cc150394fc5region_name=RegionOne[powervm][privsep][profiler][quota][rdp][remote_debug][scheduler]driver=filter_schedulermax_attempts=3workers=2[serial_console][service_user][spice]enabled=False[upgrade_levels][vault][vendordata_dynamic_auth]project_domain_name=Defaultuser_domain_name=Default[vmware][vnc]enabled=Trueserver_listen=0.0.0.0server_proxyclient_address=192.168.187.204novncproxy_base_url=http://192.168.187.204:6080/vnc_auto.htmlnovncproxy_host=0.0.0.0novncproxy_port=6080auth_schemes=none[workarounds]enable_numa_live_migration=False[wsgi]api_paste_config=api-paste.ini[xenserver][xvp][zvm][root@controller ~(keystone_admin)]# systemctl restart openstack-nova-compute[root@controller ~(keystone_admin)]# systemctl status  openstack-nova-compute● openstack-nova-compute.service - OpenStack Nova Compute Server   Loaded: loaded (/usr/lib/systemd/system/openstack-nova-compute.service; enabled; vendor preset: disabled)   Active: active (running) since Mon 2023-03-20 09:46:53 EDT; 6s ago Main PID: 33330 (nova-compute)    Tasks: 22   CGroup: /system.slice/openstack-nova-compute.service           └─33330 /usr/bin/python2 /usr/bin/nova-computeMar 20 09:46:48 controller systemd[1]: Starting OpenStack Nova Compute Server...Mar 20 09:46:53 controller systemd[1]: Started OpenStack Nova Compute Server.[root@controller ~(keystone_admin)]# tail -n 40 /var/log/nova/nova-compute.log   &lt;guest&gt;    &lt;os_type&gt;hvm&lt;/os_type&gt;    &lt;arch name=&#39;x86_64&#39;&gt;      &lt;wordsize&gt;64&lt;/wordsize&gt;      &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt;      &lt;machine maxCpus=&#39;240&#39;&gt;pc-i440fx-rhel7.6.0&lt;/machine&gt;      &lt;machine canonical=&#39;pc-i440fx-rhel7.6.0&#39; maxCpus=&#39;240&#39;&gt;pc&lt;/machine&gt;      &lt;machine maxCpus=&#39;240&#39;&gt;pc-i440fx-rhel7.0.0&lt;/machine&gt;      &lt;machine maxCpus=&#39;384&#39;&gt;pc-q35-rhel7.6.0&lt;/machine&gt;      &lt;machine canonical=&#39;pc-q35-rhel7.6.0&#39; maxCpus=&#39;384&#39;&gt;q35&lt;/machine&gt;      &lt;machine maxCpus=&#39;240&#39;&gt;rhel6.3.0&lt;/machine&gt;      &lt;machine maxCpus=&#39;240&#39;&gt;rhel6.4.0&lt;/machine&gt;      &lt;machine maxCpus=&#39;240&#39;&gt;rhel6.0.0&lt;/machine&gt;      &lt;machine maxCpus=&#39;240&#39;&gt;pc-i440fx-rhel7.5.0&lt;/machine&gt;      &lt;machine maxCpus=&#39;240&#39;&gt;pc-i440fx-rhel7.1.0&lt;/machine&gt;      &lt;machine maxCpus=&#39;240&#39;&gt;pc-i440fx-rhel7.2.0&lt;/machine&gt;      &lt;machine maxCpus=&#39;255&#39;&gt;pc-q35-rhel7.3.0&lt;/machine&gt;      &lt;machine maxCpus=&#39;240&#39;&gt;rhel6.5.0&lt;/machine&gt;      &lt;machine maxCpus=&#39;384&#39;&gt;pc-q35-rhel7.4.0&lt;/machine&gt;      &lt;machine maxCpus=&#39;240&#39;&gt;rhel6.6.0&lt;/machine&gt;      &lt;machine maxCpus=&#39;240&#39;&gt;rhel6.1.0&lt;/machine&gt;      &lt;machine maxCpus=&#39;240&#39;&gt;rhel6.2.0&lt;/machine&gt;      &lt;machine maxCpus=&#39;240&#39;&gt;pc-i440fx-rhel7.3.0&lt;/machine&gt;      &lt;machine maxCpus=&#39;240&#39;&gt;pc-i440fx-rhel7.4.0&lt;/machine&gt;      &lt;machine maxCpus=&#39;384&#39;&gt;pc-q35-rhel7.5.0&lt;/machine&gt;      &lt;domain type=&#39;qemu&#39;/&gt;    &lt;/arch&gt;    &lt;features&gt;      &lt;cpuselection/&gt;      &lt;deviceboot/&gt;      &lt;disksnapshot default=&#39;on&#39; toggle=&#39;no&#39;/&gt;      &lt;acpi default=&#39;on&#39; toggle=&#39;yes&#39;/&gt;      &lt;apic default=&#39;on&#39; toggle=&#39;no&#39;/&gt;    &lt;/features&gt;  &lt;/guest&gt;&lt;/capabilities&gt;2023-03-20 09:46:53.684 33330 INFO nova.compute.manager [req-a4053d12-7bbf-44a5-987c-45f93434df6d - - - - -] Looking for unclaimed instances stuck in BUILDING status for nodes managed by this host2023-03-20 09:46:57.288 33330 INFO nova.virt.libvirt.host [req-a4053d12-7bbf-44a5-987c-45f93434df6d - - - - -] kernel doesn&#39;t support AMD SEV</code></pre><h1><span id="十-nova和ceph对接测试">十、Nova和Ceph对接测试</span></h1><pre><code>[root@controller ~(keystone_admin)]# lltotal 20804-rw-------. 1 root root     1260 Mar 19 19:03 anaconda-ks.cfg-rw-r--r--  1 root root 21233664 Mar 20 09:54 cirros-0.6.1-x86_64-disk.img-rw-r--r--  1 root root       40 Mar 19 22:24 client.cinder.key-rw-------  1 root root      375 Mar 19 11:30 keystonerc_admin-rw-------  1 root root      320 Mar 19 11:30 keystonerc_demo-rw-------  1 root root    51811 Mar 19 11:25 train-for-ceph.txt[root@controller ~(keystone_admin)]# file cirros-0.6.1-x86_64-disk.img cirros-0.6.1-x86_64-disk.img: QEMU QCOW Image (v3), 117440512 bytes[root@controller ~(keystone_admin)]# qemu-img info cirros-0.6.1-x86_64-disk.img image: cirros-0.6.1-x86_64-disk.imgfile format: qcow2virtual size: 112M (117440512 bytes)disk size: 20Mcluster_size: 65536Format specific information:    compat: 1.1    lazy refcounts: false    refcount bits: 16    corrupt: false[root@controller ~(keystone_admin)]# qemu-img convert -f qcow2 -O raw cirros-0.6.1-x86_64-disk.img cirros-0.6.1-x86_64-disk.raw[root@controller ~(keystone_admin)]# lltotal 42388-rw-------. 1 root root      1260 Mar 19 19:03 anaconda-ks.cfg-rw-r--r--  1 root root  21233664 Mar 20 09:54 cirros-0.6.1-x86_64-disk.img-rw-r--r--  1 root root 117440512 Mar 20 09:56 cirros-0.6.1-x86_64-disk.raw-rw-r--r--  1 root root        40 Mar 19 22:24 client.cinder.key-rw-------  1 root root       375 Mar 19 11:30 keystonerc_admin-rw-------  1 root root       320 Mar 19 11:30 keystonerc_demo-rw-------  1 root root     51811 Mar 19 11:25 train-for-ceph.txt[root@controller ~(keystone_admin)]# qemu-img info cirros-0.6.1-x86_64-disk.raw image: cirros-0.6.1-x86_64-disk.rawfile format: rawvirtual size: 112M (117440512 bytes)disk size: 21M[root@controller ~(keystone_admin)]# glance image-create --name cirros-0.6.1 --disk-format raw --container-format bare --file /root/cirros-0.6.1-x86_64-disk.raw --progress[=============================&gt;] 100%+------------------+----------------------------------------------------------------------------------+| Property         | Value                                                                            |+------------------+----------------------------------------------------------------------------------+| checksum         | 1352196d1db841ad1931906db5e76ff6                                                 || container_format | bare                                                                             || created_at       | 2023-03-20T13:58:37Z                                                             || direct_url       | rbd://b8e58b30-4568-4032-a9f4-837ed3fa9529/images/e1d50811-668a-440b-            ||                  | b1b0-aa076ec74842/snap                                                           || disk_format      | raw                                                                              || id               | e1d50811-668a-440b-b1b0-aa076ec74842                                             || min_disk         | 0                                                                                || min_ram          | 0                                                                                || name             | cirros-0.6.1                                                                     || os_hash_algo     | sha512                                                                           || os_hash_value    | 84914dd1e9d4b01d7411b716cab927eeaf5c33f61fe6b72d6cee453b825dbea96423966bedebe58d ||                  | d9a8e58adfad012fcde8cd26a4c74ec29b5c540d1a0b8c11                                 || os_hidden        | False                                                                            || owner            | 536255bf80df451ebcdf8277d214661d                                                 || protected        | False                                                                            || size             | 117440512                                                                        || status           | active                                                                           || tags             | []                                                                               || updated_at       | 2023-03-20T13:58:41Z                                                             || virtual_size     | Not available                                                                    || visibility       | shared                                                                           |+------------------+----------------------------------------------------------------------------------+</code></pre><p><img src="/images/Ceph/Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/13.jpg"><br><img src="/images/Ceph/Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/14.jpg"></p><pre><code>2023-03-25 17:35:27.694 22329 ERROR nova.compute.manager [req-0bdcfc1d-e805-465b-9a18-aad1bc069f62 f8d44636051f4156ba3c737866bebf9c 9332506b38f4449285df02e63ad1091d - default default] [instance: 1ed59506-a58a-4138-bc32-325aa67d4563] Failed to build and run instance: libvirtError: internal error: qemu unexpectedly closed the monitor: 2023-03-25T09:34:55.576543Z qemu-kvm: cannot set up guest memory &#39;pc.ram&#39;: Cannot allocate memory</code></pre><blockquote><p>KVM启动报错qemu-kvm: cannot set up guest memory ‘pc.ra</p></blockquote><pre><code>nova-conductor.log 报错：ERROR nova.scheduler.utils [req-9880cb62-7a70-41aa-b6c0-db4ec5333e98 53a1cf0ad2924532aa4b7b0750dec282 0ab2dbde4f754b699e22461426cd0774 - - -] [instance: 36bb1220-f295-4205-ba2e-6e41f8b134b9] Error from last host: xiandian (node xiandian): [u&#39;Traceback (most recent call last):\n&#39;, u&#39;  File &quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py&quot;, line 1926, in _do_build_and_run_instance\n    filter_properties)\n&#39;, u&#39;  File &quot;/usr/lib/python2.7/site-packages/nova/compute/manager.py&quot;, line 2116, in _build_and_run_instance\n    instance_uuid=instance.uuid, reason=six.text_type(e))\n&#39;, u&quot;RescheduledException: Build of instance 36bb1220-f295-4205-ba2e-6e41f8b134b9 was re-scheduled: internal error: process exited while connecting to monitor: 2019-05-20T17:38:19.473598Z qemu-kvm: cannot set up guest memory &#39;pc.ram&#39;: Cannot allocate memory\n\n&quot;]错误信息：无法分配内存处理方法：1.增加计算节点内存 2.修改内核参数在这使用修改内核参数的方法：先看下主机可以分配多少内存[root@compute ~]# sysctl -a | grep overcommitvm.nr_overcommit_hugepages = 0vm.overcommit_kbytes = 0vm.overcommit_memory = 0vm.overcommit_ratio = 50 ### 内核参数overcommit_memory 它是 内存分配策略 可选值：0、1、2。0， 表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存，内存申请允许；否则，内存申请失败，并把错误返回给应用进程。1， 表示内核允许分配所有的物理内存，而不管当前的内存状态如何。2， 表示内核允许分配超过所有物理内存和交换空间总和的内存什么是Overcommit和OOMLinux对大部分申请内存的请求都回复&quot;yes&quot;，以便能跑更多更大的程序。因为申请内存后，并不会马上使用内存。这种技术叫做Overcommit。当linux发现内存不足时，会发生OOM killer(OOM=out-of-memory)。它会选择杀死一些进程(用户态进程，不是内核线程)，以便释放内存。当oom-killer发生时，linux会选择杀死哪些进程？选择进程的函数是oom_badness函数(在mm/oom_kill.c中)，该函数会计算每个进程的点数(0~1000)。点数越高，这个进程越有可能被杀死。每个进程的点数跟oom_score_adj有关，而且oom_score_adj可以被设置(-1000最低，1000最高)。解决方法三种方法：将vm.overcommit_memory 设为1即可1.编辑/etc/sysctl.conf ，改vm.overcommit_memory=1，然后sysctl -p 使配置文件生效2.sysctl vm.overcommit_memory=13.echo 1 &gt; /proc/sys/vm/overcommit_memory转载于:https://blog.51cto.com/9103824/2397175</code></pre><h1><span id="十一-重启-openstack">十一、重启 OPENSTACK</span></h1><p>要激活 Ceph 块设备驱动程序并将块设备池名称加载到配置中，您必须重新启动 OpenStack。因此，对于基于 Debian 的系统，在适当的节点上执行这些命令：</p><pre><code>sudo glance-control api restartsudo service nova-compute restartsudo service cinder-volume restartsudo service cinder-backup restart</code></pre><p>对于基于 Red Hat 的系统执行：</p><pre><code>sudo service openstack-glance-api restartsudo service openstack-nova-compute restartsudo service openstack-cinder-volume restartsudo service openstack-cinder-backup restart</code></pre><p>一旦 OpenStack 启动并运行，您应该能够创建一个卷并从中引导。</p><h1><span id="十二-从块设备启动">十二、从块设备启动</span></h1><pre><code>cinder create --image-id &#123;id of image&#125; --display-name &#123;name of volume&#125; &#123;size of volume&#125;</code></pre><p>请注意，图像必须是 RAW 格式。您可以使用qemu-img将一种格式转换为另一种格式。例如：</p><pre><code>qemu-img convert -f &#123;source-format&#125; -O &#123;output-format&#125; &#123;source-filename&#125; &#123;output-filename&#125;qemu-img convert -f qcow2 -O raw precise-cloudimg.img precise-cloudimg.raw</code></pre><p>当 Glance 和 Cinder 都使用 Ceph 块设备时，镜像是写时复制克隆，因此可以快速创建新卷。在 OpenStack 仪表板中，您可以通过执行以下步骤从该卷启动：</p><ol><li>启动一个新实例。</li><li>选择与写时复制克隆关联的图像。</li><li>选择“从卷启动”。</li><li>选择您创建的卷。</li></ol><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
          <category> OpenStack </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> OpenStack </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>openstack-cinder对接两个ceph后端配置</title>
      <link href="/2023/04/16/Ceph/15,Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/OpenStack-Cinder%E5%AF%B9%E6%8E%A5%E4%B8%A4%E4%B8%AAceph%E5%90%8E%E7%AB%AF%E9%85%8D%E7%BD%AE/"/>
      <url>/2023/04/16/Ceph/15,Ceph%E4%B8%8EOpenStack%E5%AF%B9%E6%8E%A5/OpenStack-Cinder%E5%AF%B9%E6%8E%A5%E4%B8%A4%E4%B8%AAceph%E5%90%8E%E7%AB%AF%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-%E9%9C%80%E6%B1%82">一、需求</a></li><li><a href="#%E4%BA%8C-%E7%8E%AF%E5%A2%83">二、环境</a></li><li><a href="#%E4%B8%89-cinder%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">三、cinder配置文件</a></li><li><a href="#%E5%9B%9B-%E9%85%8D%E7%BD%AEceph-cinderconf">四、配置ceph-cinder.conf</a></li><li><a href="#%E4%BA%94-nova%E8%AE%A1%E7%AE%97%E8%8A%82%E7%82%B9secret%E9%85%8D%E7%BD%AE">五、nova计算节点secret配置</a></li><li><a href="#%E5%85%AD-%E5%85%B3%E4%BA%8E%E5%8D%B7%E7%B1%BB%E5%9E%8B%E4%B8%8E%E5%AD%98%E5%82%A8%E5%90%8E%E7%AB%AF%E7%9A%84%E4%B8%80%E7%82%B9%E8%AE%A4%E8%AF%86">六、关于卷类型与存储后端的一点认识</a></li><li><a href="#%E4%B8%83-%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3">七、参考文档</a></li></ul><!-- tocstop --><h1><span id="一-需求">一、需求</span></h1><p>需要做卷迁移的工作，从一个ceph集群迁移到另一个集群，因此需要配置两个ceph后端。由此开展后续工作，将配置过程及出现的问题做一记录。<br>另外两套ceph后端的访问用户都是cinder用户，网上找的资料均为两个用户，当为同一用户时，需要增加一些额外配置，特此说明</p><h1><span id="二-环境">二、环境</span></h1><p>服务器：centos 7.9<br>openstack cinder：stein<br>ceph：nautilus</p><h1><span id="三-cinder配置文件">三、cinder配置文件</span></h1><p>在cinder-volume所在节点，修改&#x2F;etc&#x2F;cinder&#x2F;cinder.conf文件，主要修改内容如下：</p><pre><code>enabled_backends = rbd,rbd232  # 开启两个后端# 原本配置的后端[rbd] volume_driver = cinder.volume.drivers.rbd.RBDDriverrbd_pool = volumesrbd_cluster_name = cephrbd_ceph_conf = /etc/ceph/ceph-cinder.conf  # rbd_flatten_volume_from_snapshot = falserbd_max_clone_depth = 5rbd_store_chunk_size = 4rados_connect_timeout = -1glance_api_version = 2rbd_user = cinderrbd_secret_uuid = xxxx-xxxx  # 用来配置libvirt访问ceph的秘钥，在nova计算节点通过名称uuidgen生成随机字符串max_over_subscription_ratio = 20volume_backend_name = rbd# 新增后端[rbd232]volume_driver = cinder.volume.drivers.rbd.RBDDriverrbd_pool = volumesrbd_cluster_name = cephrbd_ceph_conf = /etc/ceph/232ceph-cinder.conf  # 新增后端的配置文件rbd_flatten_volume_from_snapshot = falserbd_max_clone_depth = 5rbd_store_chunk_size = 4rados_connect_timeout = -1glance_api_version = 2rbd_user = cinderrbd_secret_uuid = yyyy-yyyy  # 用来配置libvirt访问ceph的秘钥，在nova计算节点通过名称uuidgen生成随机字符串，与上边是不一样的max_over_subscription_ratio = 20volume_backend_name = rbd232  # 名称自定义rbd_keyring_conf = /etc/ceph/232ceph-cinder.client.cinder.keyring</code></pre><p>&#x2F;etc&#x2F;ceph&#x2F;232ceph-cinder.client.cinder.keyring该文件是访问ceph集群需要的秘钥环，具体内容如下：</p><pre><code>[client.cinder]    key = AQCV4n5j60KACxAA0ZX1s/ABTrWFcJN5Tnun9w==    caps mon = &quot;allow *&quot;    caps osd = &quot;allow *&quot;</code></pre><p>该内容是在ceph集群通过命令ceph auth get client.cinder得到，具体由ceph人员提供。两个存储后端分别拥有各自的keyring文件，二者不可混用。</p><p>这里配置rbd_keyring_conf在使用迁移卷的功能时需要，否则keyring会是None，导致连不上ceph集群。原有的存储后端没有配置该项是因为会通过rbd_cluster_name、rbd_user 去找默认的keyring文件，即ceph.client.cinder.keyring文件</p><p>报错栈如下：</p><pre><code>2023-02-03 15:28:58.142 29763 ERROR os_brick.initiator.linuxrbd PermissionError: [errno 1] error connecting to the cluster2023-02-03 15:28:58.142 29763 ERROR os_brick.initiator.linuxrbd 2023-02-03 15:28:58.144 29763 DEBUG os_brick.initiator.connectors.rbd [req-a13ac8ca-da18-46e1-b17c-104476292e75 bf3ab6a404d54c1092bf4dfdc7d1fe95 3b8e0f7a3d194d4083ce5226749ae472 - default default] &lt;== connect_volume: exception (73ms) BrickException(u&#39;Error connecting to dms cluster.&#39;,) trace_logging_wrapper /usr/lib/python2.7/site-packages/os_brick/utils.py:1562023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager [req-a13ac8ca-da18-46e1-b17c-104476292e75 bf3ab6a404d54c1092bf4dfdc7d1fe95 3b8e0f7a3d194d4083ce5226749ae472 - default default] Failed to copy volume b4511a73-1c79-4f7b-a9fc-1255af36e38d to 07ddc83b-232b-43ba-9aed-1d6784306b50: BrickException: Error connecting to dms cluster.2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager Traceback (most recent call last):2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager   File &quot;/usr/lib/python2.7/site-packages/cinder/volume/manager.py&quot;, line 2228, in _migrate_volume_generic2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager     self._copy_volume_data(ctxt, volume, new_volume, remote=&#39;dest&#39;)2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager   File &quot;/usr/lib/python2.7/site-packages/cinder/volume/manager.py&quot;, line 2108, in _copy_volume_data2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager     attach_encryptor=attach_encryptor)2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager   File &quot;/usr/lib/python2.7/site-packages/cinder/volume/manager.py&quot;, line 2039, in _attach_volume2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager     attach_info = self._connect_device(conn)2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager   File &quot;/usr/lib/python2.7/site-packages/cinder/volume/manager.py&quot;, line 2003, in _connect_device2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager     vol_handle = connector.connect_volume(conn[&#39;data&#39;])2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager   File &quot;/usr/lib/python2.7/site-packages/os_brick/utils.py&quot;, line 150, in trace_logging_wrapper2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager     result = f(*args, **kwargs)2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager   File &quot;/usr/lib/python2.7/site-packages/os_brick/initiator/connectors/rbd.py&quot;, line 204, in connect_volume2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager     rbd_handle = self._get_rbd_handle(connection_properties)2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager   File &quot;/usr/lib/python2.7/site-packages/os_brick/initiator/connectors/rbd.py&quot;, line 123, in _get_rbd_handle2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager     rbd_cluster_name=str(cluster_name))2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager   File &quot;/usr/lib/python2.7/site-packages/os_brick/initiator/linuxrbd.py&quot;, line 60, in __init__2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager     self.client, self.ioctx = self.connect()2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager   File &quot;/usr/lib/python2.7/site-packages/os_brick/initiator/linuxrbd.py&quot;, line 89, in connect2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager     raise exception.BrickException(message=msg)2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager BrickException: Error connecting to dms cluster.2023-02-03 15:28:58.145 29763 ERROR cinder.volume.manager </code></pre><p>源码简单分析如下，报错的最后落脚点在os_brick&#x2F;initiator&#x2F;linuxrbd.py，也就是下边的代码，红框圈出来的是配置文件，是个临时路径，在&#x2F;tmp&#x2F;目录下</p><p><img src="/images/Ceph/Openstack-cinder%E5%AF%B9%E6%8E%A5%E4%B8%A4%E4%B8%AAceph%E5%90%8E%E7%AB%AF%E9%85%8D%E7%BD%AE/1.jpg"></p><p>根据报错信息，往上定位该文件生成位置os_brick&#x2F;initiator&#x2F;connectors&#x2F;rbd.py</p><p><img src="/images/Ceph/Openstack-cinder%E5%AF%B9%E6%8E%A5%E4%B8%A4%E4%B8%AAceph%E5%90%8E%E7%AB%AF%E9%85%8D%E7%BD%AE/2.jpg"></p><p>图上的conf就是生成的临时配置文件路径，其中的keyring是None，至此想到rbd_keyring_conf这个参数可能是在这里生效的，配置后不出所料，迁移卷时不再报错。再深入的代码没有分析。</p><h1><span id="四-配置ceph-cinderconf">四、配置ceph-cinder.conf</span></h1><p>对新的后端配置&#x2F;etc&#x2F;ceph&#x2F;232ceph-cinder.conf，内容如下：</p><pre><code>[global]fsid = xxxxxxxpublic_network = 192.168.111.0/24cluster_network = 192.168.1111.0/24mon_initial_members = executormon_host = 192.168.111.112auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxmon_max_pg_per_osd = 1000osd_crush_update_on_start = false[client.cinder]keyring = /etc/ceph/232ceph-cinder.client.cinder.keyring</code></pre><p>核心点在[client.cinder]配置项，这样才会在指定的文件中拿到keyring从而正确访问集群。否则会报错如下：<br>Error connecting to ceph cluster.: PermissionError: [errno 1] error cluster 权限问题导致无法连接ceph集群。<br>这里的配置和cinder.conf中的配置二者起作用的方式不同，因此均需配置<br>查看存储后端<br>重启cinder-volume，命令</p><pre><code>systemctl restart openstack-cinder-volume</code></pre><p>查看后端是否增加，命令cinder get-pools 发现已经是两个了</p><p><img src="/images/Ceph/Openstack-cinder%E5%AF%B9%E6%8E%A5%E4%B8%A4%E4%B8%AAceph%E5%90%8E%E7%AB%AF%E9%85%8D%E7%BD%AE/3.jpg"></p><h1><span id="五-nova计算节点secret配置">五、nova计算节点secret配置</span></h1><p>为了使虚拟机能够连接使用新增的后端存储，需要配置libvirt访问ceph的秘钥。</p><p>在计算节点使用命令</p><p>uuigen生成秘钥，如yyyy-yyyy，注意和cinder.conf文件中的rbd_secret_uuid保持一致</p><p>创建secret.xml文件，文件名随便起，输入如下内容注册secret_uuid</p><pre><code>virsh secret-define --file secret.xml</code></pre><p>设置secret的value</p><pre><code>virsh secret-set-value --secret yyyy-yyyy xmeIHtliXhOrERAAw0icYbcwrilE5hL8bCNefw==</code></pre><p>其中的xmeIHtliXhOrERAAw0icYbcwrilE5hL8bCNefw&#x3D;&#x3D;是从 &#x2F;etc&#x2F;ceph&#x2F;232ceph-cinder.client.cinder.keyring获取得到的</p><p>最后通过命令virsh secret-get-value yyyy-yyyy查看是否设置成功,此时设置该秘钥的计算节点上的虚拟机就能正常连接新存储后端了,如果有更多计算节点，使用同样方法均设置后即可</p><h1><span id="六-关于卷类型与存储后端的一点认识">六、关于卷类型与存储后端的一点认识</span></h1><p>在配置好上述两个存储后端后，测试时发现使用之前的卷类型创建卷，也会跑到rbd232环境中创建卷，至此研究了一下卷类型和存储后端的关系。<br>首先要看下<br>cinder.conf配置文件的中的几个参数</p><pre><code>enabled_backends：要使用的后端名称列表，上边例子中的 rbd,rbd232，要在文件中定义这两个后端，即[rbd]和[rbd232]default_volume_type：默认使用的卷类型（volume type），也就是原生页面中，管理员-卷-卷类型 中定义的。配置文件设置该参数，也要在实际创建该卷类型volume_backend_name：该参数所处位置参考上边的cinder.conf文件。解释，驱动程序的后端名称，可以和enabled_backends中不一致，自定义随便起</code></pre><blockquote><p>出现上边说的问题是因为原本存在的卷类型rbd，没有实际关联特定存储后端，因为只有一个存储后端，scheduler调度时，只能选择这一个后端进行卷的创建，当有两个存储后端时，调度器就会在两个中调度更适合的，然后发现rbd232后端更适合创建，就全部都建到了232上。</p></blockquote><blockquote><p>当我考虑给rbd卷类型添加扩展规格或元数据时，发现无法修改，提示卷类型状态是 “in-use”，因为之前创建的卷正在使用中，不能修改。</p></blockquote><blockquote><p>问题解决方法参考：新建一个卷类型，如rbd222，然后将其添加扩展规格（volume_backend_name&#x3D;rbd，就是这个参数将卷类型与实际的存储后端关联起来了，这里写rbd是因为之前的存储后端定义的volume_backend_name就是rbd，参考上文中的cinder.conf中[rbd]区域），最后修改cinder.conf中default_volume_type&#x3D;rbd222，重启</p></blockquote><pre><code>systemctl restart openstack-cinder-api  # 修改cinder.conf中的default_volume_type需要重启api才能生效systemctl restart openstack-cinder-volumesystemctl restart openstack-cinder-scheduler</code></pre><p>这样选择创建卷时选择rbd222卷类型，创建的卷就都在原来的存储上了。<br>关联过程图示</p><p><img src="/images/Ceph/Openstack-cinder%E5%AF%B9%E6%8E%A5%E4%B8%A4%E4%B8%AAceph%E5%90%8E%E7%AB%AF%E9%85%8D%E7%BD%AE/4.jpg"><br><img src="/images/Ceph/Openstack-cinder%E5%AF%B9%E6%8E%A5%E4%B8%A4%E4%B8%AAceph%E5%90%8E%E7%AB%AF%E9%85%8D%E7%BD%AE/5.jpg"></p><blockquote><p>通过以上知道，创建卷类型后，要及时关联存储后端，即便只有一个存储后端，也要进行关联，防止后期出现更多后端时，导致不必要的麻烦</p></blockquote><h1><span id="七-参考文档">七、参考文档</span></h1><blockquote><p><a href="https://blog.csdn.net/HYESC/article/details/128863523">openstack cinder对接两个ceph后端配置</a></p></blockquote><ol><li><a href="https://medium.com/walmartglobaltech/deploying-cinder-with-multiple-ceph-cluster-backends-2cd90d64b10">https://medium.com/walmartglobaltech/deploying-cinder-with-multiple-ceph-cluster-backends-2cd90d64b10</a></li><li><a href="https://www.yisu.com/zixun/252458.html">https://www.yisu.com/zixun/252458.html</a></li><li><a href="https://docs.openstack.org/cinder/stein/configuration/block-storage/drivers/ceph-rbd-volume-driver.html">https://docs.openstack.org/cinder/stein/configuration/block-storage/drivers/ceph-rbd-volume-driver.html</a></li><li><a href="https://blog.csdn.net/weixin_40579389/article/details/120875159">https://blog.csdn.net/weixin_40579389/article/details/120875159</a></li></ol><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
          <category> OpenStack </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> OpenStack </tag>
            
            <tag> Cinder </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KVM虚拟化进阶之虚拟网络高级配置</title>
      <link href="/2023/04/12/KVM/1.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/"/>
      <url>/2023/04/12/KVM/1.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#1-%E9%85%8D%E7%BD%AE%E7%BD%91%E5%8D%A1%E7%BB%91%E5%AE%9Abond">1、配置网卡绑定（bond）</a><ul><li><a href="#1-%E7%BB%91%E5%AE%9A%E7%BD%91%E5%8D%A1">1、绑定网卡</a><ul><li><a href="#1-bond%E7%9A%84%E6%A8%A1%E5%BC%8F">1、Bond的模式</a></li></ul></li><li><a href="#2-%E9%85%8D%E7%BD%AE%E7%BD%91%E6%A1%A5">2、配置网桥</a></li></ul></li><li><a href="#2-%E9%85%8D%E7%BD%AEvlan">2、配置Vlan</a><ul><li><a href="#1-nmcli%E6%96%B9%E6%B3%95">1、nmcli方法</a></li><li><a href="#2-%E9%80%9A%E8%BF%87%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%88%9B%E5%BB%BA">2、通过命令行创建</a></li><li><a href="#3-%E9%80%9A%E8%BF%87virt-manager%E5%88%9B%E5%BB%BA">3、通过virt-manager创建</a></li><li><a href="#4-%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%BD%BF%E7%94%A8vlan">4、虚拟机使用vlan</a></li></ul></li><li><a href="#3-%E9%85%8D%E7%BD%AE%E7%BD%91%E7%BB%9C%E8%BF%87%E6%BB%A4">3、配置网络过滤</a></li></ul><!-- tocstop --><h1><span id="1-配置网卡绑定bond">1、配置网卡绑定（bond）</span></h1><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/1.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/2.jpg"></p><h2><span id="1-绑定网卡">1、绑定网卡</span></h2><pre><code>[root@kvm ~]# ip a  #查看需要绑定的网卡，ens33和ens361: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet 192.168.187.134/24 brd 192.168.187.255 scope global noprefixroute dynamic ens33       valid_lft 1066sec preferred_lft 1066sec    inet6 fe80::1fc1:66d:29af:eabe/64 scope link noprefixroute        valid_lft forever preferred_lft forever3: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0       valid_lft forever preferred_lft forever4: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff5: ens36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:e6 brd ff:ff:ff:ff:ff:ff    inet 192.168.187.135/24 brd 192.168.187.255 scope global noprefixroute dynamic ens36       valid_lft 1783sec preferred_lft 1783sec    inet6 fe80::84be:5ac8:542c:705f/64 scope link noprefixroute        valid_lft forever preferred_lft forever[root@kvm ~]# virsh net-list #查看当前kvm网络情况 名称               状态     自动开始  持久---------------------------------------------------------- default              活动     是           是[root@kvm ~]# brctl show #查看当前宿主机网桥情况bridge name    bridge id        STP enabled    interfacesvirbr0        8000.525400ccc5d5    yes        virbr0-nic[root@kvm ~]# virsh net-dumpxml --network default  #查看default的xml文件&lt;network&gt;  &lt;name&gt;default&lt;/name&gt;  &lt;uuid&gt;8e682a63-b77a-4aad-bc25-5d4d63a2a66e&lt;/uuid&gt;  &lt;forward mode=&#39;nat&#39;&gt;    &lt;nat&gt;      &lt;port start=&#39;1024&#39; end=&#39;65535&#39;/&gt;    &lt;/nat&gt;  &lt;/forward&gt;  &lt;bridge name=&#39;virbr0&#39; stp=&#39;on&#39; delay=&#39;0&#39;/&gt;  &lt;mac address=&#39;52:54:00:cc:c5:d5&#39;/&gt;  &lt;ip address=&#39;192.168.122.1&#39; netmask=&#39;255.255.255.0&#39;&gt;    &lt;dhcp&gt;      &lt;range start=&#39;192.168.122.2&#39; end=&#39;192.168.122.254&#39;/&gt;    &lt;/dhcp&gt;  &lt;/ip&gt;&lt;/network&gt;[root@kvm ~]# virsh iface-list #查看宿主机网卡情况，没有认到新增加的网卡（ens36），需要增加ifcfg-ens36的配置文件，可以通过virt-manager进行添加 名称               状态     MAC 地址--------------------------------------------------- ens33                活动     00:0c:29:0b:57:dc lo                   活动     00:00:00:00:00:00</code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/3.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/4.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/5.jpg"></p><pre><code>[root@kvm ~]# cat /etc/sysconfig/network-scripts/ifcfg-ens36  #查看通过virt-nanager自动生成的配置文件DEVICE=&quot;ens36&quot;HWADDR=&quot;00:0c:29:0b:57:e6&quot;ONBOOT=&quot;no&quot;BOOTPROTO=&quot;dhcp&quot;[root@kvm ~]# virsh iface-list --all #可以识别到ens36网卡了 名称               状态     MAC 地址--------------------------------------------------- ens33                活动     00:0c:29:0b:57:dc ens36                活动     00:0c:29:0b:57:e6 lo                   活动     00:00:00:00:00:00 [root@kvm ~]# modprobe --first-time bonding #查看系统是否安装了网桥的模块，如果没有就自动加载，这里没有报错，意思是当前系统没有加载，但是已经通过--first-time进行加载了[root@kvm ~]# lsmod | grep bonding #查看已经正常加载bonding               157075  0 [root@kvm network-scripts]# pwd/etc/sysconfig/network-scripts[root@kvm network-scripts]# ls -lha ifcfg-ens* #查看当前网卡的配置文件-rw-r--r--. 1 root root 280 4月   2 15:41 ifcfg-ens33-rw-r--r--. 1 root root  71 4月   2 19:37 ifcfg-ens36[root@kvm network-scripts]# cp ifcfg-ens33 ifcfg-ens33-bak #备份文件[root@kvm network-scripts]# cp ifcfg-ens36 ifcfg-ens36-bak [root@kvm network-scripts]# vim ifcfg-ens33 #编辑网卡文件TYPE=EthernetBOOTPROTO=noneNAME=ens33DEVICE=ens33ONBOOT=yesMASTER=bond0SLAVE=yesNM_CONTROLLERD=no #非必须USERCTL=no #非必须[root@kvm network-scripts]# vim ifcfg-ens36 #编辑网卡文件TYPE=EthernetBOOTPROTO=noneNAME=ens36DEVICE=ens36ONBOOT=yesMASTER=bond0SLAVE=yesNM_CONTROLLERD=noUSERCTL=no [root@kvm network-scripts]# vim ifcfg-bond0 #新建bond0文件DEVICE=bond0ONBOOT=yesNM_CONTROLLERD=noUSERCTL=noBONDING_OPTS=&quot;mode=1 miimon=100 fail_over_mac=1&quot;BOOTPROTO=staticIPADDR=192.168.187.134NETMASK=255.255.255.0GATEWAY=192.168.187.1DNS1=223.5.5.5[root@kvm network-scripts]# systemctl restart network #重启网络服务，远程时谨慎！连接已成功激活（master waiting for slaves）（D-Bus 活动路径：/org/freedesktop/NetworkManager/ActiveConnection/5）[root@kvm network-scripts]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff3: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0       valid_lft forever preferred_lft forever4: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff5: ens36: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff6: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet 192.168.187.134/24 brd 192.168.187.255 scope global noprefixroute bond0       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fe0b:57dc/64 scope link        valid_lft forever preferred_lft forever[root@kvm network-scripts]# dmesg |grep bond0 #查看日志[ 2586.969003] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2586.969152] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2586.971585] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2586.979903] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2587.011775] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2635.311460] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2635.457459] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2635.590868] bond0: making interface ens33 the new active one[ 2635.596124] bond0: Enslaving ens33 as an active interface with an up link[ 2635.596466] IPv6: ADDRCONF(NETDEV_CHANGE): bond0: link becomes ready[ 2635.700504] bond0: Releasing backup interface ens33[ 2637.395032] bond0: Enslaving ens33 as a backup interface with a down link[ 2637.483876] bond0: link status definitely up for interface ens33, 1000 Mbps full duplex[ 2637.483882] bond0: making interface ens33 the new active one[ 2637.491125] bond0: first active interface up![ 2637.764573] bond0: Enslaving ens36 as a backup interface with a down link[ 2637.793398] bond0: link status definitely up for interface ens36, 1000 Mbps full duplex[root@kvm network-scripts]# cat /proc/net/bonding/bond0  #查看内存中的文件Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)Bonding Mode: fault-tolerance (active-backup) #bond模式Primary Slave: NoneCurrently Active Slave: ens33 #当前的主设备是ens33MII Status: up #主设备的当前状态MII Polling Interval (ms): 100Up Delay (ms): 0Down Delay (ms): 0Slave Interface: ens33MII Status: upSpeed: 1000 MbpsDuplex: fullLink Failure Count: 0Permanent HW addr: 00:0c:29:0b:57:dcSlave queue ID: 0Slave Interface: ens36MII Status: upSpeed: 1000 MbpsDuplex: fullLink Failure Count: 0Permanent HW addr: 00:0c:29:0b:57:e6Slave queue ID: 0</code></pre><blockquote><p>与真机环境有所不同，vmware虚拟机下给linux系统做bond0网卡配置，照这样做完后，测试发现down掉eth0后，bond0网卡ping不通，无法起到网卡备份效果。BONDING_OPTS&#x3D;“fail_over_mac&#x3D;1”配置解释: 默认fail_over_mac&#x3D;0，当发生错误时，只改slave的mac不改bond；fail_over_mac&#x3D;1时，只改blave。</p></blockquote><h3><span id="1-bond的模式">1、Bond的模式</span></h3><blockquote><p><a href="https://www.likecs.com/show-308299629.html">网卡的7种bond模式</a><br><a href="https://blog.csdn.net/weixin_45548465/article/details/122625777">Linux网卡bond的七种模式详解，⽹卡绑定(bond)怎么实现？有哪些绑定⽅式</a>：</p></blockquote><table><thead><tr><th>模式</th><th>名称</th><th>特点</th><th>负载均衡</th><th>交换机配置</th><th>条件</th></tr></thead><tbody><tr><td>0</td><td>(balance-rr) Round-robin policy（平衡抡循环策略）</td><td>输数据包顺序是依次传输（即：第1个包走eth0，下一个包就走eth1….一直循环下去，直到最后一个传 输完毕），此模式提供负载平衡和容错能力。</td><td>是</td><td>静态聚合</td><td></td></tr><tr><td>1</td><td>(active-backup) Active-backup policy（主-备份策略）</td><td>只有一个设备处于活动状态，当一个宕掉另一个马上由备份转换为主设备。mac地址是外部可见得，从外面看来，bond的MAC地址是唯一的，以避免switch(交换机)发生混乱。此模式只提供了容错能力；由此可见此算法的优点是可以提供高网络连接的可用性，但是它的资源利用率较低，只有一个接口处于工作状态，在有 N 个网络接口的情况下，资源利用率为1&#x2F;N。</td><td>否</td><td>否</td><td></td></tr><tr><td>2</td><td>(balance-xor) XOR policy（平衡策略）</td><td>基于指定的传输HASH策略传输数据包。缺省的策略是：(源MAC地址 XOR 目标MAC地址) % slave数量。</td><td>是</td><td>静态聚合</td><td></td></tr><tr><td>3</td><td>broadcast（广播策略</td><td>每个slave接口上传输每个数据包，此模式提供了容错能力。</td><td>否</td><td>静态聚合</td><td></td></tr><tr><td>4</td><td>(802.3ad) IEEE 802.3adDynamic link aggregation（IEEE 802.3ad 动态链接聚合）</td><td>创建一个聚合组，它们共享同样的速率和双工设定。根据802.3ad规范将多个slave工作在同一个激活的聚合体下。</td><td>是</td><td>支持IEEE 802.3ad动态聚合</td><td>条件1：ethtool支持获取每个slave的速率和双工设定。<br>条件2：switch(交换机)支持IEEE 802.3ad Dynamic link aggregation。<br>条件3：大多数switch(交换机)需要经过特定配置才能支持802.3ad模式。<br></td></tr><tr><td>5</td><td>(balance-tlb) Adaptive transmit load balancing（适配器传输负载均衡）</td><td>不需要任何特别的switch(交换机)支持的通道bonding。在每个slave上根据当前的负载（根据速度计算）分配外出流量。如果正在接受数据的slave出故障了，另一个slave接管失败的slave的MAC地址。</td><td>是</td><td>否</td><td></td></tr><tr><td>6</td><td>(balance-alb) Adaptive load balancing（适配器适应性负载均衡）</td><td>该模式包含了balance-tlb模式，同时加上针对IPV4流量的接收负载均衡(receive load balance,rlb)，而且不需要任何switch(交换机)的支持。</td><td>是</td><td>否</td><td></td></tr></tbody></table><h2><span id="2-配置网桥">2、配置网桥</span></h2><pre><code>[root@kvm ~]# vim /etc/sysconfig/network-scripts/ifcfg-virbr1 #新建网桥virbr1的配置文件DEVICE=virbr1ONBOOT=yesTYPE=BridgeNM_CONTROLLER=noUSERCTL=noBOOTPROTO=staticIPADDR=192.168.187.134NETMASK=255.255.255.0GATEWAY=192.168.187.1[root@kvm ~]# vim /etc/sysconfig/network-scripts/ifcfg-bond0  #修改bond0的配置文件DEVICE=bond0ONBOOT=yesNM_CONTROLLERD=noUSERCTL=noBONDING_OPTS=&quot;mode=1 miimon=100 fail_over_mac=1&quot;BOOTPROTO=noneBRIDGE=virbr1[root@kvm ~]# systemctl restart network #重启网络服务[root@kvm ~]# brctl show  #查看当前系统网桥bridge name    bridge id        STP enabled    interfacesvirbr0        8000.525400ccc5d5    yes        virbr0-nicvirbr1        8000.000c290b57dc    no        bond0[root@kvm ~]# ip addr show virbr1 #查看IP已经到网桥1中了18: virbr1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet 192.168.187.134/24 brd 192.168.187.255 scope global noprefixroute virbr1       valid_lft forever preferred_lft forever    inet6 fe80::5c15:77ff:fece:bc31/64 scope link        valid_lft forever preferred_lft forever[root@kvm qemu]# cat /etc/libvirt/qemu/generic.xml #查看连接的虚拟机接口配置    &lt;interface type=&#39;bridge&#39;&gt;      &lt;mac address=&#39;52:54:00:8e:67:0a&#39;/&gt;      &lt;source bridge=&#39;virbr1&#39;/&gt;      &lt;model type=&#39;rtl8139&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x03&#39; function=&#39;0x0&#39;/&gt;    &lt;/interface&gt;[root@kvm qemu]# brctl show #查看多了个net0bridge name    bridge id        STP enabled    interfacesvirbr0        8000.525400ccc5d5    yes        virbr0-nicvirbr1        8000.000c290b57dc    no        bond0                            vnet0[root@kvm qemu]# virsh domiflist --domain generic  #查看接口信息，模式是8139，可以修改成virto优化性能Interface  Type       Source     Model       MAC-------------------------------------------------------vnet0      bridge     virbr1     rtl8139     52:54:00:8e:67:0a</code></pre><h1><span id="2-配置vlan">2、配置Vlan</span></h1><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/6.jpg"></p><p><strong>不建议使用nmcli创建</strong></p><table><thead><tr><th>网络</th><th>地址段</th><th>Vlan-id</th></tr></thead><tbody><tr><td>管理网络</td><td>192.168.200.0&#x2F;24</td><td>&#x2F;</td></tr><tr><td>生产网络</td><td>172.16.11.0&#x2F;24 172.16.11.11</td><td>11</td></tr><tr><td>生产网络</td><td>172.16.12.0&#x2F;24 172.16.12.12</td><td>12</td></tr></tbody></table><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/7.jpg"></p><h2><span id="1-nmcli方法">1、nmcli方法</span></h2><pre><code>[root@kvm ~]# nmcli --help #查看帮助文档Usage: nmcli [OPTIONS] OBJECT &#123; COMMAND | help &#125;OPTIONS  -a, --ask                                ask for missing parameters  -c, --colors auto|yes|no                 whether to use colors in output  -e, --escape yes|no                      escape columns separators in values  -f, --fields &lt;field,...&gt;|all|common      specify fields to output  -g, --get-values &lt;field,...&gt;|all|common  shortcut for -m tabular -t -f  -h, --help                               print this help  -m, --mode tabular|multiline             output mode  -o, --overview                           overview mode  -p, --pretty                             pretty output  -s, --show-secrets                       allow displaying passwords  -t, --terse                              terse output  -v, --version                            show program version  -w, --wait &lt;seconds&gt;                     set timeout waiting for finishing operationsOBJECT  g[eneral]       NetworkManager&#39;s general status and operations  n[etworking]    overall networking control  r[adio]         NetworkManager radio switches  c[onnection]    NetworkManager&#39;s connections  d[evice]        devices managed by NetworkManager  a[gent]         NetworkManager secret agent or polkit agent  m[onitor]       monitor NetworkManager changes[root@kvm network-scripts]# nmcli connection show  #查看当前网络信息NAME          UUID                                  TYPE      DEVICE System ens33  c96bc909-188e-ec64-3a96-6a90982b08ad  ethernet  ens33  System ens36  418da202-9a8c-b73c-e8a1-397e00f3c6b2  ethernet  ens36  virbr0        41366568-f66f-4c8b-a2f6-cb4df1714e06  bridge    virbr0 有线连接 1    52522e57-4b72-3d75-a602-ce47588f745d  ethernet  --     [root@kvm network-scripts]# nmcli connection add type vlan ifname VLAN11 dev ens33 id 11 #基于ens33创建vlan11Connection &#39;vlan-VLAN11&#39; (193348c2-73b4-4962-981c-4fdd7a966926) successfully added.[root@kvm network-scripts]# nmcli connection add type vlan ifname VLAN12 dev ens33 id 12 #基于ens33创建vlan12    Connection &#39;vlan-VLAN12&#39; (60496196-b1ee-4ff6-93fc-e167f25e63c7) successfully added.[root@kvm network-scripts]# pwd /etc/sysconfig/network-scripts[root@kvm network-scripts]# ls -lha | grep vlan  #查看自动创建的文件-rw-r--r--. 1 root root  357 Apr  8 05:08 ifcfg-vlan-VLAN11-rw-r--r--. 1 root root  357 Apr  8 05:08 ifcfg-vlan-VLAN12[root@kvm network-scripts]# cat ifcfg-vlan-VLAN11 #查看具体信息VLAN=yesTYPE=VlanPHYSDEV=ens33VLAN_ID=11REORDER_HDR=yesGVRP=noMVRP=noHWADDR=PROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=dhcpDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=vlan-VLAN11UUID=193348c2-73b4-4962-981c-4fdd7a966926DEVICE=VLAN11ONBOOT=yes[root@kvm network-scripts]# nmcli connection show  #查看创建的链接NAME          UUID                                  TYPE      DEVICE System ens33  c96bc909-188e-ec64-3a96-6a90982b08ad  ethernet  ens33  System ens36  418da202-9a8c-b73c-e8a1-397e00f3c6b2  ethernet  ens36  virbr0        41366568-f66f-4c8b-a2f6-cb4df1714e06  bridge    virbr0 vlan-VLAN11   193348c2-73b4-4962-981c-4fdd7a966926  vlan      --     vlan-VLAN12   60496196-b1ee-4ff6-93fc-e167f25e63c7  vlan      --     有线连接 1    52522e57-4b72-3d75-a602-ce47588f745d  ethernet  --  [root@kvm network-scripts]# ip a #查看创建结果1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet 192.168.200.128/24 brd 192.168.200.255 scope global noprefixroute ens33       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fe0b:57dc/64 scope link        valid_lft forever preferred_lft forever3: ens36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:e6 brd ff:ff:ff:ff:ff:ff    inet 192.168.200.135/24 brd 192.168.200.255 scope global noprefixroute ens36       valid_lft forever preferred_lft forever6: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0       valid_lft forever preferred_lft forever7: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff13: vnet0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 1000    link/ether fe:54:00:8e:67:0a brd ff:ff:ff:ff:ff:ff    inet6 fe80::fc54:ff:fe8e:670a/64 scope link        valid_lft forever preferred_lft forever25: VLAN12@ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet6 fe80::eb51:86e8:ea1f:f322/64 scope link noprefixroute        valid_lft forever preferred_lft forever26: VLAN11@ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet6 fe80::a47d:f2c7:75fc:7e54/64 scope link tentative noprefixroute        valid_lft forever preferred_lft forever[root@kvm network-scripts]# tail -n 20 /var/log/messages #查看日志Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5135] dhcp4 (VLAN12): canceled DHCP transaction, DHCP client pid 15817Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5136] dhcp4 (VLAN12): state changed timeout -&gt; doneApr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5141] device (VLAN12): state change: ip-config -&gt; failed (reason &#39;ip-config-unavailable&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;warn&gt;  [1680902267.5150] device (VLAN12): Activation: failed for connection &#39;vlan-VLAN12&#39;Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5152] device (VLAN12): state change: failed -&gt; disconnected (reason &#39;none&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5602] device (VLAN12): state change: disconnected -&gt; unmanaged (reason &#39;user-requested&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5609] policy: auto-activating connection &#39;vlan-VLAN12&#39; (60496196-b1ee-4ff6-93fc-e167f25e63c7)Apr  8 05:17:47 kvm kernel: IPv6: ADDRCONF(NETDEV_UP): VLAN12: link is not readyApr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5637] device (VLAN12): state change: unmanaged -&gt; unavailable (reason &#39;managed&#39;, sys-iface-state: &#39;external&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5862] device (VLAN12): carrier: link connectedApr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5881] device (VLAN12): state change: unavailable -&gt; disconnected (reason &#39;user-requested&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6013] device (VLAN12): Activation: starting connection &#39;vlan-VLAN12&#39; (60496196-b1ee-4ff6-93fc-e167f25e63c7)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6023] device (VLAN12): state change: disconnected -&gt; prepare (reason &#39;none&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6033] device (VLAN12): state change: prepare -&gt; config (reason &#39;none&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6366] device (VLAN12): state change: config -&gt; ip-config (reason &#39;none&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6373] dhcp4 (VLAN12): activation: beginning transaction (timeout in 45 seconds)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6465] dhcp4 (VLAN12): dhclient started with pid 15847Apr  8 05:17:47 kvm dhclient[15847]: DHCPDISCOVER on VLAN12 to 255.255.255.255 port 67 interval 7 (xid=0x1a408bc7)Apr  8 05:17:52 kvm dhclient[15834]: DHCPDISCOVER on VLAN11 to 255.255.255.255 port 67 interval 6 (xid=0x6c741821)Apr  8 05:17:54 kvm dhclient[15847]: DHCPDISCOVER on VLAN12 to 255.255.255.255 port 67 interval 20 (xid=0x1a408bc7)[root@kvm network-scripts]# virsh iface-list #在kvm上看 Name                 State      MAC Address--------------------------------------------------- ens33                active     00:0c:29:0b:57:dc ens36                active     00:0c:29:0b:57:e6 lo                   active     00:00:00:00:00:00 VLAN11               active     00:0c:29:0b:57:dc VLAN12               active     00:0c:29:0b:57:dc[root@kvm network-scripts]# rm -rf ifcfg-vlan-VLAN1* #恢复设置[root@kvm network-scripts]# systemctl restart network #重启[root@kvm network-scripts]# nmtui #图形化界面，不好用</code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/8.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/9.jpg"></p><h2><span id="2-通过命令行创建">2、通过命令行创建</span></h2><pre><code>[root@kvm network-scripts]# vim ifcfg-ens36.11 #创建vlan11配置文件DEVICE=&quot;ens36.11&quot;VLAN=&quot;yes&quot;ONBOOT=&quot;yes&quot;BOOTPROTO=&quot;none&quot;IPADDR=&quot;172.16.11.11&quot;NETMASK=&quot;255.255.255.0&quot;[root@kvm network-scripts]# cp ifcfg-ens36.11 ifcfg-ens36.12 #创建vlan11配置文件并修改[root@kvm network-scripts]# vim ifcfg-ens36.12 #配置DEVICE=&quot;ens36.12&quot;VLAN=&quot;yes&quot;ONBOOT=&quot;yes&quot;BOOTPROTO=&quot;none&quot;IPADDR=&quot;172.16.12.12&quot;NETMASK=&quot;255.255.255.0&quot;[root@kvm network-scripts]# systemctl restart network #重启网络配置[root@kvm network-scripts]# nmcli connection show  #查看配置是否生效NAME           UUID                                  TYPE      DEVICE   System ens33   c96bc909-188e-ec64-3a96-6a90982b08ad  ethernet  ens33    Vlan ens36.11  5aaffcd1-84a5-3046-15e7-adb62160402b  vlan      ens36.11 Vlan ens36.12  d100f2b5-85f7-4961-2105-8874b119178a  vlan      ens36.12 System ens36   418da202-9a8c-b73c-e8a1-397e00f3c6b2  ethernet  ens36    virbr0         41366568-f66f-4c8b-a2f6-cb4df1714e06  bridge    virbr0   有线连接 1     52522e57-4b72-3d75-a602-ce47588f745d  ethernet  --      [root@kvm network-scripts]# ip addr #查看IP1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet 192.168.200.128/24 brd 192.168.200.255 scope global noprefixroute ens33       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fe0b:57dc/64 scope link        valid_lft forever preferred_lft forever3: ens36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:e6 brd ff:ff:ff:ff:ff:ff    inet 192.168.200.135/24 brd 192.168.200.255 scope global noprefixroute ens36       valid_lft forever preferred_lft forever6: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0       valid_lft forever preferred_lft forever7: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff13: vnet0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 1000    link/ether fe:54:00:8e:67:0a brd ff:ff:ff:ff:ff:ff    inet6 fe80::fc54:ff:fe8e:670a/64 scope link        valid_lft forever preferred_lft forever32: ens36.12@ens36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether 00:0c:29:0b:57:e6 brd ff:ff:ff:ff:ff:ff    inet 172.16.12.12/24 brd 172.16.12.255 scope global noprefixroute ens36.12       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fe0b:57e6/64 scope link        valid_lft forever preferred_lft forever33: ens36.11@ens36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether 00:0c:29:0b:57:e6 brd ff:ff:ff:ff:ff:ff    inet 172.16.11.11/24 brd 172.16.11.255 scope global noprefixroute ens36.11       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fe0b:57e6/64 scope link        valid_lft forever preferred_lft forever[root@kvm network-scripts]# virsh iface-list --all  #查看 Name                 State      MAC Address--------------------------------------------------- ens33                active     00:0c:29:0b:57:dc ens36                active     00:0c:29:0b:57:e6 ens36.11             active     00:0c:29:0b:57:e6 ens36.12             active     00:0c:29:0b:57:e6 lo                   active     00:00:00:00:00:00</code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/10.jpg"></p><h2><span id="3-通过virt-manager创建">3、通过virt-manager创建</span></h2><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/11.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/12.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/13.jpg"></p><pre><code>[root@kvm network-scripts]# cat ifcfg-ens36.11 DEVICE=&quot;ens36.11&quot;VLAN=&quot;yes&quot;ONBOOT=&quot;yes&quot;BOOTPROTO=&quot;none&quot;IPADDR=&quot;192.168.200.135&quot;NETMASK=&quot;255.255.255.0&quot;GATEWAY=&quot;192.168.200.2&quot;</code></pre><h2><span id="4-虚拟机使用vlan">4、虚拟机使用vlan</span></h2><p><strong>一个vlan使用一个网桥，调用即可</strong><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/14.jpg"></p><h1><span id="3-配置网络过滤">3、配置网络过滤</span></h1><p><strong>通过libvirtd使用ebtailes创建的规则</strong><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/15.jpg"></p><pre><code>[root@kvm network-scripts]# virsh --help  Network Filter (help keyword &#39;filter&#39;)    nwfilter-define                define or update a network filter from an XML file    nwfilter-dumpxml               network filter information in XML    nwfilter-edit                  edit XML configuration for a network filter    nwfilter-list                  list network filters    nwfilter-undefine              undefine a network filter        nwfilter-binding-create        create a network filter binding from an XML file    nwfilter-binding-delete        delete a network filter binding    nwfilter-binding-dumpxml       network filter information in XML    nwfilter-binding-list          list network filter bindings[root@kvm network-scripts]#  virsh nwfilter-list  #查看网络过滤列表 UUID                                  Name                 ------------------------------------------------------------------ 3ce9cf03-2589-4b6a-8c1b-6e8b24e38f8e  allow-arp            a803defa-b342-47b4-af29-613250ff5281  allow-dhcp           5deae120-0ca4-4451-b8d4-92b6cd165962  allow-dhcp-server    49bdc080-9c7a-4278-9d83-59371bc9b981  allow-incoming-ipv4  64495a87-4efc-44f3-9aa2-1ef8e0a08600  allow-ipv4           cbd5d978-e475-4859-863d-1395d03da480  clean-traffic        #干净的流量 14ed106c-6baa-4ffd-8f28-d5038fea01ef  clean-traffic-gateway f88bd332-81af-480f-af36-57a7cd35104e  no-arp-ip-spoofing   a7611c9f-2a3a-4f62-9039-4e5be77881b3  no-arp-mac-spoofing  92dfc42c-0ed9-4482-a8f4-eee46f168cb8  no-arp-spoofing      20951a13-4570-4a87-9aa5-587d128c899e  no-ip-multicast      be8c0f6f-c811-451d-ae18-7cef13c56380  no-ip-spoofing       d85ecc2e-5573-43a7-85c1-e88b4daa6fdd  no-mac-broadcast     108c0c1a-8471-48b5-998a-6d92226f084e  no-mac-spoofing      7101e610-2e76-495f-976a-e4a37dd3f6ef  no-other-l2-traffic  60911d33-fb36-46bf-8eec-eeacc2f0fe62  no-other-rarp-traffic 40be9024-b924-4df0-b4c4-839e1026873f  qemu-announce-self   8bea3031-76ab-4077-9712-04d5b850e5fa  qemu-announce-self-rarp[root@kvm network-scripts]# virsh nwfilter-dumpxml clean-traffic #查看详细信息&lt;filter name=&#39;clean-traffic&#39; chain=&#39;root&#39;&gt; #默认规则及规则名称  &lt;uuid&gt;cbd5d978-e475-4859-863d-1395d03da480&lt;/uuid&gt; #规则UUID  &lt;filterref filter=&#39;no-mac-spoofing&#39;/&gt; #禁止MAC地址七篇  &lt;filterref filter=&#39;no-ip-spoofing&#39;/&gt; #禁止IP地址欺骗  &lt;rule action=&#39;accept&#39; direction=&#39;out&#39; priority=&#39;-650&#39;&gt; #accept代表动作，direction代表方向，in，out，inout，prioiy优先级，优先级值越低，优先级越高    &lt;mac protocolid=&#39;ipv4&#39;/&gt; #ipv4的mac地址  &lt;/rule&gt;  &lt;filterref filter=&#39;allow-incoming-ipv4&#39;/&gt; #调用其他规则  &lt;filterref filter=&#39;no-arp-spoofing&#39;/&gt;  &lt;rule action=&#39;accept&#39; direction=&#39;inout&#39; priority=&#39;-500&#39;&gt;    &lt;mac protocolid=&#39;arp&#39;/&gt;  &lt;/rule&gt;  &lt;filterref filter=&#39;no-other-l2-traffic&#39;/&gt; #调用其他规则  &lt;filterref filter=&#39;qemu-announce-self&#39;/&gt;&lt;/filter&gt;[root@kvm network-scripts]# ebtables -t nat -L #查看ebtables的NAT表规则Bridge table: natBridge chain: PREROUTING, entries: 1, policy: ACCEPT-j PREROUTING_directBridge chain: OUTPUT, entries: 1, policy: ACCEPT-j OUTPUT_directBridge chain: POSTROUTING, entries: 1, policy: ACCEPT-j POSTROUTING_directBridge chain: PREROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: POSTROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: OUTPUT_direct, entries: 1, policy: ACCEPT-j RETURN [root@kvm network-scripts]# virsh list --all  #查看虚拟机，准备于过滤规则绑定 Id    Name                           State---------------------------------------------------- 5     generic                        running -     centos7.0                      shut off[root@kvm network-scripts]# virsh dumpxml --domain generic    &lt;interface type=&#39;bridge&#39;&gt;      &lt;mac address=&#39;52:54:00:8e:67:0a&#39;/&gt;      &lt;source bridge=&#39;virbr1&#39;/&gt;      &lt;target dev=&#39;vnet0&#39;/&gt;      &lt;model type=&#39;rtl8139&#39;/&gt;      &lt;alias name=&#39;net0&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x03&#39; function=&#39;0x0&#39;/&gt;    &lt;/interface&gt;[root@kvm ~]# vim nwtest.xml    &lt;interface type=&#39;bridge&#39;&gt;      &lt;mac address=&#39;52:54:00:8e:67:0a&#39;/&gt;      &lt;source bridge=&#39;virbr1&#39;/&gt;      &lt;target dev=&#39;vnet0&#39;/&gt;      &lt;model type=&#39;rtl8139&#39;/&gt;      &lt;alias name=&#39;net0&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x03&#39; function=&#39;0x0&#39;/&gt;      &lt;filterref filter=&#39;clean-traffic&#39;/&gt; #增加过滤规则，调用clean-traffic    &lt;/interface&gt;[root@kvm ~]# virsh update-device --domain generic --file /root/nwtest.xml --persistent --live #更新generic主机，--file选择创建的文件，--persistent 代表永久生效，--live代表当前主机正在运行Device updated successfully[root@kvm ~]#  ebtables -t nat -L  #查看规则生效情况Bridge table: natBridge chain: PREROUTING, entries: 2, policy: ACCEPT-j PREROUTING_direct-i vnet0 -j libvirt-I-vnet0Bridge chain: OUTPUT, entries: 1, policy: ACCEPT-j OUTPUT_directBridge chain: POSTROUTING, entries: 1, policy: ACCEPT-j POSTROUTING_directBridge chain: PREROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: POSTROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: OUTPUT_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: libvirt-I-vnet0, entries: 4, policy: ACCEPT-s ! 52:54:0:8e:67:a -j DROP -p IPv4 -j ACCEPT -p ARP -j ACCEPT -j DROP [root@kvm ~]# virsh shutdown --domain generic  #关闭虚拟机Domain generic is being shutdown[root@kvm ~]# virsh list --all  #查看主机是否正常关闭 Id    Name                           State---------------------------------------------------- -     centos7.0                      shut off -     generic                        shut off[root@kvm ~]#  ebtables -t nat -L  #查看规则清空Bridge table: natBridge chain: PREROUTING, entries: 1, policy: ACCEPT-j PREROUTING_directBridge chain: OUTPUT, entries: 1, policy: ACCEPT-j OUTPUT_directBridge chain: POSTROUTING, entries: 1, policy: ACCEPT-j POSTROUTING_directBridge chain: PREROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: POSTROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: OUTPUT_direct, entries: 1, policy: ACCEPT-j RETURN [root@kvm ~]# virsh start --domain generic  #重新启动主机Domain generic started[root@kvm ~]#  ebtables -t nat -L  #再次查看规则Bridge table: natBridge chain: PREROUTING, entries: 2, policy: ACCEPT-j PREROUTING_direct-i vnet0 -j libvirt-I-vnet0Bridge chain: OUTPUT, entries: 1, policy: ACCEPT-j OUTPUT_directBridge chain: POSTROUTING, entries: 1, policy: ACCEPT-j POSTROUTING_directBridge chain: PREROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: POSTROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: OUTPUT_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: libvirt-I-vnet0, entries: 4, policy: ACCEPT-s ! 52:54:0:8e:67:a -j DROP -p IPv4 -j ACCEPT -p ARP -j ACCEPT -j DROP </code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/16.jpg"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KVM </tag>
            
            <tag> KVM虚拟网络高级配置 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KVM虚拟化进阶之Linux HA群集体系架构</title>
      <link href="/2023/04/12/KVM/3.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8BLinux_HA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/"/>
      <url>/2023/04/12/KVM/3.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8BLinux_HA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-linux-ha%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84">一、Linux HA群集体系架构</a></li></ul><!-- tocstop --><h1><span id="一-linux-ha群集体系架构">一、Linux HA群集体系架构</span></h1><p><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/48.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/49.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/50.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/51.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/52.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/53.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/54.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/55.png"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/56.png"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/58.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/58.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/59.png"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KVM </tag>
            
            <tag> Linux HA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KVM虚拟化进阶之虚拟机迁移</title>
      <link href="/2023/04/12/KVM/2.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/"/>
      <url>/2023/04/12/KVM/2.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#1-%E9%80%9A%E8%BF%87uri%E8%BF%9E%E6%8E%A5%E8%BF%9E%E6%8E%A5kvm%E4%B8%BB%E6%9C%BA">1、通过URI连接连接KVM主机</a></li><li><a href="#2-%E9%9D%99%E6%80%81%E8%BF%81%E7%A7%BB">2、静态迁移</a><ul><li><a href="#1-%E5%90%8C%E4%B8%80%E5%AE%BF%E4%B8%BB%E6%9C%BA%E5%86%85%E8%BF%81%E7%A7%BB">1、同一宿主机内迁移</a></li><li><a href="#2-%E4%B8%8D%E5%90%8C%E5%AE%BF%E4%B8%BB%E6%9C%BA%E8%BF%81%E7%A7%BB">2、不同宿主机迁移</a><ul><li><a href="#kvm">kvm：</a></li><li><a href="#kvm2">kvm2：</a></li></ul></li><li><a href="#3-%E4%B8%8D%E5%90%8C%E5%AE%BF%E4%B8%BB%E6%9C%BA%E8%BF%81%E7%A7%BB%E4%BD%BF%E7%94%A8virsh-migrate%E5%91%BD%E4%BB%A4">3、不同宿主机迁移使用virsh migrate命令</a></li></ul></li><li><a href="#3-%E5%9F%BA%E4%BA%8E%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8%E7%9A%84%E5%8A%A8%E6%80%81%E8%BF%81%E7%A7%BB">3、基于共享存储的动态迁移</a><ul><li><a href="#1-%E5%AE%89%E8%A3%85nfs%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BB%84%E4%BB%B6">1、安装NFS服务器组件</a></li><li><a href="#2-%E5%88%9B%E5%BB%BAnfs%E5%85%B1%E4%BA%AB%E6%96%87%E4%BB%B6%E5%A4%B9">2、创建NFS共享文件夹</a></li><li><a href="#3-%E5%B0%86kvm1%E4%B8%AD%E7%9A%84%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A1%AC%E7%9B%98%E8%BF%81%E7%A7%BB%E5%88%B0nfs%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8%E4%B8%8A">3、将KVM1中的虚拟机硬盘迁移到NFS服务器的共享存储上</a></li><li><a href="#4-%E4%BD%BF%E7%94%A8virt-manager%E8%BF%9B%E8%A1%8C%E8%BF%81%E7%A7%BB">4、使用virt-manager进行迁移</a></li><li><a href="#5-%E4%BD%BF%E7%94%A8virsh%E5%91%BD%E4%BB%A4%E8%BF%9B%E8%A1%8C%E8%BF%81%E7%A7%BB">5、使用virsh命令进行迁移</a><ul><li><a href="#1-%E6%9F%A5%E7%9C%8B%E5%B8%AE%E5%8A%A9">1、查看帮助</a></li><li><a href="#2-%E8%BF%81%E7%A7%BB">2、迁移</a></li></ul></li></ul></li><li><a href="#4-%E5%9F%BA%E4%BA%8E%E6%9C%AC%E5%9C%B0%E5%AD%98%E5%82%A8%E7%9A%84%E5%8A%A8%E6%80%81%E8%BF%81%E7%A7%BB">4、基于本地存储的动态迁移</a></li></ul><!-- tocstop --><p><strong>静态迁移中可以通过将虚拟机暂停，迁移完成后重置虚拟机状态，完成迁移，但是也会影响业务</strong></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/17.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/18.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/19.jpg"></p><h1><span id="1-通过uri连接连接kvm主机">1、通过URI连接连接KVM主机</span></h1><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/20.jpg"></p><pre><code>[root@kvm ~]# virsh --helpvirsh [options]... [&lt;command_string&gt;]virsh [options]... &lt;command&gt; [args...]  options:    -c | --connect=URI      hypervisor connection URI[root@kvm ~]# virsh -c qemu+ssh://root@122.200.93.9/system #远程连接其他KVM主机root@122.200.93.9&#39;s password: Welcome to virsh, the virtualization interactive terminal.Type:  &#39;help&#39; for help with commands       &#39;quit&#39; to quitvirsh # list --all Id    Name                           State---------------------------------------------------- -     centos7.0                      shut off -     centos7.0-2                    shut off -     centos7.0-3                    shut off -     generic          virsh # hostname kvm.novalocal</code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/21.jpg"></p><h1><span id="2-静态迁移">2、静态迁移</span></h1><h2><span id="1-同一宿主机内迁移">1、同一宿主机内迁移</span></h2><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/22.jpg"></p><pre><code>[root@kvm ~]# virsh list --all  #查看虚拟机状态 Id    Name                           State---------------------------------------------------- 7     generic                        running -     centos7.0                      shut off[root@kvm ~]# virsh domblklist --domain generic  #查看硬盘路径Target     Source------------------------------------------------hda        /vm/centos7.6.rawhdb        -[root@kvm ~]# virsh shutdown --domain generic #关闭虚拟机Domain generic is being shutdown[root@kvm ~]# virsh list --all #再次查看状态 Id    Name                           State---------------------------------------------------- -     centos7.0                      shut off -     generic                        shut off[root@kvm ~]# mkdir /vm1 #创建迁移后的文件夹[root@kvm ~]# mv /vm/centos7.6.raw /vm1/ #迁移硬盘[root@kvm ~]# ls -lha /vm1 #查看迁移结果total 21Gdrwxr-xr-x.  2 root root  27 Apr  8 06:41 .dr-xr-xr-x. 22 root root 279 Apr  8 06:41 ..-rw-------.  1 root root 20G Apr  8 06:40 centos7.6.raw[root@kvm ~]# virsh start --domain generic  #尝试启动虚拟机，报错，需要修改配置文件error: Failed to start domain genericerror: Cannot access storage file &#39;/vm/centos7.6.raw&#39;: 没有那个文件或目录[root@kvm ~]# virsh edit --domain generic    &lt;disk type=&#39;file&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;source file=&#39;/vm1/centos7.6.raw&#39;/&gt; #修改源文件路径      &lt;target dev=&#39;hda&#39; bus=&#39;ide&#39;/&gt;      &lt;address type=&#39;drive&#39; controller=&#39;0&#39; bus=&#39;0&#39; target=&#39;0&#39; unit=&#39;0&#39;/&gt;    &lt;/disk&gt;[root@kvm ~]# virsh start --domain generic #启动成功，迁移完毕Domain generic started</code></pre><h2><span id="2-不同宿主机迁移">2、不同宿主机迁移</span></h2><pre><code>![](images/KVM/虚拟机迁移/23.jpg)![](images/KVM/虚拟机迁移/24.jpg)| 序号 | 主机名称 | 主机IP || --- | --- | --- || 1 | kvm | 192.168.200.128 |</code></pre><p>| 2 | kvm2 | 192.168.200.129 |</p><h3><span id="kvm">kvm：</span></h3><p><strong>静态迁移要确保虚拟机处于关闭状态</strong></p><blockquote><p>迁移可以使用scp或者rsync -avSHP进行数据传输</p></blockquote><pre><code>[root@kvm ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.128 kvm192.168.200.129 kvm2[root@kvm ~]# virsh list --all  Id    Name                           State---------------------------------------------------- -     centos7.0                      shut off -     generic                        shut off #迁移此台主机 [root@kvm ~]# scp /etc/libvirt/qemu/generic.xml root@192.168.200.129:/etc/libvirt/qemu/ #将配置文件传送过去，确保主机当前状态是关机The authenticity of host &#39;192.168.200.129 (192.168.200.129)&#39; can&#39;t be established.ECDSA key fingerprint is SHA256:s2qiffBOYjuMJQOHEI9r87NTNjd0XyoEf81myM78kbQ.ECDSA key fingerprint is MD5:a5:8f:de:d4:07:9f:b7:b3:6f:7c:b0:c3:55:eb:bb:b0.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &#39;192.168.200.129&#39; (ECDSA) to the list of known hosts.root@192.168.200.129&#39;s password: generic.xml                                                                                                                                                                                                                                                                               100% 4329     5.0MB/s   00:00    [root@kvm ~]# virsh domblklist --domain generic #查看硬盘文件Target     Source------------------------------------------------hda        /vm1/centos7.6.rawhdb        -[root@kvm ~]# scp /vm1/centos7.6.raw root@kvm2:/vm1/ #传输硬盘文件到目标主机上，路径最好保持一致，否则需要修改XML文件</code></pre><h3><span id="kvm2">kvm2：</span></h3><pre><code>[root@kvm2 ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.128 kvm192.168.200.129 kvm2[root@kvm2 ~]# virsh list --all  Id    Name                           State----------------------------------------------------[root@kvm2 ~]# mkdir /vm1 #需要提前创建出目录，否则上述传输的时候会报错[root@kvm2 vm1]# ls -lha /etc/libvirt/qemu/generic.xml  #查看传输过来的xml配置文件-rw-------. 1 root root 4.3K Apr 10 16:02 /etc/libvirt/qemu/generic.xml[root@kvm2 vm1]# ls /vm1/centos7.6.raw #查看硬盘文件 /vm1/centos7.6.raw[root@kvm2 vm1]# virsh define --file /etc/libvirt/qemu/generic.xml  #定义硬盘文件Domain generic defined from /etc/libvirt/qemu/generic.xml[root@kvm2 vm1]# virsh list --all #查看识别到此主机，准备启动 Id    Name                           State---------------------------------------------------- -     generic                        shut off[root@kvm2 vm1]# virsh start generic #启动成功Domain generic started</code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/25.jpg"></p><p><strong>尝试本地远程成功，迁移完毕</strong></p><pre><code>[root@kvm2 vm1]# ssh root@192.168.122.149root@192.168.122.149&#39;s password: Last login: Mon Apr 10 04:35:12 2023[root@localhost ~]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 52:54:00:8e:67:0a brd ff:ff:ff:ff:ff:ff    inet 192.168.122.149/24 brd 192.168.122.255 scope global noprefixroute dynamic ens3       valid_lft 3503sec preferred_lft 3503sec    inet6 fe80::aaa:8f11:d0df:a3a7/64 scope link noprefixroute        valid_lft forever preferred_lft forever</code></pre><blockquote><p><strong>确保正常启动且无问题后删除源宿主机的相关配置，防止IP、Mac等冲突，引起不必要的问题</strong></p></blockquote><h2><span id="3-不同宿主机迁移使用virsh-migrate命令">3、不同宿主机迁移使用virsh migrate命令</span></h2><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/26.jpg"></p><p><strong>如果做了上述动作，需要先还原环境，本方法只作为了解，实际和scp效果一样</strong></p><p>1、在KVM2主机中查看主机列表</p><pre><code>[root@kvm2 vm1]# virsh list --all  Id    Name                           State---------------------------------------------------- 1     generic                        running</code></pre><p>2、在KVM2主机中关闭启动的generic主机</p><pre><code>[root@kvm2 vm1]# virsh shutdown --domain generic Domain generic is being shutdown</code></pre><p>3、在KVM2主机中取消generic的xml文件定义</p><pre><code>[root@kvm2 vm1]# virsh undefine --domain genericDomain generic has been undefined</code></pre><p>4、在KVM2主机中删除generic的硬盘文件</p><pre><code>[root@kvm2 vm1]# cd /vm1[root@kvm2 vm1]# ls -lha total 20Gdrwxr-xr-x.  2 root root  27 Apr 10 16:03 .dr-xr-xr-x. 22 root root 279 Apr  8 06:41 ..-rw-------.  1 root root 20G Apr 10 16:46 centos7.6.raw[root@kvm2 vm1]# rm -rf centos7.6.raw </code></pre><p>5、在KVM1主机中将generic主机关机<br><code>[root@kvm ~]# virsh shutdown --domain generic</code></p><p>6、在KVM1主机中查看migrate帮助</p><pre><code>[root@kvm ~]# virsh migrate --help   NAME    migrate - 将域迁移到另一个主机中  SYNOPSIS    migrate &lt;domain&gt; &lt;desturi&gt; [--live] [--offline] [--p2p] [--direct] [--tunnelled] [--persistent] [--undefinesource] [--suspend] [--copy-storage-all] [--copy-storage-inc] [--change-protection] [--unsafe] [--verbose] [--compressed] [--auto-converge] [--rdma-pin-all] [--abort-on-error] [--postcopy] [--postcopy-after-precopy] [--migrateuri &lt;string&gt;] [--graphicsuri &lt;string&gt;] [--listen-address &lt;string&gt;] [--dname &lt;string&gt;] [--timeout &lt;number&gt;] [--timeout-suspend] [--timeout-postcopy] [--xml &lt;string&gt;] [--migrate-disks &lt;string&gt;] [--disks-port &lt;number&gt;] [--comp-methods &lt;string&gt;] [--comp-mt-level &lt;number&gt;] [--comp-mt-threads &lt;number&gt;] [--comp-mt-dthreads &lt;number&gt;] [--comp-xbzrle-cache &lt;number&gt;] [--auto-converge-initial &lt;number&gt;] [--auto-converge-increment &lt;number&gt;] [--persistent-xml &lt;string&gt;] [--tls]  DESCRIPTION    将域迁移到另一个主机中。热迁移时添加 --live。  OPTIONS    [--domain] &lt;string&gt;  domain name, id or uuid    [--desturi] &lt;string&gt;  客户端（常规迁移）或者源（p2p 迁移）中看到到目的地主机连接 URI    --live           热迁移    --offline        离线迁移    --p2p            点对点迁移    --direct         直接迁移    --tunnelled      管道迁移    --persistent     目的地中的持久 VM    --undefinesource  在源中取消定义 VM    --suspend        部启用目的地主机中的域    --copy-storage-all  使用全磁盘复制的非共享存储进行迁移    --copy-storage-inc  使用增值复制（源和目的地共享同一基础映像）的非共享存储进行迁移    --change-protection  迁移结束前不得对域进行任何配置更改    --unsafe         即使不安全也要强制迁移    --verbose        显示迁移进程    --compressed     实时迁移过程中压缩重复的页    --auto-converge  force convergence during live migration    --rdma-pin-all   pin all memory before starting RDMA live migration    --abort-on-error  在迁移过程中忽略软错误    --postcopy       enable post-copy migration; switch to it using migrate-postcopy command    --postcopy-after-precopy  automatically switch to post-copy migration after one pass of pre-copy    --migrateuri &lt;string&gt;  迁移 URI， 通常可省略    --graphicsuri &lt;string&gt;  无空隙图形迁移中使用的图形 URI    --listen-address &lt;string&gt;  listen address that destination should bind to for incoming migration    --dname &lt;string&gt;  在迁移过长中重新命名为一个新名称（如果支持）    --timeout &lt;number&gt;  run action specified by --timeout-* option (suspend by default) if live migration exceeds timeout (in seconds)    --timeout-suspend  suspend the guest after timeout    --timeout-postcopy  switch to post-copy after timeout    --xml &lt;string&gt;   包含为目标更新的 XML 的文件名    --migrate-disks &lt;string&gt;  comma separated list of disks to be migrated    --disks-port &lt;number&gt;  port to use by target server for incoming disks migration    --comp-methods &lt;string&gt;  comma separated list of compression methods to be used    --comp-mt-level &lt;number&gt;  compress level for multithread compression    --comp-mt-threads &lt;number&gt;  number of compression threads for multithread compression    --comp-mt-dthreads &lt;number&gt;  number of decompression threads for multithread compression    --comp-xbzrle-cache &lt;number&gt;  page cache size for xbzrle compression    --auto-converge-initial &lt;number&gt;  initial CPU throttling rate for auto-convergence    --auto-converge-increment &lt;number&gt;  CPU throttling rate increment for auto-convergence    --persistent-xml &lt;string&gt;  filename containing updated persistent XML for the target    --tls            use TLS for migration</code></pre><p>7、在KVM1主机中使用migrate命令迁移</p><pre><code>[root@kvm ~]# virsh migrate --domain generic --desturi qemu+ssh://root@kvm2/system --offline --persistent root@kvm2&#39;s password: </code></pre><p>8、在KVM2主机中查看主机是否迁移并尝试启动主机</p><pre><code>[root@kvm2 vm1]# virsh list --all  Id    Name                           State---------------------------------------------------- -     generic                        shut off[root@kvm2 vm1]# virsh start --domain generic error: Failed to start domain genericerror: Cannot access storage file &#39;/vm1/centos7.6.raw&#39;: 没有那个文件或目录</code></pre><p>9、在KVM1主机中将所需的硬盘文件复制到KVM2主机的vm1目录下</p><pre><code>[root@kvm ~]# rsync -avSHP /vm1/centos7.6.raw root@kvm2:/vm1root@kvm2&#39;s password: sending incremental file listcentos7.6.raw 21,474,836,480 100%   57.68MB/s    0:05:55 (xfr#1, to-chk=0/1)sent 21,480,079,452 bytes  received 35 bytes  59,749,873.40 bytes/sectotal size is 21,474,836,480  speedup is 1.00</code></pre><p>10、在KVM2主机中再次尝试启动主机</p><pre><code>[root@kvm2 vm1]# virsh start --domain generic Domain generic started</code></pre><h1><span id="3-基于共享存储的动态迁移">3、基于共享存储的动态迁移</span></h1><blockquote><p><strong>确保防火墙等安全防护工具处于关闭状态或者端口放行状态</strong></p></blockquote><p><strong>需要安装NFS服务器</strong></p><table><thead><tr><th>主机</th><th>Lan</th><th>Private</th><th>Stroage</th></tr></thead><tbody><tr><td>kvm1</td><td>192.168.200.135</td><td>172.16.1.135</td><td>10.0.1.231</td></tr><tr><td>kvm2</td><td>192.168.200.136</td><td>172.16.1.136</td><td>10.0.1.232</td></tr><tr><td>NFS</td><td>192.168.200.137</td><td></td><td>10.0.1.230</td></tr></tbody></table><h2><span id="1-安装nfs服务器组件">1、安装NFS服务器组件</span></h2><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/27.jpg"></p><pre><code>[root@nfs ~]# yum -y install nfs-utils[root@nfs ~]# systemctl start rpcbind[root@nfs ~]# systemctl start nfs-server[root@nfs ~]# systemctl enable rpcbind[root@nfs ~]# systemctl enable nfs-serverCreated symlink from /etc/systemd/system/multi-user.target.wants/nfs-server.service to /usr/lib/systemd/system/nfs-server.service.</code></pre><h2><span id="2-创建nfs共享文件夹">2、创建NFS共享文件夹</span></h2><pre><code>[root@nfs ~]# mkdir /vm1[root@nfs ~]# touch /vm1/testfile1.txt[root@nfs ~]# vim /etc/exports  #修改export文件/vm1 *(rw,no_root_squash,sync)[root@nfs ~]# systemctl restart nfs-server #重启服务[root@nfs ~]# showmount -e localhost #查看对外提供的共享文件夹Export list for localhost:/vm1 *</code></pre><h2><span id="3-将kvm1中的虚拟机硬盘迁移到nfs服务器的共享存储上">3、将KVM1中的虚拟机硬盘迁移到NFS服务器的共享存储上</span></h2><pre><code>[root@kvm ~]# mkdir /vmdata #在KVM1服务器中创建挂载点[root@kvm ~]# mount 192.168.200.137:/vm1 /vmdata/ #挂载NFS服务器中的vm1共享文件夹到本地的/vmdata。这里的IP建议使用专有的隧道网络，如万兆光等[root@kvm ~]# df -HT  #查看挂载情况文件系统                类型      容量  已用  可用 已用% 挂载点devtmpfs                devtmpfs  2.0G     0  2.0G    0% /devtmpfs                   tmpfs     2.0G     0  2.0G    0% /dev/shmtmpfs                   tmpfs     2.0G   13M  2.0G    1% /runtmpfs                   tmpfs     2.0G     0  2.0G    0% /sys/fs/cgroup/dev/mapper/centos-root xfs        38G   35G  3.7G   91% //dev/sda1               xfs       1.1G  206M  859M   20% /boottmpfs                   tmpfs     396M     0  396M    0% /run/user/0192.168.200.137:/vm1    nfs4       38G   15G   24G   39% /vmdata[root@kvm ~]# virsh list --all #查看迁移的主机 Id    名称                         状态---------------------------------------------------- -     generic                        关闭[root@kvm ~]# virsh edit --domain generic #编辑xml文件中的disk，如果直接修改xml文件，源硬盘会被直接迁移到nfs的共享目录中，老版本可能需要手动复制到挂载点    &lt;disk type=&#39;file&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;source file=&#39;/vmdata/centos7.6.raw&#39;/&gt; #修改      &lt;target dev=&#39;hda&#39; bus=&#39;ide&#39;/&gt;      &lt;address type=&#39;drive&#39; controller=&#39;0&#39; bus=&#39;0&#39; target=&#39;0&#39; unit=&#39;0&#39;/&gt;    &lt;/disk&gt;[root@kvm vmdata]# ls -lh /vmdata/ #查看共享文件中已经存在相关硬盘文件总用量 1.8G-rw-------. 1 root root 20G 4月  10 17:10 centos7.6.raw-rw-r--r--. 1 root root   0 4月  12 2023 testfile1.txt[root@kvm vmdata]# virsh start --domain generic  #尝试启动错误：开始域 generic 失败错误：内部错误：process exited while connecting to monitor: 2023-04-10T09:59:16.927297Z qemu-kvm: -drive file=/vmdata/centos7.6.raw,format=raw,if=none,id=drive-ide0-0-0: could not open disk image /vmdata/centos7.6.raw: Could not open &#39;/vmdata/centos7.6.raw&#39;: Permission denied[root@kvm vmdata]# sestatus #查看selinux启动，导致无法启动虚拟机SELinux status:                 enabledSELinuxfs mount:                /sys/fs/selinuxSELinux root directory:         /etc/selinuxLoaded policy name:             targetedCurrent mode:                   enforcingMode from config file:          enforcingPolicy MLS status:              enabledPolicy deny_unknown status:     allowedMax kernel policy version:      31[root@kvm vmdata]# setenforce usage:  setenforce [ Enforcing | Permissive | 1 | 0 ][root@kvm vmdata]# setenforce 0 #临时关闭selinux[root@kvm vmdata]# virsh start --domain generic  #正常启动域 generic 已开始</code></pre><blockquote><p>如果修改完后虚拟机无法正常启动，提示权限错误，可能是由于Selinux的问题</p></blockquote><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/28.jpg"></p><blockquote><p>建议不要关闭selinux，可以放行nfs的相关服务</p></blockquote><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/29.png"></p><pre><code>[root@kvm vmdata]# virsh shutdown --domain generic #关闭虚拟机域 generic 被关闭[root@kvm vmdata]# sestatus #查看当前Selinux状态为临时关闭SELinux status:                 enabledSELinuxfs mount:                /sys/fs/selinuxSELinux root directory:         /etc/selinuxLoaded policy name:             targetedCurrent mode:                   permissiveMode from config file:          enforcingPolicy MLS status:              enabledPolicy deny_unknown status:     allowedMax kernel policy version:      31[root@kvm vmdata]# setenforce 1 #启用Selinux[root@kvm vmdata]# sestatus #再次查看为启动状态SELinux status:                 enabledSELinuxfs mount:                /sys/fs/selinuxSELinux root directory:         /etc/selinuxLoaded policy name:             targetedCurrent mode:                   enforcingMode from config file:          enforcingPolicy MLS status:              enabledPolicy deny_unknown status:     allowedMax kernel policy version:      31[root@kvm vmdata]# setsebool -P virt_use_nfs 1 #需要在所有的计算节点开启[root@kvm vmdata]# virsh start --domain generic  #尝试可以正常启动域 generic 已开始[root@kvm vmdata]# virsh shutdown --domain generic #关机，准备迁移域 generic 被关闭</code></pre><h2><span id="4-使用virt-manager进行迁移">4、使用virt-manager进行迁移</span></h2><p>KVM2：</p><pre><code>[root@kvm2 ~]# mkdir /vmdata[root@kvm2 ~]# ls -lh /vmdatatotal 0[root@kvm2 ~]# mount 192.168.200.137:/vm1 /vmdata/[root@kvm2 ~]# ls -lh /vmdatatotal 1.8G-rw-------. 1 root root 20G Apr 12 17:40 centos7.6.raw-rw-r--r--. 1 root root   0 Apr 12 16:57 testfile1.txt</code></pre><p>KVM1:</p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/30.png"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/31.jpg"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/32.jpg"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/33.jpg"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/34.png"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/35.png"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/36.jpg"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/37.jpg"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/38.png"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/39.jpg"></p><pre><code>[root@kvm2 ~]# setsebool -P virt_use_nfs 1[root@kvm2 ~]# iptables -I INPUT -p tcp --dport 49152:49215 -j ACCEPT[root@kvm2 ~]# iptables -I INPUT -p tcp --dport 16509 -j ACCEPT</code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/40.jpg"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/41.jpg"></p><p>Allow unsafe：CPU缓存的模式，默认使用none<br>Temporary move：临时移动，移动后不删除源主机的相关配置</p><h2><span id="5-使用virsh命令进行迁移">5、使用virsh命令进行迁移</span></h2><h3><span id="1-查看帮助">1、查看帮助</span></h3><pre><code>[root@kvm ~]# virsh  migrate --help   名称：    migrate-将域迁移到另一台主机  简介：    migrate &lt;domain&gt; &lt;desturi&gt; [--live] [--offline] [--p2p] [--direct] [--tunnelled] [--persistent] [--undefinesource] [--suspend] [--copy-storage-all] [--copy-storage-inc] [--change-protection] [--unsafe] [--verbose] [--compressed] [--auto-converge] [--rdma-pin-all] [--abort-on-error] [--postcopy] [--postcopy-after-precopy] [--migrateuri &lt;string&gt;] [--graphicsuri &lt;string&gt;] [--listen-address &lt;string&gt;] [--dname &lt;string&gt;] [--timeout &lt;number&gt;] [--timeout-suspend] [--timeout-postcopy] [--xml &lt;string&gt;] [--migrate-disks &lt;string&gt;] [--disks-port &lt;number&gt;] [--comp-methods &lt;string&gt;] [--comp-mt-level &lt;number&gt;] [--comp-mt-threads &lt;number&gt;] [--comp-mt-dthreads &lt;number&gt;] [--comp-xbzrle-cache &lt;number&gt;] [--auto-converge-initial &lt;number&gt;] [--auto-converge-increment &lt;number&gt;] [--persistent-xml &lt;string&gt;] [--tls]  说明：    将域迁移到另一台主机。添加--实时迁移。  选项：    [--domain] &lt;string&gt;  域名、id或uuid    [--desturi] &lt;string&gt;  从客户端（正常迁移）或源（p2p迁移）看到的目标主机的连接URI    --live           热迁移    --offline        离线迁移    --p2p            对等迁移    --direct         直接迁移    --tunnelled      隧道迁移    --persistent     在目标上持久化VM    --undefinesource  源上未定义VM    --suspend        不要在目标主机上重新启动域    --copy-storage-all  使用带有完整磁盘拷贝的非共享存储进行迁移    --copy-storage-inc  使用带有增量拷贝的非共享存储进行迁移（源和目标之间共享相同的基本映像）    --change-protection  在迁移结束之前，禁止对域进行任何配置更改    --unsafe         强制迁移，即使可能不安全    --verbose        显示迁移进度    --compressed     在实时迁移过程中压缩重复的页面    --auto-converge  现场迁移过程中的力收敛    --rdma-pin-all   在开始RDMA实时迁移之前固定所有内存    --abort-on-error  迁移过程中出现软错误时中止    --postcopy       启用复制后迁移；使用“移植”后复制命令切换到它    --postcopy-after-precopy  经过一次预复制后自动切换到复制后迁移    --migrateuri &lt;string&gt;  迁移URI，通常可以省略    --graphicsuri &lt;string&gt;  用于无缝图形迁移的图形URI    --listen-address &lt;string&gt;  目标应绑定到的侦听地址以进行传入迁移    --dname &lt;string&gt;  迁移期间重命名为新名称（如果支持）    --timeout &lt;number&gt;  如果实时迁移超过超时（以秒为单位），则运行由--timeout-*选项指定的操作（默认情况下为挂起）    --timeout-suspend  超时后暂停来宾    --timeout-postcopy  超时后切换到后复制    --xml &lt;string&gt;   切换到超时后发布复制包含目标的更新XML的文件名    --migrate-disks &lt;string&gt;  要迁移的磁盘的逗号分隔列表    --disks-port &lt;number&gt;  目标服务器用于传入磁盘迁移的端口    --comp-methods &lt;string&gt;  要使用的压缩方法的逗号分隔列表    --comp-mt-level &lt;number&gt;  多线程压缩的压缩级别    --comp-mt-threads &lt;number&gt;  用于多线程压缩的压缩线程数    --comp-mt-dthreads &lt;number&gt;  用于多线程压缩的解压缩线程数    --comp-xbzrle-cache &lt;number&gt;  xbzrle压缩的页面缓存大小    --auto-converge-initial &lt;number&gt;  用于自动收敛的初始CPU节流速率    --auto-converge-increment &lt;number&gt;  用于自动收敛的CPU节流速率增量    --persistent-xml &lt;string&gt;  包含目标的更新的持久XML的文件名    --tls            使用TLS进行迁移</code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/42.jpg"></p><h3><span id="2-迁移">2、迁移</span></h3><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/43.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/44.jpg"></p><p><strong>使用TCP指定迁移时使用的网络</strong></p><p><code> virsh migrate centos64a gemu+ssh://root@labkvml/system--migrateuri tcp://172.16.1.231 --live--persistent --undefinesource</code></p><p><strong>查看kvm2主机上的xml文件</strong></p><pre><code>[root@kvm2 qemu]# lsgeneric.xml  networks[root@kvm2 qemu]# pwd/etc/libvirt/qemu</code></pre><p><strong>迁移</strong></p><pre><code>[root@kvm2 qemu]# virsh list  Id    Name                           State---------------------------------------------------- 3     generic                        running[root@kvm2 qemu]# virsh migrate --domain generic --desturi qemu+ssh://root@kvm/system --live --unsafe --persistent --undefinesource root@kvm&#39;s password: error: 无法在 &#39;kvm :49152&#39; 连接到服务器: 没有到主机的路由</code></pre><p><strong>上述报错是由于目标防火墙没开通49152端口导致的，开放相关端口即可</strong></p><pre><code>[root@kvm qemu]# iptables -I INPUT -p tcp --dport 49152 -j ACCEPT</code></pre><p><strong>重新尝试迁移</strong></p><pre><code>[root@kvm2 qemu]# virsh migrate --domain generic --desturi qemu+ssh://root@kvm/system --live --unsafe --persistent --undefinesource root@kvm&#39;s password: [root@kvm2 qemu]# ls #迁移成功，且配置文件被删除networks</code></pre><p><strong>在目标端查看迁移结果</strong></p><pre><code>[root@kvm qemu]# virsh list  Id    Name                           State---------------------------------------------------- 9     generic                        running</code></pre><h1><span id="4-基于本地存储的动态迁移">4、基于本地存储的动态迁移</span></h1><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/45.png"></p><p><strong>查看主机状态及硬盘路径</strong></p><pre><code>[root@kvm qemu]# virsh list --all Id    Name                           State---------------------------------------------------- 10    generic                        running -     centos7.0                      shut off[root@kvm qemu]# virsh domblklist --domain generic Target     Source------------------------------------------------hda        /vm/centos7.6.rawhdb        -</code></pre><p><strong>迁移</strong></p><pre><code>[root@kvm qemu]# virsh migrate --domain generic --desturi qemu+ssh://kvm2/system --live --persistent --undefinesource root@kvm2&#39;s password: error: 不安全的迁移:Migration without shared storage is unsafe[root@kvm qemu]# virsh migrate --domain generic --desturi qemu+ssh://kvm2/system --live --persistent --undefinesource --unsaferoot@kvm2&#39;s password: error: Cannot access storage file &#39;/vm/centos7.6.raw&#39;: 没有那个文件或目录[root@kvm qemu]# virsh migrate --domain generic --desturi qemu+ssh://kvm2/system --live --persistent --undefinesource --unsafe --copy-storage-all root@kvm2&#39;s password: error: Cannot access storage file &#39;/vm/centos7.6.raw&#39;: 没有那个文件或目录[root@kvm qemu]# yum install centos-release-qemu-ev qemu-kvm-ev -y #目标宿主机和本机都需要执行[root@kvm qemu]# reboot #目标宿主机和本机都需要执行</code></pre><blockquote><p>centos-release-qemu-ev qemu-kvm-ev 这两个安装包会在yum的配置文件下生成相关配置文件，并且会对现有的kvm和libvirt版本进行升级和替换，生产环境中谨慎操作，建议升级后重启宿主机。目前Centos 7版本有这个问题，Ubuntu没有测试</p></blockquote><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/46.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/47.jpg"></p><pre><code>[root@kvm ~]# virsh migrate --domain generic --desturi qemu+ssh://kvm2/system --live --persistent --undefinesource --unsafe --copy-storage-all root@kvm2&#39;s password: error: 不支持的配置：Hypervisor 不支持 CPU 型号 Broadwell-noTSX-IBRS</code></pre><blockquote><p>上述报错是由于KVM2主机上的kvm版本不一致导致的，发现qemu-kvm-ev的软件包没有没安装成功，重新安装后解决此问题；</p></blockquote><pre><code>[root@kvm ~]# virsh migrate --domain generic --desturi qemu+ssh://kvm2/system --live --persistent --undefinesource --unsafe --copy-storage-all root@kvm2&#39;s password: error: 内部错误：无法执行 QEMU 命令 &#39;drive-mirror&#39;：Failed to connect socket: No route to host</code></pre><blockquote><p>上述报错是由于KVM2主机的防火墙没有放行相关端口，执行iptables -I INPUT -p tcp -s 192.168.200.128 -j ACCEPT 即可解决；</p></blockquote><pre><code>[root@kvm ~]# virsh migrate --domain generic --desturi qemu+ssh://kvm2/system --live --persistent --undefinesource --unsafe --copy-storage-all --verboseroot@kvm2&#39;s password: [root@kvm2 ~]# virsh list --all  Id    Name                           State---------------------------------------------------- 5     generic                        running</code></pre><blockquote><p>迁移成功，需要保证主机两端的存储池都是一致的,迁移过程中可以使用sar -n DEV 1 4 （间隔一秒，采样四次），进行流量观察，也可以使用iostat等命令观察硬盘IO的情况</p></blockquote><blockquote><p>在基于本地硬盘的迁移中，如果一个虚拟机由A宿主机迁移到B宿主机正常迁移后，想再次从B宿主机迁移回A宿主机，那么此时就需要重启B宿主机的libvirtd服务，否则会报下列错误</p></blockquote><pre><code>[root@node2 /]# virsh migrate --domain Centos7.9 --desturi qemu+ssh://root@node1/system --live --persistent --undefinesource --unsafe --copy-storage-all --verbose --error: Cannot access storage file &#39;/vm/Centos7.9.qcow2&#39; (as uid:107, gid:107): 没有那个文件或目录[root@node2 /]# qemu-img info /vm/Centos7.9.qcow2 qemu-img: Could not open &#39;/vm/Centos7.9.qcow2&#39;: Failed to get shared &quot;write&quot; lockIs another process using the image [/vm/Centos7.9.qcow2]?</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KVM </tag>
            
            <tag> KVM虚拟机迁移 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KVM虚拟化进阶之Linux群集安装与配置</title>
      <link href="/2023/04/12/KVM/4.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8BLinux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"/>
      <url>/2023/04/12/KVM/4.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8BLinux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#1-%E8%A7%84%E5%88%92%E8%AE%BE%E8%AE%A1">1、规划设计</a></li><li><a href="#2-%E7%BE%A4%E9%9B%86%E7%BB%84%E4%BB%B6%E5%AE%89%E8%A3%85">2、群集组件安装</a></li><li><a href="#3-%E7%BE%A4%E9%9B%86%E8%8A%82%E7%82%B9%E5%87%86%E5%A4%87">3、群集节点准备</a><ul><li><a href="#1-%E9%85%8D%E7%BD%AE%E4%B8%BB%E6%9C%BA%E5%90%8D%E5%8F%8A%E8%A7%A3%E6%9E%90">1、配置主机名及解析</a></li><li><a href="#2-%E9%85%8D%E7%BD%AEssh-key%E4%BA%92%E4%BF%A1%E5%8F%AF%E9%80%89">2、配置SSH Key互信（可选）</a></li><li><a href="#3-%E9%85%8D%E7%BD%AE%E6%97%B6%E9%92%9F">3、配置时钟</a></li><li><a href="#4-%E9%85%8D%E7%BD%AEiptables%E9%98%B2%E7%81%AB%E5%A2%99%E5%85%81%E8%AE%B8%E9%9B%86%E7%BE%A4%E7%BB%84%E4%BB%B6%E8%BF%90%E8%A1%8C">4、配置iptables防火墙允许集群组件运行</a></li><li><a href="#5-%E9%85%8D%E7%BD%AEpcs%E5%AE%88%E6%8A%A4%E7%A8%8B%E5%BA%8F">5、配置pcs守护程序</a></li><li><a href="#6-%E9%85%8D%E7%BD%AEhacluster%E8%B4%A6%E6%88%B7%E5%AF%86%E7%A0%81">6、配置hacluster账户密码</a></li><li><a href="#7-%E7%BC%96%E8%BE%91%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">7、编辑配置文件</a></li></ul></li><li><a href="#4-%E7%BE%A4%E9%9B%86%E7%9A%84%E5%88%9B%E5%BB%BA">4、群集的创建</a></li></ul><!-- tocstop --><h1><span id="1-规划设计">1、规划设计</span></h1><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/60.jpg"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/61.jpg"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/62.png"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/63.png"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/64.png"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/65.png"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/65.png"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/66.png"></p><table><thead><tr><th>主机名称</th><th>管理地址</th><th>心跳地址</th><th>存储地址</th></tr></thead><tbody><tr><td>KVM</td><td>192.168.200.200</td><td>192.168.0.200</td><td>192.168.1.200</td></tr><tr><td>KVM2</td><td>192.168.200.201</td><td>192.168.0.201</td><td>192.168.1.201</td></tr></tbody></table><h1><span id="2-群集组件安装">2、群集组件安装</span></h1><p><strong>1、查看需要安装的软件包</strong></p><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/67.jpg"></p><pre><code>[root@kvm ~]# yum list  | grep pacemakerdrbd-pacemaker.x86_64                    9.17.0-1.el7                  epel   pacemaker.x86_64                         1.1.23-1.el7_9.1              updatespacemaker-cli.x86_64                     1.1.23-1.el7_9.1              updatespacemaker-cluster-libs.i686              1.1.23-1.el7_9.1              updatespacemaker-cluster-libs.x86_64            1.1.23-1.el7_9.1              updatespacemaker-cts.x86_64                     1.1.23-1.el7_9.1              updatespacemaker-doc.x86_64                     1.1.23-1.el7_9.1              updatespacemaker-libs.i686                      1.1.23-1.el7_9.1              updatespacemaker-libs.x86_64                    1.1.23-1.el7_9.1              updatespacemaker-libs-devel.i686                1.1.23-1.el7_9.1              updatespacemaker-libs-devel.x86_64              1.1.23-1.el7_9.1              updatespacemaker-nagios-plugins-metadata.x86_64 1.1.23-1.el7_9.1              updatespacemaker-remote.x86_64                  1.1.23-1.el7_9.1              updates</code></pre><p><strong>2、安装相关软件</strong></p><pre><code>[root@kvm ~]# yum install -y pacemaker corosync pcs psmisc policycoreutils-python fence-agents-all #所有集群节点都需要安装Installed:  corosync.x86_64 0:2.4.5-7.el7_9.2                         fence-agents-all.x86_64 0:4.2.1-41.el7_9.6                         pacemaker.x86_64 0:1.1.23-1.el7_9.1                         pcs.x86_64 0:0.9.169-3.el7.centos.3                         policycoreutils-python.x86_64 0:2.5-34.el7                        Dependency Installed:  OpenIPMI.x86_64 0:2.0.27-1.el7                      OpenIPMI-libs.x86_64 0:2.0.27-1.el7                  OpenIPMI-modalias.x86_64 0:2.0.27-1.el7            audit-libs-python.x86_64 0:2.8.5-4.el7               checkpolicy.x86_64 0:2.5-8.el7                    cifs-utils.x86_64 0:6.2-10.el7                           clufter-bin.x86_64 0:0.77.1-1.el7                   clufter-common.noarch 0:0.77.1-1.el7                 corosynclib.x86_64 0:2.4.5-7.el7_9.2               device-mapper-multipath.x86_64 0:0.4.9-136.el7_9     fence-agents-amt-ws.x86_64 0:4.2.1-41.el7_9.6     fence-agents-apc.x86_64 0:4.2.1-41.el7_9.6               fence-agents-apc-snmp.x86_64 0:4.2.1-41.el7_9.6     fence-agents-bladecenter.x86_64 0:4.2.1-41.el7_9.6   fence-agents-brocade.x86_64 0:4.2.1-41.el7_9.6     fence-agents-cisco-mds.x86_64 0:4.2.1-41.el7_9.6     fence-agents-cisco-ucs.x86_64 0:4.2.1-41.el7_9.6  fence-agents-common.x86_64 0:4.2.1-41.el7_9.6            fence-agents-compute.x86_64 0:4.2.1-41.el7_9.6      fence-agents-drac5.x86_64 0:4.2.1-41.el7_9.6         fence-agents-eaton-snmp.x86_64 0:4.2.1-41.el7_9.6  fence-agents-emerson.x86_64 0:4.2.1-41.el7_9.6       fence-agents-eps.x86_64 0:4.2.1-41.el7_9.6        fence-agents-heuristics-ping.x86_64 0:4.2.1-41.el7_9.6   fence-agents-hpblade.x86_64 0:4.2.1-41.el7_9.6      fence-agents-ibmblade.x86_64 0:4.2.1-41.el7_9.6      fence-agents-ifmib.x86_64 0:4.2.1-41.el7_9.6       fence-agents-ilo-moonshot.x86_64 0:4.2.1-41.el7_9.6  fence-agents-ilo-mp.x86_64 0:4.2.1-41.el7_9.6     fence-agents-ilo-ssh.x86_64 0:4.2.1-41.el7_9.6           fence-agents-ilo2.x86_64 0:4.2.1-41.el7_9.6         fence-agents-intelmodular.x86_64 0:4.2.1-41.el7_9.6  fence-agents-ipdu.x86_64 0:4.2.1-41.el7_9.6        fence-agents-ipmilan.x86_64 0:4.2.1-41.el7_9.6       fence-agents-kdump.x86_64 0:4.2.1-41.el7_9.6      fence-agents-mpath.x86_64 0:4.2.1-41.el7_9.6             fence-agents-redfish.x86_64 0:4.2.1-41.el7_9.6      fence-agents-rhevm.x86_64 0:4.2.1-41.el7_9.6         fence-agents-rsa.x86_64 0:4.2.1-41.el7_9.6         fence-agents-rsb.x86_64 0:4.2.1-41.el7_9.6           fence-agents-sbd.x86_64 0:4.2.1-41.el7_9.6        fence-agents-scsi.x86_64 0:4.2.1-41.el7_9.6              fence-agents-vmware-rest.x86_64 0:4.2.1-41.el7_9.6  fence-agents-vmware-soap.x86_64 0:4.2.1-41.el7_9.6   fence-agents-wti.x86_64 0:4.2.1-41.el7_9.6         fence-virt.x86_64 0:0.3.2-16.el7                     ipmitool.x86_64 0:1.8.18-10.el7_9                 liberation-fonts-common.noarch 1:1.07.2-16.el7           liberation-sans-fonts.noarch 1:1.07.2-16.el7        libldb.x86_64 0:1.5.4-2.el7                          libqb.x86_64 0:1.0.1-9.el7                         libsemanage-python.x86_64 0:2.5-14.el7               libtalloc.x86_64 0:2.1.16-1.el7                   libtdb.x86_64 0:1.3.18-1.el7                             libtevent.x86_64 0:0.9.39-1.el7                     libwbclient.x86_64 0:4.10.16-24.el7_9                libwsman1.x86_64 0:2.6.3-7.git4391e5c.el7          net-snmp-libs.x86_64 1:5.7.2-49.el7_9.2              net-snmp-utils.x86_64 1:5.7.2-49.el7_9.2          openwsman-python.x86_64 0:2.6.3-7.git4391e5c.el7         overpass-fonts.noarch 0:2.1-1.el7                   pacemaker-cli.x86_64 0:1.1.23-1.el7_9.1              pacemaker-cluster-libs.x86_64 0:1.1.23-1.el7_9.1   pacemaker-libs.x86_64 0:1.1.23-1.el7_9.1             perl-TimeDate.noarch 1:2.30-2.el7                 pexpect.noarch 0:2.3-11.el7                              python-IPy.noarch 0:0.75-6.el7                      python-clufter.noarch 0:0.77.1-1.el7                 python-lxml.x86_64 0:3.2.1-4.el7                   python2-subprocess32.x86_64 0:3.2.6-14.el7           resource-agents.x86_64 0:4.1.1-61.el7_9.18        ruby.x86_64 0:2.0.0.648-39.el7_9                         ruby-irb.noarch 0:2.0.0.648-39.el7_9                ruby-libs.x86_64 0:2.0.0.648-39.el7_9                rubygem-bigdecimal.x86_64 0:1.2.0-39.el7_9         rubygem-io-console.x86_64 0:0.4.2-39.el7_9           rubygem-json.x86_64 0:1.7.7-39.el7_9              rubygem-psych.x86_64 0:2.0.0-39.el7_9                    rubygem-rdoc.noarch 0:4.0.0-39.el7_9                rubygems.noarch 0:2.0.14.1-39.el7_9                  samba-client-libs.x86_64 0:4.10.16-24.el7_9        samba-common.noarch 0:4.10.16-24.el7_9               samba-common-libs.x86_64 0:4.10.16-24.el7_9       setools-libs.x86_64 0:3.3.8-4.el7                        telnet.x86_64 1:0.17-66.el7                        [root@kvm ~]# yum update -y</code></pre><p><strong>3、查看相关软件描述</strong></p><p><strong>pacemaker</strong></p><pre><code>[root@kvm ~]# rpm -qi pacemakerName        : pacemakerVersion     : 1.1.23Release     : 1.el7_9.1Architecture: x86_64Install Date: Thu 13 Apr 2023 03:21:10 PM CSTGroup       : System Environment/DaemonsSize        : 1285198License     : GPLv2+ and LGPLv2+Signature   : RSA/SHA256, Fri 18 Dec 2020 04:35:18 AM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : pacemaker-1.1.23-1.el7_9.1.src.rpmBuild Date  : Wed 16 Dec 2020 12:40:01 AM CSTBuild Host  : x86-02.bsys.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : http://www.clusterlabs.orgSummary     : Scalable High-Availability cluster resource managerDescription :Pacemaker是一种先进的、可扩展的高可用性集群资源Corosync、CMAN和/或Linux HA的管理器。它支持超过16个具有重要功能的节点群集用于管理资源和依赖关系。它将在初始化时运行脚本，当机器上升或下降时，当相关资源出现故障时，可以配置为定期检查资源健康。可用的重建重建选项：-with(out) :cman覆盖范围文档stonithd硬化预释放分析</code></pre><p><strong>corosync</strong></p><pre><code>[root@kvm ~]# rpm -qi corosyncName        : corosyncVersion     : 2.4.5Release     : 7.el7_9.2Architecture: x86_64Install Date: Thu 13 Apr 2023 03:21:06 PM CSTGroup       : System Environment/BaseSize        : 485644License     : BSDSignature   : RSA/SHA256, Wed 01 Dec 2021 10:14:09 PM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : corosync-2.4.5-7.el7_9.2.src.rpmBuild Date  : Thu 25 Nov 2021 12:33:10 AM CSTBuild Host  : x86-02.bsys.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : http://corosync.github.io/corosync/Summary     : The Corosync Cluster Engine and Application Programming InterfacesDescription :此软件包包含Corosync Cluster Engine Executive，几个默认配置API和库、默认配置文件以及init脚本。</code></pre><p><strong>pcs</strong></p><pre><code>[root@kvm ~]# rpm -qi pcsName        : pcsVersion     : 0.9.169Release     : 3.el7.centos.3Architecture: x86_64Install Date: Thu 13 Apr 2023 03:21:17 PM CSTGroup       : System Environment/BaseSize        : 11483086License     : GPLv2Signature   : RSA/SHA256, Sat 12 Nov 2022 12:15:48 AM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : pcs-0.9.169-3.el7.centos.3.src.rpmBuild Date  : Thu 10 Nov 2022 09:56:56 PM CSTBuild Host  : x86-02.bsys.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : https://github.com/ClusterLabs/pcsSummary     : Pacemaker Configuration SystemDescription :pcs是一种同步器和起搏器配置工具。它允许用户轻松查看、修改和创建基于起搏器的集群。</code></pre><p><strong>psmisc</strong></p><pre><code>[root@kvm ~]# rpm -qi psmiscName        : psmiscVersion     : 22.20Release     : 17.el7Architecture: x86_64Install Date: Sun 02 Apr 2023 03:58:57 PM CSTGroup       : Applications/SystemSize        : 486607License     : GPLv2+Signature   : RSA/SHA256, Thu 15 Oct 2020 02:59:05 AM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : psmisc-22.20-17.el7.src.rpmBuild Date  : Thu 01 Oct 2020 01:20:29 AM CSTBuild Host  : x86-01.bsys.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : http://sourceforge.net/projects/psmiscSummary     : Utilities for managing processes on your systemDescription :psmisc包包含用于管理系统：pstree、killall和fuser。pstree命令显示一个树系统上所有正在运行的进程的结构。杀手命令将指定的信号（如果未指定任何内容，则为SIGTERM）发送到通过名称标识的进程。热熔器命令识别PID使用指定文件或文件系统的进程。</code></pre><p><strong>policycoreutils-python</strong></p><pre><code>[root@kvm ~]# rpm -qi policycoreutils-python Name        : policycoreutils-pythonVersion     : 2.5Release     : 34.el7Architecture: x86_64Install Date: Thu 13 Apr 2023 03:21:11 PM CSTGroup       : System Environment/BaseSize        : 1304826License     : GPLv2Signature   : RSA/SHA256, Sat 04 Apr 2020 05:05:35 AM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : policycoreutils-2.5-34.el7.src.rpmBuild Date  : Wed 01 Apr 2020 12:04:58 PM CSTBuild Host  : x86-02.bsys.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : http://www.selinuxproject.orgSummary     : SELinux policy core python utilitiesDescription :policycoreutils-python包包含用于管理的管理工具SELinux环境。</code></pre><p><strong>fence-agents-all</strong></p><pre><code>[root@kvm ~]# rpm -qi fence-agents-all Name        : fence-agents-allVersion     : 4.2.1Release     : 41.el7_9.6Architecture: x86_64Install Date: Thu 13 Apr 2023 03:21:15 PM CSTGroup       : System Environment/BaseSize        : 0License     : GPLv2+ and LGPLv2+ and ASL 2.0Signature   : RSA/SHA256, Thu 07 Apr 2022 01:04:19 AM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : fence-agents-4.2.1-41.el7_9.6.src.rpmBuild Date  : Wed 06 Apr 2022 12:26:21 AM CSTBuild Host  : x86-01.bsys.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : https://github.com/ClusterLabs/fence-agentsSummary     : Fence agentsDescription :Red Hat围栏代理是所有受支持的围栏代理的集合。</code></pre><h1><span id="3-群集节点准备">3、群集节点准备</span></h1><h2><span id="1-配置主机名及解析">1、配置主机名及解析</span></h2><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/67.jpg"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/68.jpg"><br>node1:</p><pre><code>[root@kvm ~]# hostnamectl set-hostname node1[root@kvm ~]# bash[root@node1 ~]# vim /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.200 node1-manage192.168.200.201 node2-manage192.168.0.200 node1-heartbeat192.168.0.201 node2-heartbeat192.168.1.200 node1-storage192.168.1.201 node2-storage[root@node1 ~]# ping node2-manage PING node2-manage (192.168.200.201) 56(84) bytes of data.64 bytes from node2-manage (192.168.200.201): icmp_seq=1 ttl=64 time=1.26 ms--- node2-manage ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 1.268/1.268/1.268/0.000 ms[root@node1 ~]# ping node2-heartbeat PING node2-heartbeat (192.168.0.201) 56(84) bytes of data.64 bytes from node2-heartbeat (192.168.0.201): icmp_seq=1 ttl=64 time=9.27 ms--- node2-heartbeat ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 9.276/9.276/9.276/0.000 ms[root@node1 ~]# ping node2-storage PING node2-storage (192.168.1.201) 56(84) bytes of data.64 bytes from node2-storage (192.168.1.201): icmp_seq=1 ttl=64 time=0.700 ms--- node2-storage ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.700/0.700/0.700/0.000 ms</code></pre><p>node2:</p><pre><code>[root@kvm2 ~]# hostnamectl set-hostname node2[root@kvm2 ~]# bash[root@node2 ~]# vim /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.200 node1-manage192.168.200.201 node2-manage192.168.0.200 node1-heartbeat192.168.0.201 node2-heartbeat192.168.1.200 node1-storage192.168.1.201 node2-storage[root@node2 ~]# ping node1-manage PING node1-manage (192.168.200.200) 56(84) bytes of data.64 bytes from node1-manage (192.168.200.200): icmp_seq=1 ttl=64 time=1.06 ms--- node1-manage ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 1.063/1.063/1.063/0.000 ms[root@node2 ~]# ping node1-heartbeat PING node1-heartbeat (192.168.0.200) 56(84) bytes of data.64 bytes from node1-heartbeat (192.168.0.200): icmp_seq=1 ttl=64 time=10.6 ms--- node1-heartbeat ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 10.616/10.616/10.616/0.000 ms[root@node2 ~]# ping node1-storage PING node1-storage (192.168.1.200) 56(84) bytes of data.64 bytes from node1-storage (192.168.1.200): icmp_seq=1 ttl=64 time=0.269 ms--- node1-storage ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.269/0.269/0.269/0.000 ms</code></pre><h2><span id="2-配置ssh-key互信可选">2、配置SSH Key互信（可选）</span></h2><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/69.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# ssh-keygen -t rsa -P &#39;&#39;Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:OGWrktFMBJXAHiwVbzg5pX9ZTg5Do99QQyCJWxjlUaI root@node1The key&#39;s randomart image is:+---[RSA 2048]----+|   +*XO+=o+      ||  . *X+* o .     ||   oE+* * o      ||    oO = %       ||    . * S +      ||     o +         ||    o .          ||     .           ||                 |+----[SHA256]-----+[root@node1 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@node2/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: ERROR: ssh: Could not resolve hostname node2: Name or service not known[root@node1 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@node2-manage/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;The authenticity of host &#39;node2-manage (192.168.200.201)&#39; can&#39;t be established.ECDSA key fingerprint is SHA256:s2qiffBOYjuMJQOHEI9r87NTNjd0XyoEf81myM78kbQ.ECDSA key fingerprint is MD5:a5:8f:de:d4:07:9f:b7:b3:6f:7c:b0:c3:55:eb:bb:b0.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@node2-manage&#39;s password: Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#39;root@node2-manage&#39;&quot;and check to make sure that only the key(s) you wanted were added.[root@node1 ~]# ssh root@node2-manageLast login: Thu Apr 13 15:16:28 2023 from 192.168.200.1[root@node2 ~]# exit logoutConnection to node2-manage closed.</code></pre><p>node2:</p><pre><code>[root@node2 ~]# ssh-keygen -t rsa -P &#39;&#39;Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:E9njGHnhnwZtQGy5CmomfuxGQENWeVNRWHe9W0q9QRw root@node2The key&#39;s randomart image is:+---[RSA 2048]----+| .o... .oB*.. oE.|| .o . o .=+= . o.|| . . . .=.*.o ...||  .   .  *.= ..oo||   . . .S.. +. .=||  . =   .. .  .o || . *             ||  . +            ||   +.            |+----[SHA256]-----+[root@node2 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@node1-manage/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;The authenticity of host &#39;node1-manage (192.168.200.200)&#39; can&#39;t be established.ECDSA key fingerprint is SHA256:s2qiffBOYjuMJQOHEI9r87NTNjd0XyoEf81myM78kbQ.ECDSA key fingerprint is MD5:a5:8f:de:d4:07:9f:b7:b3:6f:7c:b0:c3:55:eb:bb:b0.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@node1-manage&#39;s password: Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#39;root@node1-manage&#39;&quot;and check to make sure that only the key(s) you wanted were added.[root@node2 ~]# ssh root@node1-manageLast login: Thu Apr 13 15:16:27 2023 from 192.168.200.1[root@node1 ~]# exit logoutConnection to node1-manage closed.</code></pre><h2><span id="3-配置时钟">3、配置时钟</span></h2><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/70.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# ntpdate time.windows.com13 Apr 16:20:11 ntpdate[6059]: adjust time server 52.231.114.183 offset 0.021235 sec[root@node1 ~]# crontab -eno crontab for root - using an empty onecrontab: installing new crontab[root@node1 ~]# crontab -l*/30 * * * * /sbin/ntpdate ntp1.aliyun.com &amp;&gt; /dev/null</code></pre><p>node2:</p><pre><code>[root@node1 ~]# ntpdate time.windows.com13 Apr 16:20:11 ntpdate[6059]: adjust time server 52.231.114.183 offset 0.021235 sec[root@node1 ~]# crontab -eno crontab for root - using an empty onecrontab: installing new crontab[root@node1 ~]# crontab -l*/30 * * * * /sbin/ntpdate ntp1.aliyun.com &amp;&gt; /dev/null</code></pre><h2><span id="4-配置iptables防火墙允许集群组件运行">4、配置iptables防火墙允许集群组件运行</span></h2><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/71.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# firewall-cmd --permanent --add-service=high-availabilitysuccess[root@node1 ~]# firewall-cmd --add-service=high-availabilitysuccess[root@node1 ~]# firewall-cmd --reloadsuccess[root@node1 ~]# firewall-cmd --get-serviceRH-Satellite-6 RH-Satellite-6-capsule amanda-client amanda-k5-client amqp amqps apcupsd audit bacula bacula-client bgp bitcoin bitcoin-rpc bitcoin-testnet bitcoin-testnet-rpc ceph ceph-mon cfengine condor-collector ctdb dhcp dhcpv6 dhcpv6-client distcc dns docker-registry docker-swarm dropbox-lansync elasticsearch etcd-client etcd-server finger freeipa-ldap freeipa-ldaps freeipa-replication freeipa-trust ftp ganglia-client ganglia-master git gre high-availability http https imap imaps ipp ipp-client ipsec irc ircs iscsi-target isns jenkins kadmin kerberos kibana klogin kpasswd kprop kshell ldap ldaps libvirt libvirt-tls lightning-network llmnr managesieve matrix mdns minidlna mongodb mosh mountd mqtt mqtt-tls ms-wbt mssql murmur mysql nfs nfs3 nmea-0183 nrpe ntp nut openvpn ovirt-imageio ovirt-storageconsole ovirt-vmconsole plex pmcd pmproxy pmwebapi pmwebapis pop3 pop3s postgresql privoxy proxy-dhcp ptp pulseaudio puppetmaster quassel radius redis rpc-bind rsh rsyncd rtsp salt-master samba samba-client samba-dc sane sip sips slp smtp smtp-submission smtps snmp snmptrap spideroak-lansync squid ssh steam-streaming svdrp svn syncthing syncthing-gui synergy syslog syslog-tls telnet tftp tftp-client tinc tor-socks transmission-client upnp-client vdsm vnc-server wbem-http wbem-https wsman wsmans xdmcp xmpp-bosh xmpp-client xmpp-local xmpp-server zabbix-agent zabbix-server</code></pre><p>node2:</p><pre><code>[root@node1 ~]# firewall-cmd --permanent --add-service=high-availabilitysuccess[root@node1 ~]# firewall-cmd --add-service=high-availabilitysuccess[root@node1 ~]# firewall-cmd --reloadsuccess</code></pre><h2><span id="5-配置pcs守护程序">5、配置pcs守护程序</span></h2><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/72.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# systemctl start pcsd[root@node1 ~]# systemctl enable pcsdCreated symlink from /etc/systemd/system/multi-user.target.wants/pcsd.service to /usr/lib/systemd/system/pcsd.service.[root@node1 ~]# systemctl status pcsd● pcsd.service - PCS GUI and remote configuration interface   Loaded: loaded (/usr/lib/systemd/system/pcsd.service; enabled; vendor preset: disabled)   Active: active (running) since Thu 2023-04-13 16:36:13 CST; 8s ago     Docs: man:pcsd(8)           man:pcs(8) Main PID: 6161 (pcsd)   CGroup: /system.slice/pcsd.service           └─6161 /usr/bin/ruby /usr/lib/pcsd/pcsdApr 13 16:36:13 node1 systemd[1]: Starting PCS GUI and remote configuration interface...Apr 13 16:36:13 node1 systemd[1]: Started PCS GUI and remote configuration interface.</code></pre><p>node2:</p><pre><code>[root@node2 ~]# systemctl start pcsd[root@node2 ~]# systemctl enable pcsdCreated symlink from /etc/systemd/system/multi-user.target.wants/pcsd.service to /usr/lib/systemd/system/pcsd.service.[root@node2 ~]# systemctl status pcsd● pcsd.service - PCS GUI and remote configuration interface   Loaded: loaded (/usr/lib/systemd/system/pcsd.service; enabled; vendor preset: disabled)   Active: active (running) since Thu 2023-04-13 16:35:20 CST; 19s ago     Docs: man:pcsd(8)           man:pcs(8) Main PID: 6483 (pcsd)   CGroup: /system.slice/pcsd.service           └─6483 /usr/bin/ruby /usr/lib/pcsd/pcsdApr 13 16:35:20 node2 systemd[1]: Starting PCS GUI and remote configuration interface...Apr 13 16:35:20 node2 systemd[1]: Started PCS GUI and remote configuration interface.</code></pre><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/73.jpg"></p><h2><span id="6-配置hacluster账户密码">6、配置hacluster账户密码</span></h2><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/74.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# echo &quot;password&quot; | passwd --stdin hacluster Changing password for user hacluster.passwd: all authentication tokens updated successfully.</code></pre><p>node2:</p><pre><code>[root@node2 ~]# echo &quot;password&quot; | passwd --stdin hacluster Changing password for user hacluster.passwd: all authentication tokens updated successfully.</code></pre><h2><span id="7-编辑配置文件">7、编辑配置文件</span></h2><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/75.jpg"></p><h1><span id="4-群集的创建">4、群集的创建</span></h1><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/76.jpg"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/77.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# pcs cluster auth node1-heartbeat node2-heartbeatUsername: haclusterPassword: passwordnode1-heartbeat: Authorizednode2-heartbeat: Authorized[root@node1 corosync]# pcs cluster setup --name cluster1 node1-heartbeat node2-heartbeat[root@node1 corosync]# pwd/etc/corosync[root@node1 corosync]# ls -lhtotal 16K-rw-r--r--. 1 root root  397 Apr 13 16:56 corosync.conf-rw-r--r--. 1 root root 2.9K Nov 25  2021 corosync.conf.example-rw-r--r--. 1 root root  767 Nov 25  2021 corosync.conf.example.udpu-rw-r--r--. 1 root root 3.3K Nov 25  2021 corosync.xml.exampledrwxr-xr-x. 2 root root    6 Nov 25  2021 uidgid.d[root@node1 corosync]# pcs cluster status Error: cluster is not currently running on this node[root@node1 corosync]# cat corosync.conftotem &#123;    version: 2 #集群版本    cluster_name: cluster1 #集群名称    secauth: off #安全身份验证协议    transport: udpu #节点之间的传输协议&#125;nodelist &#123; #节点的列表，可以使用Ip或者定义的主机名称    node &#123;        ring0_addr: node1-heartbeat #心跳网络        nodeid: 1    &#125;    node &#123;        ring0_addr: node1-heartbeat #心跳网络        nodeid: 2    &#125;&#125;quorum &#123;     provider: corosync_votequorum     two_node: 1  #告诉集群当前是双节点的，只剩一个节点时不要将集群停止&#125;logging &#123; #日志相关    to_logfile: yes    logfile: /var/log/cluster/corosync.log    to_syslog: yes&#125;[root@node1 corosync]# pcs cluster start --help  #查看启动帮助Usage: pcs cluster start...    start [--all | &lt;node&gt;... ] [--wait[=&lt;n&gt;]] [--request-timeout=&lt;seconds&gt;]        Start a cluster on specified node(s). If no nodes are specified then        start a cluster on the local node. If --all is specified then start        a cluster on all nodes. If the cluster has many nodes then the start        request may time out. In that case you should consider setting        --request-timeout to a suitable value. If --wait is specified, pcs        waits up to &#39;n&#39; seconds for the cluster to get ready to provide        services after the cluster has successfully started.[root@node1 corosync]# pcs cluster start  --all  #启动所有节点，不使用--all的时候默认启动当前节点，此时可以在其他主机的syslog中查看相关日志信息node1-heartbeat: Starting Cluster (corosync)...node2-heartbeat: Starting Cluster (corosync)...node1-heartbeat: Starting Cluster (pacemaker)...node2-heartbeat: Starting Cluster (pacemaker)...[root@node1 corosync]# pcs status #查看pcs状态Cluster name: cluster1 #集群的名称WARNINGS:No stonith devices and stonith-enabled is not false #目前是正常现象，还没有配置stonithStack: corosync #使用corosync作为心跳Current DC: node1-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition WITHOUT quorumLast updated: Fri Apr 14 09:25:54 2023 #获取信息时间Last change: Fri Apr 14 09:24:26 2023 by hacluster via crmd on node1-heartbeat #获取信息时间2 nodes configured #双节点的集群0 resource instances configured  #目前没有配置资源Online: [ node1-heartbeat node2-heartbeat ] #当前集群运行的主机No resourcesDaemon Status: #响应的守护程序  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled[root@node1 corosync]# pcs cluster status #查看集群当前状态Cluster Status: Stack: corosync Current DC: node1-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition WITHOUT quorum #当前的DC Last updated: Fri Apr 14 09:25:00 2023 Last change: Fri Apr 14 09:24:26 2023 by hacluster via crmd on node1-heartbeat 2 nodes configured 0 resource instances configuredPCSD Status: #当前集群两个节点的状态都是在线的  node1-heartbeat: Online  node2-heartbeat: Online  [root@node1 corosync]# systemctl status pacemaker.service -l #查看响应的服务状态● pacemaker.service - Pacemaker High Availability Cluster Manager   Loaded: loaded (/usr/lib/systemd/system/pacemaker.service; disabled; vendor preset: disabled)   Active: active (running) since Fri 2023-04-14 09:32:57 CST; 4min 10s ago     Docs: man:pacemakerd           https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/1.1/html-single/Pacemaker_Explained/index.html Main PID: 12054 (pacemakerd)    Tasks: 7   CGroup: /system.slice/pacemaker.service           ├─12054 /usr/sbin/pacemakerd -f           ├─12055 /usr/libexec/pacemaker/cib           ├─12056 /usr/libexec/pacemaker/stonithd           ├─12057 /usr/libexec/pacemaker/lrmd           ├─12058 /usr/libexec/pacemaker/attrd           ├─12059 /usr/libexec/pacemaker/pengine           └─12060 /usr/libexec/pacemaker/crmdApr 14 09:32:58 node1 crmd[12060]:   notice: Quorum acquiredApr 14 09:32:58 node1 crmd[12060]:   notice: Node node1-heartbeat state is now memberApr 14 09:32:58 node1 crmd[12060]:   notice: Node node2-heartbeat state is now memberApr 14 09:32:58 node1 crmd[12060]:   notice: The local CRM is operationalApr 14 09:32:58 node1 crmd[12060]:   notice: State transition S_STARTING -&gt; S_PENDINGApr 14 09:33:00 node1 crmd[12060]:   notice: Fencer successfully connectedApr 14 09:33:19 node1 crmd[12060]:  warning: Input I_DC_TIMEOUT received in state S_PENDING from crm_timer_poppedApr 14 09:33:19 node1 crmd[12060]:   notice: State transition S_ELECTION -&gt; S_PENDINGApr 14 09:33:19 node1 crmd[12060]:   notice: State transition S_PENDING -&gt; S_NOT_DCApr 14 09:33:19 node1 attrd[12058]:   notice: Updating all attributes after cib_refresh_notify event[root@node1 corosync]# systemctl status corosync -l ● corosync.service - Corosync Cluster Engine   Loaded: loaded (/usr/lib/systemd/system/corosync.service; disabled; vendor preset: disabled)   Active: active (running) since Fri 2023-04-14 09:32:57 CST; 4min 39s ago     Docs: man:corosync           man:corosync.conf           man:corosync_overview  Process: 12025 ExecStart=/usr/share/corosync/corosync start (code=exited, status=0/SUCCESS) Main PID: 12034 (corosync)    Tasks: 2   CGroup: /system.slice/corosync.service           └─12034 corosyncApr 14 09:32:56 node1 corosync[12034]:  [QUORUM] Members[1]: 1Apr 14 09:32:56 node1 corosync[12034]:  [MAIN  ] Completed service synchronization, ready to provide service.Apr 14 09:32:56 node1 corosync[12034]:  [TOTEM ] A new membership (192.168.0.200:14) was formed. Members joined: 2Apr 14 09:32:56 node1 corosync[12034]:  [CPG   ] downlist left_list: 0 receivedApr 14 09:32:56 node1 corosync[12034]:  [CPG   ] downlist left_list: 0 receivedApr 14 09:32:56 node1 corosync[12034]:  [QUORUM] This node is within the primary component and will provide service.Apr 14 09:32:56 node1 corosync[12034]:  [QUORUM] Members[2]: 1 2Apr 14 09:32:56 node1 corosync[12034]:  [MAIN  ] Completed service synchronization, ready to provide service.Apr 14 09:32:57 node1 corosync[12025]: Starting Corosync Cluster Engine (corosync): [  确定  ]Apr 14 09:32:57 node1 systemd[1]: Started Corosync Cluster Engine.[root@node1 corosync]# systemctl status pcsd -l● pcsd.service - PCS GUI and remote configuration interface   Loaded: loaded (/usr/lib/systemd/system/pcsd.service; enabled; vendor preset: disabled)   Active: active (running) since Thu 2023-04-13 16:56:20 CST; 16h ago     Docs: man:pcsd(8)           man:pcs(8) Main PID: 6555 (pcsd)    Tasks: 6   CGroup: /system.slice/pcsd.service           └─6555 /usr/bin/ruby /usr/lib/pcsd/pcsdApr 13 16:56:19 node1 systemd[1]: Stopped PCS GUI and remote configuration interface.Apr 13 16:56:19 node1 systemd[1]: Starting PCS GUI and remote configuration interface...Apr 13 16:56:20 node1 systemd[1]: Started PCS GUI and remote configuration interface.[root@node1 corosync]# netstat -ntlp | grep 2224 #集群安装并启动后会运行一个GUI的服务，默认端口使用的是2224，使用https的方式来进行访问，用户名密码使用上述创建的，hacluster/passwordtcp6       0      0 :::2224                 :::*                    LISTEN      6555/ruby</code></pre><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/78.jpg"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/79.png"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KVM </tag>
            
            <tag> Linux HA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KVM虚拟化进阶之基于NFS的KVM群集构建</title>
      <link href="/2023/04/12/KVM/5.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%B6%E6%9E%84/"/>
      <url>/2023/04/12/KVM/5.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#1-%E8%A7%84%E5%88%92%E8%AE%BE%E8%AE%A1">1、规划设计</a></li><li><a href="#2-%E8%8A%82%E7%82%B9%E5%87%86%E5%A4%87">2、节点准备</a><ul><li><a href="#1-%E9%98%B6%E6%AE%B51%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85">1、阶段1：操作系统安装</a></li><li><a href="#2-%E9%98%B6%E6%AE%B52%E7%BE%A4%E9%9B%86%E7%BB%84%E4%BB%B6%E5%AE%89%E8%A3%85">2、阶段2：群集组件安装</a></li><li><a href="#3-%E9%98%B6%E6%AE%B53%E7%BE%A4%E9%9B%86%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85">3、阶段3：群集节点安装</a></li></ul></li><li><a href="#3-%E9%85%8D%E7%BD%AEnfs%E8%B5%84%E6%BA%90">3、配置NFS资源</a></li><li><a href="#4-%E5%87%86%E5%A4%87%E8%99%9A%E6%8B%9F%E6%9C%BA">4、准备虚拟机</a></li><li><a href="#5-%E5%90%91%E7%BE%A4%E9%9B%86%E6%B7%BB%E5%8A%A0%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%B5%84%E6%BA%90">5、向群集添加虚拟机资源</a></li><li><a href="#6-%E7%BE%A4%E9%9B%86%E6%B5%8B%E8%AF%95">6、群集测试</a></li><li><a href="#7-%E9%85%8D%E7%BD%AEstonthipmi">7、配置STONTH（IPMI）</a></li></ul><!-- tocstop --><h1><span id="1-规划设计">1、规划设计</span></h1><table><thead><tr><th>主机名称</th><th>管理地址</th><th>心跳地址</th><th>存储地址</th></tr></thead><tbody><tr><td>node1</td><td>192.168.200.200</td><td>192.168.0.200</td><td>192.168.1.200</td></tr><tr><td>node2</td><td>192.168.200.201</td><td>192.168.0.201</td><td>192.168.1.201</td></tr><tr><td>stor1</td><td>192.168.200.202</td><td></td><td>192.168.1.202</td></tr></tbody></table><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/80.jpg"></p><h1><span id="2-节点准备">2、节点准备</span></h1><h2><span id="1-阶段1操作系统安装">1、阶段1：操作系统安装</span></h2><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/81.jpg"></p><h2><span id="2-阶段2群集组件安装">2、阶段2：群集组件安装</span></h2><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/82.jpg"></p><h2><span id="3-阶段3群集节点安装">3、阶段3：群集节点安装</span></h2><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/83.jpg"></p><blockquote><p>需要注意各个节点的防火墙配置！</p></blockquote><h1><span id="3-配置nfs资源">3、配置NFS资源</span></h1><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/84.jpg"></p><blockquote><p>防火墙是指NFS服务器防火墙</p></blockquote><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/85.jpg"></p><h1><span id="4-准备虚拟机">4、准备虚拟机</span></h1><p><strong>所有节点的host应一致</strong></p><pre><code>[root@node1 corosync]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.200 node1192.168.200.201 node2192.168.0.200 node1-heartbeat192.168.0.201 node2-heartbeat192.168.1.200 node1-storage192.168.1.201 node2-storage192.168.1.202 nfs</code></pre><p><strong>所有主机都应该测试能否访问NFS服务器</strong></p><pre><code>[root@node1 vmdata]# showmount -e 192.168.1.202Export list for 192.168.1.202:/vmdata *[root@node1 vmdata]# mount nfs:/vmdata /vmdata #将NFS服务器的共享目录挂载到本地的/vmdata中，所有主机都执行[root@node1 vmdata]# df -HT  #查看挂载情况Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  2.0G     0  2.0G   0% /devtmpfs                   tmpfs     2.0G   40M  2.0G   3% /dev/shmtmpfs                   tmpfs     2.0G   13M  2.0G   1% /runtmpfs                   tmpfs     2.0G     0  2.0G   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        38G   37G  1.1G  98% //dev/sda1               xfs       1.1G  206M  859M  20% /boottmpfs                   tmpfs     396M     0  396M   0% /run/user/0nfs:/vmdata             nfs4       38G   13G   26G  34% /vmdata[root@node1 ~]# cat /etc/fstab ## /etc/fstab# Created by anaconda on Sun Apr  2 23:28:50 2023## Accessible filesystems, by reference, are maintained under &#39;/dev/disk&#39;# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#/dev/mapper/centos-root /                       xfs     defaults        0 0UUID=b283ae1c-89a3-40a1-b8f9-21699c857148 /boot                   xfs     defaults        0 0/dev/mapper/centos-swap swap                    swap    defaults        0 0nfs:/vmdata /vmdata nfs4    defaults 0 0 [root@node1 ~]# setsebool -P virt_use_nfs 1</code></pre><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/86.jpg"><br><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/87.jpg"></p><p><strong>配置虚拟机使用NFS作为后端存储</strong></p><pre><code>[root@node1 vmdata]# virsh list --all  Id    Name                           State---------------------------------------------------- 8     Centos7.9                      running[root@node1 vmdata]# virsh shutdown --domain Centos7.9 Domain Centos7.9 is being shutdown[root@node1 vmdata]# virsh edit --domain Centos7.9       &lt;source file=&#39;/vmdata/Centos7.9.qcow2&#39;/&gt;      Domain Centos7.9 XML configuration edited.[root@node1 vmdata]# virsh start --domain Centos7.9 Domain Centos7.9 started</code></pre><p><strong>测试基于NFS的动态迁移</strong></p><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/88.jpg"></p><p>node1 &gt; node2</p><pre><code>[root@node1 vmdata]# virsh migrate --domain Centos7.9 --desturi qemu+ssh://root@node2/system --live --persistent --undefinesource --unsafe  --verboseMigration: [100 %][root@node1 vmdata]# iptables -I INPUT -p tcp -s 192.168.1.0/24 -j ACCEPT</code></pre><p>node2 &gt; node1</p><pre><code>[root@node2 vmdata]# virsh migrate --domain Centos7.9 --desturi qemu+ssh://root@node1/system --live --persistent --undefinesource --unsafe --verbose --migrateuri tcp://192.168.1.200Migration: [100 %]</code></pre><h1><span id="5-向群集添加虚拟机资源">5、向群集添加虚拟机资源</span></h1><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/89.jpg"></p><p><strong>迁移虚拟机的配置文件到共享存储中</strong></p><pre><code>[root@node1 ~]# df -HT Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  2.0G     0  2.0G   0% /devtmpfs                   tmpfs     2.0G   40M  2.0G   3% /dev/shmtmpfs                   tmpfs     2.0G   13M  2.0G   1% /runtmpfs                   tmpfs     2.0G     0  2.0G   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        38G  3.8G   34G  10% //dev/sda1               xfs       1.1G  206M  858M  20% /bootnfs:/vmdata             nfs4       38G   22G   17G  57% /vmdatatmpfs                   tmpfs     396M     0  396M   0% /run/user/0[root@node1 ~]# cd /vmdata/[root@node1 vmdata]# lsCentos7.9.qcow2[root@node1 vmdata]# mkdir qemu_config[root@node1 vmdata]# virsh list  Id    Name                           State---------------------------------------------------- 15    Centos7.9                      running[root@node1 vmdata]# virsh shutdown --domain Centos7.9 Domain Centos7.9 is being shutdown[root@node1 vmdata]# cp /etc/libvirt/qemu/Centos7.9.xml /vmdata/qemu_config/[root@node1 vmdata]# virsh undefine --domain Centos7.9 Domain Centos7.9 has been undefined  取消libvirtd的控制权</code></pre><p><strong>启动虚拟机</strong></p><pre><code>[root@node1 qemu_config]# pcs status Cluster name: cluster1WARNINGS:No stonith devices and stonith-enabled is not falseStack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 14:18:02 2023Last change: Fri Apr 14 15:20:02 2023 by hacluster via crmd on node1-heartbeat2 nodes configured0 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]No resourcesDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled [root@node1 qemu_config]# pcs resource list |grep -i &quot;virt&quot; #查看支持的列表ocf:heartbeat:IPaddr - Manages virtual IPv4 and IPv6 addresses (Linux specificocf:heartbeat:IPaddr2 - Manages virtual IPv4 and IPv6 addresses (Linux specificocf:heartbeat:VirtualDomain - Manages virtual domains through the libvirt                              virtualization frameworkservice:libvirt-guests - systemd unit file for libvirt-guestsservice:libvirtd - systemd unit file for libvirtdservice:virtlockd - systemd unit file for virtlockdservice:virtlockd-admin.socket - systemd unit file for virtlockd-admin.socketservice:virtlockd.socket - systemd unit file for virtlockd.socketservice:virtlogd - systemd unit file for virtlogdservice:virtlogd-admin.socket - systemd unit file for virtlogd-admin.socketservice:virtlogd.socket - systemd unit file for virtlogd.socketsystemd:libvirt-guests - systemd unit file for libvirt-guestssystemd:libvirtd - systemd unit file for libvirtdsystemd:virtlockd - systemd unit file for virtlockdsystemd:virtlockd-admin.socket - systemd unit file for virtlockd-admin.socketsystemd:virtlockd.socket - systemd unit file for virtlockd.socketsystemd:virtlogd - systemd unit file for virtlogdsystemd:virtlogd-admin.socket - systemd unit file for virtlogd-admin.socketsystemd:virtlogd.socket - systemd unit file for virtlogd.socket[root@node1 qemu_config]# pcs resource describe  ocf:heartbeat:VirtualDomain #查看使用帮助ocf:heartbeat:VirtualDomain - Manages virtual domains through the libvirt virtualization frameworkResource agent for a virtual domain (a.k.a. domU, virtual machine,virtual environment etc., depending on context) managed by libvirtd.Resource options:  config (required) (unique): Absolute path to the libvirt configuration file, for this virtual domain.  hypervisor: Hypervisor URI to connect to. See the libvirt documentation for details on supported URI formats. The default is system dependent. Determine              the system&#39;s default uri by running &#39;virsh --quiet uri&#39;.  force_stop: Always forcefully shut down (&quot;destroy&quot;) the domain on stop. The default behavior is to resort to a forceful shutdown only after a graceful              shutdown attempt has failed. You should only set this to true if your virtual domain (or your virtualization backend) does not support graceful              shutdown.  migration_transport: Transport used to connect to the remote hypervisor while migrating. Please refer to the libvirt documentation for details on                       transports available. If this parameter is omitted, the resource will use libvirt&#39;s default transport to connect to the remote                       hypervisor.  migration_user: The username will be used in the remote libvirt remoteuri/migrateuri. No user will be given (which means root) in the username if omitted                  If remoteuri is set, migration_user will be ignored.  migration_downtime: Define max downtime during live migration in milliseconds  migration_speed: Define live migration speed per resource in MiB/s  migration_network_suffix: Use a dedicated migration network. The migration URI is composed by adding this parameters value to the end of the node name. If                            the node name happens to be an FQDN (as opposed to an unqualified host name), insert the suffix immediately prior to the first                            period (.) in the FQDN. At the moment Qemu/KVM and Xen migration via a dedicated network is supported. Note: Be sure this                            composed host name is locally resolveable and the associated IP is reachable through the favored network. This suffix will be                            added to the remoteuri and migrateuri parameters. See also the migrate_options parameter below.  migrateuri: You can also specify here if the calculated migrate URI is unsuitable for your environment. If migrateuri is set then migration_network_suffix,              migrateport and --migrateuri in migrate_options are effectively ignored. Use &quot;%n&quot; as the placeholder for the target node name. Please refer to              the libvirt documentation for details on guest migration.  migrate_options: Extra virsh options for the guest live migration. You can also specify here --migrateuri if the calculated migrate URI is unsuitable for                   your environment. If --migrateuri is set then migration_network_suffix and migrateport are effectively ignored. Use &quot;%n&quot; as the                   placeholder for the target node name. Please refer to the libvirt documentation for details on guest migration.  monitor_scripts: To additionally monitor services within the virtual domain, add this parameter with a list of scripts to monitor. Note: when monitor                   scripts are used, the start and migrate_from operations will complete only when all monitor scripts have completed successfully. Be sure                   to set the timeout of these operations to accommodate this delay.  autoset_utilization_cpu: If set true, the agent will detect the number of domainU&#39;s vCPUs from virsh, and put it into the CPU utilization of the resource                           when the monitor is executed.  autoset_utilization_hv_memory: If set true, the agent will detect the number of *Max memory* from virsh, and put it into the hv_memory utilization of the                                 resource when the monitor is executed.  migrateport: This port will be used in the qemu migrateuri. If unset, the port will be a random highport.  remoteuri: Use this URI as virsh connection URI to commuicate with a remote hypervisor. If remoteuri is set then migration_user and             migration_network_suffix are effectively ignored. Use &quot;%n&quot; as the placeholder for the target node name. Please refer to the libvirt             documentation for details on guest migration.  save_config_on_stop: Changes to a running VM&#39;s config are normally lost on stop. This parameter instructs the RA to save the configuration back to the xml                       file provided in the &quot;config&quot; parameter.  sync_config_on_stop: Setting this automatically enables save_config_on_stop. When enabled this parameter instructs the RA to call csync2 -x to synchronize                       the file to all nodes. csync2 must be properly set up for this to work.  snapshot: Path to the snapshot directory where the virtual machine image will be stored. When this parameter is set, the virtual machine&#39;s RAM state will            be saved to a file in the snapshot directory when stopped. If on start a state file is present for the domain, the domain will be restored to the            same state it was in right before it stopped last. This option is incompatible with the &#39;force_stop&#39; option.  backingfile: When the VM is used in Copy-On-Write mode, this is the backing file to use (with its full path). The VMs image will be created based on this               backing file. This backing file will never be changed during the life of the VM.  stateless: If set to true and backingfile is defined, the start of the VM will systematically create a new qcow2 based on the backing file, therefore the             VM will always be stateless. If set to false, the start of the VM will use the COW (&lt;vmname&gt;.qcow2) file if it exists, otherwise the first start             will create a new qcow2 based on the backing file given as backingfile.  copyindirs: List of directories for the virt-copy-in before booting the VM. Used only in stateless mode.  shutdown_mode: virsh shutdown method to use. Please verify that it is supported by your virsh toolsed with &#39;virsh help shutdown&#39; When this parameter is set                 --mode shutdown_mode is passed as an additional argument to the &#39;virsh shutdown&#39; command. One can use this option in case default acpi                 method does not work. Verify that this mode is supported by your VM. By default --mode is not passed.Default operations:  start: interval=0s timeout=90s  stop: interval=0s timeout=90s  monitor: interval=10s timeout=30s  migrate_from: interval=0s timeout=60s  migrate_to: interval=0s timeout=120s[root@node1 qemu_config]# pcs cluster statusCluster Status: Stack: corosync Current DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorum Last updated: Sat Apr 15 14:18:17 2023 Last change: Fri Apr 14 15:20:02 2023 by hacluster via crmd on node1-heartbeat 2 nodes configured 0 resource instances configuredPCSD Status:  node1-heartbeat: Online  node2-heartbeat: Online[root@node1 vmdata]# pcs resource create Centos7.9_res VirtualDomain \ #Centos7.9是资源名称，VirtualDomain是简化的写法，OCF自带的脚本或资源类型hypervisor=&quot;qemu:///system&quot; \ #固定的写法，虚拟化是kvm的虚拟化config=&quot;/vmdata/qemu_config/Centos7.9.xml&quot; \ #配置文件的路径migration_transport=ssh \ #migrate使用的协议meta allow-migrate=&quot;true&quot; priority=&quot;100&quot; \#上述是必须项，从这里开始是可选项，meta是元数据，允许migrate，priority是优先级op start timeout=&quot;120s&quot; \ #虚拟机启动超时的时间op stop timeout=&quot;120s&quot; \ #虚拟机关闭超时的时间op monitor timeout=&quot;30s&quot; interval=&quot;10&quot; \ #设置一些超时的时间op migrate_from interval=&quot;0&quot; timeout=&quot;120s&quot; \ #设置一些超时的时间op migrate_to interval=&quot;0&quot; timeout=&quot;120s&quot; #设置一些超时的时间[root@node1 qemu_config]# pcs status #查看虚拟机启动情况Cluster name: cluster1WARNINGS:No stonith devices and stonith-enabled is not falseStack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 14:42:15 2023Last change: Sat Apr 15 14:42:03 2023 by root via cibadmin on node1-heartbeat2 nodes configured1 resource instance configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): Stopped #因为没有设置stonith devices and stonith-enabled is not false，导致虚拟机无法启动Daemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled删除虚拟机：[root@node1 qemu_config]# pcs resource  Centos7.9_res  (ocf::heartbeat:VirtualDomain): Stopped[root@node1 qemu_config]# pcs resource delete Centos7.9_resDeleting Resource - Centos7.9_res[root@node1 qemu_config]# pcs status Cluster name: cluster1WARNINGS:No stonith devices and stonith-enabled is not falseStack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 14:43:22 2023Last change: Sat Apr 15 14:43:19 2023 by root via cibadmin on node1-heartbeat2 nodes configured0 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]No resourcesDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled关闭stonith：[root@node1 qemu_config]# pcs property list Cluster Properties: cluster-infrastructure: corosync cluster-name: cluster1 dc-version: 1.1.23-1.el7_9.1-9acf116022 have-watchdog: false[root@node1 qemu_config]# pcs property set stonith-enabled=false[root@node1 qemu_config]# pcs property list Cluster Properties: cluster-infrastructure: corosync cluster-name: cluster1 dc-version: 1.1.23-1.el7_9.1-9acf116022 have-watchdog: false stonith-enabled: false重新启动虚拟机：[root@node1 qemu_config]# pcs resource create Centos7.9_res VirtualDomain \&gt; hypervisor=&quot;qemu:///system&quot; \&gt; config=&quot;/vmdata/qemu_config/Centos7.9.xml&quot; \&gt; migration_transport=&quot;ssh&quot; \&gt; meta allow-migrate=&quot;true&quot; priority=&quot;100&quot; \&gt; op start timeout=&quot;120s&quot; \&gt; op stop timeout=&quot;120s&quot; \&gt; op monitor timeout=&quot;30s&quot; interval=&quot;10&quot; \&gt; op migrate_from interval=&quot;0&quot; timeout=&quot;120s&quot; \&gt; op migrate_to interval=&quot;0&quot; timeout=&quot;120s&quot;Assumed agent name &#39;ocf:heartbeat:VirtualDomain&#39; (deduced from &#39;VirtualDomain&#39;)[root@node1 qemu_config]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 14:45:02 2023Last change: Sat Apr 15 14:44:59 2023 by root via crm_resource on node1-heartbeat2 nodes configured1 resource instance configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): Started node1-heartbeatDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled[root@node1 qemu_config]# virsh list --all  Id    Name                           State---------------------------------------------------- 16    Centos7.9                      running</code></pre><h1><span id="6-群集测试">6、群集测试</span></h1><p><strong>迁移测试</strong></p><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/90.jpg"></p><pre><code>[root@node1 qemu_config]# pcs resource  Centos7.9_res  (ocf::heartbeat:VirtualDomain): Started node1-heartbeat[root@node1 qemu_config]# pcs resource move Centos7.9_res #提示创建了一些规则，尽量不要再从另外一个主机迁移回来Warning: Creating location constraint cli-ban-Centos7.9_res-on-node1-heartbeat with a score of -INFINITY for resource Centos7.9_res on node node1-heartbeat.This will prevent Centos7.9_res from running on node1-heartbeat until the constraint is removed. This will be the case even if node1-heartbeat is the last node in the cluster.[root@node1 qemu_config]# pcs constraint --full #可以看到创建了一个cli-ban-Centos7.9_res-on-node1-heartbeat的规则，如果想再迁移到node1上可以将这个规则进行删除Location Constraints:  Resource: Centos7.9_res    Disabled on: node1-heartbeat (score:-INFINITY) (role: Started) (id:cli-ban-Centos7.9_res-on-node1-heartbeat)Ordering Constraints:Colocation Constraints:Ticket Constraints:[root@node1 qemu_config]# pcs constraint remove  cli-ban-Centos7.9_res-on-node1-heartbeat #删除这个规则[root@node1 qemu_config]# pcs constraint --full #查看已经没有规则了Location Constraints:Ordering Constraints:Colocation Constraints:Ticket Constraints:[root@node1 qemu_config]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 15:01:15 2023Last change: Sat Apr 15 14:58:36 2023 by root via crm_resource on node1-heartbeat2 nodes configured1 resource instance configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): Started node2-heartbeatDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled</code></pre><p><strong>尝试再次迁移到node1上</strong></p><pre><code>[root@node1 qemu_config]# pcs cluster standby node2-heartbeat #将节点2暂停[root@node1 qemu_config]# pcs status  #查看主机已经迁移到node1上了Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 15:10:55 2023Last change: Sat Apr 15 15:10:40 2023 by root via cibadmin on node1-heartbeat2 nodes configured1 resource instance configuredNode node2-heartbeat: standbyOnline: [ node1-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): Started node1-heartbeatDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled[root@node1 qemu_config]# pcs constraint --full #查看没有创建规则Location Constraints:Ordering Constraints:Colocation Constraints:Ticket Constraints:[root@node1 qemu_config]# pcs cluster unstandby node2-heartbeat #取消节点2stanbdy[root@node2 .ssh]# pcs cluster stop node1-heartbeat #在节点2服务器中将节点1的服务停止node1-heartbeat: Stopping Cluster (pacemaker)...node1-heartbeat: Stopping Cluster (corosync)...[root@node2 .ssh]# pcs status  #查看主机迁移到node2中，实现高可用Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 17:37:47 2023Last change: Sat Apr 15 17:35:52 2023 by root via cibadmin on node1-heartbeat2 nodes configured1 resource instance configuredOnline: [ node2-heartbeat ]OFFLINE: [ node1-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): Started node2-heartbeatDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled测试节点2直接关机，模拟中断，节点是否迁移，通过vmware直接关机：[root@node1 qemu_config]# pcs status  #在节点1中查看，主机不会热迁移，断电仍受影响Cluster name: cluster1Stack: corosyncCurrent DC: node1-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 17:40:05 2023Last change: Sat Apr 15 17:35:52 2023 by root via cibadmin on node1-heartbeat2 nodes configured1 resource instance configuredOnline: [ node1-heartbeat ]OFFLINE: [ node2-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): StoppedFailed Resource Actions:* Centos7.9_res_start_0 on node1-heartbeat &#39;unknown error&#39; (1): call=6, status=complete, exitreason=&#39;Failed to start virtual domain Centos7.9.&#39;,    last-rc-change=&#39;Sat Apr 15 17:39:50 2023&#39;, queued=0ms, exec=1310msDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled</code></pre><p><strong>报错处理</strong></p><p>1、</p><pre><code>Apr 15 16:25:23 node1 libvirtd: 2023-04-15 08:25:23.523+0000: 9724: error : qemuMigrationSrcIsSafe:1224 : Unsafe migration: Migration may lead to data corruption if disks use cache != none or cache != directsync</code></pre><p>由于xml文件的硬盘高级选项使用的不是none，导致这个问题，需要修改</p><pre><code>[root@node1 vmdata]# virsh dumpxml --domain Centos7.9 | grep none      &lt;driver name=&#39;qemu&#39; type=&#39;qcow2&#39; cache=&#39;none&#39;/&gt;</code></pre><p>2、</p><pre><code>Apr 15 17:23:55 node1 VirtualDomain(Centos7.9_res)[18439]: INFO: Centos7.9: Starting live migration to node2-heartbeat (using: virsh --connect=qemu:///system --quiet migrate --live  Centos7.9 qemu://node2-heartbeat/system ).Apr 15 17:23:55 node1 VirtualDomain(Centos7.9_res)[18439]: ERROR: Centos7.9: live migration to node2-heartbeat failed: 1</code></pre><pre><code>由于创建的时候ssh加了双引号导致的无法识别ssh，体现在qemu://node2-heartbeat/system这个位置，没有变成qemu+ssh</code></pre><p>3、</p><pre><code>[root@node2 .ssh]# virsh --connect=qemu:///system --quiet migrate --live  Centos7.9 qemu+ssh://node1-heartbeat/systemerror: unable to connect to server at &#39;node1:49152&#39;: No route to host</code></pre><p>经测试，由于主机重启后防火墙端口未被放开，导致无法建立连接，添加相关端口即可</p><pre><code>[root@kvm2 ~]# setsebool -P virt_use_nfs 1[root@kvm2 ~]# iptables -I INPUT -p tcp --dport 49152:49215 -j ACCEPT[root@kvm2 ~]# iptables -I INPUT -p tcp --dport 16509 -j ACCEPT</code></pre><h1><span id="7-配置stonthipmi">7、配置STONTH（IPMI）</span></h1><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/91.jpg"><br><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/92.jpg"></p><blockquote><p>！ 由于当前宿主机使用的是vmware中的虚拟机，所以配置完成后是无法使用的，但是实体服务器可以进行相关配置</p></blockquote><pre><code>[root@node1 qemu_config]# yum -y install fence-agents-ipmilan #所有节点均要执行此命令[root@node1 qemu_config]# pcs stonith list  #查看列表fence_amt_ws - Fence agent for AMT (WS)fence_apc - Fence agent for APC over telnet/sshfence_apc_snmp - Fence agent for APC, Tripplite PDU over SNMPfence_bladecenter - Fence agent for IBM BladeCenterfence_brocade - Fence agent for HP Brocade over telnet/sshfence_cisco_mds - Fence agent for Cisco MDSfence_cisco_ucs - Fence agent for Cisco UCSfence_compute - Fence agent for the automatic resurrection of OpenStack compute instancesfence_drac5 - Fence agent for Dell DRAC CMC/5fence_eaton_snmp - Fence agent for Eaton over SNMPfence_emerson - Fence agent for Emerson over SNMPfence_eps - Fence agent for ePowerSwitchfence_evacuate - Fence agent for the automatic resurrection of OpenStack compute instancesfence_heuristics_ping - Fence agent for ping-heuristic based fencingfence_hpblade - Fence agent for HP BladeSystemfence_ibmblade - Fence agent for IBM BladeCenter over SNMPfence_idrac - Fence agent for IPMIfence_ifmib - Fence agent for IF MIBfence_ilo - Fence agent for HP iLOfence_ilo2 - Fence agent for HP iLOfence_ilo3 - Fence agent for IPMIfence_ilo3_ssh - Fence agent for HP iLO over SSHfence_ilo4 - Fence agent for IPMIfence_ilo4_ssh - Fence agent for HP iLO over SSHfence_ilo5 - Fence agent for IPMIfence_ilo5_ssh - Fence agent for HP iLO over SSHfence_ilo_moonshot - Fence agent for HP Moonshot iLOfence_ilo_mp - Fence agent for HP iLO MPfence_ilo_ssh - Fence agent for HP iLO over SSHfence_imm - Fence agent for IPMIfence_intelmodular - Fence agent for Intel Modularfence_ipdu - Fence agent for iPDU over SNMPfence_ipmilan - Fence agent for IPMIfence_kdump - fencing agent for use with kdump crash recovery servicefence_mpath - Fence agent for multipath persistent reservationfence_redfish - I/O Fencing agent for Redfishfence_rhevm - Fence agent for RHEV-M REST APIfence_rsa - Fence agent for IBM RSAfence_rsb - I/O Fencing agent for Fujitsu-Siemens RSBfence_sbd - Fence agent for sbdfence_scsi - Fence agent for SCSI persistent reservationfence_virt - Fence agent for virtual machinesfence_vmware_rest - Fence agent for VMware REST APIfence_vmware_soap - Fence agent for VMWare over SOAP APIfence_wti - Fence agent for WTIfence_xvm - Fence agent for virtual machines[root@node1 qemu_config]# pcs stonith describe fence_ipmilan #查看使用帮助fence_ipmilan - Fence agent for IPMIfence_ipmilan is an I/O Fencing agentwhich can be used with machines controlled by IPMI.This agent calls support software ipmitool (http://ipmitool.sf.net/). WARNING! This fence agent might report success before the node is powered off. You should use -m/method onoff if your fence device works correctly with that option.Stonith options:  auth: IPMI Lan Auth type.  cipher: Ciphersuite to use (same as ipmitool -C parameter)  hexadecimal_kg: Hexadecimal-encoded Kg key for IPMIv2 authentication  ipaddr: IP address or hostname of fencing device  ipport: TCP/UDP port to use for connection with device  lanplus: Use Lanplus to improve security of connection  login: Login name  method: Method to fence  passwd: Login password or passphrase  passwd_script: Script to run to retrieve password  port: IP address or hostname of fencing device (together with --port-as-ip)  privlvl: Privilege level on IPMI device  target: Bridge IPMI requests to the remote target address  quiet: Disable logging to stderr. Does not affect --verbose or --debug-file or logging to syslog.  verbose: Verbose mode  debug: Write debug information to given file  delay: Wait X seconds before fencing is started  ipmitool_path: Path to ipmitool binary  login_timeout: Wait X seconds for cmd prompt after login  port_as_ip: Make &quot;port/plug&quot; to be an alias to IP address  power_timeout: Test X seconds for status change after ON/OFF  power_wait: Wait X seconds after issuing ON/OFF  shell_timeout: Wait X seconds for cmd prompt after issuing command  retry_on: Count of attempts to retry power on  sudo: Use sudo (without password) when calling 3rd party software  sudo_path: Path to sudo binary  pcmk_host_map: A mapping of host names to ports numbers for devices that do not support host names. Eg. node1:1;node2:2,3 would tell the                 cluster to use port 1 for node1 and ports 2 and 3 for node2  pcmk_host_list: A list of machines controlled by this device (Optional unless pcmk_host_check=static-list).  pcmk_host_check: How to determine which machines are controlled by the device. Allowed values: dynamic-list (query the device via the                   &#39;list&#39; command), static-list (check the pcmk_host_list attribute), status (query the device via the &#39;status&#39; command),                   none (assume every device can fence every machine)  pcmk_delay_max: Enable a random delay for stonith actions and specify the maximum of random delay. This prevents double fencing when using                  slow devices such as sbd. Use this to enable a random delay for stonith actions. The overall delay is derived from this                  random delay value adding a static delay so that the sum is kept below the maximum delay.  pcmk_delay_base: Enable a base delay for stonith actions and specify base delay value. This prevents double fencing when different delays                   are configured on the nodes. Use this to enable a static delay for stonith actions. The overall delay is derived from a                   random delay value adding this static delay so that the sum is kept below the maximum delay.  pcmk_action_limit: The maximum number of actions can be performed in parallel on this device Pengine property concurrent-fencing=true needs                     to be configured first. Then use this to specify the maximum number of actions can be performed in parallel on this                     device. -1 is unlimited.Default operations:  monitor: interval=60s[root@node1 ~]# pcs cluster cib s_cfg #导出配置文件到s_cfg通过下列命令修改导出的配置文件[root@node1 ~]# pcs -f s_cfg stonith create ipmi-fencing fence_ipmilan \pcmk_host_list=&quot;node1-heartbeat node2-heartbeat&quot; \ipaddr=10.0.1.1 \login=testuser \passwd=123456 \op monitor interval=60s[root@node1 ~]# pcs -f s_cfg status #查看修改后的配置文件Stack: corosyncCurrent DC: node1-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 18:05:54 2023Last change: Sat Apr 15 17:48:01 2023 by root via crm_resource on node1-heartbeat2 nodes configured2 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): Started node1-heartbeat ipmi-fencing   (stonith:fence_ipmilan):    Stopped[root@node1 ~]# pcs -f a_cfg property set stonith-enabled=true #修改 stonith-enabled=true[root@node1 ~]# pcs cluster cib-push s_cfg  #上传到运行中的集群CIB updated[root@node1 ~]# pcs status  #查看集群状态Cluster name: cluster1Stack: corosyncCurrent DC: node1-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 18:21:56 2023Last change: Sat Apr 15 18:21:49 2023 by root via cibadmin on node1-heartbeat2 nodes configured2 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): Started node1-heartbeat ipmi-fencing   (stonith:fence_ipmilan):    Starting node2-heartbeatDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KVM </tag>
            
            <tag> Linux HA </tag>
            
            <tag> NFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph-安装</title>
      <link href="/2023/04/11/Ceph/3.Ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/Ceph-%E5%AE%89%E8%A3%85/"/>
      <url>/2023/04/11/Ceph/3.Ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/Ceph-%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E7%8E%AF%E5%A2%83%E6%83%85%E5%86%B5">环境情况</a></li><li><a href="#%E4%B8%80-%E4%BF%AE%E6%94%B9hosts%E6%96%87%E4%BB%B6">一、修改HOSTS文件</a></li><li><a href="#%E4%BA%8C-%E9%85%8D%E7%BD%AE%E6%97%A0%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95">二、配置无密码登录</a></li><li><a href="#%E4%B8%89-%E5%AE%89%E5%85%A8%E8%AE%BE%E7%BD%AE">三、安全设置</a><ul><li><a href="#%E4%B8%80-%E5%85%B3%E9%97%ADselinux">一、关闭Selinux</a></li><li><a href="#%E4%BA%8C-%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99">二、关闭防火墙</a></li></ul></li><li><a href="#%E5%9B%9B-ntp%E8%AE%BE%E7%BD%AE">四、NTP设置</a></li><li><a href="#%E4%BA%94-%E9%85%8D%E7%BD%AEyum%E6%BA%90">五、配置Yum源</a></li><li><a href="#%E5%85%AD-%E5%AE%89%E8%A3%85ceph-deploy">六、安装Ceph-deploy</a></li><li><a href="#%E4%B8%83-%E5%AE%89%E8%A3%85mon%E8%8A%82%E7%82%B9">七、安装Mon节点</a><ul><li><a href="#%E4%B8%80-%E9%87%8D%E7%BD%AE%E9%9B%86%E7%BE%A4">一、重置集群</a></li><li><a href="#%E4%BA%8C-%E5%AE%89%E8%A3%85%E7%9B%B8%E5%85%B3%E8%BD%AF%E4%BB%B6">二、安装相关软件</a></li></ul></li><li><a href="#%E4%B8%89-mon-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%8A%E5%AE%89%E8%A3%85mgr">三、Mon 初始化及安装MGR</a></li><li><a href="#%E5%85%AB-%E9%83%A8%E7%BD%B2osd%E8%8A%82%E7%82%B9">八、部署OSD节点</a></li><li><a href="#%E4%B9%9D-%E6%89%A9%E5%B1%95mon%E5%92%8Cmgr">九、扩展MON和MGR</a><ul><li><a href="#%E4%B8%80-mon">一、Mon</a></li><li><a href="#%E4%BA%8C-mgr">二、MGR</a></li></ul></li><li><a href="#%E5%8D%81-%E5%88%9B%E5%BB%BA%E8%B5%84%E6%BA%90%E6%B1%A0pool">十、创建资源池Pool</a></li></ul><!-- tocstop --><h1><span id="环境情况">环境情况</span></h1><table>    <tr>        <td>操作系统</td>        <td>公共网络</td>        <td>集群网络</td>        <td>节点名称</td>    </tr>    <tr>        <td rowspan="4">Centos 7.9</td>    </tr>        <tr>            <td>192.168.187.201</td>            <td>192.168.199.201</td>            <td>node-1</td>        </tr>        <tr>            <td>192.168.187.202</td>            <td>192.168.199.202</td>            <td>node-2</td>        </tr>        <tr>            <td>192.168.187.203</td>            <td>192.168.199.203</td>            <td>node-2</td>        </tr></table><h1><span id="一-修改hosts文件">一、修改HOSTS文件</span></h1><p><strong>所有主机均要修改</strong></p><pre><code>[root@node-1 ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.199.201 node-1192.168.199.202 node-2192.168.199.203 node-3</code></pre><h1><span id="二-配置无密码登录">二、配置无密码登录</span></h1><p><strong>Node-1：</strong></p><pre><code>[root@node-1 ~]# ssh-keygen Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Created directory &#39;/root/.ssh&#39;.Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:RNd/EfeRh3t7C73PD//lsUVTQpf84hKM5Dy3CdWj6iQ root@node-1The keys randomart image is:+---[RSA 2048]----+|        . .. .o+*||       . .. o.+*=||        .+ + o.o*||       .  * = +.=||        S  = =.=o||        E o +..+o||         +   .o.*||          .    B=||               .@|+----[SHA256]-----+[root@node-1 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub node-2/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;The authenticity of host &#39;node-2 (192.168.199.202)&#39; can t be established.ECDSA key fingerprint is SHA256:ELWxoLpZlehIUk5OpR5CO/OmzpHyNco8PkV4ztDCV7w.ECDSA key fingerprint is MD5:62:87:b6:99:64:b1:b2:d3:40:f1:d5:e8:d3:a1:8a:18.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@node-2 s password: Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#39;node-2&#39;&quot;and check to make sure that only the key(s) you wanted were added.[root@node-1 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub node-3/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;The authenticity of host &#39;node-3 (192.168.199.203)&#39; can t be established.ECDSA key fingerprint is SHA256:ELWxoLpZlehIUk5OpR5CO/OmzpHyNco8PkV4ztDCV7w.ECDSA key fingerprint is MD5:62:87:b6:99:64:b1:b2:d3:40:f1:d5:e8:d3:a1:8a:18.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@node-3 s password: Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#39;node-3&#39;&quot;and check to make sure that only the key(s) you wanted were added.</code></pre><h1><span id="三-安全设置">三、安全设置</span></h1><h2><span id="一-关闭selinux">一、关闭Selinux</span></h2><p><strong>所有节点执行，将enforcing修改成disabled：</strong></p><pre><code>[root@node-1 ~]# cat /etc/selinux/config # This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:#     enforcing - SELinux security policy is enforced.#     permissive - SELinux prints warnings instead of enforcing.#     disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of three values:#     targeted - Targeted processes are protected,#     minimum - Modification of targeted policy. Only selected processes are protected. #     mls - Multi Level Security protection.SELINUXTYPE=targeted[root@node-1 ~]# setenforce 0 #临时关闭[root@node-1 ~]# getenforce #查看状态Disabled</code></pre><h2><span id="二-关闭防火墙">二、关闭防火墙</span></h2><p><strong>所有节点均执行：</strong></p><pre><code>[root@node-1 ~]# firewall-cmd --list-all #查看防火墙状态public (active)  target: default  icmp-block-inversion: no  interfaces: ens33 ens36  sources:   services: dhcpv6-client ssh  ports:   protocols:   masquerade: no  forward-ports:   source-ports:   icmp-blocks:   rich rules: [root@node-1 ~]# systemctl disable firewalld #禁止开机自启Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.[root@node-1 ~]# systemctl stop firewalld #关闭防火墙[root@node-3 ~]# firewall-cmd --list-all #查看状态FirewallD is not running</code></pre><h1><span id="四-ntp设置">四、NTP设置</span></h1><p><strong>所有节点执行：</strong></p><pre><code>[root@node-3 ~]# cd /etc/yum.repos.d/[root@node-3 yum.repos.d]# lsCentOS-Base.repo  CentOS-Debuginfo.repo  CentOS-Media.repo    CentOS-Vault.repoCentOS-CR.repo    CentOS-fasttrack.repo  CentOS-Sources.repo  CentOS-x86_64-kernel.repo[root@node-3 yum.repos.d]# rm -rf *[root@node-3 yum.repos.d]# ls[root@node-1 yum.repos.d]# curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  2523  100  2523    0     0   6577      0 --:--:-- --:--:-- --:--:--  6587[root@node-1 yum.repos.d]# [root@node-1 yum.repos.d]# lsCentOS-Base.repo[root@node-1 yum.repos.d]# yum clean all &amp;&amp; yum makecache #清除缓存[root@node-1 yum.repos.d]# yum install ntp -y #安装NTP[root@node-1 yum.repos.d]# systemctl restart ntpd #启动NTP服务[root@node-1 yum.repos.d]# systemctl enable ntpd #设置开机自启动Created symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.[root@node-1 yum.repos.d]# ntpq -pn #查看NTP状态     remote           refid      st t when poll reach   delay   offset  jitter==============================================================================+78.46.102.180   176.9.157.12     3 u   33   64    1  245.918   17.006  14.216*144.76.76.107   192.53.103.103   2 u   33   64    1  214.840    0.073   0.324 193.182.111.14  192.36.143.153   2 u   67   64    1  293.490   -4.158   1.037 [root@node-2 ~]# crontab -l #使用crontab -e 编辑，每分钟同步一次时间*/1 * * * * /usr/sbin/ntpdate node-1;/sbin/hwclock -w</code></pre><p><strong>node-2、node-3执行：</strong></p><pre><code>[root@node-3 yum.repos.d]# cat /etc/ntp.conf # For more information about this file, see the man pages# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).driftfile /var/lib/ntp/drift# Permit time synchronization with our time source, but do not# permit the source to query or modify the service on this system.restrict default nomodify notrap nopeer noquery# Permit all access over the loopback interface.  This could# be tightened as well, but to do so would effect some of# the administrative functions.restrict 127.0.0.1 restrict ::1# Hosts on local network are less restricted.#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).server node-1 iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst#broadcast 192.168.1.255 autokey    # broadcast server#broadcastclient            # broadcast client#broadcast 224.0.1.1 autokey        # multicast server#multicastclient 224.0.1.1        # multicast client#manycastserver 239.255.254.254        # manycast server#manycastclient 239.255.254.254 autokey # manycast client# Enable public key cryptography.#cryptoincludefile /etc/ntp/crypto/pw# Key file containing the keys and key identifiers used when operating# with symmetric key cryptography. keys /etc/ntp/keys# Specify the key identifiers which are trusted.#trustedkey 4 8 42# Specify the key identifier to use with the ntpdc utility.#requestkey 8# Specify the key identifier to use with the ntpq utility.#controlkey 8# Enable writing of statistics records.#statistics clockstats cryptostats loopstats peerstats# Disable the monitoring facility to prevent amplification attacks using ntpdc# monlist command when default restrict does not include the noquery flag. See# CVE-2013-5211 for more details.# Note: Monitoring will not be disabled with the limited restriction flag.disable monitor</code></pre><h1><span id="五-配置yum源">五、配置Yum源</span></h1><p><strong>所有节点执行：</strong></p><pre><code>[root@node-3 ~]# cd /etc/yum.repos.d/[root@node-3 yum.repos.d]# lsCentOS-Base.repo  CentOS-Debuginfo.repo  CentOS-Media.repo    CentOS-Vault.repoCentOS-CR.repo    CentOS-fasttrack.repo  CentOS-Sources.repo  CentOS-x86_64-kernel.repo[root@node-3 yum.repos.d]# rm -rf *[root@node-3 yum.repos.d]# ls[root@node-1 yum.repos.d]# curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  2523  100  2523    0     0   6577      0 --:--:-- --:--:-- --:--:--  6587[root@node-1 yum.repos.d]# [root@node-1 yum.repos.d]# lsCentOS-Base.repo[root@node-1 yum.repos.d]# curl -o /etc/yum.repos.d/epel.repo https://mirrors.aliyun.com/repo/epel-7.repo  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   664  100   664    0     0   2193      0 --:--:-- --:--:-- --:--:--  2191[root@node-1 yum.repos.d]# [root@node-1 yum.repos.d]# lsCentOS-Base.repo  epel.repo[root@node-1 yum.repos.d]# cat ceph.repo  #手动创建ceph.repo文件内容如下[norch]name=norchbaseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/enabled=1gpgcheck=0[x86_64]name=x86 64baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/enabled=1gpgcheck=0[root@node-1 yum.repos.d]# yum clean all &amp;&amp; yum makecache #清除缓存[root@node-1 yum.repos.d]# yum update -y</code></pre><h1><span id="六-安装ceph-deploy">六、安装Ceph-deploy</span></h1><p><strong>Node1执行</strong></p><pre><code>[root@node-1 yum.repos.d]# yum install python-setuptools ceph-deploy -y #安装核心软件[root@node-1 yum.repos.d]# ceph-deploy --version #查看版本2.0.1</code></pre><h1><span id="七-安装mon节点">七、安装Mon节点</span></h1><h2><span id="一-重置集群">一、重置集群</span></h2><p><strong>如果安装失败或者重新安装时执行：</strong></p><pre><code>ceph-deploy purge &#123;ceph-node&#125; [&#123;ceph-node&#125;]ceph-deploy purgedata  &#123;ceph-node&#125; [&#123;ceph-node&#125;]ceph-deploy forgetkeysrm ceph.*</code></pre><h2><span id="二-安装相关软件">二、安装相关软件</span></h2><p><strong>Node1执行：</strong></p><pre><code>[root@node-1 yum.repos.d]# cd /opt/[root@node-1 opt]# mkdir my-cluster[root@node-1 opt]# cd my-cluster/[root@node-1 my-cluster]# ceph-deploy new --public-network 192.168.187.0/24 --cluster-network 192.168.199.0/24 node-1</code></pre><p><strong>公共网络是外部访问集群时使用的，集群网络时内部同步使用的</strong><br><strong>所有节点执行：</strong></p><pre><code>[root@node-1 my-cluster]# yum install ceph ceph-mon ceph-mgr ceph-mds ceph-radosgw -y #安装核心软件包</code></pre><h1><span id="三-mon-初始化及安装mgr">三、Mon 初始化及安装MGR</span></h1><pre><code>[root@node-1 my-cluster]# ceph-deploy mon create-initial #初始化Mon[root@node-1 my-cluster]# ceph-deploy admin node-1 node-2 node-3 #推送最新配置到所有节点[root@node-1 my-cluster]# ceph -s  #查看集群状态  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            mon is allowing insecure global_id reclaim  #mon允许不安全的global_id回收   services:    mon: 1 daemons, quorum node-1 (age 2m)    mgr: no daemons active    osd: 0 osds: 0 up, 0 in   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   0 B used, 0 B / 0 B avail    pgs:     [root@node-1 my-cluster]# ceph config set mon auth_allow_insecure_global_id_reclaim false #取消mon允许不安全的global_id回收[root@node-1 my-cluster]# ceph -s   cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 1 daemons, quorum node-1 (age 3m)    mgr: no daemons active    osd: 0 osds: 0 up, 0 in   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   0 B used, 0 B / 0 B avail    pgs:      [root@node-1 my-cluster]# ceph-deploy mgr create node-1 #安装mgr 监控服务[root@node-1 my-cluster]#  ceph -s  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            OSD count 0 &lt; osd_pool_default_size 3   services:    mon: 1 daemons, quorum node-1 (age 6m)    mgr: node-1(active, since 55s) #查看已经成功安装    osd: 0 osds: 0 up, 0 in   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   0 B used, 0 B / 0 B avail    pgs:     </code></pre><h1><span id="八-部署osd节点">八、部署OSD节点</span></h1><p><strong>node-1 执行:</strong></p><pre><code>[root@node-1 my-cluster]# lsblk #确定硬盘NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsda               8:0    0   20G  0 disk ├─sda1            8:1    0    1G  0 part /boot└─sda2            8:2    0   19G  0 part   ├─centos-root 253:0    0   17G  0 lvm  /  └─centos-swap 253:1    0    2G  0 lvm  [SWAP]sdb               8:16   0   10G  0 disk sdc               8:32   0   10G  0 disk sr0              11:0    1 1024M  0 rom [root@node-1 my-cluster]# ceph-deploy osd create node-1 --data /dev/sdb --journal /dev/sdb # --data指定硬盘  --journal /dev/sdb 指定加速[root@node-1 my-cluster]# ceph-deploy osd create node-2 --data /dev/sdb --journal /dev/sdb # --data指定硬盘  --journal /dev/sdb 指定加速[root@node-1 my-cluster]# ceph-deploy osd create node-3 --data /dev/sdb --journal /dev/sdb # --data指定硬盘  --journal /dev/sdb 指定加速[root@node-1 my-cluster]# ceph osd tree  #查看OSD状态ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.02939 root default                            -3       0.00980     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000 -5       0.00980     host node-2                          1   hdd 0.00980         osd.1       up  1.00000 1.00000 -7       0.00980     host node-3                          2   hdd 0.00980         osd.2       up  1.00000 1.00000 </code></pre><h1><span id="九-扩展mon和mgr">九、扩展MON和MGR</span></h1><p><strong>Mon 使用Paxos算法，一般都是奇数 3、5、7</strong></p><h2><span id="一-mon">一、Mon</span></h2><pre><code>NODE-1执行：ceph-deploy mon add node-2 --address 192.168.187.202 #添加node-2成为Mon 并指定IPceph-deploy mon add node-3 --address 192.168.187.203 #添加node-3成为Mon 并指定IP[root@node-1 my-cluster]# ceph -s  #查看Mon是否添加成功  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 54s) #已经添加node-2和node-3    mgr: node-1(active, since 18m)    osd: 3 osds: 3 up (since 8m), 3 in (since 8m)   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   3.0 GiB used, 27 GiB / 30 GiB avail    pgs: [root@node-1 my-cluster]# ceph quorum_status --format json-pretty #查看Mon 仲裁情况&#123;    &quot;election_epoch&quot;: 12,    &quot;quorum&quot;: [        0,        1,        2    ],    &quot;quorum_names&quot;: [ #可以看到当前3个节点正在参加仲裁        &quot;node-1&quot;,        &quot;node-2&quot;,        &quot;node-3&quot;    ],    &quot;quorum_leader_name&quot;: &quot;node-1&quot;, #当前主Mon是Node-1    &quot;quorum_age&quot;: 234,    &quot;monmap&quot;: &#123;        &quot;epoch&quot;: 3,        &quot;fsid&quot;: &quot;e9a90625-4707-4b6b-b52f-661512ea831d&quot;,        &quot;modified&quot;: &quot;2023-03-03 12:22:43.254681&quot;,        &quot;created&quot;: &quot;2023-03-03 11:59:53.474948&quot;,        &quot;min_mon_release&quot;: 14,        &quot;min_mon_release_name&quot;: &quot;nautilus&quot;,        &quot;features&quot;: &#123;            &quot;persistent&quot;: [                &quot;kraken&quot;,                &quot;luminous&quot;,                &quot;mimic&quot;,                &quot;osdmap-prune&quot;,                &quot;nautilus&quot;            ],            &quot;optional&quot;: []        &#125;,        &quot;mons&quot;: [            &#123;                &quot;rank&quot;: 0,                &quot;name&quot;: &quot;node-1&quot;,                &quot;public_addrs&quot;: &#123;                    &quot;addrvec&quot;: [                        &#123;                            &quot;type&quot;: &quot;v2&quot;,                            &quot;addr&quot;: &quot;192.168.187.201:3300&quot;,                            &quot;nonce&quot;: 0                        &#125;,                        &#123;                            &quot;type&quot;: &quot;v1&quot;,                            &quot;addr&quot;: &quot;192.168.187.201:6789&quot;,                            &quot;nonce&quot;: 0                        &#125;                    ]                &#125;,                &quot;addr&quot;: &quot;192.168.187.201:6789/0&quot;,                &quot;public_addr&quot;: &quot;192.168.187.201:6789/0&quot;            &#125;,            &#123;                &quot;rank&quot;: 1,                &quot;name&quot;: &quot;node-2&quot;,                &quot;public_addrs&quot;: &#123;                    &quot;addrvec&quot;: [                        &#123;                            &quot;type&quot;: &quot;v2&quot;,                            &quot;addr&quot;: &quot;192.168.187.202:3300&quot;,                            &quot;nonce&quot;: 0                        &#125;,                        &#123;                            &quot;type&quot;: &quot;v1&quot;,                            &quot;addr&quot;: &quot;192.168.187.202:6789&quot;,                            &quot;nonce&quot;: 0                        &#125;                    ]                &#125;,                &quot;addr&quot;: &quot;192.168.187.202:6789/0&quot;,                &quot;public_addr&quot;: &quot;192.168.187.202:6789/0&quot;            &#125;,            &#123;                &quot;rank&quot;: 2,                &quot;name&quot;: &quot;node-3&quot;,                &quot;public_addrs&quot;: &#123;                    &quot;addrvec&quot;: [                        &#123;                            &quot;type&quot;: &quot;v2&quot;,                            &quot;addr&quot;: &quot;192.168.187.203:3300&quot;,                            &quot;nonce&quot;: 0                        &#125;,                        &#123;                            &quot;type&quot;: &quot;v1&quot;,                            &quot;addr&quot;: &quot;192.168.187.203:6789&quot;,                            &quot;nonce&quot;: 0                        &#125;                    ]                &#125;,                &quot;addr&quot;: &quot;192.168.187.203:6789/0&quot;,                &quot;public_addr&quot;: &quot;192.168.187.203:6789/0&quot;            &#125;        ]    &#125;&#125;[root@node-1 my-cluster]#  ceph mon stat #查看Mon 状态e3: 3 mons at &#123;node-1=[v2:192.168.187.201:3300/0,v1:192.168.187.201:6789/0],node-2=[v2:192.168.187.202:3300/0,v1:192.168.187.202:6789/0],node-3=[v2:192.168.187.203:3300/0,v1:192.168.187.203:6789/0]&#125;, election epoch 12, leader 0 node-1, quorum 0,1,2 node-1,node-2,node-3[root@node-1 my-cluster]# ceph mon dump #查看Mon 状态epoch 3fsid e9a90625-4707-4b6b-b52f-661512ea831dlast_changed 2023-03-03 12:22:43.254681created 2023-03-03 11:59:53.474948min_mon_release 14 (nautilus)0: [v2:192.168.187.201:3300/0,v1:192.168.187.201:6789/0] mon.node-11: [v2:192.168.187.202:3300/0,v1:192.168.187.202:6789/0] mon.node-22: [v2:192.168.187.203:3300/0,v1:192.168.187.203:6789/0] mon.node-3dumped monmap epoch 3</code></pre><h2><span id="二-mgr">二、MGR</span></h2><p><strong>MGR默认是主备模式</strong></p><p><strong>node-1：执行</strong></p><pre><code>[root@node-1 my-cluster]#  ceph-deploy mgr create node-2 node-3 #添加node-2 node-3 成为mgr[root@node-1 my-cluster]#  ceph -s  #查看MGR状态  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 11m)    mgr: node-1(active, since 28m), standbys: node-2, node-3 #可以看到备MGR为：node-2和node-3    osd: 3 osds: 3 up (since 18m), 3 in (since 18m)   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   3.0 GiB used, 27 GiB / 30 GiB avail    pgs:[root@node-1 my-cluster]# ceph mgr dump #查看具体MAP图</code></pre><h1><span id="十-创建资源池pool">十、创建资源池Pool</span></h1><pre><code>Node-1执行：[root@node-1 my-cluster]# ceph osd lspools #查看当前pool[root@node-1 my-cluster]# ceph osd pool create ceph-demo 64 64 #创建一个叫做ceph-demo的pool 并指定PG数为64（第一个数字），pgp数64（第二个数字）pool &#39;ceph-demo&#39; created[root@node-1 my-cluster]# ceph osd lspools #再次查看当前pool 新增了要给ceph-demo的pool1 ceph-demo[root@node-1 my-cluster]#  ceph osd pool get ceph-demo pg_num #查看ceph-demo的PG数量pg_num: 64[root@node-1 my-cluster]#  ceph osd pool get ceph-demo pgp_num #查看ceph-demo的PGP数量pgp_num: 64[root@node-1 my-cluster]#  ceph osd pool get ceph-demo size #查看ceph-demo的副本数size: 3[root@node-1 my-cluster]#  ceph osd pool get ceph-demo crush_rule #查看crush的调度算法crush_rule: replicated_rule #默认的复制规则[root@node-1 my-cluster]#   ceph osd pool set ceph-demo size 2  #可以通过set的方式调整副本数量为2set pool 1 size to 2[root@node-1 my-cluster]# ceph osd pool set ceph-demo pg_num 128 #可以通过set的方式调整pg数量为128set pool 1 pg_num to 128[root@node-1 my-cluster]# ceph osd pool set ceph-demo pgp_num 128 #可以通过set的方式调整pgp数量为128,PGP数量应该与PG数一致set pool 1 pgp_num to 12[root@node-1 my-cluster]# rbd pool init &lt;pool_name&gt; #初始化pool</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RGW对象存储</title>
      <link href="/2023/04/11/Ceph/5.RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/"/>
      <url>/2023/04/11/Ceph/5.RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#ceph-%E5%AF%B9%E8%B1%A1%E7%BD%91%E5%85%B3%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D">CEPH 对象网关架构介绍</a></li><li><a href="#%E9%83%A8%E7%BD%B2rgw">部署RGW</a><ul><li><a href="#%E6%9F%A5%E7%9C%8B%E5%B8%AE%E5%8A%A9%E6%96%87%E6%A1%A3">查看帮助文档</a></li><li><a href="#%E4%BD%BF%E7%94%A8node-1%E4%BD%9C%E4%B8%BA%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3">使用node-1作为对象存储网关</a></li><li><a href="#%E6%A3%80%E6%9F%A5%E6%98%AF%E5%90%A6%E6%B7%BB%E5%8A%A0%E6%88%90%E5%8A%9F">检查是否添加成功</a></li><li><a href="#%E6%A3%80%E6%9F%A5%E6%9C%8D%E5%8A%A1%E8%BF%90%E8%A1%8C%E6%83%85%E5%86%B5">检查服务运行情况</a></li></ul></li><li><a href="#%E4%BF%AE%E6%94%B9rgw%E9%BB%98%E8%AE%A4%E7%AB%AF%E5%8F%A3">修改RGW默认端口</a><ul><li><a href="#%E4%BF%AE%E6%94%B9%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">修改节点配置文件</a></li><li><a href="#%E5%B0%86%E9%85%8D%E7%BD%AE%E5%A5%BD%E7%9A%84cephconf-%E6%8E%A8%E9%80%81%E5%88%B0%E6%89%80%E6%9C%89%E8%8A%82%E7%82%B9%E4%B8%AD">将配置好的ceph.conf 推送到所有节点中</a></li><li><a href="#%E9%87%8D%E5%90%AFrgw%E6%9C%8D%E5%8A%A1">重启rgw服务</a></li><li><a href="#%E6%A0%A1%E9%AA%8C%E9%85%8D%E7%BD%AE%E6%98%AF%E5%90%A6%E7%94%9F%E6%95%88-%E5%8F%AF%E4%BF%AE%E6%94%B9%E6%88%90443%E7%AB%AF%E5%8F%A3%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6%E5%8F%AF%E4%BD%BF%E7%94%A8%E7%99%BE%E5%BA%A6%E8%BF%9B%E8%A1%8C%E6%9F%A5%E8%AF%A2%E5%85%B7%E4%BD%93%E9%85%8D%E7%BD%AE">校验配置是否生效 可修改成443端口，添加证书可使用百度进行查询具体配置</a></li></ul></li><li><a href="#rgw%E4%B9%8Bs3%E6%8E%A5%E5%8F%A3%E4%BD%BF%E7%94%A8">RGW之s3接口使用</a><ul><li><a href="#%E6%9F%A5%E7%9C%8B%E5%B8%AE%E5%8A%A9%E6%96%87%E6%A1%A3%E4%BD%BF%E7%94%A8user-create%E8%BF%9B%E8%A1%8C%E5%88%9B%E5%BB%BA">查看帮助文档，使用user create进行创建</a></li><li><a href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%98%BE%E7%A4%BA%E5%90%8D%E4%B8%BAceph-s3-user-demo%E7%9A%84%E7%94%A8%E6%88%B7%E5%B9%B6%E6%8C%87%E5%AE%9Auid%E4%B8%BAceph-s3-user">创建一个显示名为Ceph S3 User Demo的用户，并指定uid为ceph-s3-user</a></li><li><a href="#%E4%BF%9D%E5%AD%98%E7%94%A8%E6%88%B7%E5%92%8C%E5%AF%86%E7%A0%81%E4%BF%A1%E6%81%AF">保存用户和密码信息</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8Dradosgw%E5%88%9B%E5%BB%BA%E7%9A%84%E7%94%A8%E6%88%B7">查看当前radosgw创建的用户</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E7%94%A8%E6%88%B7%E7%9A%84%E5%85%B7%E4%BD%93%E4%BF%A1%E6%81%AF">查看用户的具体信息</a></li><li><a href="#%E5%AE%89%E8%A3%85python-boto%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%8C%85">安装python-boto的软件包</a></li><li><a href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAs3-clientpy%E7%9A%84python%E8%84%9A%E6%9C%AC">创建一个s3-client.py的python脚本</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%9A%84pool">查看自动生成的pool</a></li><li><a href="#%E5%88%9B%E5%BB%BAceph-s3-bucket%E7%9A%84bucket">创建ceph-s3-bucket的bucket</a></li><li><a href="#%E5%86%8D%E6%AC%A1%E6%9F%A5%E7%9C%8B%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%9A%84pool">再次查看自动生成的pool</a></li><li><a href="#%E5%AE%89%E8%A3%85s3cmd%E7%9A%84%E5%B7%A5%E5%85%B7%E5%AE%9E%E7%8E%B0%E4%B8%8A%E4%BC%A0%E4%B8%8B%E8%BD%BD%E5%8A%9F%E8%83%BD">安装s3cmd的工具实现上传下载功能</a></li><li><a href="#%E9%85%8D%E7%BD%AEs3cmd%E7%9A%84%E5%B7%A5%E5%85%B7">配置s3cmd的工具</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E7%94%9F%E6%88%90%E7%9A%84%E6%96%87%E4%BB%B6">查看生成的文件</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8Dbuck%E6%83%85%E5%86%B5">查看当前buck情况</a></li><li><a href="#%E5%88%9B%E5%BB%BAbucket">创建bucket</a></li><li><a href="#%E6%9F%A5%E7%9C%8Bbucket">查看bucket</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6">上传文件</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E5%A4%B9">上传文件夹</a></li><li><a href="#%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6">下载文件</a></li><li><a href="#%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6">删除文件</a></li><li><a href="#%E5%88%A0%E9%99%A4%E7%9B%AE%E5%BD%95">删除目录</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E5%AD%98%E5%82%A8%E6%B1%A0%E5%8F%98%E5%8C%96">查看存储池变化</a></li></ul></li><li><a href="#rgw%E4%B9%8Bswift%E6%8E%A5%E5%8F%A3%E4%BD%BF%E7%94%A8">RGW之Swift接口使用</a><ul><li><a href="#%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8D%E7%94%A8%E6%88%B7">查看当前用户</a></li><li><a href="#%E6%A0%B9%E6%8D%AE%E5%BD%93%E5%89%8D%E7%94%A8%E6%88%B7%E5%88%9B%E5%BB%BAswift%E7%94%A8%E6%88%B7">根据当前用户创建swift用户</a></li><li><a href="#%E7%94%9F%E6%88%90swift%E7%9A%84key">生成Swift的key</a></li><li><a href="#%E5%AE%89%E8%A3%85swift%E5%AE%A2%E6%88%B7%E7%AB%AF">安装Swift客户端</a></li><li><a href="#%E6%9F%A5%E7%9C%8Bbucket-1">查看bucket</a></li><li><a href="#%E5%88%9B%E5%BB%BAswift%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E6%96%87%E4%BB%B6">创建Swift环境变量文件</a></li><li><a href="#%E9%AA%8C%E8%AF%81%E5%8F%98%E9%87%8F%E6%98%AF%E5%90%A6%E7%94%9F%E6%95%88">验证变量是否生效</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6-1">上传文件</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E7%9B%AE%E5%BD%95">上传目录</a></li><li><a href="#%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6-1">下载文件</a></li><li><a href="#%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6-1">删除文件</a></li></ul></li></ul><!-- tocstop --><h1><span id="ceph-对象网关架构介绍">CEPH 对象网关架构介绍</span></h1><p>Ceph 对象网关是建立在 librados. 它在应用程序和 Ceph 存储集群之间提供了一个 RESTful 网关。Ceph 对象存储支持两种接口：</p><ol><li>S3 兼容：通过与 Amazon S3 RESTful API 的大部分子集兼容的接口提供对象存储功能。</li><li>Swift 兼容：通过与 OpenStack Swift API 的大部分子集兼容的接口提供对象存储功能。</li></ol><p>Ceph 对象存储使用 Ceph 对象网关守护进程 ( radosgw)，这是一个设计用于与 Ceph 存储集群交互的 HTTP 服务器。</p><p>Ceph 对象网关提供与 Amazon S3 和 OpenStack Swift 兼容的接口，并且有自己的用户管理。Ceph 对象网关可以将数据存储在同一个 Ceph 存储集群中，其中存储了来自 Ceph 文件系统客户端和 Ceph 块设备客户端的数据。S3 API 和 Swift API 共享一个公共命名空间，这使得可以使用一个 API 将数据写入 Ceph 存储集群，然后使用另一个 API 检索该数据。</p><p><img src="/images/Ceph/RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3/1.jpg"></p><blockquote><p>Ceph 对象存储不使用Ceph 元数据服务器。</p></blockquote><h1><span id="部署rgw">部署RGW</span></h1><h2><span id="查看帮助文档">查看帮助文档</span></h2><pre><code>[root@node-1 my-cluster]# ceph-deploy rgw -h  usage: ceph-deploy rgw [-h] &#123;create&#125; ...Ceph RGW daemon managementpositional arguments:  &#123;create&#125;    create    Create an RGW instanceoptional arguments:  -h, --help  show this help message and exit</code></pre><h2><span id="使用node-1作为对象存储网关">使用node-1作为对象存储网关</span></h2><pre><code>[root@node-1 my-cluster]# ceph-deploy rgw create [HOST[:NAME] ...]usage: ceph-deploy rgw create [-h] HOST[:NAME] [HOST[:NAME] ...][root@node-1 my-cluster]# ceph-deploy rgw create node-1</code></pre><h2><span id="检查是否添加成功">检查是否添加成功</span></h2><pre><code>[root@node-1 my-cluster]# ceph -s  #查看集群多了一个rgw的对象网关  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 32s)    mgr: node-1(active, since 18h), standbys: node-2, node-3    osd: 3 osds: 3 up (since 2h), 3 in (since 5h)    rgw: 1 daemon active (node-1)   task status:   data:    pools:   5 pools, 192 pgs    objects: 468 objects, 1.0 GiB    usage:   6.2 GiB used, 24 GiB / 30 GiB avail    pgs:     192 active+clean[root@node-1 my-cluster]# netstat -antupl | grep 7480 #查看监听端口tcp        0      0 0.0.0.0:7480            0.0.0.0:*               LISTEN      66157/radosgw       tcp6       0      0 :::7480                 :::*                    LISTEN      66157/radosgw [root@node-1 my-cluster]#  yum whatprovides &quot;*bin/netstat&quot; #如果netstat执行失败可查看是否没有安装net-toolsLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.aliyun.comnet-tools-2.0-0.25.20131004git.el7.x86_64 : Basic networking toolsRepo        : baseMatched from:Filename    : /bin/netstatnet-tools-2.0-0.25.20131004git.el7.x86_64 : Basic networking toolsRepo        : @baseMatched from:Filename    : /bin/netsta[root@node-1 my-cluster]#  yum install -y net-tools</code></pre><h2><span id="检查服务运行情况">检查服务运行情况</span></h2><pre><code>[root@node-1 my-cluster]# curl http://node-1:7480 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ListAllMyBucketsResult xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;</code></pre><h1><span id="修改rgw默认端口">修改RGW默认端口</span></h1><h2><span id="修改节点配置文件">修改节点配置文件</span></h2><p><strong>node-1节点执行：</strong></p><pre><code>[root@node-1 my-cluster]# cat ceph.conf [global]fsid = e9a90625-4707-4b6b-b52f-661512ea831dpublic_network = 192.168.187.0/24cluster_network = 192.168.199.0/24mon_initial_members = node-1mon_host = 192.168.187.201auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephx[client.rgw.node-1] #新增如下配置</code></pre><h2><span id="将配置好的cephconf-推送到所有节点中">将配置好的ceph.conf 推送到所有节点中</span></h2><blockquote><p>–overwrite-conf 由于其他节点已经有ceph.conf 文件，所以需要加此参数进行覆盖</p></blockquote><p>[root@node-1 my-cluster]# ceph-deploy –overwrite-conf config push node-1 node-2 node-3 </p><h2><span id="重启rgw服务">重启rgw服务</span></h2><pre><code>[root@node-1 my-cluster]#  systemctl restart ceph-radosgw.target</code></pre><h2><span id="校验配置是否生效-可修改成443端口添加证书可使用百度进行查询具体配置">校验配置是否生效 可修改成443端口，添加证书可使用百度进行查询具体配置</span></h2><pre><code>[root@node-1 my-cluster]# netstat -antupl | grep 80 tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      67161/radosgw</code></pre><h1><span id="rgw之s3接口使用">RGW之s3接口使用</span></h1><blockquote><p> <strong>使用对象存储的前提是需要创建用户，一种是S3风格的，一种是swift风格的</strong></p></blockquote><h2><span id="查看帮助文档使用user-create进行创建">查看帮助文档，使用user create进行创建</span></h2><pre><code>[root@node-1 my-cluster]# radosgw-admin -h | grep user   user create                create a new user  user modify                modify user  user info                  get user info  user rm                    remove user  user suspend               suspend a user  user enable                re-enable user after suspension  user check                 check user info  user stats                 show user stats as accounted by quota subsystem  user list                  list users  caps add                   add user capabilities  caps rm                    remove user capabilities  subuser create             create a new subuser  subuser modify             modify subuser  subuser rm                 remove subuser  bucket link                link bucket to specified user  bucket unlink              unlink bucket from specified user  usage show                 show usage (by user, by bucket, date range)  usage trim                 trim usage (by user, by bucket, date range)   --uid=&lt;id&gt;                user id   --subuser=&lt;name&gt;          subuser name   --email=&lt;email&gt;           user&#39;s email address   --access=&lt;access&gt;         Set access permissions for sub-user, should be one   --display-name=&lt;name&gt;     user&#39;s display name   --max-buckets             max number of buckets for a user   --admin                   set the admin flag on the user   --system                  set the system flag on the user   --op-mask                 set the op mask on the user   --purge-data              when specified, user removal will also purge all the                             user data   --purge-keys              when specified, subuser removal will also purge all the                             subuser keys   --sync-stats              option to &#39;user stats&#39;, update user stats with current                             stats reported by user&#39;s buckets indexes   --reset-stats             option to &#39;user stats&#39;, reset stats in accordance with user buckets   --caps=&lt;caps&gt;             list of caps (e.g., &quot;usage=read, write; user=read&quot;)   --quota-scope             scope of quota (bucket, user)  --setuser USER    set uid to user or uid (and gid to user&#39;s gid)</code></pre><h2><span id="创建一个显示名为ceph-s3-user-demo的用户并指定uid为ceph-s3-user">创建一个显示名为Ceph S3 User Demo的用户，并指定uid为ceph-s3-user</span></h2><pre><code>[root@node-1 my-cluster]# radosgw-admin user create --uid ceph-s3-user --display-name &quot;Ceph S3 User Demo&quot; &#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000, #最大只允许创建1000个buckets    &quot;subusers&quot;: [],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;, #用户ID            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;, #SSH的key            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot; #  secret的key        &#125;    ],    &quot;swift_keys&quot;: [],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123; #bucket配额信息        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123; #用户的配额信息        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="保存用户和密码信息">保存用户和密码信息</span></h2><pre><code>[root@node-1 my-cluster]# cat key.txt      &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],</code></pre><h2><span id="查看当前radosgw创建的用户">查看当前radosgw创建的用户</span></h2><pre><code>[root@node-1 my-cluster]#  radosgw-admin user list [    &quot;ceph-s3-user&quot;]</code></pre><h2><span id="查看用户的具体信息">查看用户的具体信息</span></h2><pre><code>[root@node-1 my-cluster]# radosgw-admin user info --uid ceph-s3-user &#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000,    &quot;subusers&quot;: [],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],    &quot;swift_keys&quot;: [],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="安装python-boto的软件包">安装python-boto的软件包</span></h2><pre><code>[root@node-1 my-cluster]# yum install python-boto</code></pre><h2><span id="创建一个s3-clientpy的python脚本">创建一个s3-client.py的python脚本</span></h2><pre><code>[root@node-1 my-cluster]# cat s3-client.pyimport botoimport boto.s3.connectionaccess_key = &#39;5SCX55201QUQ0FOPCPD6&#39; #上述保存的keysecret_key = &#39;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&#39; #上述保存的keyconn = boto.connect_s3(        aws_access_key_id = access_key,        aws_secret_access_key = secret_key,        host = &#39;192.168.187.201&#39;, port=80, #服务器的IP及端口        is_secure=False,calling_format=boto.s3.connection.OrdinaryCallingFormat(),        )bucket = conn.create_bucket(&#39;ceph-s3-bucket&#39;)for bucket in conn.get_all_buckets():        print(&quot;&#123;name&#125;\t&#123;created&#125;&quot;.format(                name = bucket.name,                created = bucket.creation_date,        ))</code></pre><h2><span id="查看自动生成的pool">查看自动生成的pool</span></h2><pre><code>[root@node-1 my-cluster]# ceph osd lspools1 ceph-demo2 .rgw.root3 default.rgw.control4 default.rgw.meta5 default.rgw.log</code></pre><h2><span id="创建ceph-s3-bucket的bucket">创建ceph-s3-bucket的bucket</span></h2><pre><code>[root@node-1 my-cluster]#  python  s3-client.py ceph-s3-bucket    2023-03-04T12:52:01.787Z</code></pre><h2><span id="再次查看自动生成的pool">再次查看自动生成的pool</span></h2><blockquote><p>执行后可以看到多了一个6 default.rgw.buckets.index的pool，写入数据后还会增加一个data的pool</p></blockquote><pre><code>[root@node-1 my-cluster]# ceph osd lspools1 ceph-demo2 .rgw.root #rgw的根pool3 default.rgw.control #rgw的配置pool4 default.rgw.meta #rgw的元数据pool5 default.rgw.log #rgw的日志pool 6 default.rgw.buckets.index #rgw的索引pool</code></pre><h2><span id="安装s3cmd的工具实现上传下载功能">安装s3cmd的工具实现上传下载功能</span></h2><p><code>[root@node-1 my-cluster]#  yum install s3cmd</code></p><h2><span id="配置s3cmd的工具">配置s3cmd的工具</span></h2><pre><code>[root@node-1 my-cluster]# s3cmd --configureEnter new values or accept defaults in brackets with Enter.Refer to user manual for detailed description of all options.Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.Access Key: 5SCX55201QUQ0FOPCPD6 #输入Secret Key: k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm #输入Default Region [US]: Use &quot;s3.amazonaws.com&quot; for S3 Endpoint and not modify it to the 回车target Amazon S3.S3 Endpoint [s3.amazonaws.com]: 192.168.187.201:80 #输入回车Use &quot;%(bucket)s.s3.amazonaws.com&quot; to the target Amazon S3. &quot;%(bucket)s&quot; and &quot;%(location)s&quot; vars can be usedif the target S3 system supports dns based buckets.DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: 192.168.187.201:80/%(bucket)s #输入回车Encryption password is used to protect your files from readingby unauthorized persons while in transfer to S3Encryption password: Path to GPG program [/usr/bin/gpg]: When using secure HTTPS protocol all communication with Amazon S3servers is protected from 3rd party eavesdropping. This method isslower than plain HTTP, and can only be proxied with Python 2.7 or newerUse HTTPS protocol [Yes]: no #输入回车On some networks all internet access must go through a HTTP proxy.Try setting it here if you can&#39;t connect to S3 directlyHTTP Proxy server name: New settings:  Access Key: 5SCX55201QUQ0FOPCPD6  Secret Key: k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm  Default Region: US  S3 Endpoint: 192.168.187.201:80  DNS-style bucket+hostname:port template for accessing a bucket: 192.168.187.201:80/%(bucket)s  Encryption password:   Path to GPG program: /usr/bin/gpg  Use HTTPS protocol: False  HTTP Proxy server name:   HTTP Proxy server port: 0Test access with supplied credentials? [Y/n] y #输入Please wait, attempting to list all buckets...Success. Your access key and secret key worked fine :-)Now verifying that encryption works...Not configured. Never mind.Save settings? [y/N] y #输入Configuration saved to &#39;/root/.s3cfg&#39;You have new mail in /var/spool/mail/root</code></pre><h2><span id="查看生成的文件">查看生成的文件</span></h2><pre><code>[root@node-1 my-cluster]# cat /root/.s3cfg[default]access_key = 5SCX55201QUQ0FOPCPD6access_token = add_encoding_exts = add_headers = bucket_location = USca_certs_file = cache_file = check_ssl_certificate = Truecheck_ssl_hostname = Truecloudfront_host = cloudfront.amazonaws.comconnection_max_age = 5connection_pooling = Truecontent_disposition = content_type = default_mime_type = binary/octet-streamdelay_updates = Falsedelete_after = Falsedelete_after_fetch = Falsedelete_removed = Falsedry_run = Falseenable_multipart = Trueencrypt = Falseexpiry_date = expiry_days = expiry_prefix = follow_symlinks = Falseforce = Falseget_continue = Falsegpg_command = /usr/bin/gpggpg_decrypt = %(gpg_command)s -d --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)sgpg_encrypt = %(gpg_command)s -c --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)sgpg_passphrase = guess_mime_type = Truehost_base = 192.168.187.201:80host_bucket = 192.168.187.201:80/%(bucket)shuman_readable_sizes = Falseinvalidate_default_index_on_cf = Falseinvalidate_default_index_root_on_cf = Trueinvalidate_on_cf = Falsekms_key = limit = -1limitrate = 0list_allow_unordered = Falselist_md5 = Falselog_target_prefix = long_listing = Falsemax_delete = -1mime_type = multipart_chunk_size_mb = 15multipart_copy_chunk_size_mb = 1024multipart_max_chunks = 10000preserve_attrs = Trueprogress_meter = Trueproxy_host = proxy_port = 0public_url_use_https = Falseput_continue = Falserecursive = Falserecv_chunk = 65536reduced_redundancy = Falserequester_pays = Falserestore_days = 1restore_priority = Standardsecret_key = k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldmsend_chunk = 65536server_side_encryption = Falsesignature_v2 = Falsesignurl_use_https = Falsesimpledb_host = sdb.amazonaws.comskip_existing = Falsesocket_timeout = 300ssl_client_cert_file = ssl_client_key_file = stats = Falsestop_on_error = Falsestorage_class = throttle_max = 100upload_id = urlencoding_mode = normaluse_http_expect = Falseuse_https = Falseuse_mime_magic = Trueverbosity = WARNINGwebsite_endpoint = http://%(bucket)s.s3-website-%(location)s.amazonaws.com/website_error = website_index = index.html</code></pre><h2><span id="查看当前buck情况">查看当前buck情况</span></h2><pre><code>[root@node-1 ~]# s3cmd ls 2023-03-04 12:52  s3://ceph-s3-bucket</code></pre><h2><span id="创建bucket">创建bucket</span></h2><blockquote><p>使用cmd创建一个叫做s3cmd-demo的bucket</p></blockquote><pre><code>[root@node-1 ~]# s3cmd mb s3://s3cmd-demo ERROR: S3 error: 403 (SignatureDoesNotMatch) #由于当前设置的版本不一致导致</code></pre><blockquote><p>修改版本</p></blockquote><pre><code>[root@node-1 ~]# vi /root/.s3cfg signature_v2 = True</code></pre><blockquote><p>重新创建</p></blockquote><pre><code>[root@node-1 ~]# s3cmd mb s3://s3cmd-demo Bucket &#39;s3://s3cmd-demo/&#39; created</code></pre><h2><span id="查看bucket">查看bucket</span></h2><pre><code>[root@node-1 ~]# s3cmd ls 2023-03-04 12:52  s3://ceph-s3-bucket2023-03-04 17:38  s3://s3cmd-demo</code></pre><h2><span id="上传文件">上传文件</span></h2><pre><code>[root@node-1 ~]#  s3cmd put /etc/fstab s3://s3cmd-demo/fastab-demo #报错，经查询最简单的解决办法是添加OSD或者修改ceph.conf 文件upload: &#39;/etc/fstab&#39; -&gt; &#39;s3://s3cmd-demo/fastab-demo&#39;  [1 of 1] 465 of 465   100% in    0s   866.93 B/s  doneERROR: S3 error: 416 (InvalidRange)</code></pre><blockquote><p>报错，经查询最简单的解决办法是添加OSD或者修改ceph.conf 文件,原因是当前OSD数量太少；</p></blockquote><blockquote><p>添加OSD</p></blockquote><pre><code>[root@node-1 my-cluster]# ceph-deploy osd create node-1 --data /dev/sdc --journal /dev/sdc[root@node-1 my-cluster]# ceph-deploy osd create node-2 --data /dev/sdc --journal /dev/sdc[root@node-1 my-cluster]# ceph-deploy osd create node-3 --data /dev/sdc --journal /dev/sdc</code></pre><blockquote><p>尝试重新上传</p></blockquote><pre><code>[root@node-1 my-cluster]#  s3cmd put /etc/fstab s3://s3cmd-demo/fastab-demoupload: &#39;/etc/fstab&#39; -&gt; &#39;s3://s3cmd-demo/fastab-demo&#39;  [1 of 1] 465 of 465   100% in    1s   317.00 B/s  done</code></pre><blockquote><p>查看上传的文件</p></blockquote><pre><code>[root@node-1 my-cluster]# s3cmd ls s3://s3cmd-demo 2023-03-04 17:47          465  s3://s3cmd-demo/fastab-demo</code></pre><h2><span id="上传文件夹">上传文件夹</span></h2><blockquote><p>–recursive 上传目录需要使用递归的方式</p></blockquote><p><code>[root@node-1 my-cluster]# s3cmd put /etc s3://s3cmd-demo/etc/ --recursive </code></p><blockquote><p>查看上传结果</p></blockquote><pre><code>[root@node-1 my-cluster]# s3cmd ls s3://s3cmd-demo/etc/etc/                          DIR  s3://s3cmd-demo/etc/etc/NetworkManager/                          DIR  s3://s3cmd-demo/etc/etc/X11/                          DIR  s3://s3cmd-demo/etc/etc/audisp/                          DIR  s3://s3cmd-demo/etc/etc/audit/                          DIR  s3://s3cmd-demo/etc/etc/bash_completion.d/                          DIR  s3://s3cmd-demo/etc/etc/ceph/                          DIR  s3://s3cmd-demo/etc/etc/cron.d/</code></pre><h2><span id="下载文件">下载文件</span></h2><blockquote><p>#下载文件并重命名为yum.conf-download</p></blockquote><pre><code>[root@node-1 ~]# s3cmd get s3://s3cmd-demo/etc/etc/yum.conf yum.conf-download download: &#39;s3://s3cmd-demo/etc/etc/yum.conf&#39; -&gt; &#39;yum.conf-download&#39;  [1 of 1] 970 of 970   100% in    0s    22.27 KB/s  done</code></pre><h2><span id="删除文件">删除文件</span></h2><pre><code>[root@node-1 ~]#  s3cmd rm s3://s3cmd-demo/fastab-demo #删除操作delete: &#39;s3://s3cmd-demo/fstab-demo&#39;</code></pre><h2><span id="删除目录">删除目录</span></h2><p><code>s3cmd rm s3://s3cmd-demo/etc --recursive</code></p><h2><span id="查看存储池变化">查看存储池变化</span></h2><blockquote><p>完成上述操作后ceph会多出一个default.rgw.buckets.data的pool</p></blockquote><pre><code>[root@node-1 ~]# ceph osd lspools1 ceph-demo2 .rgw.root3 default.rgw.control4 default.rgw.meta5 default.rgw.log6 default.rgw.buckets.index7 default.rgw.buckets.data #rgw的数据pool</code></pre><blockquote><p>查看内容</p></blockquote><pre><code>[root@node-1 ~]# rados -p default.rgw.buckets.data ls 3db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_23db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_1</code></pre><blockquote><p>#重新上传文件</p></blockquote><p><code>root@node-1 ~]#  s3cmd put /etc/fstab s3://s3cmd-demo/fastab-demo</code></p><blockquote><p>再次查看内容</p></blockquote><pre><code>[root@node-1 ~]# rados -p default.rgw.buckets.data ls  #可以看到多了一个3db4c650-9c23-4843-8015-97b79566b3b0.5970.2_fastab-demo的对象3db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_23db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_13db4c650-9c23-4843-8015-97b79566b3b0.5970.2_fastab-demo #可以看到这个文件多了写前缀，前缀是存放在default.rgw.buckets.index的pool中的</code></pre><blockquote><p>查看索引</p></blockquote><pre><code>[root@node-1 ~]# rados -p default.rgw.buckets.index ls .dir.3db4c650-9c23-4843-8015-97b79566b3b0.5970.2</code></pre><h1><span id="rgw之swift接口使用">RGW之Swift接口使用</span></h1><blockquote><p><strong>需要在原有用户的基础上进行添加</strong></p></blockquote><h2><span id="查看当前用户">查看当前用户</span></h2><pre><code>[root@node-1 ~]# radosgw-admin user list [    &quot;ceph-s3-user&quot;]</code></pre><h2><span id="根据当前用户创建swift用户">根据当前用户创建swift用户</span></h2><pre><code>[root@node-1 ~]# radosgw-admin subuser create --uid ceph-s3-user --subuser=ceph-s3-user:swift --access=full&#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000,    &quot;subusers&quot;: [        &#123;            &quot;id&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;permissions&quot;: &quot;full-control&quot;        &#125;    ],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],    &quot;swift_keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;secret_key&quot;: &quot;9sWyeIFQphNButuZp2zNs2t7xU9qEkgW27yHxqBL&quot;        &#125;    ],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="生成swift的key">生成Swift的key</span></h2><pre><code>[root@node-1 ~]# radosgw-admin key create --subuser=ceph-s3-user:swift --key-type=swift --gen-secret &#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000,    &quot;subusers&quot;: [        &#123;            &quot;id&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;permissions&quot;: &quot;full-control&quot;        &#125;    ],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],    &quot;swift_keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;secret_key&quot;: &quot;mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjh&quot; #提前保存        &#125;    ],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="安装swift客户端">安装Swift客户端</span></h2><blockquote><p>安装Swift客户端需要先安装pip</p></blockquote><pre><code>[root@node-1 ~]# yum install python-pip #安装pip 由于当前pip版本较老（8.1），无法通过pip install -U pip 进行升级，只能手动进行升级[root@node-1 ~]wget https://files.pythonhosted.org/packages/0b/f5/be8e741434a4bf4ce5dbc235aa28ed0666178ea8986ddc10d035023744e6/pip-20.2.4.tar.gz  #下载安装包[root@node-1 ~]tar -zxvf pip-20.2.4.tar.gz  # 解压[root@node-1 ~]cd pip-20.2.4/[root@node-1 ~]sudo python setup.py install #给予权限不然可能安装失败[root@node-1 ~]pip install -U pip #再次更新DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.Collecting pip  Downloading pip-20.3.4-py2.py3-none-any.whl (1.5 MB)     |████████████████████████████████| 1.5 MB 1.0 MB/s Installing collected packages: pip  Attempting uninstall: pip    Found existing installation: pip 20.2.4    Uninstalling pip-20.2.4:      Successfully uninstalled pip-20.2.4Successfully installed pip-20.3.4</code></pre><blockquote><p>再次安装Swift客户端</p></blockquote><pre><code>[root@node-1 pip-20.2.4]# pip install  python-swiftclient #安装成功DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.Collecting python-swiftclient  Downloading python_swiftclient-3.13.1-py2.py3-none-any.whl (87 kB)     |████████████████████████████████| 87 kB 6.4 kB/s Collecting futures&gt;=3.0.0; python_version == &quot;2.7&quot;  Downloading futures-3.4.0-py2-none-any.whl (16 kB)Requirement already satisfied: requests&gt;=1.1.0 in /usr/lib/python2.7/site-packages (from python-swiftclient) (2.6.0)Requirement already satisfied: six&gt;=1.9.0 in /usr/lib/python2.7/site-packages (from python-swiftclient) (1.9.0)Installing collected packages: futures, python-swiftclientSuccessfully installed futures-3.4.0 python-swiftclient-3.13.1</code></pre><h2><span id="查看bucket">查看bucket</span></h2><blockquote><p>查看，list可换成upload或download实现上传下载</p></blockquote><pre><code>[root@node-1 ~]# swift -A http://192.168.187.201:80/auth -U ceph-s3-user:swift -K mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjh list ceph-s3-buckets3cmd-demo</code></pre><h2><span id="创建swift环境变量文件">创建Swift环境变量文件</span></h2><pre><code>[root@node-1 ~]# cat swift-openrc.shexport ST_AUTH=http://192.168.187.201:80/authexport ST_USER=ceph-s3-user:swiftexport ST_KEY=mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjh</code></pre><h2><span id="验证变量是否生效">验证变量是否生效</span></h2><pre><code>[root@node-1 ~]# set |grep ST #查看环境变量是否生效DIRSTACK=()HISTCONTROL=ignoredupsHISTFILE=/root/.bash_historyHISTFILESIZE=1000HISTSIZE=1000HOSTNAME=node-1HOSTTYPE=x86_64OSTYPE=linux-gnuPIPESTATUS=([0]=&quot;0&quot;)SELINUX_LEVEL_REQUESTED=SELINUX_ROLE_REQUESTED=ST_AUTH=http://192.168.187.201:80/authST_KEY=mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjhexport[root@node-1 ~]# swift list  #验证变量成功ceph-s3-buckets3cmd-demo</code></pre><h2><span id="上传文件">上传文件</span></h2><p><code>[root@node-1 ~]# swift upload swift-demo /etc/shadow </code></p><h2><span id="上传目录">上传目录</span></h2><p><code>[root@node-1 ~]# swift upload swift-demo /etc/ </code></p><h2><span id="下载文件">下载文件</span></h2><pre><code>[root@node-1 ~]# swift download swift-demo etc/passwdetc/passwd [auth 0.008s, headers 0.011s, total 0.012s, 0.278 MB/s]</code></pre><p>##查看详细信息</p><pre><code>[root@node-1 ~]# swift stat swift-demo                      Account: v1                    Container: swift-demo                      Objects: 1694                        Bytes: 35940093                     Read ACL:                    Write ACL:                      Sync To:                     Sync Key:X-Container-Bytes-Used-Actual: 40927232                Accept-Ranges: bytes             X-Storage-Policy: default-placement              X-Storage-Class: STANDARD                Last-Modified: Sat, 04 Mar 2023 19:14:33 GMT                  X-Timestamp: 1677957085.09881                   X-Trans-Id: tx0000000000000000000d1-0064043339-85b3-default                 Content-Type: text/plain; charset=utf-8       X-Openstack-Request-Id: tx0000000000000000000d1-0064043339-85b3-default</code></pre><h2><span id="删除文件">删除文件</span></h2><blockquote><p>#删除文件，不要使用 swift delete -a 会删除所有bucket</p></blockquote><pre><code>[root@node-1 ~]# swift delete swift-demo etc/shadow #删除文件，不要使用 swift delete -a 会删除所有bucketetc/shadow[root@node-1 ~]# swift list swift-demo</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> RGW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CephFS文件存储</title>
      <link href="/2023/04/11/Ceph/6.Ceph%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/Ceph%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"/>
      <url>/2023/04/11/Ceph/6.Ceph%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/Ceph%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E5%AE%89%E8%A3%85mds%E6%9C%8D%E5%8A%A1">安装MDS服务</a></li><li><a href="#%E5%88%9B%E5%BB%BAceph-fs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F">创建Ceph FS文件系统</a></li><li><a href="#%E6%9F%A5%E7%9C%8Bfs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F">查看fs文件系统</a></li><li><a href="#%E5%86%85%E6%A0%B8%E6%8C%82%E8%BD%BD">内核挂载</a></li><li><a href="#%E7%94%A8%E6%88%B7%E6%80%81%E6%8C%82%E8%BD%BD">用户态挂载</a></li></ul><!-- tocstop --><p>Ceph FS文件存储时由NAS衍生过来的。</p><p>Ceph 文件系统或CephFS是一个 POSIX 兼容的文件系统，构建在 Ceph 的分布式对象存储RADOS之上。CephFS致力于为各种应用程序提供最先进、多用途、高可用性和高性能的文件存储，包括共享主目录、HPC 暂存空间和分布式工作流共享存储等传统用例。</p><p>CephFS 通过使用一些新颖的架构选择来实现这些目标。值得注意的是，文件元数据存储在与文件数据不同的 RADOS 池中，并通过可调整大小的元数据服务器集群或MDS提供服务，它可以扩展以支持更高吞吐量的元数据工作负载。文件系统的客户端可以直接访问 RADOS 来读写文件数据块。因此，工作负载可能会随着底层 RADOS 对象存储的大小线性扩展；也就是说，没有网关或代理为客户端调解数据 I&#x2F;O。</p><p>对数据的访问通过 MDS 集群进行协调，MDS 集群充当由客户端和 MDS 共同维护的分布式元数据缓存状态的权威。每个 MDS 将元数据的变化聚合成一系列高效写入 RADOS 上的日志；MDS 没有在本地存储任何元数据状态。该模型允许在 POSIX 文件系统的上下文中在客户端之间进行连贯和快速的协作。</p><p><img src="/images/Ceph/Ceph_FS%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/1.jpg"></p><p>CephFS 因其新颖的设计和对文件系统研究的贡献而成为众多学术论文的主题。它是 Ceph 中最古老的存储接口，曾经是 RADOS的主要用例。现在它与另外两个存储接口一起组成了一个现代统一存储系统：RBD（Ceph 块设备）和 RGW（Ceph 对象存储网关）。</p><h1><span id="安装mds服务">安装MDS服务</span></h1><p>在三个节点上同时部署MDS</p><pre><code>[root@node-1 my-cluster]# ceph-deploy mds create node-1 node-2 node-3</code></pre><p>查看当前MDS状态，因为当前没有文件系统，所以3台主机都是备状态</p><pre><code>[root@node-1 my-cluster]#  3 up:standby </code></pre><h1><span id="创建ceph-fs文件系统">创建Ceph FS文件系统</span></h1><p>创建pool</p><pre><code>[root@node-1 my-cluster]# ceph osd pool create cephfs_data 16 16[root@node-1 my-cluster]# ceph osd pool create cephfs_metadata 16 16</code></pre><p>查看使用帮助</p><pre><code>[root@node-1 my-cluster]# ceph -h|grep fs |grep new fs new &lt;fs_name&gt; &lt;metadata&gt; &lt;data&gt; &#123;--force&#125; &#123;--allow-dangerous-make new filesystem using named pools &lt;metadata&gt; and &lt;data&gt;</code></pre><p>创建一个fs文件系统</p><pre><code>[root@node-1 my-cluster]# ceph fs new cephfs-demo cephfs_metadata cephfs_data  new fs with metadata pool 9 and data pool 8</code></pre><h1><span id="查看fs文件系统">查看fs文件系统</span></h1><pre><code>[root@node-1 my-cluster]# ceph fs ls name: cephfs-demo, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</code></pre><p>查看集群状态</p><pre><code>[root@node-1 my-cluster]#  ceph -s  #可以看到MDS状态已经又一个active了  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            1 pools have too many placement groups            too many PGs per OSD (368 &gt; max 250)   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 4m)    mgr: node-2(active, since 10m), standbys: node-3, node-1    mds: cephfs-demo:1 &#123;0=node-2=up:active&#125; 2 up:standby    osd: 6 osds: 6 up (since 4m), 6 in (since 14h)    rgw: 1 daemon active (node-1)   task status:   data:    pools:   9 pools, 736 pgs    objects: 534 objects, 1.0 GiB    usage:   9.6 GiB used, 50 GiB / 60 GiB avail    pgs:     736 active+clean</code></pre><h1><span id="内核挂载">内核挂载</span></h1><p>创建一个挂载点</p><pre><code>[root@node-1 my-cluster]# mkdir /mnt/cephfs #创建一个挂载点</code></pre><p>使用命令进行挂载</p><pre><code>[root@node-1 my-cluster]# which mount.ceph #使用这个命令进行挂载/usr/sbin/mount.ceph [root@node-1 my-cluster]# rpm -qf /usr/sbin/mount.ceph #安装ceph-common的时候添加的命令ceph-common-14.2.22-0.el7.x86_64[root@node-1 my-cluster]# mount -t ceph 192.168.187.201:6789:/ /mnt/cephfs/ -o name=admin #挂载</code></pre><p>查看挂载</p><pre><code>[root@node-1 my-cluster]# df -HT # 查看挂载Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   11M  944M   2% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.6G   16G  14% //dev/sda1               xfs       1.1G  204M  860M  20% /boottmpfs                   tmpfs     954M   25k  954M   1% /var/lib/ceph/osd/ceph-0tmpfs                   tmpfs     954M   25k  954M   1% /var/lib/ceph/osd/ceph-3tmpfs                   tmpfs     191M     0  191M   0% /run/user/0/dev/rbd0               xfs        11G  1.2G  9.7G  11% /data192.168.187.201:6789:/  ceph       17G     0   17G   0% /mnt/cephfs [root@node-1 my-cluster]# lsmod | grep ceph #会自动添加ceph的模块，此方法性能比较高ceph                  363016  1 libceph               314775  2 rbd,cephdns_resolver           13140  1 libcephlibcrc32c              12644  2 xfs,libceph</code></pre><h1><span id="用户态挂载">用户态挂载</span></h1><p>安装软件包</p><pre><code>[root@node-1 my-cluster]#  yum install ceph-fuse -y</code></pre><p>查看帮助文档</p><pre><code>[root@node-1 my-cluster]# ceph-fuse -h #查看使用帮助usage: ceph-fuse [-n client.username] [-m mon-ip-addr:mon-port] &lt;mount point&gt; [OPTIONS]  --client_mountpoint/-r &lt;sub_directory&gt;                    use sub_directory as the mounted root, rather than the full Ceph tree.usage: ceph-fuse mountpoint [options]general options:    -o opt,[opt...]        mount options    -h   --help            print help    -V   --version         print versionFUSE options:    -d   -o debug          enable debug output (implies -f)    -f                     foreground operation    -s                     disable multi-threaded operation  --conf/-c FILE    read configuration from the given configuration file  --id ID           set ID portion of my name  --name/-n TYPE.ID set name  --cluster NAME    set cluster name (default: ceph)  --setuser USER    set uid to user or uid (and gid to user&#39;s gid)  --setgroup GROUP  set gid to group or gid  --version         show version and quit</code></pre><p>挂载</p><p>可以看到与内核态挂载的地点文件是共享的 </p><pre><code>[root@node-1 my-cluster]# ceph-fuse -n client.admin -m 192.168.187.201:6789,192.168.187.202:6789,192.168.187.203:6789 /mnt/ceph-fuse/ #使用ceph-fuse进行挂载，-n 指定用户名 -m 指定mon节点，可指定1个或者多个，或者不指定，最后写上挂载地点ceph-fuse[205722023-03-05 17:10:28.202 7f6b40549f80 -1 init, newargv = 0x5623b0d084e0 newargc=9]: starting ceph clientceph-fuse[20572]: starting fuse</code></pre><p>确定挂载情况</p><pre><code>[root@node-1 my-cluster]# cd /mnt/ceph-fuse/ [root@node-1 ceph-fuse]# ls[root@node-1 ceph-fuse]# lltotal 0[root@node-1 ceph-fuse]# [root@node-1 ceph-fuse]# ls[root@node-1 ceph-fuse]# echo aaa &gt; test[root@node-1 ceph-fuse]# lstest[root@node-1 ceph-fuse]# ls /mnt/cephfstest</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> CephFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph故障排查</title>
      <link href="/2023/04/10/Ceph/0.Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/"/>
      <url>/2023/04/10/Ceph/0.Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-application-not-enabled-on-1-pools">一、application not enabled on 1 pool(s)</a></li></ul><!-- tocstop --><h1><span id="一-application-not-enabled-on-1-pools">一、application not enabled on 1 pool(s)</span></h1><blockquote><p>故障现象：</p></blockquote><pre><code>[root@node-1 data]# ceph -s   cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            application not enabled on 1 pool(s) #提示有一个pool没有启用application ，是由于创建pool的时候没有使用rbd pool init &lt;pool-name&gt;进行初始化，使用此命令会默认指定成RBD格式的application   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 71m)    mgr: node-1(active, since 14h), standbys: node-2, node-3    osd: 3 osds: 3 up (since 39m), 3 in (since 65m)   data:    pools:   1 pools, 64 pgs    objects: 281 objects, 1.0 GiB    usage:   6.2 GiB used, 24 GiB / 30 GiB avail    pgs:     64 active+clean</code></pre><blockquote><p> 查看当前集群的健康状态</p></blockquote><pre><code>[root@node-1 data]# ceph health detailHEALTH_WARN application not enabled on 1 pool(s)POOL_APP_NOT_ENABLED application not enabled on 1 pool(s)    application not enabled on pool &#39;ceph-demo&#39;    use &#39;ceph osd pool application enable &lt;pool-name&gt; &lt;app-name&gt;&#39;, where &lt;app-name&gt; is &#39;cephfs&#39;, &#39;rbd&#39;, &#39;rgw&#39;, or freeform for custom applications</code></pre><pre><code>crash info &lt;id&gt;                                                       show crash dump metadatacrash json_report &lt;hours&gt;                                             Crashes in the last &lt;hours&gt; hourscrash ls                                                              Show new and archived crash dumpscrash ls-new                                                          Show new crash dumpscrash post                                                            Add a crash dump (use -i &lt;jsonfile&gt;)crash prune &lt;keep&gt;                                                    Remove crashes older than &lt;keep&gt; dayscrash rm &lt;id&gt;                                                         Remove a saved crash &lt;id&gt;crash stat                                                            Summarize recorded crashes</code></pre><blockquote><p>查看当前状态</p></blockquote><pre><code>[root@node-1 data]# ceph crash ls</code></pre><p><img src="/images/Ceph/Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/1.jpg"></p><blockquote><p>查看该ID的具体信息</p></blockquote><pre><code>[root@node-1 data]# ceph crash info &lt;id&gt;</code></pre><blockquote><p>使用ceph crash archive <id>进行标记，视为已读，或者重启所有OSD也能解决告警问题</id></p></blockquote><p><img src="/images/Ceph/Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/2.jpg"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> 故障排查 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph简介</title>
      <link href="/2023/04/10/Ceph/2.Ceph%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84/Ceph-%E7%AE%80%E4%BB%8B/"/>
      <url>/2023/04/10/Ceph/2.Ceph%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84/Ceph-%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#ceph-%E7%AE%80%E4%BB%8B">CEPH 简介</a><ul><li><a href="#%E5%BB%BA%E8%AE%AE">建议</a></li><li><a href="#%E7%A1%AC%E4%BB%B6%E6%8E%A8%E8%8D%90">硬件推荐</a><ul><li><a href="#cpu">CPU</a></li><li><a href="#%E5%86%85%E5%AD%98">内存</a></li><li><a href="#%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8">数据存储</a></li><li><a href="#%E7%BD%91%E7%BB%9C">网络</a></li><li><a href="#%E6%95%85%E9%9A%9C%E5%9F%9F">故障域</a></li></ul></li><li><a href="#%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%BB%BA%E8%AE%AE">操作系统建议</a><ul><li><a href="#ceph-%E4%BE%9D%E8%B5%96%E9%A1%B9">CEPH 依赖项</a></li><li><a href="#%E5%B9%B3%E5%8F%B0">平台</a></li></ul></li></ul></li></ul><!-- tocstop --><h1><span id="ceph-简介">CEPH 简介</span></h1><p>无论您是想为云平台提供Ceph 对象存储和&#x2F;或 Ceph 块设备服务、部署Ceph 文件系统还是将 Ceph 用于其他目的，所有 Ceph 存储集群部署都从设置每个 Ceph 节点、您的网络和 Ceph存储集群。一个 Ceph 存储集群至少需要一个 Ceph Monitor、Ceph Manager 和 Ceph OSD（Object Storage Daemon）。运行 Ceph 文件系统客户端时也需要 Ceph 元数据服务器。<br><img src="/images/Ceph/Ceph%E7%AE%80%E4%BB%8B/1.jpg"></p><ul><li>Monitors：Ceph Monitor ( ceph-mon) 维护集群状态图，包括监视器图、管理器图、OSD 图、MDS 图和 CRUSH 图。这些映射是 Ceph 守护进程相互协调所需的关键集群状态。监视器还负责管理守护进程和客户端之间的身份验证。通常至少需要三个监视器才能实现冗余和高可用性。</li><li>管理器：Ceph 管理器守护进程 ( ceph-mgr) 负责跟踪运行时指标和 Ceph 集群的当前状态，包括存储利用率、当前性能指标和系统负载。Ceph Manager 守护进程还托管基于 python 的模块来管理和公开 Ceph 集群信息，包括基于 Web 的Ceph Dashboard和 REST API。高可用性通常至少需要两个管理器。</li><li>Ceph OSDs：一个对象存储守护进程（Ceph OSD， ceph-osd）存储数据，处理数据复制，恢复，重新平衡，并通过检查其他 Ceph OSD 守护进程的心跳向 Ceph Monitors 和 Managers 提供一些监控信息。通常至少需要三个 Ceph OSD 才能实现冗余和高可用性。</li><li>MDS：Ceph 元数据服务器（MDS ceph-mds）代表Ceph 文件系统存储元数据（即 Ceph 块设备和 Ceph 对象存储不使用 MDS）。Ceph 元数据服务器允许 POSIX 文件系统用户执行基本命令（如 ls、find等），而不会给 Ceph 存储集群带来巨大负担。</li></ul><p>Ceph 将数据作为对象存储在逻辑存储池中。使用 CRUSH算法，Ceph 计算出哪个归置组 (PG) 应该包含该对象，以及哪个 OSD 应该存储该归置组。CRUSH 算法使 Ceph 存储集群能够动态扩展、重新平衡和恢复。</p><h2><span id="建议">建议</span></h2><p>要开始在生产中使用 Ceph，您应该查看我们的硬件建议和操作系统建议。</p><ul><li>硬件推荐<ul><li>中央处理器</li><li>内存</li><li>记忆</li><li>数据存储</li><li>网络</li><li>故障域</li><li>最低硬件建议</li></ul></li><li>操作系统建议<ul><li>Ceph 依赖项</li><li>平台</li></ul></li></ul><h2><span id="硬件推荐">硬件推荐</span></h2><p>Ceph 被设计为在商用硬件上运行，这使得构建和维护 PB 级数据集群在经济上是可行的。在规划集群硬件时，您需要平衡许多考虑因素，包括故障域和潜在的性能问题。硬件规划应该包括在许多主机上分布 Ceph 守护进程和其他使用 Ceph 的进程。通常，我们建议在为特定类型的守护进程配置的主机上运行特定类型的 Ceph 守护进程。我们建议将其他主机用于利用您的数据集群的进程（例如，OpenStack、CloudStack 等）。<br>也可以查看<a href="https://ceph.com/community/blog/">Ceph 博客。</a></p><h3><span id="cpu">CPU</span></h3><p>CephFS 元数据服务器 (MDS) 是 CPU 密集型的。因此，CephFS 元数据服务器 (MDS) 应具有四核（或更好）CPU 和高时钟频率 (GHz)。OSD 节点需要足够的处理能力来运行 RADOS 服务、使用 CRUSH 计算数据放置、复制数据以及维护它们自己的集群映射副本。</p><blockquote><p><strong>一个 Ceph 集群的要求与另一个集群的要求不同，但这里有一些通用准则。</strong></p></blockquote><p>在 Ceph 的早期版本中，我们会根据每个 OSD 的核心数来提出硬件建议，但这个每个 OSD 的核心数指标不再像每个 IOP 的周期数和每个 OSD 的 IOP 数一样有用。例如，对于 NVMe 驱动器，Ceph 可以轻松地在真实集群上使用五个或六个内核，并在单个 OSD 上隔离使用多达大约十四个内核。因此，每个 OSD 的核心不再像以前那样紧迫。选择硬件时，选择每个内核的 IOP。</p><p>监控节点和管理器节点对 CPU 的要求不高，只需要适度的处理器。如果您的主机除了运行 Ceph 守护进程外还将运行 CPU 密集型进程，请确保您有足够的处理能力来运行 CPU 密集型进程和 Ceph 守护进程。（OpenStack Nova 是 CPU 密集型进程的一个例子。）我们建议您在单独的主机上（即，在不是您的监视器和管理器节点的主机上）运行非 Ceph CPU 密集型进程，以避免占用资源争论。</p><h3><span id="内存">内存</span></h3><p>通常，RAM 越大越好。适度集群的监视器&#x2F;管理器节点可能使用 64GB 就可以了；对于拥有数百个 OSD 的更大集群，128GB 是一个合理的目标。BlueStore OSD 的内存目标默认为 4GB。考虑到操作系统和管理任务（如监控和指标）的谨慎余量以及恢复期间增加的消耗：建议为每个 BlueStore OSD 预配 ~8GB。</p><h4><span id="监视器和管理器ceph-mon-和-ceph-mgr">监视器和管理器（CEPH-MON 和 CEPH-MGR）</span></h4><p>监视器和管理器守护进程的内存使用量通常随集群的大小而变化。请注意，在启动时以及拓扑更改和恢复期间，这些守护程序将需要比稳态操作期间更多的 RAM，因此请计划使用高峰期。对于非常小的集群，32 GB 就足够了。对于多达 300 个 OSD 的集群，需要 64GB</p><h4><span id="元数据服务器-ceph-mds">元数据服务器 (CEPH-MDS)</span></h4><p>元数据守护程序内存利用率取决于其缓存配置为消耗的内存量。对于大多数系统，我们建议至少使用 1 GB。 看mds_cache_memory。</p><h4><span id="记忆">记忆</span></h4><p>Bluestore 使用自己的内存来缓存数据，而不是依赖操作系统的页面缓存。在 Bluestore 中，您可以通过更改配置选项来调整 OSD 尝试消耗的内存量osd_memory_target 。</p><ul><li>通常不建议设置osd_memory_target低于 2GB（Ceph 可能无法将内存消耗保持在 2GB 以下，这可能会导致性能极慢）。</li><li>将内存目标设置在 2GB 和 4GB 之间通常可行，但可能会导致性能下降，因为元数据可能会在 IO 期间从磁盘读取，除非活动数据集相对较小。</li><li>4GB 是当前的默认osd_memory_target大小。此默认值是为典型用例选择的，旨在平衡内存需求和 OSD 性能。</li><li>osd_memory_target当有很多（小）对象或处理大（256GB&#x2F;OSD 或更多）数据集时，设置高于 4GB 可以提高性能。</li></ul><blockquote><p>OSD 内存自动调整是“尽力而为”。虽然 OSD 可以取消映射内存以允许内核回收它，但不能保证内核会在特定时间范围内实际回收释放的内存。这尤其适用于旧版本的 Ceph，其中透明大页面可以防止内核回收从碎片大页面中释放的内存。现代版本的 Ceph 在应用程序级别禁用透明大页面以避免这种情况，尽管这仍然不能保证内核会立即回收未映射的内存。OSD 有时仍可能超出其内存目标。我们建议在您的系统上预算大约 20% 的额外内存，以防止 OSD 在临时峰值期间或由于内核回收已释放页面的任何延迟而出现 OOM。</p></blockquote><p>使用旧版 FileStore 后端时，页面缓存用于缓存数据，因此通常不需要调整。<br>使用旧版 FileStore 后端时，OSD 内存消耗与系统中每个守护进程的 PG 数量有关。</p><h3><span id="数据存储">数据存储</span></h3><p>仔细规划您的数据存储配置。在规划数据存储时，需要考虑显着的成本和性能权衡。同时进行的操作系统操作以及多个守护进程对单个驱动器的读写操作的同时请求会大大降低性能。</p><h4><span id="硬盘驱动器">硬盘驱动器</span></h4><p>OSD 应该有足够的硬盘驱动器空间来存储对象数据。我们建议最小硬盘驱动器大小为 1 TB。考虑更大磁盘的每 GB 成本优势。我们建议将硬盘驱动器的价格除以千兆字节数，得出每千兆字节的成本，因为更大的驱动器可能会对每千兆字节的成本产生重大影响。例如，价格为 75.00 美元的 1 TB 硬盘的成本为每 GB 0.07 美元（即 75 美元 &#x2F; 1024 &#x3D; 0.0732）。相比之下，价格为 150.00 美元的 3 TB 硬盘的成本为每 GB 0.05 美元（即 150 美元 &#x2F; 3072 &#x3D; 0.0488）。在前面的示例中，使用 1 TB 的磁盘通常会使每 GB 的成本增加 40%——使您的集群的成本效率大大降低。</p><blockquote><p>在单个 SAS &#x2F; SATA 驱动器上运行多个 OSD 不是一个好主意。但是，NVMe 驱动器可以通过拆分为两个或更多 OSD 来提高性能。<br>在单个驱动器上运行 OSD 和监视器或元数据服务器也不是一个好主意。</p></blockquote><p>存储驱动器受寻道时间、访问时间、读写时间以及总吞吐量的限制。这些物理限制会影响整体系统性能，尤其是在恢复期间。我们建议为操作系统和软件使用专用（最好是镜像）驱动器，并为主机上运行的每个 Ceph OSD 守护进程使用一个驱动器（上面的模数 NVMe）。许多不是由硬件故障引起的“慢 OSD”问题是由于在同一驱动器上运行操作系统和多个 OSD 而引起的。由于在小型集群上解决性能问题的成本可能超过额外磁盘驱动器的成本，因此您可以通过避免让 OSD 存储驱动器负担过重的诱惑来优化您的集群设计规划。</p><blockquote><p>您可以在每个 SAS &#x2F; SATA 驱动器上运行多个 Ceph OSD 守护进程，但这可能会导致资源争用并降低整体吞吐量。</p></blockquote><h4><span id="态硬盘">态硬盘</span></h4><p>性能改进的一个机会是使用固态驱动器 (SSD) 来减少随机访问时间和读取延迟，同时加快吞吐量。与硬盘驱动器相比，SSD 每 GB 的成本通常是硬盘驱动器的 10 倍以上，但 SSD 的访问时间通常至少比硬盘驱动器快 100 倍。<br>SSD 没有移动机械部件，因此它们不一定受到与硬盘驱动器相同类型的限制。SSD 确实有很大的局限性。在评估 SSD 时，重要的是要考虑顺序读写的性能。</p><blockquote><p>我们建议探索使用 SSD 来提高性能。但是，在对 SSD 进行重大投资之前，我们强烈建议查看 SSD 的性能指标并在测试配置中测试 SSD 以衡量性能。</p></blockquote><p>相对便宜的 SSD 可能会吸引您的经济意识。谨慎使用。选择用于 Ceph 的 SSD 时，可接受的 IOPS 是不够的。</p><p>SSD 在历史上一直是对象存储的成本高昂，但新兴的 QLC 驱动器正在缩小差距。通过将 WAL+DB 卸载到 SSD，HDD OSD 可能会看到显着的性能提升。</p><p>Ceph 加速 CephFS 文件系统性能的一种方法是将 CephFS 元数据的存储与 CephFS 文件内容的存储分开。Ceph 为 CephFS 元数据提供了一个默认metadata池。您永远不必为 CephFS 元数据创建一个池，但您可以为您的 CephFS 元数据池创建一个仅指向主机的 SSD 存储介质的 CRUSH 映射层次结构。有关详细信息，请参阅 <a href="https://docs.ceph.com/en/quincy/rados/operations/crush-map-edits/#crush-map-device-class">CRUSH 设备类</a>。</p><h4><span id="控制器">控制器</span></h4><p>磁盘控制器 (HBA) 会对写入吞吐量产生重大影响。仔细考虑您的选择以确保它们不会造成性能瓶颈。值得注意的是，RAID 模式 (IR) HBA 可能比更简单的“JBOD”(IT) 模式 HBA 表现出更高的延迟，并且 RAID SoC、写缓存和电池备份会显着增加硬件和维护成本。某些 RAID HBA 可以配置有 IT 模式“个性”。</p><p><a href="https://ceph.com/community/blog/">Ceph 博客</a>通常是有关 Ceph 性能问题的极佳信息来源。有关更多详细信息，请参阅<a href="https://ceph.com/community/ceph-performance-part-1-disk-controller-write-throughput/">Ceph 写入吞吐量 1</a>和<a href="https://ceph.com/community/ceph-performance-part-2-write-throughput-without-ssd-journals/">Ceph 写入吞吐量 2</a>。</p><h4><span id="对标">对标</span></h4><p>BlueStore 在 O_DIRECT 中打开块设备，并频繁使用 fsync 以确保数据安全地持久化到介质中。您可以使用 评估驱动器的低级写入性能fio。例如，4kB 随机写性能测量如下：</p><pre><code>#fio --name=/dev/sdX --ioengine=libaio --direct=1 --fsync=1 --readwrite=randwrite --blocksize=4k --runtime=300</code></pre><h4><span id="写缓存">写缓存</span></h4><p>企业 SSD 和 HDD 通常包括断电保护功能，使用多级缓存来加速直接或同步写入。这些设备可以在两种缓存模式之间切换——易失性缓存通过 fsync 刷新到持久介质，或同步写入的非易失性缓存。</p><p>通过“启用”或“禁用”写入（易失性）缓存来选择这两种模式。当启用易失性缓存时，Linux 使用“回写”模式的设备，禁用时，它使用“直写”。</p><p>默认配置（通常启用缓存）可能不是最佳配置，并且 OSD 性能可能会通过禁用写缓存在增加 IOPS 和减少 commit_latency 方面得到显着提高。</p><p>因此，鼓励用户fio如前所述对他们的设备进行基准测试，并为他们的设备保留最佳缓存配置。<br>hdparm可以使用、sdparm或 smartctl读取中的值来查询缓存配置&#x2F;sys&#x2F;class&#x2F;scsi_disk&#x2F;*&#x2F;cache_type，例如：</p><pre><code>#hdparm -W /dev/sda/dev/sda: write-caching =  1 (on)# sdparm --get WCE /dev/sda    /dev/sda: ATA       TOSHIBA MG07ACA1  0101WCE           1  [cha: y]# smartctl -g wcache /dev/sdasmartctl 7.1 2020-04-05 r5049 [x86_64-linux-4.18.0-305.19.1.el8_4.x86_64] (local build)Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.orgWrite cache is:   Enabled# cat /sys/class/scsi_disk/0\:0\:0\:0/cache_typewrite back</code></pre><p>可以使用相同的工具禁用写缓存：</p><pre><code>#hdparm -W0 /dev/sda/dev/sda: setting drive write-caching to 0 (off) write-caching =  0 (off)# sdparm --clear WCE /dev/sda    /dev/sda: ATA       TOSHIBA MG07ACA1  0101# smartctl -s wcache,off /dev/sdasmartctl 7.1 2020-04-05 r5049 [x86_64-linux-4.18.0-305.19.1.el8_4.x86_64] (local build)Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.org=== START OF ENABLE/DISABLE COMMANDS SECTION ===Write cache disabled</code></pre><p>hdparm通常，使用、sdparm或禁用缓存smartctl 会导致 cache_type 自动更改为“write through”。如果不是这样，你可以尝试直接如下设置。（用户应注意，设置 cache_type 也会正确保留设备的缓存模式，直到下一次重启）：</p><pre><code>#echo &quot;write through&quot; &gt; /sys/class/scsi_disk/0\:0\:0\:0/cache_type# hdparm -W /dev/sda/dev/sda: write-caching =  0 (off)</code></pre><p>这个 udev 规则（在 CentOS 8 上测试过）会将所有 SATA&#x2F;SAS 设备 cache_types 设置为“write through”：</p><pre><code>#cat /etc/udev/rules.d/99-ceph-write-through.rulesACTION==&quot;add&quot;, SUBSYSTEM==&quot;scsi_disk&quot;, ATTR&#123;cache_type&#125;:=&quot;write through&quot;</code></pre><p>这个 udev 规则（在 CentOS 7 上测试过）会将所有 SATA&#x2F;SAS 设备 cache_types 设置为“write through”：</p><pre><code>#cat /etc/udev/rules.d/99-ceph-write-through-el7.rulesACTION==&quot;add&quot;, SUBSYSTEM==&quot;scsi_disk&quot;, RUN+=&quot;/bin/sh -c &#39;echo write through &gt; /sys/class/scsi_disk/$kernel/cache_type&#39;&quot;</code></pre><p>该sdparm实用程序可用于一次查看&#x2F;更改多个设备上的易失性写入缓存：</p><pre><code>#sdparm --get WCE /dev/sd*    /dev/sda: ATA       TOSHIBA MG07ACA1  0101WCE           0  [cha: y]    /dev/sdb: ATA       TOSHIBA MG07ACA1  0101WCE           0  [cha: y]# sdparm --clear WCE /dev/sd*    /dev/sda: ATA       TOSHIBA MG07ACA1  0101    /dev/sdb: ATA       TOSHIBA MG07ACA1  0101</code></pre><h4><span id="其他注意事项">其他注意事项</span></h4><p>您通常会在每个主机上运行多个 OSD，但您应该确保 OSD 驱动器的总吞吐量不超过满足客户端读取或写入数据需求所需的网络带宽。您还应该考虑集群在每个主机上存储的总数据的百分比。如果特定主机上的百分比很大并且该主机发生故障，则可能导致诸如超过 之类的问题，这会导致 Ceph 停止操作以作为防止数据丢失的安全预防措施。full ratio</p><p>当您在每个主机上运行多个 OSD 时，您还需要确保内核是最新的。请参阅<a href="https://docs.ceph.com/en/quincy/start/os-recommendations">OS Recommendations</a>了解有关注意事项glibc并 syncfs(2)确保您的硬件在每个主机运行多个 OSD 时按预期运行。</p><h3><span id="网络">网络</span></h3><p>在您的机架中提供至少 10 Gb&#x2F;s 的网络。</p><h4><span id="速度">速度</span></h4><p>在 1 Gb&#x2F;s 网络上复制 1 TB 数据需要三个小时，在 1 Gb&#x2F;s 网络上复制 10 TB 数据需要三十个小时。但是在 10 Gb&#x2F;s 网络上复制 1 TB 数据只需要 20 分钟，而在 10 Gb&#x2F;s 网络上复制 10 TB 数据只需要一个小时。</p><h4><span id="成本">成本</span></h4><p>Ceph 集群越大，OSD 故障就越常见。degraded归置组 (PG) 从一种状态恢复到另一种状态的速度越快越好。值得注意的是，快速恢复将可能导致数据暂时不可用甚至丢失的多重重叠故障的可能性降至最低。当然，在配置您的网络时，您必须在价格与性能之间取得平衡。active + clean</p><p>一些部署工具使用 VLAN 来使硬件和网络布线更易于管理。使用 802.1q 协议的 VLAN 需要支持 VLAN 的 NIC 和交换机。该硬件的额外费用可能会被网络设置和维护方面节省的运营成本所抵消。当使用 VLAN 处理集群和计算堆栈（例如 OpenStack、CloudStack 等）之间的 VM 流量时，使用 10 Gb&#x2F;s 以太网或更好的以太网具有额外的价值；截至 2022 年，40 Gb&#x2F;s 或25&#x2F;50&#x2F;100 Gb&#x2F;s 网络在生产集群中很常见。</p><blockquote><p>架顶式 (TOR) 交换机还需要快速和冗余的上行链路来旋转主干交换机&#x2F;路由器，通常至少为 40 Gb&#x2F;s。</p></blockquote><h4><span id="底板管理控制器-bmc">底板管理控制器 (BMC)</span></h4><p>您的服务器机箱应该有底板管理控制器 (BMC)。众所周知的例子是 iDRAC (Dell)、CIMC (Cisco UCS) 和 iLO (HPE)。管理和部署工具也可能广泛使用 BMC，尤其是通过 IPMI 或 Redfish，因此请考虑带外网络在安全性和管理方面的成本&#x2F;收益权衡。Hypervisor SSH 访问、VM 映像上传、OS 映像安装、管理套接字等会给网络带来巨大的负载。运行三个网络似乎有点矫枉过正，但每个流量路径都代表潜在的容量、吞吐量和&#x2F;或性能瓶颈，您在部署大规模数据集群之前应该仔细考虑。</p><h3><span id="故障域">故障域</span></h3><p>故障域是阻止访问一个或多个 OSD 的任何故障。那可能是主机上停止的守护进程；硬盘故障、操作系统崩溃、网卡故障、电源故障、网络中断、停电等等。在规划您的硬件需求时，您必须权衡通过将过多的责任置于过少的故障域中来降低成本的诱惑与隔离每个潜在故障域的额外成本之间的平衡。</p><h4><span id="最低硬件建议">最低硬件建议</span></h4><p>Ceph 可以在廉价的商用硬件上运行。小型生产集群和开发集群可以使用适度的硬件成功运行。</p><table>    <tr>        <td>过程</td>        <td>标准</td>        <td>最低推荐</td>    </tr>    <tr>        <td rowspan="5">ceph-osd</td>    </tr>        <tr>            <td>处理器</td>            <td>最少 1 个核心 <br>  每 200-500 MB/s 1 个核心 <br> 每 1000-3000 IOPS 1 个核心  <br>  结果在复制之前。 <br>  结果可能因不同的 CPU 型号和 Ceph 功能而异。（纠删码、压缩等）。<br> ARM 处理器可能特别需要额外的内核。 <br> 实际性能取决于许多因素，包括驱动器、网络和客户端吞吐量和延迟。强烈建议进行基准测试。 </td>        </tr>        <tr>            <td>内存</td>            <td>每个守护进程 4GB+（越多越好）<br> 2-4GB 经常运行（可能很慢） <br> 小于 2GB 不推荐 </td>        </tr>        <tr>            <td>卷存储</td>            <td>每个守护进程 1 个存储驱动器 </td>        </tr>        <tr>            <td>数据库/文件</td>            <td>每个守护进程 1 个 SSD 分区（可选） </td>        </tr>        <tr>            <td>网络</td>            <td>1x 1GbE+ NIC（推荐 10GbE+） </td>        </tr>    <tr>        <td rowspan="4">ceph-mon</td>    </tr>        <tr>            <td>处理器</td>            <td>最少 2 个核心 </td>        </tr>        <tr>            <td>内存</td>            <td>每个守护进程 2-4GB+ </td>        </tr>        <tr>        <td>磁盘空间</td>        <td>每个守护进程 60 GB </td>        </tr>        <tr>            <td>网络</td>            <td>1x 1GbE+ NIC</td>        </tr>    <tr>        <td rowspan="4">ceph-mds</td>    </tr>        <tr>            <td>处理器</td>            <td>最少 2 个核心 </td>        </tr>        <tr>            <td>内存</td>            <td>每个守护进程 2GB+ </td>        </tr>        <tr>            <td>磁盘空间</td>            <td>每个守护进程 1MB</td>        </tr>        <tr>            <td>网络</td>            <td>1x 1GbE+ NIC</td>        </tr></table><blockquote><p>如果您使用单个磁盘运行 OSD，请为您的卷存储创建一个分区，该分区与包含操作系统的分区分开。通常，我们建议操作系统和卷存储使用单独的磁盘。</p></blockquote><h2><span id="操作系统建议">操作系统建议</span></h2><h3><span id="ceph-依赖项">CEPH 依赖项</span></h3><p>作为一般规则，我们建议在较新版本的 Linux 上部署 Ceph。我们还建议在具有长期支持的版本上进行部署。</p><h4><span id="内核">内核</span></h4><blockquote><p>Ceph 内核客户端</p></blockquote><p>如果您使用内核客户端映射 RBD 块设备或挂载 CephFS，一般建议是使用 <a href="http://kernel.org/">http://kernel.org</a> 或您在任何客户端上的 Linux 发行版提供的“稳定”或“长期维护”内核系列主机。</p><p>对于 RBD，如果选择跟踪长期内核，我们目前推荐基于 4.x 的“长期维护”内核系列或更高版本：</p><p>对于 RBD，如果选择跟踪长期内核，我们目前推荐基于 4.x 的“长期维护”内核系列或更高版本：</p><ul><li>4.19.z</li><li>4.14.z</li><li>5.x</li></ul><p>对于 CephFS，请参阅有关使用内核驱动程序安装 CephFS 的部分 以获取内核版本指南。</p><p>较旧的内核客户端版本可能不支持您的CRUSH 可调配置文件或 Ceph 集群的其他较新功能，需要将存储集群配置为禁用这些功能。</p><h3><span id="平台">平台</span></h3><p>下面的图表显示了 Ceph 的需求如何映射到各种 Linux 平台。一般而言，对内核和系统初始化包（即 sysvinit、systemd）之外的特定发行版的依赖性非常小。</p><table>    <tr>        <td>Release Name</td>        <td>Tag</td>        <td>CentOS</td>        <td>Ubuntu</td>        <td>OpenSUSE C</td>        <td>Debian C</td>    </tr>        <tr>            <td>Quincy</td>            <td>17.2.z</td>            <td>8 A</td>            <td>20.04 A</td>            <td>15.3</td>            <td>11</td>        </tr>        <tr>            <td>Pacific</td>            <td>16.2.z</td>            <td>8 A</td>            <td>18.04 C,<br>20.04 A</td>            <td>15.2</td>            <td>10<br>11</td>        </tr>        <tr>            <td>Octopus</td>            <td>15.2.z</td>            <td>7 B <br> 8 A</td>            <td>18.04 C,<br>20.04 A</td>            <td>15.2</td>            <td>10</td>        </tr></table><ul><li>A : Ceph 提供软件包，并对其中的软件进行了全面的测试。</li><li>B : Ceph 提供了软件包，并对其中的软件做了基本的测试。</li><li>C : Ceph 只提供包。尚未对这些版本进行任何测试。</li></ul><blockquote><p>Centos 7 用户：Btrfs在 Octopus 版本中不再在 Centos 7 上进行测试。我们建议bluestore改用。</p></blockquote><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客中插入 Chart 动态图表</title>
      <link href="/2023/04/08/chatjs/"/>
      <url>/2023/04/08/chatjs/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>该文基本(全部)来自于chatjs中文文档</p><h1><span id="背景">背景</span></h1><!-- toc --><ul><li><a href="#hexo-%E4%B8%AD%E7%9A%84-chartjs">Hexo 中的 Chartjs</a></li><li><a href="#%E7%A4%BA%E4%BE%8B">示例</a><ul><li><a href="#%E6%8A%98%E7%BA%BF%E5%9B%BE">折线图</a></li><li><a href="#%E7%9B%B8%E5%85%B3%E5%B1%9E%E6%80%A7">相关属性</a></li></ul></li></ul><!-- tocstop --><span id="more"></span><p>Chartjs是一款简单优雅的数据可视化工具，对比其他图表库如echarts、highcharts、c3、flot、amchart等，它的画面效果、动态效果都更精致，它的 文档首页 就透出一股小清新，基于 HTML5 Canvas，它拥有更好的性能且响应式，基本满足了一般数据展示的需要，包括折线图，条形图，饼图，散点图，雷达图，极地图，甜甜圈图等</p><h1><span id="hexo-中的-chartjs">Hexo 中的 Chartjs</span></h1><p>为了方便在 Hexo 中使用这么漂亮的图表库，我自己写了一个 Hexo 的 Chartjs 插件。插件的安装和使用非常的简单，只需要进入博客目录，然后打开命令行，用npm安装一下：</p><pre><code>npm install hexo-tag-chart --save</code></pre><p>之后在文章内使用 chart 的 tag 就可以了</p><pre><code>&#123;% chart 90% 300 %&#125;\\TODO option goes here&#123;% endchart %&#125;</code></pre><p>其中 chart 是标签名，endchart 是结束标签，不需要更改，90% 是图表容器的相对宽度，默认是100%，300 是图表容器的高度，默认是按正常比例缩放的，你可以通过设置 options 里面的 aspectRatio 属性来调整宽高比例，另外还有许多属性可以自定义，你可以查看 官方文档。在标签之间的部分，都是需要自己填充的图表数据和属性。</p><p>我们来看一个样例，你可以把鼠标移上去看看动态效果。</p><div style="width: 90%;margin: 0 auto">    <canvas id="chart3267" style="height: 300px"></canvas></div><script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script><script type="text/javascript">    var ctx = document.getElementById('chart3267').getContext('2d');    var options =     {    type: 'line',    data: {    labels: ['January', 'February', 'March', 'April', 'May', 'June', 'July'],    datasets: [{        label: 'My First dataset',        backgroundColor: 'rgb(255, 99, 132)',        borderColor: 'rgb(255, 99, 132)',        data: [0, 10, 5, 2, 20, 30, 45]        }]    },    options: {        responsive: true,        title: {        display: true,        text: 'Chart.js Line Chart'        }    }};    new Chart(ctx, options);</script><p>上面这个样例可以通过以下代码来实现：</p><pre><code>&#123;% chart 90% 300 %&#125;    &#123;    type: 'line',    data: &#123;    labels: ['January', 'February', 'March', 'April', 'May', 'June', 'July'],    datasets: [&#123;        label: 'My First dataset',        backgroundColor: 'rgb(255, 99, 132)',        borderColor: 'rgb(255, 99, 132)',        data: [0, 10, 5, 2, 20, 30, 45]        &#125;]    &#125;,    options: &#123;        responsive: true,        title: &#123;        display: true,        text: 'Chart.js Line Chart'        &#125;    &#125;&#125;&#123;% endchart %&#125;</code></pre><h1><span id="示例">示例</span></h1><p>现在你已经基本学会了在Hexo中插入图表了，我再展示一些更炫酷的图表吧，你可以自己去尝试一下。</p><h2><span id="折线图">折线图</span></h2><p>效果</p><div style="width: 90%;margin: 0 auto">    <canvas id="chart6215" style="height: 300px"></canvas></div><script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script><script type="text/javascript">    var ctx = document.getElementById('chart6215').getContext('2d');    var options =   //其中 chart 是标签名，endchart 是结束标签，不需要更改，可以通过设置 options 里面的  aspectRatio 属性来调整图表的宽高比例，默认 width 为 100%，height 为 300px。 {    type: 'line',    data: { //  折线图数据    labels: ['8.4', '8.5', '8.6', '8.7', '8.8', '8.9', '8.10'], //  横坐标    datasets: [{ //  纵坐标变量名称        label: 'My First dataset',        lineTension: 0,        borderColor: 'rgb(255, 99, 132)', //  线条颜色        data: [0, 10, 5, 2, 20, 30, 45] //  纵坐标        }]    },    options: {        responsive: true,        title: {        display: true,        text: 'Chart.js 折线图' //  图表名称        }    }};    new Chart(ctx, options);</script><pre><code>&#123;% chart 90% 300 %&#125;  //其中 chart 是标签名，endchart 是结束标签，不需要更改，可以通过设置 options 里面的               aspectRatio 属性来调整图表的宽高比例，默认 width 为 100%，height 为 300px。 &#123;    type: 'line',    data: &#123; //  折线图数据    labels: ['8.4', '8.5', '8.6', '8.7', '8.8', '8.9', '8.10'], //  横坐标    datasets: [&#123; //  纵坐标变量名称        label: 'My First dataset',        lineTension: 0,        borderColor: 'rgb(255, 99, 132)', //  线条颜色        data: [0, 10, 5, 2, 20, 30, 45] //  纵坐标        &#125;]    &#125;,    options: &#123;        responsive: true,        title: &#123;        display: true,        text: 'Chart.js 折线图' //  图表名称        &#125;    &#125;&#125;&#123;% endchart %&#125;</code></pre><h2><span id="相关属性">相关属性</span></h2><table><thead><tr><th>名称</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>backgroundColor</td><td>Color&#x2F;Color[]</td><td>线条背景色</td></tr><tr><td></td><td></td><td></td></tr><tr><td>borderColor</td><td>Color&#x2F;Color[]</td><td>线条颜色</td></tr></tbody></table><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatjs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用存储介绍</title>
      <link href="/2023/04/08/Ceph/1.%E5%AD%98%E5%82%A8%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E5%AD%98%E5%82%A8%E4%BB%8B%E7%BB%8D/"/>
      <url>/2023/04/08/Ceph/1.%E5%AD%98%E5%82%A8%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E5%AD%98%E5%82%A8%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><h1><span id="常用存储介绍">常用存储介绍</span></h1><p><img src="/images/Ceph/%E5%B8%B8%E7%94%A8%E5%AD%98%E5%82%A8%E4%BB%8B%E7%BB%8D/1.jpg"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
