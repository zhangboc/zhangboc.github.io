<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2023/04/12/Ceph/7.OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/"/>
      <url>/2023/04/12/Ceph/7.OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>KVM虚拟化进阶与提高视频课程</title>
      <link href="/2023/04/12/KVM/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/"/>
      <url>/2023/04/12/KVM/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE">一、虚拟网络高级配置</a><ul><li><a href="#1-%E9%85%8D%E7%BD%AE%E7%BD%91%E5%8D%A1%E7%BB%91%E5%AE%9Abond">1、配置网卡绑定（bond）</a><ul><li><a href="#1-%E7%BB%91%E5%AE%9A%E7%BD%91%E5%8D%A1">1、绑定网卡</a></li><li><a href="#2-%E9%85%8D%E7%BD%AE%E7%BD%91%E6%A1%A5">2、配置网桥</a></li></ul></li><li><a href="#2-%E9%85%8D%E7%BD%AEvlan">2、配置Vlan</a><ul><li><a href="#1-nmcli%E6%96%B9%E6%B3%95">1、nmcli方法</a></li><li><a href="#2-%E9%80%9A%E8%BF%87%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%88%9B%E5%BB%BA">2、通过命令行创建</a></li><li><a href="#3-%E9%80%9A%E8%BF%87virt-manager%E5%88%9B%E5%BB%BA">3、通过virt-manager创建</a></li><li><a href="#4-%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%BD%BF%E7%94%A8vlan">4、虚拟机使用vlan</a></li></ul></li><li><a href="#3-%E9%85%8D%E7%BD%AE%E7%BD%91%E7%BB%9C%E8%BF%87%E6%BB%A4">3、配置网络过滤</a></li></ul></li><li><a href="#%E4%BA%8C-%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB">二、虚拟机迁移</a><ul><li><a href="#1-%E9%80%9A%E8%BF%87uri%E8%BF%9E%E6%8E%A5%E8%BF%9E%E6%8E%A5kvm%E4%B8%BB%E6%9C%BA">1、通过URI连接连接KVM主机</a></li><li><a href="#2-%E9%9D%99%E6%80%81%E8%BF%81%E7%A7%BB">2、静态迁移</a><ul><li><a href="#1-%E5%90%8C%E4%B8%80%E5%AE%BF%E4%B8%BB%E6%9C%BA%E5%86%85%E8%BF%81%E7%A7%BB">1、同一宿主机内迁移</a></li><li><a href="#2-%E4%B8%8D%E5%90%8C%E5%AE%BF%E4%B8%BB%E6%9C%BA%E8%BF%81%E7%A7%BB">2、不同宿主机迁移</a></li><li><a href="#3-%E4%B8%8D%E5%90%8C%E5%AE%BF%E4%B8%BB%E6%9C%BA%E8%BF%81%E7%A7%BB%E4%BD%BF%E7%94%A8virsh-migrate%E5%91%BD%E4%BB%A4">3、不同宿主机迁移使用virsh migrate命令</a></li></ul></li><li><a href="#3-%E5%9F%BA%E4%BA%8E%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8%E7%9A%84%E5%8A%A8%E6%80%81%E8%BF%81%E7%A7%BB">3、基于共享存储的动态迁移</a><ul><li><a href="#1-%E5%AE%89%E8%A3%85nfs%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BB%84%E4%BB%B6">1、安装NFS服务器组件</a></li><li><a href="#%E5%88%9B%E5%BB%BAnfs%E5%85%B1%E4%BA%AB%E6%96%87%E4%BB%B6%E5%A4%B9">创建NFS共享文件夹</a></li><li><a href="#%E5%B0%86kvm1%E4%B8%AD%E7%9A%84%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A1%AC%E7%9B%98%E8%BF%81%E7%A7%BB%E5%88%B0nfs%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8%E4%B8%8A">将KVM1中的虚拟机硬盘迁移到NFS服务器的共享存储上</a></li><li><a href="#2-%E4%BD%BF%E7%94%A8virt-manager%E8%BF%9B%E8%A1%8C%E8%BF%81%E7%A7%BB">2、使用virt-manager进行迁移</a></li><li><a href="#3-%E4%BD%BF%E7%94%A8virsh%E5%91%BD%E4%BB%A4%E8%BF%9B%E8%A1%8C%E8%BF%81%E7%A7%BB">3、使用virsh命令进行迁移</a></li></ul></li><li><a href="#4-%E5%9F%BA%E4%BA%8E%E6%9C%AC%E5%9C%B0%E5%AD%98%E5%82%A8%E7%9A%84%E5%8A%A8%E6%80%81%E8%BF%81%E7%A7%BB">4、基于本地存储的动态迁移</a></li></ul></li><li><a href="#%E4%B8%89-kvm%E9%9B%86%E7%BE%A4">三、KVM集群</a></li><li><a href="#%E5%9B%9B-%E5%B5%8C%E5%A5%97%E8%99%9A%E6%8B%9F%E5%8C%96">四、嵌套虚拟化</a></li><li><a href="#%E4%BA%94-%E6%80%A7%E8%83%BD%E7%9B%91%E8%A7%86%E4%B8%8E%E4%BC%98%E5%8C%96">五、性能监视与优化</a></li><li><a href="#%E5%85%AD-p2v%E5%92%8Cv2v%E8%BF%81%E7%A7%BB">六、P2V和V2V迁移</a></li><li><a href="#%E4%B8%83-%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D">七、备份与恢复</a></li><li><a href="#%E5%85%AB-ovirtrhev%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E7%AE%A1%E7%90%86">八、oVirt(RHEV)安装与基本管理</a></li></ul><!-- tocstop --><h1><span id="一-虚拟网络高级配置">一、虚拟网络高级配置</span></h1><h2><span id="1-配置网卡绑定bond">1、配置网卡绑定（bond）</span></h2><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/1.jpg"><br><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/2.jpg"></p><h3><span id="1-绑定网卡">1、绑定网卡</span></h3><pre><code>[root@kvm ~]# ip a  #查看需要绑定的网卡，ens33和ens361: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet 192.168.187.134/24 brd 192.168.187.255 scope global noprefixroute dynamic ens33       valid_lft 1066sec preferred_lft 1066sec    inet6 fe80::1fc1:66d:29af:eabe/64 scope link noprefixroute        valid_lft forever preferred_lft forever3: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0       valid_lft forever preferred_lft forever4: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff5: ens36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:e6 brd ff:ff:ff:ff:ff:ff    inet 192.168.187.135/24 brd 192.168.187.255 scope global noprefixroute dynamic ens36       valid_lft 1783sec preferred_lft 1783sec    inet6 fe80::84be:5ac8:542c:705f/64 scope link noprefixroute        valid_lft forever preferred_lft forever[root@kvm ~]# virsh net-list #查看当前kvm网络情况 名称               状态     自动开始  持久---------------------------------------------------------- default              活动     是           是[root@kvm ~]# brctl show #查看当前宿主机网桥情况bridge name    bridge id        STP enabled    interfacesvirbr0        8000.525400ccc5d5    yes        virbr0-nic[root@kvm ~]# virsh net-dumpxml --network default  #查看default的xml文件&lt;network&gt;  &lt;name&gt;default&lt;/name&gt;  &lt;uuid&gt;8e682a63-b77a-4aad-bc25-5d4d63a2a66e&lt;/uuid&gt;  &lt;forward mode=&#39;nat&#39;&gt;    &lt;nat&gt;      &lt;port start=&#39;1024&#39; end=&#39;65535&#39;/&gt;    &lt;/nat&gt;  &lt;/forward&gt;  &lt;bridge name=&#39;virbr0&#39; stp=&#39;on&#39; delay=&#39;0&#39;/&gt;  &lt;mac address=&#39;52:54:00:cc:c5:d5&#39;/&gt;  &lt;ip address=&#39;192.168.122.1&#39; netmask=&#39;255.255.255.0&#39;&gt;    &lt;dhcp&gt;      &lt;range start=&#39;192.168.122.2&#39; end=&#39;192.168.122.254&#39;/&gt;    &lt;/dhcp&gt;  &lt;/ip&gt;&lt;/network&gt;[root@kvm ~]# virsh iface-list #查看宿主机网卡情况，没有认到新增加的网卡（ens36），需要增加ifcfg-ens36的配置文件，可以通过virt-manager进行添加 名称               状态     MAC 地址--------------------------------------------------- ens33                活动     00:0c:29:0b:57:dc lo                   活动     00:00:00:00:00:00</code></pre><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/3.jpg"><br><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/4.jpg"><br><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/5.jpg"></p><pre><code>[root@kvm ~]# cat /etc/sysconfig/network-scripts/ifcfg-ens36  #查看通过virt-nanager自动生成的配置文件DEVICE=&quot;ens36&quot;HWADDR=&quot;00:0c:29:0b:57:e6&quot;ONBOOT=&quot;no&quot;BOOTPROTO=&quot;dhcp&quot;[root@kvm ~]# virsh iface-list --all #可以识别到ens36网卡了 名称               状态     MAC 地址--------------------------------------------------- ens33                活动     00:0c:29:0b:57:dc ens36                活动     00:0c:29:0b:57:e6 lo                   活动     00:00:00:00:00:00 [root@kvm ~]# modprobe --first-time bonding #查看系统是否安装了网桥的模块，如果没有就自动加载，这里没有报错，意思是当前系统没有加载，但是已经通过--first-time进行加载了[root@kvm ~]# lsmod | grep bonding #查看已经正常加载bonding               157075  0 [root@kvm network-scripts]# pwd/etc/sysconfig/network-scripts[root@kvm network-scripts]# ls -lha ifcfg-ens* #查看当前网卡的配置文件-rw-r--r--. 1 root root 280 4月   2 15:41 ifcfg-ens33-rw-r--r--. 1 root root  71 4月   2 19:37 ifcfg-ens36[root@kvm network-scripts]# cp ifcfg-ens33 ifcfg-ens33-bak #备份文件[root@kvm network-scripts]# cp ifcfg-ens36 ifcfg-ens36-bak [root@kvm network-scripts]# vim ifcfg-ens33 #编辑网卡文件TYPE=EthernetBOOTPROTO=noneNAME=ens33DEVICE=ens33ONBOOT=yesMASTER=bond0SLAVE=yesNM_CONTROLLERD=no #非必须USERCTL=no #非必须[root@kvm network-scripts]# vim ifcfg-ens36 #编辑网卡文件TYPE=EthernetBOOTPROTO=noneNAME=ens36DEVICE=ens36ONBOOT=yesMASTER=bond0SLAVE=yesNM_CONTROLLERD=noUSERCTL=no [root@kvm network-scripts]# vim ifcfg-bond0 #新建bond0文件DEVICE=bond0ONBOOT=yesNM_CONTROLLERD=noUSERCTL=noBONDING_OPTS=&quot;mode=1 miimon=100 fail_over_mac=1&quot;BOOTPROTO=staticIPADDR=192.168.187.134NETMASK=255.255.255.0GATEWAY=192.168.187.1DNS1=223.5.5.5[root@kvm network-scripts]# systemctl restart network #重启网络服务，远程时谨慎！连接已成功激活（master waiting for slaves）（D-Bus 活动路径：/org/freedesktop/NetworkManager/ActiveConnection/5）[root@kvm network-scripts]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff3: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0       valid_lft forever preferred_lft forever4: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff5: ens36: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff6: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet 192.168.187.134/24 brd 192.168.187.255 scope global noprefixroute bond0       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fe0b:57dc/64 scope link        valid_lft forever preferred_lft forever[root@kvm network-scripts]# dmesg |grep bond0 #查看日志[ 2586.969003] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2586.969152] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2586.971585] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2586.979903] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2587.011775] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2635.311460] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2635.457459] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2635.590868] bond0: making interface ens33 the new active one[ 2635.596124] bond0: Enslaving ens33 as an active interface with an up link[ 2635.596466] IPv6: ADDRCONF(NETDEV_CHANGE): bond0: link becomes ready[ 2635.700504] bond0: Releasing backup interface ens33[ 2637.395032] bond0: Enslaving ens33 as a backup interface with a down link[ 2637.483876] bond0: link status definitely up for interface ens33, 1000 Mbps full duplex[ 2637.483882] bond0: making interface ens33 the new active one[ 2637.491125] bond0: first active interface up![ 2637.764573] bond0: Enslaving ens36 as a backup interface with a down link[ 2637.793398] bond0: link status definitely up for interface ens36, 1000 Mbps full duplex[root@kvm network-scripts]# cat /proc/net/bonding/bond0  #查看内存中的文件Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)Bonding Mode: fault-tolerance (active-backup) #bond模式Primary Slave: NoneCurrently Active Slave: ens33 #当前的主设备是ens33MII Status: up #主设备的当前状态MII Polling Interval (ms): 100Up Delay (ms): 0Down Delay (ms): 0Slave Interface: ens33MII Status: upSpeed: 1000 MbpsDuplex: fullLink Failure Count: 0Permanent HW addr: 00:0c:29:0b:57:dcSlave queue ID: 0Slave Interface: ens36MII Status: upSpeed: 1000 MbpsDuplex: fullLink Failure Count: 0Permanent HW addr: 00:0c:29:0b:57:e6Slave queue ID: 0</code></pre><blockquote><p>与真机环境有所不同，vmware虚拟机下给linux系统做bond0网卡配置，照这样做完后，测试发现down掉eth0后，bond0网卡ping不通，无法起到网卡备份效果。BONDING_OPTS&#x3D;“fail_over_mac&#x3D;1”配置解释: 默认fail_over_mac&#x3D;0，当发生错误时，只改slave的mac不改bond；fail_over_mac&#x3D;1时，只改bond不改slave。</p></blockquote><h4><span id="1-bond的模式">1、Bond的模式</span></h4><blockquote><p><a href="https://www.likecs.com/show-308299629.html">网卡的7种bond模式</a><br><a href="https://blog.csdn.net/weixin_45548465/article/details/122625777">Linux网卡bond的七种模式详解，⽹卡绑定(bond)怎么实现？有哪些绑定⽅式</a>：</p></blockquote><table><thead><tr><th>模式</th><th>名称</th><th>特点</th><th>负载均衡</th><th>交换机配置</th><th>条件</th></tr></thead><tbody><tr><td>0</td><td>(balance-rr) Round-robin policy（平衡抡循环策略）</td><td>输数据包顺序是依次传输（即：第1个包走eth0，下一个包就走eth1….一直循环下去，直到最后一个传 输完毕），此模式提供负载平衡和容错能力。</td><td>是</td><td>静态聚合</td><td></td></tr><tr><td>1</td><td>(active-backup) Active-backup policy（主-备份策略）</td><td>只有一个设备处于活动状态，当一个宕掉另一个马上由备份转换为主设备。mac地址是外部可见得，从外面看来，bond的MAC地址是唯一的，以避免switch(交换机)发生混乱。此模式只提供了容错能力；由此可见此算法的优点是可以提供高网络连接的可用性，但是它的资源利用率较低，只有一个接口处于工作状态，在有 N 个网络接口的情况下，资源利用率为1&#x2F;N。</td><td>否</td><td>否</td><td></td></tr><tr><td>2</td><td>(balance-xor) XOR policy（平衡策略）</td><td>基于指定的传输HASH策略传输数据包。缺省的策略是：(源MAC地址 XOR 目标MAC地址) % slave数量。</td><td>是</td><td>静态聚合</td><td></td></tr><tr><td>3</td><td>broadcast（广播策略</td><td>每个slave接口上传输每个数据包，此模式提供了容错能力。</td><td>否</td><td>静态聚合</td><td></td></tr><tr><td>4</td><td>(802.3ad) IEEE 802.3adDynamic link aggregation（IEEE 802.3ad 动态链接聚合）</td><td>创建一个聚合组，它们共享同样的速率和双工设定。根据802.3ad规范将多个slave工作在同一个激活的聚合体下。</td><td>是</td><td>支持IEEE 802.3ad动态聚合</td><td>条件1：ethtool支持获取每个slave的速率和双工设定。<br>条件2：switch(交换机)支持IEEE 802.3ad Dynamic link aggregation。<br>条件3：大多数switch(交换机)需要经过特定配置才能支持802.3ad模式。<br></td></tr><tr><td>5</td><td>(balance-tlb) Adaptive transmit load balancing（适配器传输负载均衡）</td><td>不需要任何特别的switch(交换机)支持的通道bonding。在每个slave上根据当前的负载（根据速度计算）分配外出流量。如果正在接受数据的slave出故障了，另一个slave接管失败的slave的MAC地址。</td><td>是</td><td>否</td><td></td></tr><tr><td>6</td><td>(balance-alb) Adaptive load balancing（适配器适应性负载均衡）</td><td>该模式包含了balance-tlb模式，同时加上针对IPV4流量的接收负载均衡(receive load balance,rlb)，而且不需要任何switch(交换机)的支持。</td><td>是</td><td>否</td><td></td></tr></tbody></table><h3><span id="2-配置网桥">2、配置网桥</span></h3><pre><code>[root@kvm ~]# vim /etc/sysconfig/network-scripts/ifcfg-virbr1 #新建网桥virbr1的配置文件DEVICE=virbr1ONBOOT=yesTYPE=BridgeNM_CONTROLLER=noUSERCTL=noBOOTPROTO=staticIPADDR=192.168.187.134NETMASK=255.255.255.0GATEWAY=192.168.187.1[root@kvm ~]# vim /etc/sysconfig/network-scripts/ifcfg-bond0  #修改bond0的配置文件DEVICE=bond0ONBOOT=yesNM_CONTROLLERD=noUSERCTL=noBONDING_OPTS=&quot;mode=1 miimon=100 fail_over_mac=1&quot;BOOTPROTO=noneBRIDGE=virbr1[root@kvm ~]# systemctl restart network #重启网络服务[root@kvm ~]# brctl show  #查看当前系统网桥bridge name    bridge id        STP enabled    interfacesvirbr0        8000.525400ccc5d5    yes        virbr0-nicvirbr1        8000.000c290b57dc    no        bond0[root@kvm ~]# ip addr show virbr1 #查看IP已经到网桥1中了18: virbr1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet 192.168.187.134/24 brd 192.168.187.255 scope global noprefixroute virbr1       valid_lft forever preferred_lft forever    inet6 fe80::5c15:77ff:fece:bc31/64 scope link        valid_lft forever preferred_lft forever[root@kvm qemu]# cat /etc/libvirt/qemu/generic.xml #查看连接的虚拟机接口配置    &lt;interface type=&#39;bridge&#39;&gt;      &lt;mac address=&#39;52:54:00:8e:67:0a&#39;/&gt;      &lt;source bridge=&#39;virbr1&#39;/&gt;      &lt;model type=&#39;rtl8139&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x03&#39; function=&#39;0x0&#39;/&gt;    &lt;/interface&gt;[root@kvm qemu]# brctl show #查看多了个net0bridge name    bridge id        STP enabled    interfacesvirbr0        8000.525400ccc5d5    yes        virbr0-nicvirbr1        8000.000c290b57dc    no        bond0                            vnet0[root@kvm qemu]# virsh domiflist --domain generic  #查看接口信息，模式是8139，可以修改成virto优化性能Interface  Type       Source     Model       MAC-------------------------------------------------------vnet0      bridge     virbr1     rtl8139     52:54:00:8e:67:0a</code></pre><h2><span id="2-配置vlan">2、配置Vlan</span></h2><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/6.jpg"></p><p><strong>不建议使用nmcli创建</strong></p><table><thead><tr><th>网络</th><th>地址段</th><th>Vlan-id</th></tr></thead><tbody><tr><td>管理网络</td><td>192.168.200.0&#x2F;24</td><td>&#x2F;</td></tr><tr><td>生产网络</td><td>172.16.11.0&#x2F;24 172.16.11.11</td><td>11</td></tr><tr><td>生产网络</td><td>172.16.12.0&#x2F;24 172.16.12.12</td><td>12</td></tr></tbody></table><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/7.jpg"></p><h3><span id="1-nmcli方法">1、nmcli方法</span></h3><pre><code>[root@kvm ~]# nmcli --help #查看帮助文档Usage: nmcli [OPTIONS] OBJECT &#123; COMMAND | help &#125;OPTIONS  -a, --ask                                ask for missing parameters  -c, --colors auto|yes|no                 whether to use colors in output  -e, --escape yes|no                      escape columns separators in values  -f, --fields &lt;field,...&gt;|all|common      specify fields to output  -g, --get-values &lt;field,...&gt;|all|common  shortcut for -m tabular -t -f  -h, --help                               print this help  -m, --mode tabular|multiline             output mode  -o, --overview                           overview mode  -p, --pretty                             pretty output  -s, --show-secrets                       allow displaying passwords  -t, --terse                              terse output  -v, --version                            show program version  -w, --wait &lt;seconds&gt;                     set timeout waiting for finishing operationsOBJECT  g[eneral]       NetworkManager&#39;s general status and operations  n[etworking]    overall networking control  r[adio]         NetworkManager radio switches  c[onnection]    NetworkManager&#39;s connections  d[evice]        devices managed by NetworkManager  a[gent]         NetworkManager secret agent or polkit agent  m[onitor]       monitor NetworkManager changes[root@kvm network-scripts]# nmcli connection show  #查看当前网络信息NAME          UUID                                  TYPE      DEVICE System ens33  c96bc909-188e-ec64-3a96-6a90982b08ad  ethernet  ens33  System ens36  418da202-9a8c-b73c-e8a1-397e00f3c6b2  ethernet  ens36  virbr0        41366568-f66f-4c8b-a2f6-cb4df1714e06  bridge    virbr0 有线连接 1    52522e57-4b72-3d75-a602-ce47588f745d  ethernet  --     [root@kvm network-scripts]# nmcli connection add type vlan ifname VLAN11 dev ens33 id 11 #基于ens33创建vlan11Connection &#39;vlan-VLAN11&#39; (193348c2-73b4-4962-981c-4fdd7a966926) successfully added.[root@kvm network-scripts]# nmcli connection add type vlan ifname VLAN12 dev ens33 id 12 #基于ens33创建vlan12    Connection &#39;vlan-VLAN12&#39; (60496196-b1ee-4ff6-93fc-e167f25e63c7) successfully added.[root@kvm network-scripts]# pwd /etc/sysconfig/network-scripts[root@kvm network-scripts]# ls -lha | grep vlan  #查看自动创建的文件-rw-r--r--. 1 root root  357 Apr  8 05:08 ifcfg-vlan-VLAN11-rw-r--r--. 1 root root  357 Apr  8 05:08 ifcfg-vlan-VLAN12[root@kvm network-scripts]# cat ifcfg-vlan-VLAN11 #查看具体信息VLAN=yesTYPE=VlanPHYSDEV=ens33VLAN_ID=11REORDER_HDR=yesGVRP=noMVRP=noHWADDR=PROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=dhcpDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=vlan-VLAN11UUID=193348c2-73b4-4962-981c-4fdd7a966926DEVICE=VLAN11ONBOOT=yes[root@kvm network-scripts]# nmcli connection show  #查看创建的链接NAME          UUID                                  TYPE      DEVICE System ens33  c96bc909-188e-ec64-3a96-6a90982b08ad  ethernet  ens33  System ens36  418da202-9a8c-b73c-e8a1-397e00f3c6b2  ethernet  ens36  virbr0        41366568-f66f-4c8b-a2f6-cb4df1714e06  bridge    virbr0 vlan-VLAN11   193348c2-73b4-4962-981c-4fdd7a966926  vlan      --     vlan-VLAN12   60496196-b1ee-4ff6-93fc-e167f25e63c7  vlan      --     有线连接 1    52522e57-4b72-3d75-a602-ce47588f745d  ethernet  --  [root@kvm network-scripts]# ip a #查看创建结果1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet 192.168.200.128/24 brd 192.168.200.255 scope global noprefixroute ens33       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fe0b:57dc/64 scope link        valid_lft forever preferred_lft forever3: ens36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:e6 brd ff:ff:ff:ff:ff:ff    inet 192.168.200.135/24 brd 192.168.200.255 scope global noprefixroute ens36       valid_lft forever preferred_lft forever6: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0       valid_lft forever preferred_lft forever7: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff13: vnet0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 1000    link/ether fe:54:00:8e:67:0a brd ff:ff:ff:ff:ff:ff    inet6 fe80::fc54:ff:fe8e:670a/64 scope link        valid_lft forever preferred_lft forever25: VLAN12@ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet6 fe80::eb51:86e8:ea1f:f322/64 scope link noprefixroute        valid_lft forever preferred_lft forever26: VLAN11@ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet6 fe80::a47d:f2c7:75fc:7e54/64 scope link tentative noprefixroute        valid_lft forever preferred_lft forever[root@kvm network-scripts]# tail -n 20 /var/log/messages #查看日志Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5135] dhcp4 (VLAN12): canceled DHCP transaction, DHCP client pid 15817Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5136] dhcp4 (VLAN12): state changed timeout -&gt; doneApr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5141] device (VLAN12): state change: ip-config -&gt; failed (reason &#39;ip-config-unavailable&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;warn&gt;  [1680902267.5150] device (VLAN12): Activation: failed for connection &#39;vlan-VLAN12&#39;Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5152] device (VLAN12): state change: failed -&gt; disconnected (reason &#39;none&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5602] device (VLAN12): state change: disconnected -&gt; unmanaged (reason &#39;user-requested&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5609] policy: auto-activating connection &#39;vlan-VLAN12&#39; (60496196-b1ee-4ff6-93fc-e167f25e63c7)Apr  8 05:17:47 kvm kernel: IPv6: ADDRCONF(NETDEV_UP): VLAN12: link is not readyApr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5637] device (VLAN12): state change: unmanaged -&gt; unavailable (reason &#39;managed&#39;, sys-iface-state: &#39;external&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5862] device (VLAN12): carrier: link connectedApr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5881] device (VLAN12): state change: unavailable -&gt; disconnected (reason &#39;user-requested&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6013] device (VLAN12): Activation: starting connection &#39;vlan-VLAN12&#39; (60496196-b1ee-4ff6-93fc-e167f25e63c7)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6023] device (VLAN12): state change: disconnected -&gt; prepare (reason &#39;none&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6033] device (VLAN12): state change: prepare -&gt; config (reason &#39;none&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6366] device (VLAN12): state change: config -&gt; ip-config (reason &#39;none&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6373] dhcp4 (VLAN12): activation: beginning transaction (timeout in 45 seconds)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6465] dhcp4 (VLAN12): dhclient started with pid 15847Apr  8 05:17:47 kvm dhclient[15847]: DHCPDISCOVER on VLAN12 to 255.255.255.255 port 67 interval 7 (xid=0x1a408bc7)Apr  8 05:17:52 kvm dhclient[15834]: DHCPDISCOVER on VLAN11 to 255.255.255.255 port 67 interval 6 (xid=0x6c741821)Apr  8 05:17:54 kvm dhclient[15847]: DHCPDISCOVER on VLAN12 to 255.255.255.255 port 67 interval 20 (xid=0x1a408bc7)[root@kvm network-scripts]# virsh iface-list #在kvm上看 Name                 State      MAC Address--------------------------------------------------- ens33                active     00:0c:29:0b:57:dc ens36                active     00:0c:29:0b:57:e6 lo                   active     00:00:00:00:00:00 VLAN11               active     00:0c:29:0b:57:dc VLAN12               active     00:0c:29:0b:57:dc[root@kvm network-scripts]# rm -rf ifcfg-vlan-VLAN1* #恢复设置[root@kvm network-scripts]# systemctl restart network #重启[root@kvm network-scripts]# nmtui #图形化界面，不好用</code></pre><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/8.jpg"><br><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/9.jpg"></p><h3><span id="2-通过命令行创建">2、通过命令行创建</span></h3><pre><code>[root@kvm network-scripts]# vim ifcfg-ens36.11 #创建vlan11配置文件DEVICE=&quot;ens36.11&quot;VLAN=&quot;yes&quot;ONBOOT=&quot;yes&quot;BOOTPROTO=&quot;none&quot;IPADDR=&quot;172.16.11.11&quot;NETMASK=&quot;255.255.255.0&quot;[root@kvm network-scripts]# cp ifcfg-ens36.11 ifcfg-ens36.12 #创建vlan11配置文件并修改[root@kvm network-scripts]# vim ifcfg-ens36.12 #配置DEVICE=&quot;ens36.12&quot;VLAN=&quot;yes&quot;ONBOOT=&quot;yes&quot;BOOTPROTO=&quot;none&quot;IPADDR=&quot;172.16.12.12&quot;NETMASK=&quot;255.255.255.0&quot;[root@kvm network-scripts]# systemctl restart network #重启网络配置[root@kvm network-scripts]# nmcli connection show  #查看配置是否生效NAME           UUID                                  TYPE      DEVICE   System ens33   c96bc909-188e-ec64-3a96-6a90982b08ad  ethernet  ens33    Vlan ens36.11  5aaffcd1-84a5-3046-15e7-adb62160402b  vlan      ens36.11 Vlan ens36.12  d100f2b5-85f7-4961-2105-8874b119178a  vlan      ens36.12 System ens36   418da202-9a8c-b73c-e8a1-397e00f3c6b2  ethernet  ens36    virbr0         41366568-f66f-4c8b-a2f6-cb4df1714e06  bridge    virbr0   有线连接 1     52522e57-4b72-3d75-a602-ce47588f745d  ethernet  --      [root@kvm network-scripts]# ip addr #查看IP1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet 192.168.200.128/24 brd 192.168.200.255 scope global noprefixroute ens33       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fe0b:57dc/64 scope link        valid_lft forever preferred_lft forever3: ens36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:e6 brd ff:ff:ff:ff:ff:ff    inet 192.168.200.135/24 brd 192.168.200.255 scope global noprefixroute ens36       valid_lft forever preferred_lft forever6: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0       valid_lft forever preferred_lft forever7: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff13: vnet0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 1000    link/ether fe:54:00:8e:67:0a brd ff:ff:ff:ff:ff:ff    inet6 fe80::fc54:ff:fe8e:670a/64 scope link        valid_lft forever preferred_lft forever32: ens36.12@ens36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether 00:0c:29:0b:57:e6 brd ff:ff:ff:ff:ff:ff    inet 172.16.12.12/24 brd 172.16.12.255 scope global noprefixroute ens36.12       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fe0b:57e6/64 scope link        valid_lft forever preferred_lft forever33: ens36.11@ens36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether 00:0c:29:0b:57:e6 brd ff:ff:ff:ff:ff:ff    inet 172.16.11.11/24 brd 172.16.11.255 scope global noprefixroute ens36.11       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fe0b:57e6/64 scope link        valid_lft forever preferred_lft forever[root@kvm network-scripts]# virsh iface-list --all  #查看 Name                 State      MAC Address--------------------------------------------------- ens33                active     00:0c:29:0b:57:dc ens36                active     00:0c:29:0b:57:e6 ens36.11             active     00:0c:29:0b:57:e6 ens36.12             active     00:0c:29:0b:57:e6 lo                   active     00:00:00:00:00:00</code></pre><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/10.jpg"></p><h3><span id="3-通过virt-manager创建">3、通过virt-manager创建</span></h3><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/11.jpg"><br><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/12.jpg"><br><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/13.jpg"></p><pre><code>[root@kvm network-scripts]# cat ifcfg-ens36.11 DEVICE=&quot;ens36.11&quot;VLAN=&quot;yes&quot;ONBOOT=&quot;yes&quot;BOOTPROTO=&quot;none&quot;IPADDR=&quot;192.168.200.135&quot;NETMASK=&quot;255.255.255.0&quot;GATEWAY=&quot;192.168.200.2&quot;</code></pre><h3><span id="4-虚拟机使用vlan">4、虚拟机使用vlan</span></h3><p><strong>一个vlan使用一个网桥，调用即可</strong><br><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/14.jpg"></p><h2><span id="3-配置网络过滤">3、配置网络过滤</span></h2><p><strong>通过libvirtd使用ebtailes创建的规则</strong><br><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/15.jpg"></p><pre><code>[root@kvm network-scripts]# virsh --help  Network Filter (help keyword &#39;filter&#39;)    nwfilter-define                define or update a network filter from an XML file    nwfilter-dumpxml               network filter information in XML    nwfilter-edit                  edit XML configuration for a network filter    nwfilter-list                  list network filters    nwfilter-undefine              undefine a network filter        nwfilter-binding-create        create a network filter binding from an XML file    nwfilter-binding-delete        delete a network filter binding    nwfilter-binding-dumpxml       network filter information in XML    nwfilter-binding-list          list network filter bindings[root@kvm network-scripts]#  virsh nwfilter-list  #查看网络过滤列表 UUID                                  Name                 ------------------------------------------------------------------ 3ce9cf03-2589-4b6a-8c1b-6e8b24e38f8e  allow-arp            a803defa-b342-47b4-af29-613250ff5281  allow-dhcp           5deae120-0ca4-4451-b8d4-92b6cd165962  allow-dhcp-server    49bdc080-9c7a-4278-9d83-59371bc9b981  allow-incoming-ipv4  64495a87-4efc-44f3-9aa2-1ef8e0a08600  allow-ipv4           cbd5d978-e475-4859-863d-1395d03da480  clean-traffic        #干净的流量 14ed106c-6baa-4ffd-8f28-d5038fea01ef  clean-traffic-gateway f88bd332-81af-480f-af36-57a7cd35104e  no-arp-ip-spoofing   a7611c9f-2a3a-4f62-9039-4e5be77881b3  no-arp-mac-spoofing  92dfc42c-0ed9-4482-a8f4-eee46f168cb8  no-arp-spoofing      20951a13-4570-4a87-9aa5-587d128c899e  no-ip-multicast      be8c0f6f-c811-451d-ae18-7cef13c56380  no-ip-spoofing       d85ecc2e-5573-43a7-85c1-e88b4daa6fdd  no-mac-broadcast     108c0c1a-8471-48b5-998a-6d92226f084e  no-mac-spoofing      7101e610-2e76-495f-976a-e4a37dd3f6ef  no-other-l2-traffic  60911d33-fb36-46bf-8eec-eeacc2f0fe62  no-other-rarp-traffic 40be9024-b924-4df0-b4c4-839e1026873f  qemu-announce-self   8bea3031-76ab-4077-9712-04d5b850e5fa  qemu-announce-self-rarp[root@kvm network-scripts]# virsh nwfilter-dumpxml clean-traffic #查看详细信息&lt;filter name=&#39;clean-traffic&#39; chain=&#39;root&#39;&gt; #默认规则及规则名称  &lt;uuid&gt;cbd5d978-e475-4859-863d-1395d03da480&lt;/uuid&gt; #规则UUID  &lt;filterref filter=&#39;no-mac-spoofing&#39;/&gt; #禁止MAC地址七篇  &lt;filterref filter=&#39;no-ip-spoofing&#39;/&gt; #禁止IP地址欺骗  &lt;rule action=&#39;accept&#39; direction=&#39;out&#39; priority=&#39;-650&#39;&gt; #accept代表动作，direction代表方向，in，out，inout，prioiy优先级，优先级值越低，优先级越高    &lt;mac protocolid=&#39;ipv4&#39;/&gt; #ipv4的mac地址  &lt;/rule&gt;  &lt;filterref filter=&#39;allow-incoming-ipv4&#39;/&gt; #调用其他规则  &lt;filterref filter=&#39;no-arp-spoofing&#39;/&gt;  &lt;rule action=&#39;accept&#39; direction=&#39;inout&#39; priority=&#39;-500&#39;&gt;    &lt;mac protocolid=&#39;arp&#39;/&gt;  &lt;/rule&gt;  &lt;filterref filter=&#39;no-other-l2-traffic&#39;/&gt; #调用其他规则  &lt;filterref filter=&#39;qemu-announce-self&#39;/&gt;&lt;/filter&gt;[root@kvm network-scripts]# ebtables -t nat -L #查看ebtables的NAT表规则Bridge table: natBridge chain: PREROUTING, entries: 1, policy: ACCEPT-j PREROUTING_directBridge chain: OUTPUT, entries: 1, policy: ACCEPT-j OUTPUT_directBridge chain: POSTROUTING, entries: 1, policy: ACCEPT-j POSTROUTING_directBridge chain: PREROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: POSTROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: OUTPUT_direct, entries: 1, policy: ACCEPT-j RETURN [root@kvm network-scripts]# virsh list --all  #查看虚拟机，准备于过滤规则绑定 Id    Name                           State---------------------------------------------------- 5     generic                        running -     centos7.0                      shut off[root@kvm network-scripts]# virsh dumpxml --domain generic    &lt;interface type=&#39;bridge&#39;&gt;      &lt;mac address=&#39;52:54:00:8e:67:0a&#39;/&gt;      &lt;source bridge=&#39;virbr1&#39;/&gt;      &lt;target dev=&#39;vnet0&#39;/&gt;      &lt;model type=&#39;rtl8139&#39;/&gt;      &lt;alias name=&#39;net0&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x03&#39; function=&#39;0x0&#39;/&gt;    &lt;/interface&gt;[root@kvm ~]# vim nwtest.xml    &lt;interface type=&#39;bridge&#39;&gt;      &lt;mac address=&#39;52:54:00:8e:67:0a&#39;/&gt;      &lt;source bridge=&#39;virbr1&#39;/&gt;      &lt;target dev=&#39;vnet0&#39;/&gt;      &lt;model type=&#39;rtl8139&#39;/&gt;      &lt;alias name=&#39;net0&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x03&#39; function=&#39;0x0&#39;/&gt;      &lt;filterref filter=&#39;clean-traffic&#39;/&gt; #增加过滤规则，调用clean-traffic    &lt;/interface&gt;[root@kvm ~]# virsh update-device --domain generic --file /root/nwtest.xml --persistent --live #更新generic主机，--file选择创建的文件，--persistent 代表永久生效，--live代表当前主机正在运行Device updated successfully[root@kvm ~]#  ebtables -t nat -L  #查看规则生效情况Bridge table: natBridge chain: PREROUTING, entries: 2, policy: ACCEPT-j PREROUTING_direct-i vnet0 -j libvirt-I-vnet0Bridge chain: OUTPUT, entries: 1, policy: ACCEPT-j OUTPUT_directBridge chain: POSTROUTING, entries: 1, policy: ACCEPT-j POSTROUTING_directBridge chain: PREROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: POSTROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: OUTPUT_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: libvirt-I-vnet0, entries: 4, policy: ACCEPT-s ! 52:54:0:8e:67:a -j DROP -p IPv4 -j ACCEPT -p ARP -j ACCEPT -j DROP [root@kvm ~]# virsh shutdown --domain generic  #关闭虚拟机Domain generic is being shutdown[root@kvm ~]# virsh list --all  #查看主机是否正常关闭 Id    Name                           State---------------------------------------------------- -     centos7.0                      shut off -     generic                        shut off[root@kvm ~]#  ebtables -t nat -L  #查看规则清空Bridge table: natBridge chain: PREROUTING, entries: 1, policy: ACCEPT-j PREROUTING_directBridge chain: OUTPUT, entries: 1, policy: ACCEPT-j OUTPUT_directBridge chain: POSTROUTING, entries: 1, policy: ACCEPT-j POSTROUTING_directBridge chain: PREROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: POSTROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: OUTPUT_direct, entries: 1, policy: ACCEPT-j RETURN [root@kvm ~]# virsh start --domain generic  #重新启动主机Domain generic started[root@kvm ~]#  ebtables -t nat -L  #再次查看规则Bridge table: natBridge chain: PREROUTING, entries: 2, policy: ACCEPT-j PREROUTING_direct-i vnet0 -j libvirt-I-vnet0Bridge chain: OUTPUT, entries: 1, policy: ACCEPT-j OUTPUT_directBridge chain: POSTROUTING, entries: 1, policy: ACCEPT-j POSTROUTING_directBridge chain: PREROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: POSTROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: OUTPUT_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: libvirt-I-vnet0, entries: 4, policy: ACCEPT-s ! 52:54:0:8e:67:a -j DROP -p IPv4 -j ACCEPT -p ARP -j ACCEPT -j DROP </code></pre><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/16.jpg"></p><h1><span id="二-虚拟机迁移">二、虚拟机迁移</span></h1><p><strong>静态迁移中可以通过将虚拟机暂停，迁移完成后重置虚拟机状态，完成迁移，但是也会影响业务</strong></p><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/17.jpg"><br><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/18.jpg"><br><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/19.jpg"></p><h2><span id="1-通过uri连接连接kvm主机">1、通过URI连接连接KVM主机</span></h2><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/20.jpg"></p><pre><code>[root@kvm ~]# virsh --helpvirsh [options]... [&lt;command_string&gt;]virsh [options]... &lt;command&gt; [args...]  options:    -c | --connect=URI      hypervisor connection URI[root@kvm ~]# virsh -c qemu+ssh://root@122.200.93.9/system #远程连接其他KVM主机root@122.200.93.9&#39;s password: Welcome to virsh, the virtualization interactive terminal.Type:  &#39;help&#39; for help with commands       &#39;quit&#39; to quitvirsh # list --all Id    Name                           State---------------------------------------------------- -     centos7.0                      shut off -     centos7.0-2                    shut off -     centos7.0-3                    shut off -     generic          virsh # hostname kvm.novalocal</code></pre><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/21.jpg"></p><h2><span id="2-静态迁移">2、静态迁移</span></h2><h3><span id="1-同一宿主机内迁移">1、同一宿主机内迁移</span></h3><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/22.jpg"></p><pre><code>[root@kvm ~]# virsh list --all  #查看虚拟机状态 Id    Name                           State---------------------------------------------------- 7     generic                        running -     centos7.0                      shut off[root@kvm ~]# virsh domblklist --domain generic  #查看硬盘路径Target     Source------------------------------------------------hda        /vm/centos7.6.rawhdb        -[root@kvm ~]# virsh shutdown --domain generic #关闭虚拟机Domain generic is being shutdown[root@kvm ~]# virsh list --all #再次查看状态 Id    Name                           State---------------------------------------------------- -     centos7.0                      shut off -     generic                        shut off[root@kvm ~]# mkdir /vm1 #创建迁移后的文件夹[root@kvm ~]# mv /vm/centos7.6.raw /vm1/ #迁移硬盘[root@kvm ~]# ls -lha /vm1 #查看迁移结果total 21Gdrwxr-xr-x.  2 root root  27 Apr  8 06:41 .dr-xr-xr-x. 22 root root 279 Apr  8 06:41 ..-rw-------.  1 root root 20G Apr  8 06:40 centos7.6.raw[root@kvm ~]# virsh start --domain generic  #尝试启动虚拟机，报错，需要修改配置文件error: Failed to start domain genericerror: Cannot access storage file &#39;/vm/centos7.6.raw&#39;: 没有那个文件或目录[root@kvm ~]# virsh edit --domain generic    &lt;disk type=&#39;file&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;source file=&#39;/vm1/centos7.6.raw&#39;/&gt; #修改源文件路径      &lt;target dev=&#39;hda&#39; bus=&#39;ide&#39;/&gt;      &lt;address type=&#39;drive&#39; controller=&#39;0&#39; bus=&#39;0&#39; target=&#39;0&#39; unit=&#39;0&#39;/&gt;    &lt;/disk&gt;[root@kvm ~]# virsh start --domain generic #启动成功，迁移完毕Domain generic started</code></pre><h3><span id="2-不同宿主机迁移">2、不同宿主机迁移</span></h3><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/23.jpg"><br><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/24.jpg"></p><table><thead><tr><th>序号</th><th>主机名称</th><th>主机IP</th></tr></thead><tbody><tr><td>1</td><td>kvm</td><td>192.168.200.128</td></tr><tr><td>2</td><td>kvm2</td><td>192.168.200.129</td></tr></tbody></table><h4><span id="kvm">kvm：</span></h4><p><strong>静态迁移要确保虚拟机处于关闭状态</strong></p><blockquote><p>迁移可以使用scp或者rsync -avSHP进行数据传输</p></blockquote><pre><code>[root@kvm ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.128 kvm192.168.200.129 kvm2[root@kvm ~]# virsh list --all  Id    Name                           State---------------------------------------------------- -     centos7.0                      shut off -     generic                        shut off #迁移此台主机 [root@kvm ~]# scp /etc/libvirt/qemu/generic.xml root@192.168.200.129:/etc/libvirt/qemu/ #将配置文件传送过去，确保主机当前状态是关机The authenticity of host &#39;192.168.200.129 (192.168.200.129)&#39; can&#39;t be established.ECDSA key fingerprint is SHA256:s2qiffBOYjuMJQOHEI9r87NTNjd0XyoEf81myM78kbQ.ECDSA key fingerprint is MD5:a5:8f:de:d4:07:9f:b7:b3:6f:7c:b0:c3:55:eb:bb:b0.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &#39;192.168.200.129&#39; (ECDSA) to the list of known hosts.root@192.168.200.129&#39;s password: generic.xml                                                                                                                                                                                                                                                                               100% 4329     5.0MB/s   00:00    [root@kvm ~]# virsh domblklist --domain generic #查看硬盘文件Target     Source------------------------------------------------hda        /vm1/centos7.6.rawhdb        -[root@kvm ~]# scp /vm1/centos7.6.raw root@kvm2:/vm1/ #传输硬盘文件到目标主机上，路径最好保持一致，否则需要修改XML文件</code></pre><h4><span id="kvm2">kvm2：</span></h4><pre><code>[root@kvm2 ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.128 kvm192.168.200.129 kvm2[root@kvm2 ~]# virsh list --all  Id    Name                           State----------------------------------------------------[root@kvm2 ~]# mkdir /vm1 #需要提前创建出目录，否则上述传输的时候会报错[root@kvm2 vm1]# ls -lha /etc/libvirt/qemu/generic.xml  #查看传输过来的xml配置文件-rw-------. 1 root root 4.3K Apr 10 16:02 /etc/libvirt/qemu/generic.xml[root@kvm2 vm1]# ls /vm1/centos7.6.raw #查看硬盘文件 /vm1/centos7.6.raw[root@kvm2 vm1]# virsh define --file /etc/libvirt/qemu/generic.xml  #定义硬盘文件Domain generic defined from /etc/libvirt/qemu/generic.xml[root@kvm2 vm1]# virsh list --all #查看识别到此主机，准备启动 Id    Name                           State---------------------------------------------------- -     generic                        shut off[root@kvm2 vm1]# virsh start generic #启动成功Domain generic started</code></pre><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/25.jpg"></p><p><strong>尝试本地远程成功，迁移完毕</strong></p><pre><code>[root@kvm2 vm1]# ssh root@192.168.122.149root@192.168.122.149&#39;s password: Last login: Mon Apr 10 04:35:12 2023[root@localhost ~]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 52:54:00:8e:67:0a brd ff:ff:ff:ff:ff:ff    inet 192.168.122.149/24 brd 192.168.122.255 scope global noprefixroute dynamic ens3       valid_lft 3503sec preferred_lft 3503sec    inet6 fe80::aaa:8f11:d0df:a3a7/64 scope link noprefixroute        valid_lft forever preferred_lft forever</code></pre><blockquote><p><strong>确保正常启动且无问题后删除源宿主机的相关配置，防止IP、Mac等冲突，引起不必要的问题</strong></p></blockquote><h3><span id="3-不同宿主机迁移使用virsh-migrate命令">3、不同宿主机迁移使用virsh migrate命令</span></h3><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/26.jpg"></p><p><strong>如果做了上述动作，需要先还原环境，本方法只作为了解，实际和scp效果一样</strong></p><p>1、在KVM2主机中查看主机列表</p><pre><code>[root@kvm2 vm1]# virsh list --all  Id    Name                           State---------------------------------------------------- 1     generic                        running</code></pre><p>2、在KVM2主机中关闭启动的generic主机</p><pre><code>[root@kvm2 vm1]# virsh shutdown --domain generic Domain generic is being shutdown</code></pre><p>3、在KVM2主机中取消generic的xml文件定义</p><pre><code>[root@kvm2 vm1]# virsh undefine --domain genericDomain generic has been undefined</code></pre><p>4、在KVM2主机中删除generic的硬盘文件</p><pre><code>[root@kvm2 vm1]# cd /vm1[root@kvm2 vm1]# ls -lha total 20Gdrwxr-xr-x.  2 root root  27 Apr 10 16:03 .dr-xr-xr-x. 22 root root 279 Apr  8 06:41 ..-rw-------.  1 root root 20G Apr 10 16:46 centos7.6.raw[root@kvm2 vm1]# rm -rf centos7.6.raw </code></pre><p>5、在KVM1主机中将generic主机关机<br><code>[root@kvm ~]# virsh shutdown --domain generic</code></p><p>6、在KVM1主机中查看migrate帮助</p><pre><code>[root@kvm ~]# virsh migrate --help   NAME    migrate - 将域迁移到另一个主机中  SYNOPSIS    migrate &lt;domain&gt; &lt;desturi&gt; [--live] [--offline] [--p2p] [--direct] [--tunnelled] [--persistent] [--undefinesource] [--suspend] [--copy-storage-all] [--copy-storage-inc] [--change-protection] [--unsafe] [--verbose] [--compressed] [--auto-converge] [--rdma-pin-all] [--abort-on-error] [--postcopy] [--postcopy-after-precopy] [--migrateuri &lt;string&gt;] [--graphicsuri &lt;string&gt;] [--listen-address &lt;string&gt;] [--dname &lt;string&gt;] [--timeout &lt;number&gt;] [--timeout-suspend] [--timeout-postcopy] [--xml &lt;string&gt;] [--migrate-disks &lt;string&gt;] [--disks-port &lt;number&gt;] [--comp-methods &lt;string&gt;] [--comp-mt-level &lt;number&gt;] [--comp-mt-threads &lt;number&gt;] [--comp-mt-dthreads &lt;number&gt;] [--comp-xbzrle-cache &lt;number&gt;] [--auto-converge-initial &lt;number&gt;] [--auto-converge-increment &lt;number&gt;] [--persistent-xml &lt;string&gt;] [--tls]  DESCRIPTION    将域迁移到另一个主机中。热迁移时添加 --live。  OPTIONS    [--domain] &lt;string&gt;  domain name, id or uuid    [--desturi] &lt;string&gt;  客户端（常规迁移）或者源（p2p 迁移）中看到到目的地主机连接 URI    --live           热迁移    --offline        离线迁移    --p2p            点对点迁移    --direct         直接迁移    --tunnelled      管道迁移    --persistent     目的地中的持久 VM    --undefinesource  在源中取消定义 VM    --suspend        部启用目的地主机中的域    --copy-storage-all  使用全磁盘复制的非共享存储进行迁移    --copy-storage-inc  使用增值复制（源和目的地共享同一基础映像）的非共享存储进行迁移    --change-protection  迁移结束前不得对域进行任何配置更改    --unsafe         即使不安全也要强制迁移    --verbose        显示迁移进程    --compressed     实时迁移过程中压缩重复的页    --auto-converge  force convergence during live migration    --rdma-pin-all   pin all memory before starting RDMA live migration    --abort-on-error  在迁移过程中忽略软错误    --postcopy       enable post-copy migration; switch to it using migrate-postcopy command    --postcopy-after-precopy  automatically switch to post-copy migration after one pass of pre-copy    --migrateuri &lt;string&gt;  迁移 URI， 通常可省略    --graphicsuri &lt;string&gt;  无空隙图形迁移中使用的图形 URI    --listen-address &lt;string&gt;  listen address that destination should bind to for incoming migration    --dname &lt;string&gt;  在迁移过长中重新命名为一个新名称（如果支持）    --timeout &lt;number&gt;  run action specified by --timeout-* option (suspend by default) if live migration exceeds timeout (in seconds)    --timeout-suspend  suspend the guest after timeout    --timeout-postcopy  switch to post-copy after timeout    --xml &lt;string&gt;   包含为目标更新的 XML 的文件名    --migrate-disks &lt;string&gt;  comma separated list of disks to be migrated    --disks-port &lt;number&gt;  port to use by target server for incoming disks migration    --comp-methods &lt;string&gt;  comma separated list of compression methods to be used    --comp-mt-level &lt;number&gt;  compress level for multithread compression    --comp-mt-threads &lt;number&gt;  number of compression threads for multithread compression    --comp-mt-dthreads &lt;number&gt;  number of decompression threads for multithread compression    --comp-xbzrle-cache &lt;number&gt;  page cache size for xbzrle compression    --auto-converge-initial &lt;number&gt;  initial CPU throttling rate for auto-convergence    --auto-converge-increment &lt;number&gt;  CPU throttling rate increment for auto-convergence    --persistent-xml &lt;string&gt;  filename containing updated persistent XML for the target    --tls            use TLS for migration</code></pre><p>7、在KVM1主机中使用migrate命令迁移</p><pre><code>[root@kvm ~]# virsh migrate --domain generic --desturi qemu+ssh://root@kvm2/system --offline --persistent root@kvm2&#39;s password: </code></pre><p>8、在KVM2主机中查看主机是否迁移并尝试启动主机</p><pre><code>[root@kvm2 vm1]# virsh list --all  Id    Name                           State---------------------------------------------------- -     generic                        shut off[root@kvm2 vm1]# virsh start --domain generic error: Failed to start domain genericerror: Cannot access storage file &#39;/vm1/centos7.6.raw&#39;: 没有那个文件或目录</code></pre><p>9、在KVM1主机中将所需的硬盘文件复制到KVM2主机的vm1目录下</p><pre><code>[root@kvm ~]# rsync -avSHP /vm1/centos7.6.raw root@kvm2:/vm1root@kvm2&#39;s password: sending incremental file listcentos7.6.raw 21,474,836,480 100%   57.68MB/s    0:05:55 (xfr#1, to-chk=0/1)sent 21,480,079,452 bytes  received 35 bytes  59,749,873.40 bytes/sectotal size is 21,474,836,480  speedup is 1.00</code></pre><p>10、在KVM2主机中再次尝试启动主机</p><pre><code>[root@kvm2 vm1]# virsh start --domain generic Domain generic started</code></pre><h2><span id="3-基于共享存储的动态迁移">3、基于共享存储的动态迁移</span></h2><blockquote><p><strong>确保防火墙等安全防护工具处于关闭状态或者端口放行状态</strong></p></blockquote><p><strong>需要安装NFS服务器</strong></p><table><thead><tr><th>主机</th><th>Lan</th><th>Private</th><th>Stroage</th></tr></thead><tbody><tr><td>kvm1</td><td>192.168.200.135</td><td>172.16.1.135</td><td>10.0.1.231</td></tr><tr><td>kvm2</td><td>192.168.200.136</td><td>172.16.1.136</td><td>10.0.1.232</td></tr><tr><td>NFS</td><td>192.168.200.137</td><td></td><td>10.0.1.230</td></tr></tbody></table><h3><span id="1-安装nfs服务器组件">1、安装NFS服务器组件</span></h3><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/27.jpg"></p><pre><code>[root@nfs ~]# yum -y install nfs-utils[root@nfs ~]# systemctl start rpcbind[root@nfs ~]# systemctl start nfs-server[root@nfs ~]# systemctl enable rpcbind[root@nfs ~]# systemctl enable nfs-serverCreated symlink from /etc/systemd/system/multi-user.target.wants/nfs-server.service to /usr/lib/systemd/system/nfs-server.service.</code></pre><h3><span id="创建nfs共享文件夹">创建NFS共享文件夹</span></h3><pre><code>[root@nfs ~]# mkdir /vm1[root@nfs ~]# touch /vm1/testfile1.txt[root@nfs ~]# vim /etc/exports  #修改export文件/vm1 *(rw,no_root_squash,sync)[root@nfs ~]# systemctl restart nfs-server #重启服务[root@nfs ~]# showmount -e localhost #查看对外提供的共享文件夹Export list for localhost:/vm1 *</code></pre><h3><span id="将kvm1中的虚拟机硬盘迁移到nfs服务器的共享存储上">将KVM1中的虚拟机硬盘迁移到NFS服务器的共享存储上</span></h3><pre><code>[root@kvm ~]# mkdir /vmdata #在KVM1服务器中创建挂载点[root@kvm ~]# mount 192.168.200.137:/vm1 /vmdata/ #挂载NFS服务器中的vm1共享文件夹到本地的/vmdata。这里的IP建议使用专有的隧道网络，如万兆光等[root@kvm ~]# df -HT  #查看挂载情况文件系统                类型      容量  已用  可用 已用% 挂载点devtmpfs                devtmpfs  2.0G     0  2.0G    0% /devtmpfs                   tmpfs     2.0G     0  2.0G    0% /dev/shmtmpfs                   tmpfs     2.0G   13M  2.0G    1% /runtmpfs                   tmpfs     2.0G     0  2.0G    0% /sys/fs/cgroup/dev/mapper/centos-root xfs        38G   35G  3.7G   91% //dev/sda1               xfs       1.1G  206M  859M   20% /boottmpfs                   tmpfs     396M     0  396M    0% /run/user/0192.168.200.137:/vm1    nfs4       38G   15G   24G   39% /vmdata[root@kvm ~]# virsh list --all #查看迁移的主机 Id    名称                         状态---------------------------------------------------- -     generic                        关闭[root@kvm ~]# virsh edit --domain generic #编辑xml文件中的disk，如果直接修改xml文件，源硬盘会被直接迁移到nfs的共享目录中，老版本可能需要手动复制到挂载点    &lt;disk type=&#39;file&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;source file=&#39;/vmdata/centos7.6.raw&#39;/&gt; #修改      &lt;target dev=&#39;hda&#39; bus=&#39;ide&#39;/&gt;      &lt;address type=&#39;drive&#39; controller=&#39;0&#39; bus=&#39;0&#39; target=&#39;0&#39; unit=&#39;0&#39;/&gt;    &lt;/disk&gt;[root@kvm vmdata]# ls -lh /vmdata/ #查看共享文件中已经存在相关硬盘文件总用量 1.8G-rw-------. 1 root root 20G 4月  10 17:10 centos7.6.raw-rw-r--r--. 1 root root   0 4月  12 2023 testfile1.txt[root@kvm vmdata]# virsh start --domain generic  #尝试启动错误：开始域 generic 失败错误：内部错误：process exited while connecting to monitor: 2023-04-10T09:59:16.927297Z qemu-kvm: -drive file=/vmdata/centos7.6.raw,format=raw,if=none,id=drive-ide0-0-0: could not open disk image /vmdata/centos7.6.raw: Could not open &#39;/vmdata/centos7.6.raw&#39;: Permission denied[root@kvm vmdata]# sestatus #查看selinux启动，导致无法启动虚拟机SELinux status:                 enabledSELinuxfs mount:                /sys/fs/selinuxSELinux root directory:         /etc/selinuxLoaded policy name:             targetedCurrent mode:                   enforcingMode from config file:          enforcingPolicy MLS status:              enabledPolicy deny_unknown status:     allowedMax kernel policy version:      31[root@kvm vmdata]# setenforce usage:  setenforce [ Enforcing | Permissive | 1 | 0 ][root@kvm vmdata]# setenforce 0 #临时关闭selinux[root@kvm vmdata]# virsh start --domain generic  #正常启动域 generic 已开始</code></pre><blockquote><p>如果修改完后虚拟机无法正常启动，提示权限错误，可能是由于Selinux的问题</p></blockquote><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/28.jpg"></p><blockquote><p>建议不要关闭selinux，可以放行nfs的相关服务</p></blockquote><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/29.png"></p><pre><code>[root@kvm vmdata]# virsh shutdown --domain generic #关闭虚拟机域 generic 被关闭[root@kvm vmdata]# sestatus #查看当前Selinux状态为临时关闭SELinux status:                 enabledSELinuxfs mount:                /sys/fs/selinuxSELinux root directory:         /etc/selinuxLoaded policy name:             targetedCurrent mode:                   permissiveMode from config file:          enforcingPolicy MLS status:              enabledPolicy deny_unknown status:     allowedMax kernel policy version:      31[root@kvm vmdata]# setenforce 1 #启用Selinux[root@kvm vmdata]# sestatus #再次查看为启动状态SELinux status:                 enabledSELinuxfs mount:                /sys/fs/selinuxSELinux root directory:         /etc/selinuxLoaded policy name:             targetedCurrent mode:                   enforcingMode from config file:          enforcingPolicy MLS status:              enabledPolicy deny_unknown status:     allowedMax kernel policy version:      31[root@kvm vmdata]# setsebool -P virt_use_nfs 1 #需要在所有的计算节点开启[root@kvm vmdata]# virsh start --domain generic  #尝试可以正常启动域 generic 已开始[root@kvm vmdata]# virsh shutdown --domain generic #关机，准备迁移域 generic 被关闭</code></pre><h3><span id="2-使用virt-manager进行迁移">2、使用virt-manager进行迁移</span></h3><p>KVM2：</p><pre><code>[root@kvm2 ~]# mkdir /vmdata[root@kvm2 ~]# ls -lh /vmdata-ooal 0t root   0 Apr 12 16:57 testfile1.txt</code></pre><p>KVM1:</p><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/30.png"></p><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/31.jpg"></p><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/32.jpg"></p><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/33.jpg"></p><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/34.png"></p><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/35.png"></p><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/36.jpg"></p><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/37.jpg"></p><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/38.png"></p><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/39.jpg"></p><pre><code>[root@kvm2 ~]# setsebool -P virt_use_nfs 1[root@kvm2 ~]# iptables -I INPUT -p tcp --dport 49152:49215 -j ACCEPT[root@kvm2 ~]# iptables -I INPUT -p tcp --dport 16509 -j ACCEPT</code></pre><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/40.jpg"></p><p><img src="/images/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B/41.jpg"></p><p>Allow unsafe：CPU缓存的模式，默认使用none<br>Temporary move：临时移动，移动后不删除源主机的相关配置</p><h3><span id="3-使用virsh命令进行迁移">3、使用virsh命令进行迁移</span></h3><h2><span id="4-基于本地存储的动态迁移">4、基于本地存储的动态迁移</span></h2><h1><span id="三-kvm集群">三、KVM集群</span></h1><h1><span id="四-嵌套虚拟化">四、嵌套虚拟化</span></h1><h1><span id="五-性能监视与优化">五、性能监视与优化</span></h1><h1><span id="六-p2v和v2v迁移">六、P2V和V2V迁移</span></h1><h1><span id="七-备份与恢复">七、备份与恢复</span></h1><h1><span id="八-ovirtrhev安装与基本管理">八、oVirt(RHEV)安装与基本管理</span></h1><p>-root@kvm2 ~]# mount 192.168.200.137:&#x2F;vm1 &#x2F;vmdata&#x2F;<br>-root@kvm2 ~]# ls -lh &#x2F;vmdata<br>wotal 1.8G<br>-rw——-. 1 root root 20G Apr 12</p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TEST</title>
      <link href="/2023/04/11/test/"/>
      <url>/2023/04/11/test/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><!-- toc --><!-- tocstop --><p>创建脚注格式类似这样 [^RUNOOB]。</p><p>[^RUNOOB]: 菜鸟教程 – 学的不仅是技术，更是梦想！！！</p><blockquote><p>最外层</p><blockquote><p>第一层嵌套</p><blockquote><p>第二层嵌套</p></blockquote></blockquote></blockquote><pre><code>%% 语法示例        gantt        dateFormat  YYYY-MM-DD        title 软件开发甘特图        section 设计        需求                      :done,    des1, 2014-01-06,2014-01-08        原型                      :active,  des2, 2014-01-09, 3d        UI设计                     :         des3, after des2, 5d    未来任务                     :         des4, after des3, 5d        section 开发        学习准备理解需求                      :crit, done, 2014-01-06,24h        设计框架                             :crit, done, after des2, 2d        开发                                 :crit, active, 3d        未来任务                              :crit, 5d        耍                                   :2d        section 测试        功能测试                              :active, a1, after des3, 3d        压力测试                               :after a1  , 20h        测试报告                               : 48h</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Test </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Test </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph-安装</title>
      <link href="/2023/04/11/Ceph/3.Ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/Ceph-%E5%AE%89%E8%A3%85/"/>
      <url>/2023/04/11/Ceph/3.Ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/Ceph-%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E7%8E%AF%E5%A2%83%E6%83%85%E5%86%B5">环境情况</a></li><li><a href="#%E4%B8%80-%E4%BF%AE%E6%94%B9hosts%E6%96%87%E4%BB%B6">一、修改HOSTS文件</a></li><li><a href="#%E4%BA%8C-%E9%85%8D%E7%BD%AE%E6%97%A0%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95">二、配置无密码登录</a></li><li><a href="#%E4%B8%89-%E5%AE%89%E5%85%A8%E8%AE%BE%E7%BD%AE">三、安全设置</a><ul><li><a href="#%E4%B8%80-%E5%85%B3%E9%97%ADselinux">一、关闭Selinux</a></li><li><a href="#%E4%BA%8C-%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99">二、关闭防火墙</a></li></ul></li><li><a href="#%E5%9B%9B-ntp%E8%AE%BE%E7%BD%AE">四、NTP设置</a></li><li><a href="#%E4%BA%94-%E9%85%8D%E7%BD%AEyum%E6%BA%90">五、配置Yum源</a></li><li><a href="#%E5%85%AD-%E5%AE%89%E8%A3%85ceph-deploy">六、安装Ceph-deploy</a></li><li><a href="#%E4%B8%83-%E5%AE%89%E8%A3%85mon%E8%8A%82%E7%82%B9">七、安装Mon节点</a><ul><li><a href="#%E4%B8%80-%E9%87%8D%E7%BD%AE%E9%9B%86%E7%BE%A4">一、重置集群</a></li><li><a href="#%E4%BA%8C-%E5%AE%89%E8%A3%85%E7%9B%B8%E5%85%B3%E8%BD%AF%E4%BB%B6">二、安装相关软件</a></li></ul></li><li><a href="#%E4%B8%89-mon-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%8A%E5%AE%89%E8%A3%85mgr">三、Mon 初始化及安装MGR</a></li><li><a href="#%E5%85%AB-%E9%83%A8%E7%BD%B2osd%E8%8A%82%E7%82%B9">八、部署OSD节点</a></li><li><a href="#%E4%B9%9D-%E6%89%A9%E5%B1%95mon%E5%92%8Cmgr">九、扩展MON和MGR</a><ul><li><a href="#%E4%B8%80-mon">一、Mon</a></li><li><a href="#%E4%BA%8C-mgr">二、MGR</a></li></ul></li><li><a href="#%E5%8D%81-%E5%88%9B%E5%BB%BA%E8%B5%84%E6%BA%90%E6%B1%A0pool">十、创建资源池Pool</a></li></ul><!-- tocstop --><h1><span id="环境情况">环境情况</span></h1><table>    <tr>        <td>操作系统</td>        <td>公共网络</td>        <td>集群网络</td>        <td>节点名称</td>    </tr>    <tr>        <td rowspan="4">Centos 7.9</td>    </tr>        <tr>            <td>192.168.187.201</td>            <td>192.168.199.201</td>            <td>node-1</td>        </tr>        <tr>            <td>192.168.187.202</td>            <td>192.168.199.202</td>            <td>node-2</td>        </tr>        <tr>            <td>192.168.187.203</td>            <td>192.168.199.203</td>            <td>node-2</td>        </tr></table><h1><span id="一-修改hosts文件">一、修改HOSTS文件</span></h1><p><strong>所有主机均要修改</strong></p><pre><code>[root@node-1 ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.199.201 node-1192.168.199.202 node-2192.168.199.203 node-3</code></pre><h1><span id="二-配置无密码登录">二、配置无密码登录</span></h1><p><strong>Node-1：</strong></p><pre><code>[root@node-1 ~]# ssh-keygen Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Created directory &#39;/root/.ssh&#39;.Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:RNd/EfeRh3t7C73PD//lsUVTQpf84hKM5Dy3CdWj6iQ root@node-1The keys randomart image is:+---[RSA 2048]----+|        . .. .o+*||       . .. o.+*=||        .+ + o.o*||       .  * = +.=||        S  = =.=o||        E o +..+o||         +   .o.*||          .    B=||               .@|+----[SHA256]-----+[root@node-1 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub node-2/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;The authenticity of host &#39;node-2 (192.168.199.202)&#39; can t be established.ECDSA key fingerprint is SHA256:ELWxoLpZlehIUk5OpR5CO/OmzpHyNco8PkV4ztDCV7w.ECDSA key fingerprint is MD5:62:87:b6:99:64:b1:b2:d3:40:f1:d5:e8:d3:a1:8a:18.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@node-2 s password: Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#39;node-2&#39;&quot;and check to make sure that only the key(s) you wanted were added.[root@node-1 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub node-3/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;The authenticity of host &#39;node-3 (192.168.199.203)&#39; can t be established.ECDSA key fingerprint is SHA256:ELWxoLpZlehIUk5OpR5CO/OmzpHyNco8PkV4ztDCV7w.ECDSA key fingerprint is MD5:62:87:b6:99:64:b1:b2:d3:40:f1:d5:e8:d3:a1:8a:18.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@node-3 s password: Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#39;node-3&#39;&quot;and check to make sure that only the key(s) you wanted were added.</code></pre><h1><span id="三-安全设置">三、安全设置</span></h1><h2><span id="一-关闭selinux">一、关闭Selinux</span></h2><p><strong>所有节点执行，将enforcing修改成disabled：</strong></p><pre><code>[root@node-1 ~]# cat /etc/selinux/config # This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:#     enforcing - SELinux security policy is enforced.#     permissive - SELinux prints warnings instead of enforcing.#     disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of three values:#     targeted - Targeted processes are protected,#     minimum - Modification of targeted policy. Only selected processes are protected. #     mls - Multi Level Security protection.SELINUXTYPE=targeted[root@node-1 ~]# setenforce 0 #临时关闭[root@node-1 ~]# getenforce #查看状态Disabled</code></pre><h2><span id="二-关闭防火墙">二、关闭防火墙</span></h2><p><strong>所有节点均执行：</strong></p><pre><code>[root@node-1 ~]# firewall-cmd --list-all #查看防火墙状态public (active)  target: default  icmp-block-inversion: no  interfaces: ens33 ens36  sources:   services: dhcpv6-client ssh  ports:   protocols:   masquerade: no  forward-ports:   source-ports:   icmp-blocks:   rich rules: [root@node-1 ~]# systemctl disable firewalld #禁止开机自启Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.[root@node-1 ~]# systemctl stop firewalld #关闭防火墙[root@node-3 ~]# firewall-cmd --list-all #查看状态FirewallD is not running</code></pre><h1><span id="四-ntp设置">四、NTP设置</span></h1><p><strong>所有节点执行：</strong></p><pre><code>[root@node-3 ~]# cd /etc/yum.repos.d/[root@node-3 yum.repos.d]# lsCentOS-Base.repo  CentOS-Debuginfo.repo  CentOS-Media.repo    CentOS-Vault.repoCentOS-CR.repo    CentOS-fasttrack.repo  CentOS-Sources.repo  CentOS-x86_64-kernel.repo[root@node-3 yum.repos.d]# rm -rf *[root@node-3 yum.repos.d]# ls[root@node-1 yum.repos.d]# curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  2523  100  2523    0     0   6577      0 --:--:-- --:--:-- --:--:--  6587[root@node-1 yum.repos.d]# [root@node-1 yum.repos.d]# lsCentOS-Base.repo[root@node-1 yum.repos.d]# yum clean all &amp;&amp; yum makecache #清除缓存[root@node-1 yum.repos.d]# yum install ntp -y #安装NTP[root@node-1 yum.repos.d]# systemctl restart ntpd #启动NTP服务[root@node-1 yum.repos.d]# systemctl enable ntpd #设置开机自启动Created symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.[root@node-1 yum.repos.d]# ntpq -pn #查看NTP状态     remote           refid      st t when poll reach   delay   offset  jitter==============================================================================+78.46.102.180   176.9.157.12     3 u   33   64    1  245.918   17.006  14.216*144.76.76.107   192.53.103.103   2 u   33   64    1  214.840    0.073   0.324 193.182.111.14  192.36.143.153   2 u   67   64    1  293.490   -4.158   1.037 [root@node-2 ~]# crontab -l #使用crontab -e 编辑，每分钟同步一次时间*/1 * * * * /usr/sbin/ntpdate node-1;/sbin/hwclock -w</code></pre><p><strong>node-2、node-3执行：</strong></p><pre><code>[root@node-3 yum.repos.d]# cat /etc/ntp.conf # For more information about this file, see the man pages# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).driftfile /var/lib/ntp/drift# Permit time synchronization with our time source, but do not# permit the source to query or modify the service on this system.restrict default nomodify notrap nopeer noquery# Permit all access over the loopback interface.  This could# be tightened as well, but to do so would effect some of# the administrative functions.restrict 127.0.0.1 restrict ::1# Hosts on local network are less restricted.#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).server node-1 iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst#broadcast 192.168.1.255 autokey    # broadcast server#broadcastclient            # broadcast client#broadcast 224.0.1.1 autokey        # multicast server#multicastclient 224.0.1.1        # multicast client#manycastserver 239.255.254.254        # manycast server#manycastclient 239.255.254.254 autokey # manycast client# Enable public key cryptography.#cryptoincludefile /etc/ntp/crypto/pw# Key file containing the keys and key identifiers used when operating# with symmetric key cryptography. keys /etc/ntp/keys# Specify the key identifiers which are trusted.#trustedkey 4 8 42# Specify the key identifier to use with the ntpdc utility.#requestkey 8# Specify the key identifier to use with the ntpq utility.#controlkey 8# Enable writing of statistics records.#statistics clockstats cryptostats loopstats peerstats# Disable the monitoring facility to prevent amplification attacks using ntpdc# monlist command when default restrict does not include the noquery flag. See# CVE-2013-5211 for more details.# Note: Monitoring will not be disabled with the limited restriction flag.disable monitor</code></pre><h1><span id="五-配置yum源">五、配置Yum源</span></h1><p><strong>所有节点执行：</strong></p><pre><code>[root@node-3 ~]# cd /etc/yum.repos.d/[root@node-3 yum.repos.d]# lsCentOS-Base.repo  CentOS-Debuginfo.repo  CentOS-Media.repo    CentOS-Vault.repoCentOS-CR.repo    CentOS-fasttrack.repo  CentOS-Sources.repo  CentOS-x86_64-kernel.repo[root@node-3 yum.repos.d]# rm -rf *[root@node-3 yum.repos.d]# ls[root@node-1 yum.repos.d]# curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  2523  100  2523    0     0   6577      0 --:--:-- --:--:-- --:--:--  6587[root@node-1 yum.repos.d]# [root@node-1 yum.repos.d]# lsCentOS-Base.repo[root@node-1 yum.repos.d]# curl -o /etc/yum.repos.d/epel.repo https://mirrors.aliyun.com/repo/epel-7.repo  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   664  100   664    0     0   2193      0 --:--:-- --:--:-- --:--:--  2191[root@node-1 yum.repos.d]# [root@node-1 yum.repos.d]# lsCentOS-Base.repo  epel.repo[root@node-1 yum.repos.d]# cat ceph.repo  #手动创建ceph.repo文件内容如下[norch]name=norchbaseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/enabled=1gpgcheck=0[x86_64]name=x86 64baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/enabled=1gpgcheck=0[root@node-1 yum.repos.d]# yum clean all &amp;&amp; yum makecache #清除缓存[root@node-1 yum.repos.d]# yum update -y</code></pre><h1><span id="六-安装ceph-deploy">六、安装Ceph-deploy</span></h1><p><strong>Node1执行</strong></p><pre><code>[root@node-1 yum.repos.d]# yum install python-setuptools ceph-deploy -y #安装核心软件[root@node-1 yum.repos.d]# ceph-deploy --version #查看版本2.0.1</code></pre><h1><span id="七-安装mon节点">七、安装Mon节点</span></h1><h2><span id="一-重置集群">一、重置集群</span></h2><p><strong>如果安装失败或者重新安装时执行：</strong></p><pre><code>ceph-deploy purge &#123;ceph-node&#125; [&#123;ceph-node&#125;]ceph-deploy purgedata  &#123;ceph-node&#125; [&#123;ceph-node&#125;]ceph-deploy forgetkeysrm ceph.*</code></pre><h2><span id="二-安装相关软件">二、安装相关软件</span></h2><p><strong>Node1执行：</strong></p><pre><code>[root@node-1 yum.repos.d]# cd /opt/[root@node-1 opt]# mkdir my-cluster[root@node-1 opt]# cd my-cluster/[root@node-1 my-cluster]# ceph-deploy new --public-network 192.168.187.0/24 --cluster-network 192.168.199.0/24 node-1</code></pre><p><strong>公共网络是外部访问集群时使用的，集群网络时内部同步使用的</strong><br><strong>所有节点执行：</strong></p><pre><code>[root@node-1 my-cluster]# yum install ceph ceph-mon ceph-mgr ceph-mds ceph-radosgw -y #安装核心软件包</code></pre><h1><span id="三-mon-初始化及安装mgr">三、Mon 初始化及安装MGR</span></h1><pre><code>[root@node-1 my-cluster]# ceph-deploy mon create-initial #初始化Mon[root@node-1 my-cluster]# ceph-deploy admin node-1 node-2 node-3 #推送最新配置到所有节点[root@node-1 my-cluster]# ceph -s  #查看集群状态  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            mon is allowing insecure global_id reclaim  #mon允许不安全的global_id回收   services:    mon: 1 daemons, quorum node-1 (age 2m)    mgr: no daemons active    osd: 0 osds: 0 up, 0 in   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   0 B used, 0 B / 0 B avail    pgs:     [root@node-1 my-cluster]# ceph config set mon auth_allow_insecure_global_id_reclaim false #取消mon允许不安全的global_id回收[root@node-1 my-cluster]# ceph -s   cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 1 daemons, quorum node-1 (age 3m)    mgr: no daemons active    osd: 0 osds: 0 up, 0 in   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   0 B used, 0 B / 0 B avail    pgs:      [root@node-1 my-cluster]# ceph-deploy mgr create node-1 #安装mgr 监控服务[root@node-1 my-cluster]#  ceph -s  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            OSD count 0 &lt; osd_pool_default_size 3   services:    mon: 1 daemons, quorum node-1 (age 6m)    mgr: node-1(active, since 55s) #查看已经成功安装    osd: 0 osds: 0 up, 0 in   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   0 B used, 0 B / 0 B avail    pgs:     </code></pre><h1><span id="八-部署osd节点">八、部署OSD节点</span></h1><p><strong>node-1 执行:</strong></p><pre><code>[root@node-1 my-cluster]# lsblk #确定硬盘NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsda               8:0    0   20G  0 disk ├─sda1            8:1    0    1G  0 part /boot└─sda2            8:2    0   19G  0 part   ├─centos-root 253:0    0   17G  0 lvm  /  └─centos-swap 253:1    0    2G  0 lvm  [SWAP]sdb               8:16   0   10G  0 disk sdc               8:32   0   10G  0 disk sr0              11:0    1 1024M  0 rom [root@node-1 my-cluster]# ceph-deploy osd create node-1 --data /dev/sdb --journal /dev/sdb # --data指定硬盘  --journal /dev/sdb 指定加速[root@node-1 my-cluster]# ceph-deploy osd create node-2 --data /dev/sdb --journal /dev/sdb # --data指定硬盘  --journal /dev/sdb 指定加速[root@node-1 my-cluster]# ceph-deploy osd create node-3 --data /dev/sdb --journal /dev/sdb # --data指定硬盘  --journal /dev/sdb 指定加速[root@node-1 my-cluster]# ceph osd tree  #查看OSD状态ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.02939 root default                            -3       0.00980     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000 -5       0.00980     host node-2                          1   hdd 0.00980         osd.1       up  1.00000 1.00000 -7       0.00980     host node-3                          2   hdd 0.00980         osd.2       up  1.00000 1.00000 </code></pre><h1><span id="九-扩展mon和mgr">九、扩展MON和MGR</span></h1><p><strong>Mon 使用Paxos算法，一般都是奇数 3、5、7</strong></p><h2><span id="一-mon">一、Mon</span></h2><pre><code>NODE-1执行：ceph-deploy mon add node-2 --address 192.168.187.202 #添加node-2成为Mon 并指定IPceph-deploy mon add node-3 --address 192.168.187.203 #添加node-3成为Mon 并指定IP[root@node-1 my-cluster]# ceph -s  #查看Mon是否添加成功  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 54s) #已经添加node-2和node-3    mgr: node-1(active, since 18m)    osd: 3 osds: 3 up (since 8m), 3 in (since 8m)   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   3.0 GiB used, 27 GiB / 30 GiB avail    pgs: [root@node-1 my-cluster]# ceph quorum_status --format json-pretty #查看Mon 仲裁情况&#123;    &quot;election_epoch&quot;: 12,    &quot;quorum&quot;: [        0,        1,        2    ],    &quot;quorum_names&quot;: [ #可以看到当前3个节点正在参加仲裁        &quot;node-1&quot;,        &quot;node-2&quot;,        &quot;node-3&quot;    ],    &quot;quorum_leader_name&quot;: &quot;node-1&quot;, #当前主Mon是Node-1    &quot;quorum_age&quot;: 234,    &quot;monmap&quot;: &#123;        &quot;epoch&quot;: 3,        &quot;fsid&quot;: &quot;e9a90625-4707-4b6b-b52f-661512ea831d&quot;,        &quot;modified&quot;: &quot;2023-03-03 12:22:43.254681&quot;,        &quot;created&quot;: &quot;2023-03-03 11:59:53.474948&quot;,        &quot;min_mon_release&quot;: 14,        &quot;min_mon_release_name&quot;: &quot;nautilus&quot;,        &quot;features&quot;: &#123;            &quot;persistent&quot;: [                &quot;kraken&quot;,                &quot;luminous&quot;,                &quot;mimic&quot;,                &quot;osdmap-prune&quot;,                &quot;nautilus&quot;            ],            &quot;optional&quot;: []        &#125;,        &quot;mons&quot;: [            &#123;                &quot;rank&quot;: 0,                &quot;name&quot;: &quot;node-1&quot;,                &quot;public_addrs&quot;: &#123;                    &quot;addrvec&quot;: [                        &#123;                            &quot;type&quot;: &quot;v2&quot;,                            &quot;addr&quot;: &quot;192.168.187.201:3300&quot;,                            &quot;nonce&quot;: 0                        &#125;,                        &#123;                            &quot;type&quot;: &quot;v1&quot;,                            &quot;addr&quot;: &quot;192.168.187.201:6789&quot;,                            &quot;nonce&quot;: 0                        &#125;                    ]                &#125;,                &quot;addr&quot;: &quot;192.168.187.201:6789/0&quot;,                &quot;public_addr&quot;: &quot;192.168.187.201:6789/0&quot;            &#125;,            &#123;                &quot;rank&quot;: 1,                &quot;name&quot;: &quot;node-2&quot;,                &quot;public_addrs&quot;: &#123;                    &quot;addrvec&quot;: [                        &#123;                            &quot;type&quot;: &quot;v2&quot;,                            &quot;addr&quot;: &quot;192.168.187.202:3300&quot;,                            &quot;nonce&quot;: 0                        &#125;,                        &#123;                            &quot;type&quot;: &quot;v1&quot;,                            &quot;addr&quot;: &quot;192.168.187.202:6789&quot;,                            &quot;nonce&quot;: 0                        &#125;                    ]                &#125;,                &quot;addr&quot;: &quot;192.168.187.202:6789/0&quot;,                &quot;public_addr&quot;: &quot;192.168.187.202:6789/0&quot;            &#125;,            &#123;                &quot;rank&quot;: 2,                &quot;name&quot;: &quot;node-3&quot;,                &quot;public_addrs&quot;: &#123;                    &quot;addrvec&quot;: [                        &#123;                            &quot;type&quot;: &quot;v2&quot;,                            &quot;addr&quot;: &quot;192.168.187.203:3300&quot;,                            &quot;nonce&quot;: 0                        &#125;,                        &#123;                            &quot;type&quot;: &quot;v1&quot;,                            &quot;addr&quot;: &quot;192.168.187.203:6789&quot;,                            &quot;nonce&quot;: 0                        &#125;                    ]                &#125;,                &quot;addr&quot;: &quot;192.168.187.203:6789/0&quot;,                &quot;public_addr&quot;: &quot;192.168.187.203:6789/0&quot;            &#125;        ]    &#125;&#125;[root@node-1 my-cluster]#  ceph mon stat #查看Mon 状态e3: 3 mons at &#123;node-1=[v2:192.168.187.201:3300/0,v1:192.168.187.201:6789/0],node-2=[v2:192.168.187.202:3300/0,v1:192.168.187.202:6789/0],node-3=[v2:192.168.187.203:3300/0,v1:192.168.187.203:6789/0]&#125;, election epoch 12, leader 0 node-1, quorum 0,1,2 node-1,node-2,node-3[root@node-1 my-cluster]# ceph mon dump #查看Mon 状态epoch 3fsid e9a90625-4707-4b6b-b52f-661512ea831dlast_changed 2023-03-03 12:22:43.254681created 2023-03-03 11:59:53.474948min_mon_release 14 (nautilus)0: [v2:192.168.187.201:3300/0,v1:192.168.187.201:6789/0] mon.node-11: [v2:192.168.187.202:3300/0,v1:192.168.187.202:6789/0] mon.node-22: [v2:192.168.187.203:3300/0,v1:192.168.187.203:6789/0] mon.node-3dumped monmap epoch 3</code></pre><h2><span id="二-mgr">二、MGR</span></h2><p><strong>MGR默认是主备模式</strong></p><p><strong>node-1：执行</strong></p><pre><code>[root@node-1 my-cluster]#  ceph-deploy mgr create node-2 node-3 #添加node-2 node-3 成为mgr[root@node-1 my-cluster]#  ceph -s  #查看MGR状态  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 11m)    mgr: node-1(active, since 28m), standbys: node-2, node-3 #可以看到备MGR为：node-2和node-3    osd: 3 osds: 3 up (since 18m), 3 in (since 18m)   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   3.0 GiB used, 27 GiB / 30 GiB avail    pgs:[root@node-1 my-cluster]# ceph mgr dump #查看具体MAP图</code></pre><h1><span id="十-创建资源池pool">十、创建资源池Pool</span></h1><pre><code>Node-1执行：[root@node-1 my-cluster]# ceph osd lspools #查看当前pool[root@node-1 my-cluster]# ceph osd pool create ceph-demo 64 64 #创建一个叫做ceph-demo的pool 并指定PG数为64（第一个数字），pgp数64（第二个数字）pool &#39;ceph-demo&#39; created[root@node-1 my-cluster]# ceph osd lspools #再次查看当前pool 新增了要给ceph-demo的pool1 ceph-demo[root@node-1 my-cluster]#  ceph osd pool get ceph-demo pg_num #查看ceph-demo的PG数量pg_num: 64[root@node-1 my-cluster]#  ceph osd pool get ceph-demo pgp_num #查看ceph-demo的PGP数量pgp_num: 64[root@node-1 my-cluster]#  ceph osd pool get ceph-demo size #查看ceph-demo的副本数size: 3[root@node-1 my-cluster]#  ceph osd pool get ceph-demo crush_rule #查看crush的调度算法crush_rule: replicated_rule #默认的复制规则[root@node-1 my-cluster]#   ceph osd pool set ceph-demo size 2  #可以通过set的方式调整副本数量为2set pool 1 size to 2[root@node-1 my-cluster]# ceph osd pool set ceph-demo pg_num 128 #可以通过set的方式调整pg数量为128set pool 1 pg_num to 128[root@node-1 my-cluster]# ceph osd pool set ceph-demo pgp_num 128 #可以通过set的方式调整pgp数量为128,PGP数量应该与PG数一致set pool 1 pgp_num to 12[root@node-1 my-cluster]# rbd pool init &lt;pool_name&gt; #初始化pool</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RBD写入流程</title>
      <link href="/2023/04/11/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B/"/>
      <url>/2023/04/11/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><!-- tocstop --><p><img src="/images/RBD%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B/1.jpg"></p><blockquote><p>块设备都是瘦分配的，意味着使用的越多，分配的空间越多</p></blockquote><pre><code>[root@node-1 ~]# rbd -p ceph-demo info rbd-demo.img #查看镜像信息rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023[root@node-1 ~]# rados  -p ceph-demo ls | grep rbd_data.11e24b04cf01 #查看objectrbd_data.11e24b04cf01.0000000000000960rbd_data.11e24b04cf01.0000000000000500rbd_data.11e24b04cf01.0000000000000780rbd_data.11e24b04cf01.0000000000000320rbd_data.11e24b04cf01.00000000000008c0rbd_data.11e24b04cf01.00000000000006e0rbd_data.11e24b04cf01.00000000000009ffrbd_data.11e24b04cf01.0000000000000640rbd_data.11e24b04cf01.0000000000000000rbd_data.11e24b04cf01.0000000000000280rbd_data.11e24b04cf01.0000000000000501rbd_data.11e24b04cf01.0000000000000001rbd_data.11e24b04cf01.00000000000003c0rbd_data.11e24b04cf01.0000000000000502rbd_data.11e24b04cf01.00000000000005a0rbd_data.11e24b04cf01.0000000000000460rbd_data.11e24b04cf01.0000000000000140rbd_data.11e24b04cf01.00000000000001e0rbd_data.11e24b04cf01.0000000000000820rbd_data.11e24b04cf01.00000000000000a0[root@node-1 data]# rados -p ceph-demo stat rbd_data.11e24b04cf01.000000000000007a#查看object 大小 size=比特，需要除以两次1024得出Mceph-demo/rbd_data.11e24b04cf01.000000000000007a mtime 2023-03-04 15:08:07.000000, size 4194304[root@node-1 ~]#  ceph osd map ceph-demo rbd_data.11e24b04cf01.000000000000007a #查看object 落到那个OSD和PG上 PG=1.20 最终落在0，1，2三个OSD上osdmap e339 pool &#39;ceph-demo&#39; (1) object &#39;rbd_data.11e24b04cf01.0000000000000960&#39; -&gt; pg 1.85b57b60 (1.20) -&gt; up ([2,1,0], p2) acting ([2,1,0], p2)[root@node-1 ~]# rados  -p ceph-demo ls |wc -l 查看当前ceph-demo的pool中一共存在25个object 每个object等于4 M，一共有25个，所以总共分配空间为100M，随着数据增加，会动态扩容分配空间。25[root@node-1 data]# dd if=/dev/zero of=test.img bs=1M count=1024  #尝试写入1G的文件到集群中1024+0 records in1024+0 records out1073741824 bytes (1.1 GB) copied, 5.97513 s, 180 MB/sYou have new mail in /var/spool/mail/root[root@node-1 data]# ls -lha total 1.1Gdrwxr-xr-x.  2 root root   34 Mar  4 15:08 .dr-xr-xr-x. 18 root root  236 Mar  4 02:27 ..-rw-r--r--.  1 root root    5 Mar  4 02:30 test-rw-r--r--.  1 root root 1.0G Mar  4 15:08 test.img[root@node-1 data]# rados  -p ceph-demo ls | grep rbd_data.11e24b04cf01 |wc -l  #可以看到object的数量由25个增加到了276个 每个object等于4 M，所以总计分配1,104M276[root@node-1 data]# df -HT #查看实际使用情况发现基本相当Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   14M  941M   2% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.7G   16G  15% //dev/sda1               xfs       1.1G  190M  874M  18% /boottmpfs                   tmpfs     191M     0  191M   0% /run/user/0tmpfs                   tmpfs     954M   54k  954M   1% /var/lib/ceph/osd/ceph-0/dev/rbd0               xfs        11G  1.2G  9.7G  11% /data</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> RBD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RBD块创建及映射</title>
      <link href="/2023/04/11/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%9D%97%E5%88%9B%E5%BB%BA%E5%8F%8A%E6%98%A0%E5%B0%84/"/>
      <url>/2023/04/11/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%9D%97%E5%88%9B%E5%BB%BA%E5%8F%8A%E6%98%A0%E5%B0%84/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#rbd%E5%9D%97%E5%88%9B%E5%BB%BA%E5%8F%8A%E6%98%A0%E5%B0%84">RBD块创建及映射</a><ul><li><a href="#%E4%B8%80-%E5%88%9B%E5%BB%BA%E5%9D%97%E8%AE%BE%E5%A4%87">一、创建块设备</a></li><li><a href="#%E4%BA%8C-%E4%BD%BF%E7%94%A8%E5%9D%97%E8%AE%BE%E5%A4%87">二、使用块设备</a></li></ul></li></ul><!-- tocstop --><h1><span id="rbd块创建及映射">RBD块创建及映射</span></h1><h2><span id="一-创建块设备">一、创建块设备</span></h2><p><strong>任意节点执行即可：</strong></p><pre><code>[root@node-1 my-cluster]#  rbd help create #查看create的帮助文档[root@node-1 my-cluster]#  rbd create -p ceph-demo --image rbd-demo.img --size 10G #在ceph-demo的pool中创建一个叫做rbd-demo.img的块设备，大小为10G[root@node-1 my-cluster]# rbd ls ceph-demo #查看创建结果rbd-demo.img[root@node-1 my-cluster]#  rbd create  ceph-demo/rbd-demo-1.img --size 10G #在ceph-demo的pool中创建一个叫做rbd-demo.img的块设备，大小为10G[root@node-1 my-cluster]# rbd ls ceph-demo  #查看创建结果rbd-demo-1.imgrbd-demo.img[root@node-1 my-cluster]# rbd -p ceph-demo ls  #与rbd ls ceph-demo 作用一致rbd-demo-1.imgrbd-demo.img[root@node-1 my-cluster]#  rbd info ceph-demo/rbd-demo.img #查看块设备的具体信息，与rbd info-p ceph-demo --image rbd-demo.img 一致rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Fri Mar  3 13:03:28 2023    access_timestamp: Fri Mar  3 13:03:28 2023    modify_timestamp: Fri Mar  3 13:03:28 2023[root@node-1 my-cluster]# rbd rm  ceph-demo/rbd-demo-1.img #删除块设备，与rbd rm -p ceph-demo --image rbd-demo-1.img用法一致Removing image: 100% complete...done.</code></pre><h2><span id="二-使用块设备">二、使用块设备</span></h2><pre><code>[root@node-1 my-cluster]#  rbd map ceph-demo/rbd-demo.img #映射块设备到本地rbd: sysfs write failedRBD image feature set mismatch. You can disable features unsupported by the kernel with &quot;rbd feature disable ceph-demo/rbd-demo.img object-map fast-diff deep-flatten&quot;.In some cases useful info is found in syslog - try &quot;dmesg | tail&quot;.rbd: map failed: (6) No such device or address</code></pre><blockquote><p>上述报错主要是由于Centos 7 不支持某些特性，需要手动禁用，具体可使用rbd info ceph-demo&#x2F;rbd-demo.img 进行查看，特性主要记录在features中</p></blockquote><p><strong>禁用相关特性：</strong></p><pre><code>[root@node-1 my-cluster]#  rbd info ceph-demo/rbd-demo.img #查看当前特性rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Fri Mar  3 13:03:28 2023    access_timestamp: Fri Mar  3 13:03:28 2023    modify_timestamp: Fri Mar  3 13:03:28 2023[root@node-1 my-cluster]# rbd help feature disable #查看使用文档usage: rbd feature disable [--pool &lt;pool&gt;] [--namespace &lt;namespace&gt;]                            [--image &lt;image&gt;]                            &lt;image-spec&gt; &lt;features&gt; [&lt;features&gt; ...]Disable the specified image feature.Positional arguments  &lt;image-spec&gt;         image specification                       (example: [&lt;pool-name&gt;/[&lt;namespace&gt;/]]&lt;image-name&gt;)  &lt;features&gt;           image features                       [exclusive-lock, object-map, journaling]Optional arguments  -p [ --pool ] arg    pool name  --namespace arg      namespace name  --image arg          image name[root@node-1 my-cluster]# rbd feature disable ceph-demo/rbd-demo.img deep-flatten #禁用rbd-demo.img块设备的deep-flatten特性[root@node-1 my-cluster]# rbd feature disable ceph-demo/rbd-demo.img fast-diff #禁用rbd-demo.img块设备的fast-diff特性[root@node-1 my-cluster]# rbd feature disable ceph-demo/rbd-demo.img exclusive-lock #禁用rbd-demo.img块设备的exclusive-lock特性[root@node-1 my-cluster]# rbd info ceph-demo/rbd-demo.img #查看相关特性已经被禁用rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering #layering不影响使用，可不禁用    op_features:     flags:     create_timestamp: Fri Mar  3 13:03:28 2023    access_timestamp: Fri Mar  3 13:03:28 2023    modify_timestamp: Fri Mar  3 13:03:28 2023 [root@node-1 my-cluster]#  rbd map ceph-demo/rbd-demo.img #挂载到本地/dev/rbd0[root@node-1 my-cluster]# lsblkNAME                                                                                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsda                                                                                                     8:0    0   20G  0 disk ├─sda1                                                                                                  8:1    0    1G  0 part /boot└─sda2                                                                                                  8:2    0   19G  0 part   ├─centos-root                                                                                       253:0    0   17G  0 lvm  /  └─centos-swap                                                                                       253:1    0    2G  0 lvm  [SWAP]sdb                                                                                                     8:16   0   10G  0 disk └─ceph--53398982--af97--4b05--9a0b--bf91741b7f6a-osd--block--68c09ae0--7d72--4984--b5c0--6f1476698c2b 253:2    0   10G  0 lvm  sdc                                                                                                     8:32   0   10G  0 disk sr0                                                                                                    11:0    1 1024M  0 rom  rbd0                                                                                                  252:0    0   10G  0 disk [root@node-1 my-cluster]# rbd device list #使用此命令进行查看id pool      namespace image        snap device    0  ceph-demo           rbd-demo.img -    /dev/rbd0[root@node-1 my-cluster]#  mkfs.xfs /dev/rbd0 #格式化为xfs格式Discarding blocks...Done.meta-data=/dev/rbd0              isize=512    agcount=16, agsize=163840 blks         =                       sectsz=512   attr=2, projid32bit=1         =                       crc=1        finobt=0, sparse=0data     =                       bsize=4096   blocks=2621440, imaxpct=25         =                       sunit=1024   swidth=1024 blksnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1log      =internal log           bsize=4096   blocks=2560, version=2         =                       sectsz=512   sunit=8 blks, lazy-count=1realtime =none                   extsz=4096   blocks=0, rtextents=0[root@node-1 my-cluster]# mkdir /data[root@node-1 my-cluster]# mount /dev/rbd0 /data #进行挂载[root@node-1 my-cluster]# df -HT #查看挂载情况Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   11M  944M   2% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.6G   16G  15% //dev/sda1               xfs       1.1G  190M  874M  18% /boottmpfs                   tmpfs     191M     0  191M   0% /run/user/0tmpfs                   tmpfs     954M   54k  954M   1% /var/lib/ceph/osd/ceph-0/dev/rbd0               xfs        11G   35M   11G   1% /data</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> RBD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RBD存储扩容/缩容</title>
      <link href="/2023/04/11/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%9D%97%E5%AD%98%E5%82%A8%E6%89%A9%E5%AE%B9%E5%8F%8A%E7%BC%A9%E5%AE%B9/"/>
      <url>/2023/04/11/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%9D%97%E5%AD%98%E5%82%A8%E6%89%A9%E5%AE%B9%E5%8F%8A%E7%BC%A9%E5%AE%B9/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#rbd%E5%AD%98%E5%82%A8%E6%89%A9%E5%AE%B9">RBD存储扩容</a></li></ul><!-- tocstop --><h1><span id="rbd存储扩容">RBD存储扩容</span></h1><p><strong>查看需要扩容的镜像</strong></p><pre><code>[root@node-1 data]# rbd ls ceph-demo rbd-demo.img</code></pre><p><strong>查看需要扩容的镜像</strong></p><pre><code>[root@node-1 data]# rbd -p ceph-demo info rbd-demo.img rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023</code></pre><p><strong>使用rbd resize进行扩容帮助</strong></p><pre><code>[root@node-1 data]# rbd help resize  #使用rbd resize进行扩容usage: rbd resize [--pool &lt;pool&gt;] [--namespace &lt;namespace&gt;]                   [--image &lt;image&gt;] --size &lt;size&gt; [--allow-shrink]                   [--no-progress]                   &lt;image-spec&gt; Resize (expand or shrink) image.Positional arguments  &lt;image-spec&gt;         image specification                       (example: [&lt;pool-name&gt;/[&lt;namespace&gt;/]]&lt;image-name&gt;)Optional arguments  -p [ --pool ] arg    pool name  --namespace arg      namespace name  --image arg          image name  -s [ --size ] arg    image size (in M/G/T) [default: M]  --allow-shrink       permit shrinking  #缩容操作，不建议，可能会导致数据丢失  --no-progress        disable progress output</code></pre><p><strong>将ceph-demo池下的rbd-demo.img镜像扩容到20G</strong></p><pre><code>[root@node-1 data]#  rbd resize ceph-demo/rbd-demo.img --size 20G Resizing image: 100% complete...done.[root@node-1 data]# rbd -p ceph-demo info rbd-demo.img #查看已经扩容到20G了rbd image &#39;rbd-demo.img&#39;:    size 20 GiB in 5120 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023</code></pre><p><strong>#将上述扩容到20G的硬盘缩容到15G</strong></p><pre><code>[root@node-1 data]#  rbd resize ceph-demo/rbd-demo.img --allow-shrink --size 15GResizing image: 100% complete...done.[root@node-1 data]# rbd -p ceph-demo info rbd-demo.imgrbd image &#39;rbd-demo.img&#39;:    size 15 GiB in 3840 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> RBD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RGW对象存储</title>
      <link href="/2023/04/11/Ceph/5.RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/"/>
      <url>/2023/04/11/Ceph/5.RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#ceph-%E5%AF%B9%E8%B1%A1%E7%BD%91%E5%85%B3%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D">CEPH 对象网关架构介绍</a></li><li><a href="#%E9%83%A8%E7%BD%B2rgw">部署RGW</a><ul><li><a href="#%E6%9F%A5%E7%9C%8B%E5%B8%AE%E5%8A%A9%E6%96%87%E6%A1%A3">查看帮助文档</a></li><li><a href="#%E4%BD%BF%E7%94%A8node-1%E4%BD%9C%E4%B8%BA%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3">使用node-1作为对象存储网关</a></li><li><a href="#%E6%A3%80%E6%9F%A5%E6%98%AF%E5%90%A6%E6%B7%BB%E5%8A%A0%E6%88%90%E5%8A%9F">检查是否添加成功</a></li><li><a href="#%E6%A3%80%E6%9F%A5%E6%9C%8D%E5%8A%A1%E8%BF%90%E8%A1%8C%E6%83%85%E5%86%B5">检查服务运行情况</a></li></ul></li><li><a href="#%E4%BF%AE%E6%94%B9rgw%E9%BB%98%E8%AE%A4%E7%AB%AF%E5%8F%A3">修改RGW默认端口</a><ul><li><a href="#%E4%BF%AE%E6%94%B9%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">修改节点配置文件</a></li><li><a href="#%E5%B0%86%E9%85%8D%E7%BD%AE%E5%A5%BD%E7%9A%84cephconf-%E6%8E%A8%E9%80%81%E5%88%B0%E6%89%80%E6%9C%89%E8%8A%82%E7%82%B9%E4%B8%AD">将配置好的ceph.conf 推送到所有节点中</a></li><li><a href="#%E9%87%8D%E5%90%AFrgw%E6%9C%8D%E5%8A%A1">重启rgw服务</a></li><li><a href="#%E6%A0%A1%E9%AA%8C%E9%85%8D%E7%BD%AE%E6%98%AF%E5%90%A6%E7%94%9F%E6%95%88-%E5%8F%AF%E4%BF%AE%E6%94%B9%E6%88%90443%E7%AB%AF%E5%8F%A3%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6%E5%8F%AF%E4%BD%BF%E7%94%A8%E7%99%BE%E5%BA%A6%E8%BF%9B%E8%A1%8C%E6%9F%A5%E8%AF%A2%E5%85%B7%E4%BD%93%E9%85%8D%E7%BD%AE">校验配置是否生效 可修改成443端口，添加证书可使用百度进行查询具体配置</a></li></ul></li><li><a href="#rgw%E4%B9%8Bs3%E6%8E%A5%E5%8F%A3%E4%BD%BF%E7%94%A8">RGW之s3接口使用</a><ul><li><a href="#%E6%9F%A5%E7%9C%8B%E5%B8%AE%E5%8A%A9%E6%96%87%E6%A1%A3%E4%BD%BF%E7%94%A8user-create%E8%BF%9B%E8%A1%8C%E5%88%9B%E5%BB%BA">查看帮助文档，使用user create进行创建</a></li><li><a href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%98%BE%E7%A4%BA%E5%90%8D%E4%B8%BAceph-s3-user-demo%E7%9A%84%E7%94%A8%E6%88%B7%E5%B9%B6%E6%8C%87%E5%AE%9Auid%E4%B8%BAceph-s3-user">创建一个显示名为Ceph S3 User Demo的用户，并指定uid为ceph-s3-user</a></li><li><a href="#%E4%BF%9D%E5%AD%98%E7%94%A8%E6%88%B7%E5%92%8C%E5%AF%86%E7%A0%81%E4%BF%A1%E6%81%AF">保存用户和密码信息</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8Dradosgw%E5%88%9B%E5%BB%BA%E7%9A%84%E7%94%A8%E6%88%B7">查看当前radosgw创建的用户</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E7%94%A8%E6%88%B7%E7%9A%84%E5%85%B7%E4%BD%93%E4%BF%A1%E6%81%AF">查看用户的具体信息</a></li><li><a href="#%E5%AE%89%E8%A3%85python-boto%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%8C%85">安装python-boto的软件包</a></li><li><a href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAs3-clientpy%E7%9A%84python%E8%84%9A%E6%9C%AC">创建一个s3-client.py的python脚本</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%9A%84pool">查看自动生成的pool</a></li><li><a href="#%E5%88%9B%E5%BB%BAceph-s3-bucket%E7%9A%84bucket">创建ceph-s3-bucket的bucket</a></li><li><a href="#%E5%86%8D%E6%AC%A1%E6%9F%A5%E7%9C%8B%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%9A%84pool">再次查看自动生成的pool</a></li><li><a href="#%E5%AE%89%E8%A3%85s3cmd%E7%9A%84%E5%B7%A5%E5%85%B7%E5%AE%9E%E7%8E%B0%E4%B8%8A%E4%BC%A0%E4%B8%8B%E8%BD%BD%E5%8A%9F%E8%83%BD">安装s3cmd的工具实现上传下载功能</a></li><li><a href="#%E9%85%8D%E7%BD%AEs3cmd%E7%9A%84%E5%B7%A5%E5%85%B7">配置s3cmd的工具</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E7%94%9F%E6%88%90%E7%9A%84%E6%96%87%E4%BB%B6">查看生成的文件</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8Dbuck%E6%83%85%E5%86%B5">查看当前buck情况</a></li><li><a href="#%E5%88%9B%E5%BB%BAbucket">创建bucket</a></li><li><a href="#%E6%9F%A5%E7%9C%8Bbucket">查看bucket</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6">上传文件</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E5%A4%B9">上传文件夹</a></li><li><a href="#%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6">下载文件</a></li><li><a href="#%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6">删除文件</a></li><li><a href="#%E5%88%A0%E9%99%A4%E7%9B%AE%E5%BD%95">删除目录</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E5%AD%98%E5%82%A8%E6%B1%A0%E5%8F%98%E5%8C%96">查看存储池变化</a></li></ul></li><li><a href="#rgw%E4%B9%8Bswift%E6%8E%A5%E5%8F%A3%E4%BD%BF%E7%94%A8">RGW之Swift接口使用</a><ul><li><a href="#%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8D%E7%94%A8%E6%88%B7">查看当前用户</a></li><li><a href="#%E6%A0%B9%E6%8D%AE%E5%BD%93%E5%89%8D%E7%94%A8%E6%88%B7%E5%88%9B%E5%BB%BAswift%E7%94%A8%E6%88%B7">根据当前用户创建swift用户</a></li><li><a href="#%E7%94%9F%E6%88%90swift%E7%9A%84key">生成Swift的key</a></li><li><a href="#%E5%AE%89%E8%A3%85swift%E5%AE%A2%E6%88%B7%E7%AB%AF">安装Swift客户端</a></li><li><a href="#%E6%9F%A5%E7%9C%8Bbucket-1">查看bucket</a></li><li><a href="#%E5%88%9B%E5%BB%BAswift%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E6%96%87%E4%BB%B6">创建Swift环境变量文件</a></li><li><a href="#%E9%AA%8C%E8%AF%81%E5%8F%98%E9%87%8F%E6%98%AF%E5%90%A6%E7%94%9F%E6%95%88">验证变量是否生效</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6-1">上传文件</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E7%9B%AE%E5%BD%95">上传目录</a></li><li><a href="#%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6-1">下载文件</a></li><li><a href="#%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6-1">删除文件</a></li></ul></li></ul><!-- tocstop --><h1><span id="ceph-对象网关架构介绍">CEPH 对象网关架构介绍</span></h1><p>Ceph 对象网关是建立在 librados. 它在应用程序和 Ceph 存储集群之间提供了一个 RESTful 网关。Ceph 对象存储支持两种接口：</p><ol><li>S3 兼容：通过与 Amazon S3 RESTful API 的大部分子集兼容的接口提供对象存储功能。</li><li>Swift 兼容：通过与 OpenStack Swift API 的大部分子集兼容的接口提供对象存储功能。</li></ol><p>Ceph 对象存储使用 Ceph 对象网关守护进程 ( radosgw)，这是一个设计用于与 Ceph 存储集群交互的 HTTP 服务器。</p><p>Ceph 对象网关提供与 Amazon S3 和 OpenStack Swift 兼容的接口，并且有自己的用户管理。Ceph 对象网关可以将数据存储在同一个 Ceph 存储集群中，其中存储了来自 Ceph 文件系统客户端和 Ceph 块设备客户端的数据。S3 API 和 Swift API 共享一个公共命名空间，这使得可以使用一个 API 将数据写入 Ceph 存储集群，然后使用另一个 API 检索该数据。</p><p><img src="/images/RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3/1.jpg"></p><blockquote><p>Ceph 对象存储不使用Ceph 元数据服务器。</p></blockquote><h1><span id="部署rgw">部署RGW</span></h1><h2><span id="查看帮助文档">查看帮助文档</span></h2><pre><code>[root@node-1 my-cluster]# ceph-deploy rgw -h  usage: ceph-deploy rgw [-h] &#123;create&#125; ...Ceph RGW daemon managementpositional arguments:  &#123;create&#125;    create    Create an RGW instanceoptional arguments:  -h, --help  show this help message and exit</code></pre><h2><span id="使用node-1作为对象存储网关">使用node-1作为对象存储网关</span></h2><pre><code>[root@node-1 my-cluster]# ceph-deploy rgw create [HOST[:NAME] ...]usage: ceph-deploy rgw create [-h] HOST[:NAME] [HOST[:NAME] ...][root@node-1 my-cluster]# ceph-deploy rgw create node-1</code></pre><h2><span id="检查是否添加成功">检查是否添加成功</span></h2><pre><code>[root@node-1 my-cluster]# ceph -s  #查看集群多了一个rgw的对象网关  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 32s)    mgr: node-1(active, since 18h), standbys: node-2, node-3    osd: 3 osds: 3 up (since 2h), 3 in (since 5h)    rgw: 1 daemon active (node-1)   task status:   data:    pools:   5 pools, 192 pgs    objects: 468 objects, 1.0 GiB    usage:   6.2 GiB used, 24 GiB / 30 GiB avail    pgs:     192 active+clean[root@node-1 my-cluster]# netstat -antupl | grep 7480 #查看监听端口tcp        0      0 0.0.0.0:7480            0.0.0.0:*               LISTEN      66157/radosgw       tcp6       0      0 :::7480                 :::*                    LISTEN      66157/radosgw [root@node-1 my-cluster]#  yum whatprovides &quot;*bin/netstat&quot; #如果netstat执行失败可查看是否没有安装net-toolsLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.aliyun.comnet-tools-2.0-0.25.20131004git.el7.x86_64 : Basic networking toolsRepo        : baseMatched from:Filename    : /bin/netstatnet-tools-2.0-0.25.20131004git.el7.x86_64 : Basic networking toolsRepo        : @baseMatched from:Filename    : /bin/netsta[root@node-1 my-cluster]#  yum install -y net-tools</code></pre><h2><span id="检查服务运行情况">检查服务运行情况</span></h2><pre><code>[root@node-1 my-cluster]# curl http://node-1:7480 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ListAllMyBucketsResult xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;</code></pre><h1><span id="修改rgw默认端口">修改RGW默认端口</span></h1><h2><span id="修改节点配置文件">修改节点配置文件</span></h2><p><strong>node-1节点执行：</strong></p><pre><code>[root@node-1 my-cluster]# cat ceph.conf [global]fsid = e9a90625-4707-4b6b-b52f-661512ea831dpublic_network = 192.168.187.0/24cluster_network = 192.168.199.0/24mon_initial_members = node-1mon_host = 192.168.187.201auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephx[client.rgw.node-1] #新增如下配置</code></pre><h2><span id="将配置好的cephconf-推送到所有节点中">将配置好的ceph.conf 推送到所有节点中</span></h2><blockquote><p>–overwrite-conf 由于其他节点已经有ceph.conf 文件，所以需要加此参数进行覆盖</p></blockquote><p>[root@node-1 my-cluster]# ceph-deploy –overwrite-conf config push node-1 node-2 node-3 </p><h2><span id="重启rgw服务">重启rgw服务</span></h2><pre><code>[root@node-1 my-cluster]#  systemctl restart ceph-radosgw.target</code></pre><h2><span id="校验配置是否生效-可修改成443端口添加证书可使用百度进行查询具体配置">校验配置是否生效 可修改成443端口，添加证书可使用百度进行查询具体配置</span></h2><pre><code>[root@node-1 my-cluster]# netstat -antupl | grep 80 tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      67161/radosgw</code></pre><h1><span id="rgw之s3接口使用">RGW之s3接口使用</span></h1><blockquote><p> <strong>使用对象存储的前提是需要创建用户，一种是S3风格的，一种是swift风格的</strong></p></blockquote><h2><span id="查看帮助文档使用user-create进行创建">查看帮助文档，使用user create进行创建</span></h2><pre><code>[root@node-1 my-cluster]# radosgw-admin -h | grep user   user create                create a new user  user modify                modify user  user info                  get user info  user rm                    remove user  user suspend               suspend a user  user enable                re-enable user after suspension  user check                 check user info  user stats                 show user stats as accounted by quota subsystem  user list                  list users  caps add                   add user capabilities  caps rm                    remove user capabilities  subuser create             create a new subuser  subuser modify             modify subuser  subuser rm                 remove subuser  bucket link                link bucket to specified user  bucket unlink              unlink bucket from specified user  usage show                 show usage (by user, by bucket, date range)  usage trim                 trim usage (by user, by bucket, date range)   --uid=&lt;id&gt;                user id   --subuser=&lt;name&gt;          subuser name   --email=&lt;email&gt;           user&#39;s email address   --access=&lt;access&gt;         Set access permissions for sub-user, should be one   --display-name=&lt;name&gt;     user&#39;s display name   --max-buckets             max number of buckets for a user   --admin                   set the admin flag on the user   --system                  set the system flag on the user   --op-mask                 set the op mask on the user   --purge-data              when specified, user removal will also purge all the                             user data   --purge-keys              when specified, subuser removal will also purge all the                             subuser keys   --sync-stats              option to &#39;user stats&#39;, update user stats with current                             stats reported by user&#39;s buckets indexes   --reset-stats             option to &#39;user stats&#39;, reset stats in accordance with user buckets   --caps=&lt;caps&gt;             list of caps (e.g., &quot;usage=read, write; user=read&quot;)   --quota-scope             scope of quota (bucket, user)  --setuser USER    set uid to user or uid (and gid to user&#39;s gid)</code></pre><h2><span id="创建一个显示名为ceph-s3-user-demo的用户并指定uid为ceph-s3-user">创建一个显示名为Ceph S3 User Demo的用户，并指定uid为ceph-s3-user</span></h2><pre><code>[root@node-1 my-cluster]# radosgw-admin user create --uid ceph-s3-user --display-name &quot;Ceph S3 User Demo&quot; &#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000, #最大只允许创建1000个buckets    &quot;subusers&quot;: [],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;, #用户ID            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;, #SSH的key            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot; #  secret的key        &#125;    ],    &quot;swift_keys&quot;: [],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123; #bucket配额信息        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123; #用户的配额信息        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="保存用户和密码信息">保存用户和密码信息</span></h2><pre><code>[root@node-1 my-cluster]# cat key.txt      &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],</code></pre><h2><span id="查看当前radosgw创建的用户">查看当前radosgw创建的用户</span></h2><pre><code>[root@node-1 my-cluster]#  radosgw-admin user list [    &quot;ceph-s3-user&quot;]</code></pre><h2><span id="查看用户的具体信息">查看用户的具体信息</span></h2><pre><code>[root@node-1 my-cluster]# radosgw-admin user info --uid ceph-s3-user &#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000,    &quot;subusers&quot;: [],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],    &quot;swift_keys&quot;: [],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="安装python-boto的软件包">安装python-boto的软件包</span></h2><pre><code>[root@node-1 my-cluster]# yum install python-boto</code></pre><h2><span id="创建一个s3-clientpy的python脚本">创建一个s3-client.py的python脚本</span></h2><pre><code>[root@node-1 my-cluster]# cat s3-client.pyimport botoimport boto.s3.connectionaccess_key = &#39;5SCX55201QUQ0FOPCPD6&#39; #上述保存的keysecret_key = &#39;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&#39; #上述保存的keyconn = boto.connect_s3(        aws_access_key_id = access_key,        aws_secret_access_key = secret_key,        host = &#39;192.168.187.201&#39;, port=80, #服务器的IP及端口        is_secure=False,calling_format=boto.s3.connection.OrdinaryCallingFormat(),        )bucket = conn.create_bucket(&#39;ceph-s3-bucket&#39;)for bucket in conn.get_all_buckets():        print(&quot;&#123;name&#125;\t&#123;created&#125;&quot;.format(                name = bucket.name,                created = bucket.creation_date,        ))</code></pre><h2><span id="查看自动生成的pool">查看自动生成的pool</span></h2><pre><code>[root@node-1 my-cluster]# ceph osd lspools1 ceph-demo2 .rgw.root3 default.rgw.control4 default.rgw.meta5 default.rgw.log</code></pre><h2><span id="创建ceph-s3-bucket的bucket">创建ceph-s3-bucket的bucket</span></h2><pre><code>[root@node-1 my-cluster]#  python  s3-client.py ceph-s3-bucket    2023-03-04T12:52:01.787Z</code></pre><h2><span id="再次查看自动生成的pool">再次查看自动生成的pool</span></h2><blockquote><p>执行后可以看到多了一个6 default.rgw.buckets.index的pool，写入数据后还会增加一个data的pool</p></blockquote><pre><code>[root@node-1 my-cluster]# ceph osd lspools1 ceph-demo2 .rgw.root #rgw的根pool3 default.rgw.control #rgw的配置pool4 default.rgw.meta #rgw的元数据pool5 default.rgw.log #rgw的日志pool 6 default.rgw.buckets.index #rgw的索引pool</code></pre><h2><span id="安装s3cmd的工具实现上传下载功能">安装s3cmd的工具实现上传下载功能</span></h2><p><code>[root@node-1 my-cluster]#  yum install s3cmd</code></p><h2><span id="配置s3cmd的工具">配置s3cmd的工具</span></h2><pre><code>[root@node-1 my-cluster]# s3cmd --configureEnter new values or accept defaults in brackets with Enter.Refer to user manual for detailed description of all options.Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.Access Key: 5SCX55201QUQ0FOPCPD6 #输入Secret Key: k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm #输入Default Region [US]: Use &quot;s3.amazonaws.com&quot; for S3 Endpoint and not modify it to the 回车target Amazon S3.S3 Endpoint [s3.amazonaws.com]: 192.168.187.201:80 #输入回车Use &quot;%(bucket)s.s3.amazonaws.com&quot; to the target Amazon S3. &quot;%(bucket)s&quot; and &quot;%(location)s&quot; vars can be usedif the target S3 system supports dns based buckets.DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: 192.168.187.201:80/%(bucket)s #输入回车Encryption password is used to protect your files from readingby unauthorized persons while in transfer to S3Encryption password: Path to GPG program [/usr/bin/gpg]: When using secure HTTPS protocol all communication with Amazon S3servers is protected from 3rd party eavesdropping. This method isslower than plain HTTP, and can only be proxied with Python 2.7 or newerUse HTTPS protocol [Yes]: no #输入回车On some networks all internet access must go through a HTTP proxy.Try setting it here if you can&#39;t connect to S3 directlyHTTP Proxy server name: New settings:  Access Key: 5SCX55201QUQ0FOPCPD6  Secret Key: k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm  Default Region: US  S3 Endpoint: 192.168.187.201:80  DNS-style bucket+hostname:port template for accessing a bucket: 192.168.187.201:80/%(bucket)s  Encryption password:   Path to GPG program: /usr/bin/gpg  Use HTTPS protocol: False  HTTP Proxy server name:   HTTP Proxy server port: 0Test access with supplied credentials? [Y/n] y #输入Please wait, attempting to list all buckets...Success. Your access key and secret key worked fine :-)Now verifying that encryption works...Not configured. Never mind.Save settings? [y/N] y #输入Configuration saved to &#39;/root/.s3cfg&#39;You have new mail in /var/spool/mail/root</code></pre><h2><span id="查看生成的文件">查看生成的文件</span></h2><pre><code>[root@node-1 my-cluster]# cat /root/.s3cfg[default]access_key = 5SCX55201QUQ0FOPCPD6access_token = add_encoding_exts = add_headers = bucket_location = USca_certs_file = cache_file = check_ssl_certificate = Truecheck_ssl_hostname = Truecloudfront_host = cloudfront.amazonaws.comconnection_max_age = 5connection_pooling = Truecontent_disposition = content_type = default_mime_type = binary/octet-streamdelay_updates = Falsedelete_after = Falsedelete_after_fetch = Falsedelete_removed = Falsedry_run = Falseenable_multipart = Trueencrypt = Falseexpiry_date = expiry_days = expiry_prefix = follow_symlinks = Falseforce = Falseget_continue = Falsegpg_command = /usr/bin/gpggpg_decrypt = %(gpg_command)s -d --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)sgpg_encrypt = %(gpg_command)s -c --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)sgpg_passphrase = guess_mime_type = Truehost_base = 192.168.187.201:80host_bucket = 192.168.187.201:80/%(bucket)shuman_readable_sizes = Falseinvalidate_default_index_on_cf = Falseinvalidate_default_index_root_on_cf = Trueinvalidate_on_cf = Falsekms_key = limit = -1limitrate = 0list_allow_unordered = Falselist_md5 = Falselog_target_prefix = long_listing = Falsemax_delete = -1mime_type = multipart_chunk_size_mb = 15multipart_copy_chunk_size_mb = 1024multipart_max_chunks = 10000preserve_attrs = Trueprogress_meter = Trueproxy_host = proxy_port = 0public_url_use_https = Falseput_continue = Falserecursive = Falserecv_chunk = 65536reduced_redundancy = Falserequester_pays = Falserestore_days = 1restore_priority = Standardsecret_key = k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldmsend_chunk = 65536server_side_encryption = Falsesignature_v2 = Falsesignurl_use_https = Falsesimpledb_host = sdb.amazonaws.comskip_existing = Falsesocket_timeout = 300ssl_client_cert_file = ssl_client_key_file = stats = Falsestop_on_error = Falsestorage_class = throttle_max = 100upload_id = urlencoding_mode = normaluse_http_expect = Falseuse_https = Falseuse_mime_magic = Trueverbosity = WARNINGwebsite_endpoint = http://%(bucket)s.s3-website-%(location)s.amazonaws.com/website_error = website_index = index.html</code></pre><h2><span id="查看当前buck情况">查看当前buck情况</span></h2><pre><code>[root@node-1 ~]# s3cmd ls 2023-03-04 12:52  s3://ceph-s3-bucket</code></pre><h2><span id="创建bucket">创建bucket</span></h2><blockquote><p>使用cmd创建一个叫做s3cmd-demo的bucket</p></blockquote><pre><code>[root@node-1 ~]# s3cmd mb s3://s3cmd-demo ERROR: S3 error: 403 (SignatureDoesNotMatch) #由于当前设置的版本不一致导致</code></pre><blockquote><p>修改版本</p></blockquote><pre><code>[root@node-1 ~]# vi /root/.s3cfg signature_v2 = True</code></pre><blockquote><p>重新创建</p></blockquote><pre><code>[root@node-1 ~]# s3cmd mb s3://s3cmd-demo Bucket &#39;s3://s3cmd-demo/&#39; created</code></pre><h2><span id="查看bucket">查看bucket</span></h2><pre><code>[root@node-1 ~]# s3cmd ls 2023-03-04 12:52  s3://ceph-s3-bucket2023-03-04 17:38  s3://s3cmd-demo</code></pre><h2><span id="上传文件">上传文件</span></h2><pre><code>[root@node-1 ~]#  s3cmd put /etc/fstab s3://s3cmd-demo/fastab-demo #报错，经查询最简单的解决办法是添加OSD或者修改ceph.conf 文件upload: &#39;/etc/fstab&#39; -&gt; &#39;s3://s3cmd-demo/fastab-demo&#39;  [1 of 1] 465 of 465   100% in    0s   866.93 B/s  doneERROR: S3 error: 416 (InvalidRange)</code></pre><blockquote><p>报错，经查询最简单的解决办法是添加OSD或者修改ceph.conf 文件,原因是当前OSD数量太少；</p></blockquote><blockquote><p>添加OSD</p></blockquote><pre><code>[root@node-1 my-cluster]# ceph-deploy osd create node-1 --data /dev/sdc --journal /dev/sdc[root@node-1 my-cluster]# ceph-deploy osd create node-2 --data /dev/sdc --journal /dev/sdc[root@node-1 my-cluster]# ceph-deploy osd create node-3 --data /dev/sdc --journal /dev/sdc</code></pre><blockquote><p>尝试重新上传</p></blockquote><pre><code>[root@node-1 my-cluster]#  s3cmd put /etc/fstab s3://s3cmd-demo/fastab-demoupload: &#39;/etc/fstab&#39; -&gt; &#39;s3://s3cmd-demo/fastab-demo&#39;  [1 of 1] 465 of 465   100% in    1s   317.00 B/s  done</code></pre><blockquote><p>查看上传的文件</p></blockquote><pre><code>[root@node-1 my-cluster]# s3cmd ls s3://s3cmd-demo 2023-03-04 17:47          465  s3://s3cmd-demo/fastab-demo</code></pre><h2><span id="上传文件夹">上传文件夹</span></h2><blockquote><p>–recursive 上传目录需要使用递归的方式</p></blockquote><p><code>[root@node-1 my-cluster]# s3cmd put /etc s3://s3cmd-demo/etc/ --recursive </code></p><blockquote><p>查看上传结果</p></blockquote><pre><code>[root@node-1 my-cluster]# s3cmd ls s3://s3cmd-demo/etc/etc/                          DIR  s3://s3cmd-demo/etc/etc/NetworkManager/                          DIR  s3://s3cmd-demo/etc/etc/X11/                          DIR  s3://s3cmd-demo/etc/etc/audisp/                          DIR  s3://s3cmd-demo/etc/etc/audit/                          DIR  s3://s3cmd-demo/etc/etc/bash_completion.d/                          DIR  s3://s3cmd-demo/etc/etc/ceph/                          DIR  s3://s3cmd-demo/etc/etc/cron.d/</code></pre><h2><span id="下载文件">下载文件</span></h2><blockquote><p>#下载文件并重命名为yum.conf-download</p></blockquote><pre><code>[root@node-1 ~]# s3cmd get s3://s3cmd-demo/etc/etc/yum.conf yum.conf-download download: &#39;s3://s3cmd-demo/etc/etc/yum.conf&#39; -&gt; &#39;yum.conf-download&#39;  [1 of 1] 970 of 970   100% in    0s    22.27 KB/s  done</code></pre><h2><span id="删除文件">删除文件</span></h2><pre><code>[root@node-1 ~]#  s3cmd rm s3://s3cmd-demo/fastab-demo #删除操作delete: &#39;s3://s3cmd-demo/fstab-demo&#39;</code></pre><h2><span id="删除目录">删除目录</span></h2><p><code>s3cmd rm s3://s3cmd-demo/etc --recursive</code></p><h2><span id="查看存储池变化">查看存储池变化</span></h2><blockquote><p>完成上述操作后ceph会多出一个default.rgw.buckets.data的pool</p></blockquote><pre><code>[root@node-1 ~]# ceph osd lspools1 ceph-demo2 .rgw.root3 default.rgw.control4 default.rgw.meta5 default.rgw.log6 default.rgw.buckets.index7 default.rgw.buckets.data #rgw的数据pool</code></pre><blockquote><p>查看内容</p></blockquote><pre><code>[root@node-1 ~]# rados -p default.rgw.buckets.data ls 3db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_23db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_1</code></pre><blockquote><p>#重新上传文件</p></blockquote><p><code>root@node-1 ~]#  s3cmd put /etc/fstab s3://s3cmd-demo/fastab-demo</code></p><blockquote><p>再次查看内容</p></blockquote><pre><code>[root@node-1 ~]# rados -p default.rgw.buckets.data ls  #可以看到多了一个3db4c650-9c23-4843-8015-97b79566b3b0.5970.2_fastab-demo的对象3db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_23db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_13db4c650-9c23-4843-8015-97b79566b3b0.5970.2_fastab-demo #可以看到这个文件多了写前缀，前缀是存放在default.rgw.buckets.index的pool中的</code></pre><blockquote><p>查看索引</p></blockquote><pre><code>[root@node-1 ~]# rados -p default.rgw.buckets.index ls .dir.3db4c650-9c23-4843-8015-97b79566b3b0.5970.2</code></pre><h1><span id="rgw之swift接口使用">RGW之Swift接口使用</span></h1><blockquote><p><strong>需要在原有用户的基础上进行添加</strong></p></blockquote><h2><span id="查看当前用户">查看当前用户</span></h2><pre><code>[root@node-1 ~]# radosgw-admin user list [    &quot;ceph-s3-user&quot;]</code></pre><h2><span id="根据当前用户创建swift用户">根据当前用户创建swift用户</span></h2><pre><code>[root@node-1 ~]# radosgw-admin subuser create --uid ceph-s3-user --subuser=ceph-s3-user:swift --access=full&#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000,    &quot;subusers&quot;: [        &#123;            &quot;id&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;permissions&quot;: &quot;full-control&quot;        &#125;    ],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],    &quot;swift_keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;secret_key&quot;: &quot;9sWyeIFQphNButuZp2zNs2t7xU9qEkgW27yHxqBL&quot;        &#125;    ],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="生成swift的key">生成Swift的key</span></h2><pre><code>[root@node-1 ~]# radosgw-admin key create --subuser=ceph-s3-user:swift --key-type=swift --gen-secret &#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000,    &quot;subusers&quot;: [        &#123;            &quot;id&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;permissions&quot;: &quot;full-control&quot;        &#125;    ],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],    &quot;swift_keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;secret_key&quot;: &quot;mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjh&quot; #提前保存        &#125;    ],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="安装swift客户端">安装Swift客户端</span></h2><blockquote><p>安装Swift客户端需要先安装pip</p></blockquote><pre><code>[root@node-1 ~]# yum install python-pip #安装pip 由于当前pip版本较老（8.1），无法通过pip install -U pip 进行升级，只能手动进行升级[root@node-1 ~]wget https://files.pythonhosted.org/packages/0b/f5/be8e741434a4bf4ce5dbc235aa28ed0666178ea8986ddc10d035023744e6/pip-20.2.4.tar.gz  #下载安装包[root@node-1 ~]tar -zxvf pip-20.2.4.tar.gz  # 解压[root@node-1 ~]cd pip-20.2.4/[root@node-1 ~]sudo python setup.py install #给予权限不然可能安装失败[root@node-1 ~]pip install -U pip #再次更新DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.Collecting pip  Downloading pip-20.3.4-py2.py3-none-any.whl (1.5 MB)     |████████████████████████████████| 1.5 MB 1.0 MB/s Installing collected packages: pip  Attempting uninstall: pip    Found existing installation: pip 20.2.4    Uninstalling pip-20.2.4:      Successfully uninstalled pip-20.2.4Successfully installed pip-20.3.4</code></pre><blockquote><p>再次安装Swift客户端</p></blockquote><pre><code>[root@node-1 pip-20.2.4]# pip install  python-swiftclient #安装成功DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.Collecting python-swiftclient  Downloading python_swiftclient-3.13.1-py2.py3-none-any.whl (87 kB)     |████████████████████████████████| 87 kB 6.4 kB/s Collecting futures&gt;=3.0.0; python_version == &quot;2.7&quot;  Downloading futures-3.4.0-py2-none-any.whl (16 kB)Requirement already satisfied: requests&gt;=1.1.0 in /usr/lib/python2.7/site-packages (from python-swiftclient) (2.6.0)Requirement already satisfied: six&gt;=1.9.0 in /usr/lib/python2.7/site-packages (from python-swiftclient) (1.9.0)Installing collected packages: futures, python-swiftclientSuccessfully installed futures-3.4.0 python-swiftclient-3.13.1</code></pre><h2><span id="查看bucket">查看bucket</span></h2><blockquote><p>查看，list可换成upload或download实现上传下载</p></blockquote><pre><code>[root@node-1 ~]# swift -A http://192.168.187.201:80/auth -U ceph-s3-user:swift -K mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjh list ceph-s3-buckets3cmd-demo</code></pre><h2><span id="创建swift环境变量文件">创建Swift环境变量文件</span></h2><pre><code>[root@node-1 ~]# cat swift-openrc.shexport ST_AUTH=http://192.168.187.201:80/authexport ST_USER=ceph-s3-user:swiftexport ST_KEY=mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjh</code></pre><h2><span id="验证变量是否生效">验证变量是否生效</span></h2><pre><code>[root@node-1 ~]# set |grep ST #查看环境变量是否生效DIRSTACK=()HISTCONTROL=ignoredupsHISTFILE=/root/.bash_historyHISTFILESIZE=1000HISTSIZE=1000HOSTNAME=node-1HOSTTYPE=x86_64OSTYPE=linux-gnuPIPESTATUS=([0]=&quot;0&quot;)SELINUX_LEVEL_REQUESTED=SELINUX_ROLE_REQUESTED=ST_AUTH=http://192.168.187.201:80/authST_KEY=mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjhexport[root@node-1 ~]# swift list  #验证变量成功ceph-s3-buckets3cmd-demo</code></pre><h2><span id="上传文件">上传文件</span></h2><p><code>[root@node-1 ~]# swift upload swift-demo /etc/shadow </code></p><h2><span id="上传目录">上传目录</span></h2><p><code>[root@node-1 ~]# swift upload swift-demo /etc/ </code></p><h2><span id="下载文件">下载文件</span></h2><pre><code>[root@node-1 ~]# swift download swift-demo etc/passwdetc/passwd [auth 0.008s, headers 0.011s, total 0.012s, 0.278 MB/s]</code></pre><p>##查看详细信息</p><pre><code>[root@node-1 ~]# swift stat swift-demo                      Account: v1                    Container: swift-demo                      Objects: 1694                        Bytes: 35940093                     Read ACL:                    Write ACL:                      Sync To:                     Sync Key:X-Container-Bytes-Used-Actual: 40927232                Accept-Ranges: bytes             X-Storage-Policy: default-placement              X-Storage-Class: STANDARD                Last-Modified: Sat, 04 Mar 2023 19:14:33 GMT                  X-Timestamp: 1677957085.09881                   X-Trans-Id: tx0000000000000000000d1-0064043339-85b3-default                 Content-Type: text/plain; charset=utf-8       X-Openstack-Request-Id: tx0000000000000000000d1-0064043339-85b3-default</code></pre><h2><span id="删除文件">删除文件</span></h2><blockquote><p>#删除文件，不要使用 swift delete -a 会删除所有bucket</p></blockquote><pre><code>[root@node-1 ~]# swift delete swift-demo etc/shadow #删除文件，不要使用 swift delete -a 会删除所有bucketetc/shadow[root@node-1 ~]# swift list swift-demo</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> RGW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph FS文件存储</title>
      <link href="/2023/04/11/Ceph/6.Ceph_%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/Ceph_%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"/>
      <url>/2023/04/11/Ceph/6.Ceph_%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/Ceph_%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E5%AE%89%E8%A3%85mds%E6%9C%8D%E5%8A%A1">安装MDS服务</a></li><li><a href="#%E5%88%9B%E5%BB%BAceph-fs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F">创建Ceph FS文件系统</a></li><li><a href="#%E6%9F%A5%E7%9C%8Bfs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F">查看fs文件系统</a></li><li><a href="#%E5%86%85%E6%A0%B8%E6%8C%82%E8%BD%BD">内核挂载</a></li><li><a href="#%E7%94%A8%E6%88%B7%E6%80%81%E6%8C%82%E8%BD%BD">用户态挂载</a></li></ul><!-- tocstop --><p>Ceph FS文件存储时由NAS衍生过来的。</p><p>Ceph 文件系统或CephFS是一个 POSIX 兼容的文件系统，构建在 Ceph 的分布式对象存储RADOS之上。CephFS致力于为各种应用程序提供最先进、多用途、高可用性和高性能的文件存储，包括共享主目录、HPC 暂存空间和分布式工作流共享存储等传统用例。</p><p>CephFS 通过使用一些新颖的架构选择来实现这些目标。值得注意的是，文件元数据存储在与文件数据不同的 RADOS 池中，并通过可调整大小的元数据服务器集群或MDS提供服务，它可以扩展以支持更高吞吐量的元数据工作负载。文件系统的客户端可以直接访问 RADOS 来读写文件数据块。因此，工作负载可能会随着底层 RADOS 对象存储的大小线性扩展；也就是说，没有网关或代理为客户端调解数据 I&#x2F;O。</p><p>对数据的访问通过 MDS 集群进行协调，MDS 集群充当由客户端和 MDS 共同维护的分布式元数据缓存状态的权威。每个 MDS 将元数据的变化聚合成一系列高效写入 RADOS 上的日志；MDS 没有在本地存储任何元数据状态。该模型允许在 POSIX 文件系统的上下文中在客户端之间进行连贯和快速的协作。</p><p><img src="/images/Ceph_FS%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/1.jpg"></p><p>CephFS 因其新颖的设计和对文件系统研究的贡献而成为众多学术论文的主题。它是 Ceph 中最古老的存储接口，曾经是 RADOS的主要用例。现在它与另外两个存储接口一起组成了一个现代统一存储系统：RBD（Ceph 块设备）和 RGW（Ceph 对象存储网关）。</p><h1><span id="安装mds服务">安装MDS服务</span></h1><p>在三个节点上同时部署MDS</p><pre><code>[root@node-1 my-cluster]# ceph-deploy mds create node-1 node-2 node-3</code></pre><p>查看当前MDS状态，因为当前没有文件系统，所以3台主机都是备状态</p><pre><code>[root@node-1 my-cluster]#  3 up:standby </code></pre><h1><span id="创建ceph-fs文件系统">创建Ceph FS文件系统</span></h1><p>创建pool</p><pre><code>[root@node-1 my-cluster]# ceph osd pool create cephfs_data 16 16[root@node-1 my-cluster]# ceph osd pool create cephfs_metadata 16 16</code></pre><p>查看使用帮助</p><pre><code>[root@node-1 my-cluster]# ceph -h|grep fs |grep new fs new &lt;fs_name&gt; &lt;metadata&gt; &lt;data&gt; &#123;--force&#125; &#123;--allow-dangerous-make new filesystem using named pools &lt;metadata&gt; and &lt;data&gt;</code></pre><p>创建一个fs文件系统</p><pre><code>[root@node-1 my-cluster]# ceph fs new cephfs-demo cephfs_metadata cephfs_data  new fs with metadata pool 9 and data pool 8</code></pre><h1><span id="查看fs文件系统">查看fs文件系统</span></h1><pre><code>[root@node-1 my-cluster]# ceph fs ls name: cephfs-demo, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</code></pre><p>查看集群状态</p><pre><code>[root@node-1 my-cluster]#  ceph -s  #可以看到MDS状态已经又一个active了  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            1 pools have too many placement groups            too many PGs per OSD (368 &gt; max 250)   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 4m)    mgr: node-2(active, since 10m), standbys: node-3, node-1    mds: cephfs-demo:1 &#123;0=node-2=up:active&#125; 2 up:standby    osd: 6 osds: 6 up (since 4m), 6 in (since 14h)    rgw: 1 daemon active (node-1)   task status:   data:    pools:   9 pools, 736 pgs    objects: 534 objects, 1.0 GiB    usage:   9.6 GiB used, 50 GiB / 60 GiB avail    pgs:     736 active+clean</code></pre><h1><span id="内核挂载">内核挂载</span></h1><p>创建一个挂载点</p><pre><code>[root@node-1 my-cluster]# mkdir /mnt/cephfs #创建一个挂载点</code></pre><p>使用命令进行挂载</p><pre><code>[root@node-1 my-cluster]# which mount.ceph #使用这个命令进行挂载/usr/sbin/mount.ceph [root@node-1 my-cluster]# rpm -qf /usr/sbin/mount.ceph #安装ceph-common的时候添加的命令ceph-common-14.2.22-0.el7.x86_64[root@node-1 my-cluster]# mount -t ceph 192.168.187.201:6789:/ /mnt/cephfs/ -o name=admin #挂载</code></pre><p>查看挂载</p><pre><code>[root@node-1 my-cluster]# df -HT # 查看挂载Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   11M  944M   2% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.6G   16G  14% //dev/sda1               xfs       1.1G  204M  860M  20% /boottmpfs                   tmpfs     954M   25k  954M   1% /var/lib/ceph/osd/ceph-0tmpfs                   tmpfs     954M   25k  954M   1% /var/lib/ceph/osd/ceph-3tmpfs                   tmpfs     191M     0  191M   0% /run/user/0/dev/rbd0               xfs        11G  1.2G  9.7G  11% /data192.168.187.201:6789:/  ceph       17G     0   17G   0% /mnt/cephfs [root@node-1 my-cluster]# lsmod | grep ceph #会自动添加ceph的模块，此方法性能比较高ceph                  363016  1 libceph               314775  2 rbd,cephdns_resolver           13140  1 libcephlibcrc32c              12644  2 xfs,libceph</code></pre><h1><span id="用户态挂载">用户态挂载</span></h1><p>安装软件包</p><pre><code>[root@node-1 my-cluster]#  yum install ceph-fuse -y</code></pre><p>查看帮助文档</p><pre><code>[root@node-1 my-cluster]# ceph-fuse -h #查看使用帮助usage: ceph-fuse [-n client.username] [-m mon-ip-addr:mon-port] &lt;mount point&gt; [OPTIONS]  --client_mountpoint/-r &lt;sub_directory&gt;                    use sub_directory as the mounted root, rather than the full Ceph tree.usage: ceph-fuse mountpoint [options]general options:    -o opt,[opt...]        mount options    -h   --help            print help    -V   --version         print versionFUSE options:    -d   -o debug          enable debug output (implies -f)    -f                     foreground operation    -s                     disable multi-threaded operation  --conf/-c FILE    read configuration from the given configuration file  --id ID           set ID portion of my name  --name/-n TYPE.ID set name  --cluster NAME    set cluster name (default: ceph)  --setuser USER    set uid to user or uid (and gid to user&#39;s gid)  --setgroup GROUP  set gid to group or gid  --version         show version and quit</code></pre><p>挂载</p><p>可以看到与内核态挂载的地点文件是共享的 </p><pre><code>[root@node-1 my-cluster]# ceph-fuse -n client.admin -m 192.168.187.201:6789,192.168.187.202:6789,192.168.187.203:6789 /mnt/ceph-fuse/ #使用ceph-fuse进行挂载，-n 指定用户名 -m 指定mon节点，可指定1个或者多个，或者不指定，最后写上挂载地点ceph-fuse[205722023-03-05 17:10:28.202 7f6b40549f80 -1 init, newargv = 0x5623b0d084e0 newargc=9]: starting ceph clientceph-fuse[20572]: starting fuse</code></pre><p>确定挂载情况</p><pre><code>[root@node-1 my-cluster]# cd /mnt/ceph-fuse/ [root@node-1 ceph-fuse]# ls[root@node-1 ceph-fuse]# lltotal 0[root@node-1 ceph-fuse]# [root@node-1 ceph-fuse]# ls[root@node-1 ceph-fuse]# echo aaa &gt; test[root@node-1 ceph-fuse]# lstest[root@node-1 ceph-fuse]# ls /mnt/cephfstest</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Ceph FS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph简介</title>
      <link href="/2023/04/10/Ceph/2.Ceph%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84/Ceph-%E7%AE%80%E4%BB%8B/"/>
      <url>/2023/04/10/Ceph/2.Ceph%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84/Ceph-%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#ceph-%E7%AE%80%E4%BB%8B">CEPH 简介</a><ul><li><a href="#%E5%BB%BA%E8%AE%AE">建议</a></li><li><a href="#%E7%A1%AC%E4%BB%B6%E6%8E%A8%E8%8D%90">硬件推荐</a><ul><li><a href="#cpu">CPU</a></li><li><a href="#%E5%86%85%E5%AD%98">内存</a></li><li><a href="#%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8">数据存储</a></li><li><a href="#%E7%BD%91%E7%BB%9C">网络</a></li><li><a href="#%E6%95%85%E9%9A%9C%E5%9F%9F">故障域</a></li></ul></li><li><a href="#%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%BB%BA%E8%AE%AE">操作系统建议</a><ul><li><a href="#ceph-%E4%BE%9D%E8%B5%96%E9%A1%B9">CEPH 依赖项</a></li><li><a href="#%E5%B9%B3%E5%8F%B0">平台</a></li></ul></li></ul></li></ul><!-- tocstop --><h1><span id="ceph-简介">CEPH 简介</span></h1><p>无论您是想为云平台提供Ceph 对象存储和&#x2F;或 Ceph 块设备服务、部署Ceph 文件系统还是将 Ceph 用于其他目的，所有 Ceph 存储集群部署都从设置每个 Ceph 节点、您的网络和 Ceph存储集群。一个 Ceph 存储集群至少需要一个 Ceph Monitor、Ceph Manager 和 Ceph OSD（Object Storage Daemon）。运行 Ceph 文件系统客户端时也需要 Ceph 元数据服务器。<br><img src="/images/Ceph%E7%AE%80%E4%BB%8B/1.jpg"></p><ul><li>Monitors：Ceph Monitor ( ceph-mon) 维护集群状态图，包括监视器图、管理器图、OSD 图、MDS 图和 CRUSH 图。这些映射是 Ceph 守护进程相互协调所需的关键集群状态。监视器还负责管理守护进程和客户端之间的身份验证。通常至少需要三个监视器才能实现冗余和高可用性。</li><li>管理器：Ceph 管理器守护进程 ( ceph-mgr) 负责跟踪运行时指标和 Ceph 集群的当前状态，包括存储利用率、当前性能指标和系统负载。Ceph Manager 守护进程还托管基于 python 的模块来管理和公开 Ceph 集群信息，包括基于 Web 的Ceph Dashboard和 REST API。高可用性通常至少需要两个管理器。</li><li>Ceph OSDs：一个对象存储守护进程（Ceph OSD， ceph-osd）存储数据，处理数据复制，恢复，重新平衡，并通过检查其他 Ceph OSD 守护进程的心跳向 Ceph Monitors 和 Managers 提供一些监控信息。通常至少需要三个 Ceph OSD 才能实现冗余和高可用性。</li><li>MDS：Ceph 元数据服务器（MDS ceph-mds）代表Ceph 文件系统存储元数据（即 Ceph 块设备和 Ceph 对象存储不使用 MDS）。Ceph 元数据服务器允许 POSIX 文件系统用户执行基本命令（如 ls、find等），而不会给 Ceph 存储集群带来巨大负担。</li></ul><p>Ceph 将数据作为对象存储在逻辑存储池中。使用 CRUSH算法，Ceph 计算出哪个归置组 (PG) 应该包含该对象，以及哪个 OSD 应该存储该归置组。CRUSH 算法使 Ceph 存储集群能够动态扩展、重新平衡和恢复。</p><h2><span id="建议">建议</span></h2><p>要开始在生产中使用 Ceph，您应该查看我们的硬件建议和操作系统建议。</p><ul><li>硬件推荐<ul><li>中央处理器</li><li>内存</li><li>记忆</li><li>数据存储</li><li>网络</li><li>故障域</li><li>最低硬件建议</li></ul></li><li>操作系统建议<ul><li>Ceph 依赖项</li><li>平台</li></ul></li></ul><h2><span id="硬件推荐">硬件推荐</span></h2><p>Ceph 被设计为在商用硬件上运行，这使得构建和维护 PB 级数据集群在经济上是可行的。在规划集群硬件时，您需要平衡许多考虑因素，包括故障域和潜在的性能问题。硬件规划应该包括在许多主机上分布 Ceph 守护进程和其他使用 Ceph 的进程。通常，我们建议在为特定类型的守护进程配置的主机上运行特定类型的 Ceph 守护进程。我们建议将其他主机用于利用您的数据集群的进程（例如，OpenStack、CloudStack 等）。<br>也可以查看<a href="https://ceph.com/community/blog/">Ceph 博客。</a></p><h3><span id="cpu">CPU</span></h3><p>CephFS 元数据服务器 (MDS) 是 CPU 密集型的。因此，CephFS 元数据服务器 (MDS) 应具有四核（或更好）CPU 和高时钟频率 (GHz)。OSD 节点需要足够的处理能力来运行 RADOS 服务、使用 CRUSH 计算数据放置、复制数据以及维护它们自己的集群映射副本。</p><blockquote><p><strong>一个 Ceph 集群的要求与另一个集群的要求不同，但这里有一些通用准则。</strong></p></blockquote><p>在 Ceph 的早期版本中，我们会根据每个 OSD 的核心数来提出硬件建议，但这个每个 OSD 的核心数指标不再像每个 IOP 的周期数和每个 OSD 的 IOP 数一样有用。例如，对于 NVMe 驱动器，Ceph 可以轻松地在真实集群上使用五个或六个内核，并在单个 OSD 上隔离使用多达大约十四个内核。因此，每个 OSD 的核心不再像以前那样紧迫。选择硬件时，选择每个内核的 IOP。</p><p>监控节点和管理器节点对 CPU 的要求不高，只需要适度的处理器。如果您的主机除了运行 Ceph 守护进程外还将运行 CPU 密集型进程，请确保您有足够的处理能力来运行 CPU 密集型进程和 Ceph 守护进程。（OpenStack Nova 是 CPU 密集型进程的一个例子。）我们建议您在单独的主机上（即，在不是您的监视器和管理器节点的主机上）运行非 Ceph CPU 密集型进程，以避免占用资源争论。</p><h3><span id="内存">内存</span></h3><p>通常，RAM 越大越好。适度集群的监视器&#x2F;管理器节点可能使用 64GB 就可以了；对于拥有数百个 OSD 的更大集群，128GB 是一个合理的目标。BlueStore OSD 的内存目标默认为 4GB。考虑到操作系统和管理任务（如监控和指标）的谨慎余量以及恢复期间增加的消耗：建议为每个 BlueStore OSD 预配 ~8GB。</p><h4><span id="监视器和管理器ceph-mon-和-ceph-mgr">监视器和管理器（CEPH-MON 和 CEPH-MGR）</span></h4><p>监视器和管理器守护进程的内存使用量通常随集群的大小而变化。请注意，在启动时以及拓扑更改和恢复期间，这些守护程序将需要比稳态操作期间更多的 RAM，因此请计划使用高峰期。对于非常小的集群，32 GB 就足够了。对于多达 300 个 OSD 的集群，需要 64GB</p><h4><span id="元数据服务器-ceph-mds">元数据服务器 (CEPH-MDS)</span></h4><p>元数据守护程序内存利用率取决于其缓存配置为消耗的内存量。对于大多数系统，我们建议至少使用 1 GB。 看mds_cache_memory。</p><h4><span id="记忆">记忆</span></h4><p>Bluestore 使用自己的内存来缓存数据，而不是依赖操作系统的页面缓存。在 Bluestore 中，您可以通过更改配置选项来调整 OSD 尝试消耗的内存量osd_memory_target 。</p><ul><li>通常不建议设置osd_memory_target低于 2GB（Ceph 可能无法将内存消耗保持在 2GB 以下，这可能会导致性能极慢）。</li><li>将内存目标设置在 2GB 和 4GB 之间通常可行，但可能会导致性能下降，因为元数据可能会在 IO 期间从磁盘读取，除非活动数据集相对较小。</li><li>4GB 是当前的默认osd_memory_target大小。此默认值是为典型用例选择的，旨在平衡内存需求和 OSD 性能。</li><li>osd_memory_target当有很多（小）对象或处理大（256GB&#x2F;OSD 或更多）数据集时，设置高于 4GB 可以提高性能。</li></ul><blockquote><p>OSD 内存自动调整是“尽力而为”。虽然 OSD 可以取消映射内存以允许内核回收它，但不能保证内核会在特定时间范围内实际回收释放的内存。这尤其适用于旧版本的 Ceph，其中透明大页面可以防止内核回收从碎片大页面中释放的内存。现代版本的 Ceph 在应用程序级别禁用透明大页面以避免这种情况，尽管这仍然不能保证内核会立即回收未映射的内存。OSD 有时仍可能超出其内存目标。我们建议在您的系统上预算大约 20% 的额外内存，以防止 OSD 在临时峰值期间或由于内核回收已释放页面的任何延迟而出现 OOM。</p></blockquote><p>使用旧版 FileStore 后端时，页面缓存用于缓存数据，因此通常不需要调整。<br>使用旧版 FileStore 后端时，OSD 内存消耗与系统中每个守护进程的 PG 数量有关。</p><h3><span id="数据存储">数据存储</span></h3><p>仔细规划您的数据存储配置。在规划数据存储时，需要考虑显着的成本和性能权衡。同时进行的操作系统操作以及多个守护进程对单个驱动器的读写操作的同时请求会大大降低性能。</p><h4><span id="硬盘驱动器">硬盘驱动器</span></h4><p>OSD 应该有足够的硬盘驱动器空间来存储对象数据。我们建议最小硬盘驱动器大小为 1 TB。考虑更大磁盘的每 GB 成本优势。我们建议将硬盘驱动器的价格除以千兆字节数，得出每千兆字节的成本，因为更大的驱动器可能会对每千兆字节的成本产生重大影响。例如，价格为 75.00 美元的 1 TB 硬盘的成本为每 GB 0.07 美元（即 75 美元 &#x2F; 1024 &#x3D; 0.0732）。相比之下，价格为 150.00 美元的 3 TB 硬盘的成本为每 GB 0.05 美元（即 150 美元 &#x2F; 3072 &#x3D; 0.0488）。在前面的示例中，使用 1 TB 的磁盘通常会使每 GB 的成本增加 40%——使您的集群的成本效率大大降低。</p><blockquote><p>在单个 SAS &#x2F; SATA 驱动器上运行多个 OSD 不是一个好主意。但是，NVMe 驱动器可以通过拆分为两个或更多 OSD 来提高性能。<br>在单个驱动器上运行 OSD 和监视器或元数据服务器也不是一个好主意。</p></blockquote><p>存储驱动器受寻道时间、访问时间、读写时间以及总吞吐量的限制。这些物理限制会影响整体系统性能，尤其是在恢复期间。我们建议为操作系统和软件使用专用（最好是镜像）驱动器，并为主机上运行的每个 Ceph OSD 守护进程使用一个驱动器（上面的模数 NVMe）。许多不是由硬件故障引起的“慢 OSD”问题是由于在同一驱动器上运行操作系统和多个 OSD 而引起的。由于在小型集群上解决性能问题的成本可能超过额外磁盘驱动器的成本，因此您可以通过避免让 OSD 存储驱动器负担过重的诱惑来优化您的集群设计规划。</p><blockquote><p>您可以在每个 SAS &#x2F; SATA 驱动器上运行多个 Ceph OSD 守护进程，但这可能会导致资源争用并降低整体吞吐量。</p></blockquote><h4><span id="态硬盘">态硬盘</span></h4><p>性能改进的一个机会是使用固态驱动器 (SSD) 来减少随机访问时间和读取延迟，同时加快吞吐量。与硬盘驱动器相比，SSD 每 GB 的成本通常是硬盘驱动器的 10 倍以上，但 SSD 的访问时间通常至少比硬盘驱动器快 100 倍。<br>SSD 没有移动机械部件，因此它们不一定受到与硬盘驱动器相同类型的限制。SSD 确实有很大的局限性。在评估 SSD 时，重要的是要考虑顺序读写的性能。</p><blockquote><p>我们建议探索使用 SSD 来提高性能。但是，在对 SSD 进行重大投资之前，我们强烈建议查看 SSD 的性能指标并在测试配置中测试 SSD 以衡量性能。</p></blockquote><p>相对便宜的 SSD 可能会吸引您的经济意识。谨慎使用。选择用于 Ceph 的 SSD 时，可接受的 IOPS 是不够的。</p><p>SSD 在历史上一直是对象存储的成本高昂，但新兴的 QLC 驱动器正在缩小差距。通过将 WAL+DB 卸载到 SSD，HDD OSD 可能会看到显着的性能提升。</p><p>Ceph 加速 CephFS 文件系统性能的一种方法是将 CephFS 元数据的存储与 CephFS 文件内容的存储分开。Ceph 为 CephFS 元数据提供了一个默认metadata池。您永远不必为 CephFS 元数据创建一个池，但您可以为您的 CephFS 元数据池创建一个仅指向主机的 SSD 存储介质的 CRUSH 映射层次结构。有关详细信息，请参阅 <a href="https://docs.ceph.com/en/quincy/rados/operations/crush-map-edits/#crush-map-device-class">CRUSH 设备类</a>。</p><h4><span id="控制器">控制器</span></h4><p>磁盘控制器 (HBA) 会对写入吞吐量产生重大影响。仔细考虑您的选择以确保它们不会造成性能瓶颈。值得注意的是，RAID 模式 (IR) HBA 可能比更简单的“JBOD”(IT) 模式 HBA 表现出更高的延迟，并且 RAID SoC、写缓存和电池备份会显着增加硬件和维护成本。某些 RAID HBA 可以配置有 IT 模式“个性”。</p><p><a href="https://ceph.com/community/blog/">Ceph 博客</a>通常是有关 Ceph 性能问题的极佳信息来源。有关更多详细信息，请参阅<a href="https://ceph.com/community/ceph-performance-part-1-disk-controller-write-throughput/">Ceph 写入吞吐量 1</a>和<a href="https://ceph.com/community/ceph-performance-part-2-write-throughput-without-ssd-journals/">Ceph 写入吞吐量 2</a>。</p><h4><span id="对标">对标</span></h4><p>BlueStore 在 O_DIRECT 中打开块设备，并频繁使用 fsync 以确保数据安全地持久化到介质中。您可以使用 评估驱动器的低级写入性能fio。例如，4kB 随机写性能测量如下：</p><pre><code>#fio --name=/dev/sdX --ioengine=libaio --direct=1 --fsync=1 --readwrite=randwrite --blocksize=4k --runtime=300</code></pre><h4><span id="写缓存">写缓存</span></h4><p>企业 SSD 和 HDD 通常包括断电保护功能，使用多级缓存来加速直接或同步写入。这些设备可以在两种缓存模式之间切换——易失性缓存通过 fsync 刷新到持久介质，或同步写入的非易失性缓存。</p><p>通过“启用”或“禁用”写入（易失性）缓存来选择这两种模式。当启用易失性缓存时，Linux 使用“回写”模式的设备，禁用时，它使用“直写”。</p><p>默认配置（通常启用缓存）可能不是最佳配置，并且 OSD 性能可能会通过禁用写缓存在增加 IOPS 和减少 commit_latency 方面得到显着提高。</p><p>因此，鼓励用户fio如前所述对他们的设备进行基准测试，并为他们的设备保留最佳缓存配置。<br>hdparm可以使用、sdparm或 smartctl读取中的值来查询缓存配置&#x2F;sys&#x2F;class&#x2F;scsi_disk&#x2F;*&#x2F;cache_type，例如：</p><pre><code>#hdparm -W /dev/sda/dev/sda: write-caching =  1 (on)# sdparm --get WCE /dev/sda    /dev/sda: ATA       TOSHIBA MG07ACA1  0101WCE           1  [cha: y]# smartctl -g wcache /dev/sdasmartctl 7.1 2020-04-05 r5049 [x86_64-linux-4.18.0-305.19.1.el8_4.x86_64] (local build)Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.orgWrite cache is:   Enabled# cat /sys/class/scsi_disk/0\:0\:0\:0/cache_typewrite back</code></pre><p>可以使用相同的工具禁用写缓存：</p><pre><code>#hdparm -W0 /dev/sda/dev/sda: setting drive write-caching to 0 (off) write-caching =  0 (off)# sdparm --clear WCE /dev/sda    /dev/sda: ATA       TOSHIBA MG07ACA1  0101# smartctl -s wcache,off /dev/sdasmartctl 7.1 2020-04-05 r5049 [x86_64-linux-4.18.0-305.19.1.el8_4.x86_64] (local build)Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.org=== START OF ENABLE/DISABLE COMMANDS SECTION ===Write cache disabled</code></pre><p>hdparm通常，使用、sdparm或禁用缓存smartctl 会导致 cache_type 自动更改为“write through”。如果不是这样，你可以尝试直接如下设置。（用户应注意，设置 cache_type 也会正确保留设备的缓存模式，直到下一次重启）：</p><pre><code>#echo &quot;write through&quot; &gt; /sys/class/scsi_disk/0\:0\:0\:0/cache_type# hdparm -W /dev/sda/dev/sda: write-caching =  0 (off)</code></pre><p>这个 udev 规则（在 CentOS 8 上测试过）会将所有 SATA&#x2F;SAS 设备 cache_types 设置为“write through”：</p><pre><code>#cat /etc/udev/rules.d/99-ceph-write-through.rulesACTION==&quot;add&quot;, SUBSYSTEM==&quot;scsi_disk&quot;, ATTR&#123;cache_type&#125;:=&quot;write through&quot;</code></pre><p>这个 udev 规则（在 CentOS 7 上测试过）会将所有 SATA&#x2F;SAS 设备 cache_types 设置为“write through”：</p><pre><code>#cat /etc/udev/rules.d/99-ceph-write-through-el7.rulesACTION==&quot;add&quot;, SUBSYSTEM==&quot;scsi_disk&quot;, RUN+=&quot;/bin/sh -c &#39;echo write through &gt; /sys/class/scsi_disk/$kernel/cache_type&#39;&quot;</code></pre><p>该sdparm实用程序可用于一次查看&#x2F;更改多个设备上的易失性写入缓存：</p><pre><code>#sdparm --get WCE /dev/sd*    /dev/sda: ATA       TOSHIBA MG07ACA1  0101WCE           0  [cha: y]    /dev/sdb: ATA       TOSHIBA MG07ACA1  0101WCE           0  [cha: y]# sdparm --clear WCE /dev/sd*    /dev/sda: ATA       TOSHIBA MG07ACA1  0101    /dev/sdb: ATA       TOSHIBA MG07ACA1  0101</code></pre><h4><span id="其他注意事项">其他注意事项</span></h4><p>您通常会在每个主机上运行多个 OSD，但您应该确保 OSD 驱动器的总吞吐量不超过满足客户端读取或写入数据需求所需的网络带宽。您还应该考虑集群在每个主机上存储的总数据的百分比。如果特定主机上的百分比很大并且该主机发生故障，则可能导致诸如超过 之类的问题，这会导致 Ceph 停止操作以作为防止数据丢失的安全预防措施。full ratio</p><p>当您在每个主机上运行多个 OSD 时，您还需要确保内核是最新的。请参阅<a href="https://docs.ceph.com/en/quincy/start/os-recommendations">OS Recommendations</a>了解有关注意事项glibc并 syncfs(2)确保您的硬件在每个主机运行多个 OSD 时按预期运行。</p><h3><span id="网络">网络</span></h3><p>在您的机架中提供至少 10 Gb&#x2F;s 的网络。</p><h4><span id="速度">速度</span></h4><p>在 1 Gb&#x2F;s 网络上复制 1 TB 数据需要三个小时，在 1 Gb&#x2F;s 网络上复制 10 TB 数据需要三十个小时。但是在 10 Gb&#x2F;s 网络上复制 1 TB 数据只需要 20 分钟，而在 10 Gb&#x2F;s 网络上复制 10 TB 数据只需要一个小时。</p><h4><span id="成本">成本</span></h4><p>Ceph 集群越大，OSD 故障就越常见。degraded归置组 (PG) 从一种状态恢复到另一种状态的速度越快越好。值得注意的是，快速恢复将可能导致数据暂时不可用甚至丢失的多重重叠故障的可能性降至最低。当然，在配置您的网络时，您必须在价格与性能之间取得平衡。active + clean</p><p>一些部署工具使用 VLAN 来使硬件和网络布线更易于管理。使用 802.1q 协议的 VLAN 需要支持 VLAN 的 NIC 和交换机。该硬件的额外费用可能会被网络设置和维护方面节省的运营成本所抵消。当使用 VLAN 处理集群和计算堆栈（例如 OpenStack、CloudStack 等）之间的 VM 流量时，使用 10 Gb&#x2F;s 以太网或更好的以太网具有额外的价值；截至 2022 年，40 Gb&#x2F;s 或25&#x2F;50&#x2F;100 Gb&#x2F;s 网络在生产集群中很常见。</p><blockquote><p>架顶式 (TOR) 交换机还需要快速和冗余的上行链路来旋转主干交换机&#x2F;路由器，通常至少为 40 Gb&#x2F;s。</p></blockquote><h4><span id="底板管理控制器-bmc">底板管理控制器 (BMC)</span></h4><p>您的服务器机箱应该有底板管理控制器 (BMC)。众所周知的例子是 iDRAC (Dell)、CIMC (Cisco UCS) 和 iLO (HPE)。管理和部署工具也可能广泛使用 BMC，尤其是通过 IPMI 或 Redfish，因此请考虑带外网络在安全性和管理方面的成本&#x2F;收益权衡。Hypervisor SSH 访问、VM 映像上传、OS 映像安装、管理套接字等会给网络带来巨大的负载。运行三个网络似乎有点矫枉过正，但每个流量路径都代表潜在的容量、吞吐量和&#x2F;或性能瓶颈，您在部署大规模数据集群之前应该仔细考虑。</p><h3><span id="故障域">故障域</span></h3><p>故障域是阻止访问一个或多个 OSD 的任何故障。那可能是主机上停止的守护进程；硬盘故障、操作系统崩溃、网卡故障、电源故障、网络中断、停电等等。在规划您的硬件需求时，您必须权衡通过将过多的责任置于过少的故障域中来降低成本的诱惑与隔离每个潜在故障域的额外成本之间的平衡。</p><h4><span id="最低硬件建议">最低硬件建议</span></h4><p>Ceph 可以在廉价的商用硬件上运行。小型生产集群和开发集群可以使用适度的硬件成功运行。</p><table>    <tr>        <td>过程</td>        <td>标准</td>        <td>最低推荐</td>    </tr>    <tr>        <td rowspan="5">ceph-osd</td>    </tr>        <tr>            <td>处理器</td>            <td>最少 1 个核心 <br>  每 200-500 MB/s 1 个核心 <br> 每 1000-3000 IOPS 1 个核心  <br>  结果在复制之前。 <br>  结果可能因不同的 CPU 型号和 Ceph 功能而异。（纠删码、压缩等）。<br> ARM 处理器可能特别需要额外的内核。 <br> 实际性能取决于许多因素，包括驱动器、网络和客户端吞吐量和延迟。强烈建议进行基准测试。 </td>        </tr>        <tr>            <td>内存</td>            <td>每个守护进程 4GB+（越多越好）<br> 2-4GB 经常运行（可能很慢） <br> 小于 2GB 不推荐 </td>        </tr>        <tr>            <td>卷存储</td>            <td>每个守护进程 1 个存储驱动器 </td>        </tr>        <tr>            <td>数据库/文件</td>            <td>每个守护进程 1 个 SSD 分区（可选） </td>        </tr>        <tr>            <td>网络</td>            <td>1x 1GbE+ NIC（推荐 10GbE+） </td>        </tr>    <tr>        <td rowspan="4">ceph-mon</td>    </tr>        <tr>            <td>处理器</td>            <td>最少 2 个核心 </td>        </tr>        <tr>            <td>内存</td>            <td>每个守护进程 2-4GB+ </td>        </tr>        <tr>        <td>磁盘空间</td>        <td>每个守护进程 60 GB </td>        </tr>        <tr>            <td>网络</td>            <td>1x 1GbE+ NIC</td>        </tr>    <tr>        <td rowspan="4">ceph-mds</td>    </tr>        <tr>            <td>处理器</td>            <td>最少 2 个核心 </td>        </tr>        <tr>            <td>内存</td>            <td>每个守护进程 2GB+ </td>        </tr>        <tr>            <td>磁盘空间</td>            <td>每个守护进程 1MB</td>        </tr>        <tr>            <td>网络</td>            <td>1x 1GbE+ NIC</td>        </tr></table><blockquote><p>如果您使用单个磁盘运行 OSD，请为您的卷存储创建一个分区，该分区与包含操作系统的分区分开。通常，我们建议操作系统和卷存储使用单独的磁盘。</p></blockquote><h2><span id="操作系统建议">操作系统建议</span></h2><h3><span id="ceph-依赖项">CEPH 依赖项</span></h3><p>作为一般规则，我们建议在较新版本的 Linux 上部署 Ceph。我们还建议在具有长期支持的版本上进行部署。</p><h4><span id="内核">内核</span></h4><blockquote><p>Ceph 内核客户端</p></blockquote><p>如果您使用内核客户端映射 RBD 块设备或挂载 CephFS，一般建议是使用 <a href="http://kernel.org/">http://kernel.org</a> 或您在任何客户端上的 Linux 发行版提供的“稳定”或“长期维护”内核系列主机。</p><p>对于 RBD，如果选择跟踪长期内核，我们目前推荐基于 4.x 的“长期维护”内核系列或更高版本：</p><p>对于 RBD，如果选择跟踪长期内核，我们目前推荐基于 4.x 的“长期维护”内核系列或更高版本：</p><ul><li>4.19.z</li><li>4.14.z</li><li>5.x</li></ul><p>对于 CephFS，请参阅有关使用内核驱动程序安装 CephFS 的部分 以获取内核版本指南。</p><p>较旧的内核客户端版本可能不支持您的CRUSH 可调配置文件或 Ceph 集群的其他较新功能，需要将存储集群配置为禁用这些功能。</p><h3><span id="平台">平台</span></h3><p>下面的图表显示了 Ceph 的需求如何映射到各种 Linux 平台。一般而言，对内核和系统初始化包（即 sysvinit、systemd）之外的特定发行版的依赖性非常小。</p><table>    <tr>        <td>Release Name</td>        <td>Tag</td>        <td>CentOS</td>        <td>Ubuntu</td>        <td>OpenSUSE C</td>        <td>Debian C</td>    </tr>        <tr>            <td>Quincy</td>            <td>17.2.z</td>            <td>8 A</td>            <td>20.04 A</td>            <td>15.3</td>            <td>11</td>        </tr>        <tr>            <td>Pacific</td>            <td>16.2.z</td>            <td>8 A</td>            <td>18.04 C,<br>20.04 A</td>            <td>15.2</td>            <td>10<br>11</td>        </tr>        <tr>            <td>Octopus</td>            <td>15.2.z</td>            <td>7 B <br> 8 A</td>            <td>18.04 C,<br>20.04 A</td>            <td>15.2</td>            <td>10</td>        </tr></table><ul><li>A : Ceph 提供软件包，并对其中的软件进行了全面的测试。</li><li>B : Ceph 提供了软件包，并对其中的软件做了基本的测试。</li><li>C : Ceph 只提供包。尚未对这些版本进行任何测试。</li></ul><blockquote><p>Centos 7 用户：Btrfs在 Octopus 版本中不再在 Centos 7 上进行测试。我们建议bluestore改用。</p></blockquote><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph故障排查</title>
      <link href="/2023/04/10/Ceph/0.Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/"/>
      <url>/2023/04/10/Ceph/0.Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-application-not-enabled-on-1-pools">一、application not enabled on 1 pool(s)</a></li><li><a href="#%E4%BA%8C-%E6%B8%85%E7%90%86mgr%E5%91%8A%E8%AD%A6">二、清理MGR告警</a></li></ul><!-- tocstop --><h1><span id="一-application-not-enabled-on-1-pools">一、application not enabled on 1 pool(s)</span></h1><blockquote><p>故障现象：</p></blockquote><pre><code>[root@node-1 data]# ceph -s   cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            application not enabled on 1 pool(s) #提示有一个pool没有启用application ，是由于创建pool的时候没有使用rbd pool init &lt;pool-name&gt;进行初始化，使用此命令会默认指定成RBD格式的application   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 71m)    mgr: node-1(active, since 14h), standbys: node-2, node-3    osd: 3 osds: 3 up (since 39m), 3 in (since 65m)   data:    pools:   1 pools, 64 pgs    objects: 281 objects, 1.0 GiB    usage:   6.2 GiB used, 24 GiB / 30 GiB avail    pgs:     64 active+clean</code></pre><blockquote><p> 查看当前集群的健康状态</p></blockquote><pre><code>[root@node-1 data]# ceph health detailHEALTH_WARN application not enabled on 1 pool(s)POOL_APP_NOT_ENABLED application not enabled on 1 pool(s)    application not enabled on pool &#39;ceph-demo&#39;    use &#39;ceph osd pool application enable &lt;pool-name&gt; &lt;app-name&gt;&#39;, where &lt;app-name&gt; is &#39;cephfs&#39;, &#39;rbd&#39;, &#39;rgw&#39;, or freeform for custom applications</code></pre><blockquote><p>启用ceph-demo的资源池的application，并设置成RBD格式</p></blockquote><pre><code>[root@node-1 data]# ceph osd pool application enable ceph-demo rbd  enabled application &#39;rbd&#39; on pool &#39;ceph-demo&#39;</code></pre><blockquote><p>通过get命令可查看资源池的application设置</p></blockquote><pre><code>[root@node-1 data]#  ceph -h |grep application osd pool application disable &lt;poolname&gt; &lt;app&gt; &#123;--yes-i-really-mean-   disables use of an application &lt;app&gt; on pool &lt;poolname&gt;osd pool application enable &lt;poolname&gt; &lt;app&gt; &#123;--yes-i-really-mean-it&#125; enable use of an application &lt;app&gt; [cephfs,rbd,rgw] on pool osd pool application get &#123;&lt;poolname&gt;&#125; &#123;&lt;app&gt;&#125; &#123;&lt;key&gt;&#125;                 get value of key &lt;key&gt; of application &lt;app&gt; on pool &lt;poolname&gt;osd pool application rm &lt;poolname&gt; &lt;app&gt; &lt;key&gt;                        removes application &lt;app&gt; metadata key &lt;key&gt; on pool &lt;poolname&gt;osd pool application set &lt;poolname&gt; &lt;app&gt; &lt;key&gt; &lt;value&gt;               sets application &lt;app&gt; metadata key &lt;key&gt; to &lt;value&gt; on pool [root@node-1 data]# ceph osd pool application get ceph-demo #默认为空&#123;    &quot;rbd&quot;: &#123;&#125;&#125;</code></pre><blockquote><p>查看集群已经恢复</p></blockquote><pre><code>[root@node-1 data]# ceph -s  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK  services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 80m)    mgr: node-1(active, since 14h), standbys: node-2, node-3    osd: 3 osds: 3 up (since 48m), 3 in (since 73m)   data:    pools:   1 pools, 64 pgs    objects: 281 objects, 1.0 GiB    usage:   6.2 GiB used, 24 GiB / 30 GiB avail    pgs:     64 active+clean</code></pre><h1><span id="二-清理mgr告警">二、清理MGR告警</span></h1><blockquote><p>故障现象：</p></blockquote><pre><code>[root@node-1 data]# ceph -s    cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            2 daemons have recently crashed #有两个告警信息，可能是由于某个服务出现异常导致   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 80m)    mgr: node-1(active, since 14h), standbys: node-2, node-3    osd: 3 osds: 3 up (since 48m), 3 in (since 73m)   data:    pools:   1 pools, 64 pgs    objects: 281 objects, 1.0 GiB    usage:   6.2 GiB used, 24 GiB / 30 GiB avail    pgs:     64 active+clean[root@node-1 data]#  ceph health detailHEALTH_WARN 2 daemons have recently crashedRECETN_CRASH 2 daemons have recently crashed    mgr.node-1 crashed on host node-1 at 2023-03-04 15:42:01.891238Z    mgr.node-1 crashed on host node-1 at 2023-03-04 15:45:31.891238Z</code></pre><blockquote><p>查看crash用法</p></blockquote><pre><code>[root@node-1 data]# ceph -h | grep crashcrash archive &lt;id&gt;                                                    Acknowledge a crash and silence health warning(s)crash archive-all                                                     Acknowledge all new crashes and silence health warning(s)crash info &lt;id&gt;                                                       show crash dump metadatacrash json_report &lt;hours&gt;                                             Crashes in the last &lt;hours&gt; hourscrash ls                                                              Show new and archived crash dumpscrash ls-new                                                          Show new crash dumpscrash post                                                            Add a crash dump (use -i &lt;jsonfile&gt;)crash prune &lt;keep&gt;                                                    Remove crashes older than &lt;keep&gt; dayscrash rm &lt;id&gt;                                                         Remove a saved crash &lt;id&gt;crash stat                                                            Summarize recorded crashes</code></pre><blockquote><p>查看当前状态</p></blockquote><pre><code>[root@node-1 data]# ceph crash ls</code></pre><p><img src="/images/Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/1.jpg"></p><blockquote><p>查看该ID的具体信息</p></blockquote><pre><code>[root@node-1 data]# ceph crash info &lt;id&gt;</code></pre><blockquote><p>使用ceph crash archive <id>进行标记，视为已读，或者重启所有OSD也能解决告警问题</id></p></blockquote><p><img src="/images/Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/2.jpg"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> 故障排查 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客中插入 Chart 动态图表</title>
      <link href="/2023/04/08/chatjs/"/>
      <url>/2023/04/08/chatjs/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>该文基本(全部)来自于chatjs中文文档</p><h1><span id="背景">背景</span></h1><!-- toc --><ul><li><a href="#hexo-%E4%B8%AD%E7%9A%84-chartjs">Hexo 中的 Chartjs</a></li><li><a href="#%E7%A4%BA%E4%BE%8B">示例</a><ul><li><a href="#%E6%8A%98%E7%BA%BF%E5%9B%BE">折线图</a></li><li><a href="#%E7%9B%B8%E5%85%B3%E5%B1%9E%E6%80%A7">相关属性</a></li></ul></li></ul><!-- tocstop --><span id="more"></span><p>Chartjs是一款简单优雅的数据可视化工具，对比其他图表库如echarts、highcharts、c3、flot、amchart等，它的画面效果、动态效果都更精致，它的 文档首页 就透出一股小清新，基于 HTML5 Canvas，它拥有更好的性能且响应式，基本满足了一般数据展示的需要，包括折线图，条形图，饼图，散点图，雷达图，极地图，甜甜圈图等</p><h1><span id="hexo-中的-chartjs">Hexo 中的 Chartjs</span></h1><p>为了方便在 Hexo 中使用这么漂亮的图表库，我自己写了一个 Hexo 的 Chartjs 插件。插件的安装和使用非常的简单，只需要进入博客目录，然后打开命令行，用npm安装一下：</p><pre><code>npm install hexo-tag-chart --save</code></pre><p>之后在文章内使用 chart 的 tag 就可以了</p><pre><code>&#123;% chart 90% 300 %&#125;\\TODO option goes here&#123;% endchart %&#125;</code></pre><p>其中 chart 是标签名，endchart 是结束标签，不需要更改，90% 是图表容器的相对宽度，默认是100%，300 是图表容器的高度，默认是按正常比例缩放的，你可以通过设置 options 里面的 aspectRatio 属性来调整宽高比例，另外还有许多属性可以自定义，你可以查看 官方文档。在标签之间的部分，都是需要自己填充的图表数据和属性。</p><p>我们来看一个样例，你可以把鼠标移上去看看动态效果。</p><div style="width: 90%;margin: 0 auto">    <canvas id="chart1101" style="height: 300px"></canvas></div><script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script><script type="text/javascript">    var ctx = document.getElementById('chart1101').getContext('2d');    var options =     {    type: 'line',    data: {    labels: ['January', 'February', 'March', 'April', 'May', 'June', 'July'],    datasets: [{        label: 'My First dataset',        backgroundColor: 'rgb(255, 99, 132)',        borderColor: 'rgb(255, 99, 132)',        data: [0, 10, 5, 2, 20, 30, 45]        }]    },    options: {        responsive: true,        title: {        display: true,        text: 'Chart.js Line Chart'        }    }};    new Chart(ctx, options);</script><p>上面这个样例可以通过以下代码来实现：</p><pre><code>&#123;% chart 90% 300 %&#125;    &#123;    type: 'line',    data: &#123;    labels: ['January', 'February', 'March', 'April', 'May', 'June', 'July'],    datasets: [&#123;        label: 'My First dataset',        backgroundColor: 'rgb(255, 99, 132)',        borderColor: 'rgb(255, 99, 132)',        data: [0, 10, 5, 2, 20, 30, 45]        &#125;]    &#125;,    options: &#123;        responsive: true,        title: &#123;        display: true,        text: 'Chart.js Line Chart'        &#125;    &#125;&#125;&#123;% endchart %&#125;</code></pre><h1><span id="示例">示例</span></h1><p>现在你已经基本学会了在Hexo中插入图表了，我再展示一些更炫酷的图表吧，你可以自己去尝试一下。</p><h2><span id="折线图">折线图</span></h2><p>效果</p><div style="width: 90%;margin: 0 auto">    <canvas id="chart4323" style="height: 300px"></canvas></div><script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script><script type="text/javascript">    var ctx = document.getElementById('chart4323').getContext('2d');    var options =   //其中 chart 是标签名，endchart 是结束标签，不需要更改，可以通过设置 options 里面的  aspectRatio 属性来调整图表的宽高比例，默认 width 为 100%，height 为 300px。 {    type: 'line',    data: { //  折线图数据    labels: ['8.4', '8.5', '8.6', '8.7', '8.8', '8.9', '8.10'], //  横坐标    datasets: [{ //  纵坐标变量名称        label: 'My First dataset',        lineTension: 0,        borderColor: 'rgb(255, 99, 132)', //  线条颜色        data: [0, 10, 5, 2, 20, 30, 45] //  纵坐标        }]    },    options: {        responsive: true,        title: {        display: true,        text: 'Chart.js 折线图' //  图表名称        }    }};    new Chart(ctx, options);</script><pre><code>&#123;% chart 90% 300 %&#125;  //其中 chart 是标签名，endchart 是结束标签，不需要更改，可以通过设置 options 里面的               aspectRatio 属性来调整图表的宽高比例，默认 width 为 100%，height 为 300px。 &#123;    type: 'line',    data: &#123; //  折线图数据    labels: ['8.4', '8.5', '8.6', '8.7', '8.8', '8.9', '8.10'], //  横坐标    datasets: [&#123; //  纵坐标变量名称        label: 'My First dataset',        lineTension: 0,        borderColor: 'rgb(255, 99, 132)', //  线条颜色        data: [0, 10, 5, 2, 20, 30, 45] //  纵坐标        &#125;]    &#125;,    options: &#123;        responsive: true,        title: &#123;        display: true,        text: 'Chart.js 折线图' //  图表名称        &#125;    &#125;&#125;&#123;% endchart %&#125;</code></pre><h2><span id="相关属性">相关属性</span></h2><table><thead><tr><th>名称</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>backgroundColor</td><td>Color&#x2F;Color[]</td><td>线条背景色</td></tr><tr><td></td><td></td><td></td></tr><tr><td>borderColor</td><td>Color&#x2F;Color[]</td><td>线条颜色</td></tr></tbody></table><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatjs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hello-world</title>
      <link href="/2023/04/08/hello-world/"/>
      <url>/2023/04/08/hello-world/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><span id="more"></span><h2><span id="quick-start">Quick Start</span></h2><h3><span id="create-a-new-post">Create a new post</span></h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3><span id="run-server">Run server</span></h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3><span id="generate-static-files">Generate static files</span></h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3><span id="deploy-to-remote-sites">Deploy to remote sites</span></h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>常用存储介绍</title>
      <link href="/2023/04/08/Ceph/1.%E5%AD%98%E5%82%A8%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E5%AD%98%E5%82%A8%E4%BB%8B%E7%BB%8D/"/>
      <url>/2023/04/08/Ceph/1.%E5%AD%98%E5%82%A8%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E5%AD%98%E5%82%A8%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><h1><span id="常用存储介绍">常用存储介绍</span></h1><p><img src="/images/%E5%B8%B8%E7%94%A8%E5%AD%98%E5%82%A8%E4%BB%8B%E7%BB%8D/1.jpg"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
