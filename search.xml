<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Ceph-安装</title>
      <link href="/2023/04/11/Ceph/3.Ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/Ceph-%E5%AE%89%E8%A3%85/"/>
      <url>/2023/04/11/Ceph/3.Ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/Ceph-%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E7%8E%AF%E5%A2%83%E6%83%85%E5%86%B5">环境情况</a></li><li><a href="#%E4%B8%80-%E4%BF%AE%E6%94%B9hosts%E6%96%87%E4%BB%B6">一、修改HOSTS文件</a></li><li><a href="#%E4%BA%8C-%E9%85%8D%E7%BD%AE%E6%97%A0%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95">二、配置无密码登录</a></li><li><a href="#%E4%B8%89-%E5%AE%89%E5%85%A8%E8%AE%BE%E7%BD%AE">三、安全设置</a><ul><li><a href="#%E4%B8%80-%E5%85%B3%E9%97%ADselinux">一、关闭Selinux</a></li><li><a href="#%E4%BA%8C-%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99">二、关闭防火墙</a></li></ul></li><li><a href="#%E5%9B%9B-ntp%E8%AE%BE%E7%BD%AE">四、NTP设置</a></li><li><a href="#%E4%BA%94-%E9%85%8D%E7%BD%AEyum%E6%BA%90">五、配置Yum源</a></li><li><a href="#%E5%85%AD-%E5%AE%89%E8%A3%85ceph-deploy">六、安装Ceph-deploy</a></li><li><a href="#%E4%B8%83-%E5%AE%89%E8%A3%85mon%E8%8A%82%E7%82%B9">七、安装Mon节点</a><ul><li><a href="#%E4%B8%80-%E9%87%8D%E7%BD%AE%E9%9B%86%E7%BE%A4">一、重置集群</a></li><li><a href="#%E4%BA%8C-%E5%AE%89%E8%A3%85%E7%9B%B8%E5%85%B3%E8%BD%AF%E4%BB%B6">二、安装相关软件</a></li></ul></li><li><a href="#%E4%B8%89-mon-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%8A%E5%AE%89%E8%A3%85mgr">三、Mon 初始化及安装MGR</a></li><li><a href="#%E5%85%AB-%E9%83%A8%E7%BD%B2osd%E8%8A%82%E7%82%B9">八、部署OSD节点</a></li><li><a href="#%E4%B9%9D-%E6%89%A9%E5%B1%95mon%E5%92%8Cmgr">九、扩展MON和MGR</a><ul><li><a href="#%E4%B8%80-mon">一、Mon</a></li><li><a href="#%E4%BA%8C-mgr">二、MGR</a></li></ul></li><li><a href="#%E5%8D%81-%E5%88%9B%E5%BB%BA%E8%B5%84%E6%BA%90%E6%B1%A0pool">十、创建资源池Pool</a></li></ul><!-- tocstop --><h1><span id="环境情况">环境情况</span></h1><table>    <tr>        <td>操作系统</td>        <td>公共网络</td>        <td>集群网络</td>        <td>节点名称</td>    </tr>    <tr>        <td rowspan="4">Centos 7.9</td>    </tr>        <tr>            <td>192.168.187.201</td>            <td>192.168.199.201</td>            <td>node-1</td>        </tr>        <tr>            <td>192.168.187.202</td>            <td>192.168.199.202</td>            <td>node-2</td>        </tr>        <tr>            <td>192.168.187.203</td>            <td>192.168.199.203</td>            <td>node-2</td>        </tr></table><h1><span id="一-修改hosts文件">一、修改HOSTS文件</span></h1><p><strong>所有主机均要修改</strong></p><pre><code>[root@node-1 ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.199.201 node-1192.168.199.202 node-2192.168.199.203 node-3</code></pre><h1><span id="二-配置无密码登录">二、配置无密码登录</span></h1><p><strong>Node-1：</strong></p><pre><code>[root@node-1 ~]# ssh-keygen Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Created directory &#39;/root/.ssh&#39;.Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:RNd/EfeRh3t7C73PD//lsUVTQpf84hKM5Dy3CdWj6iQ root@node-1The keys randomart image is:+---[RSA 2048]----+|        . .. .o+*||       . .. o.+*=||        .+ + o.o*||       .  * = +.=||        S  = =.=o||        E o +..+o||         +   .o.*||          .    B=||               .@|+----[SHA256]-----+[root@node-1 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub node-2/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;The authenticity of host &#39;node-2 (192.168.199.202)&#39; can t be established.ECDSA key fingerprint is SHA256:ELWxoLpZlehIUk5OpR5CO/OmzpHyNco8PkV4ztDCV7w.ECDSA key fingerprint is MD5:62:87:b6:99:64:b1:b2:d3:40:f1:d5:e8:d3:a1:8a:18.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@node-2 s password: Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#39;node-2&#39;&quot;and check to make sure that only the key(s) you wanted were added.[root@node-1 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub node-3/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;The authenticity of host &#39;node-3 (192.168.199.203)&#39; can t be established.ECDSA key fingerprint is SHA256:ELWxoLpZlehIUk5OpR5CO/OmzpHyNco8PkV4ztDCV7w.ECDSA key fingerprint is MD5:62:87:b6:99:64:b1:b2:d3:40:f1:d5:e8:d3:a1:8a:18.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@node-3 s password: Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#39;node-3&#39;&quot;and check to make sure that only the key(s) you wanted were added.</code></pre><h1><span id="三-安全设置">三、安全设置</span></h1><h2><span id="一-关闭selinux">一、关闭Selinux</span></h2><p><strong>所有节点执行，将enforcing修改成disabled：</strong></p><pre><code>[root@node-1 ~]# cat /etc/selinux/config # This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:#     enforcing - SELinux security policy is enforced.#     permissive - SELinux prints warnings instead of enforcing.#     disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of three values:#     targeted - Targeted processes are protected,#     minimum - Modification of targeted policy. Only selected processes are protected. #     mls - Multi Level Security protection.SELINUXTYPE=targeted[root@node-1 ~]# setenforce 0 #临时关闭[root@node-1 ~]# getenforce #查看状态Disabled</code></pre><h2><span id="二-关闭防火墙">二、关闭防火墙</span></h2><p><strong>所有节点均执行：</strong></p><pre><code>[root@node-1 ~]# firewall-cmd --list-all #查看防火墙状态public (active)  target: default  icmp-block-inversion: no  interfaces: ens33 ens36  sources:   services: dhcpv6-client ssh  ports:   protocols:   masquerade: no  forward-ports:   source-ports:   icmp-blocks:   rich rules: [root@node-1 ~]# systemctl disable firewalld #禁止开机自启Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.[root@node-1 ~]# systemctl stop firewalld #关闭防火墙[root@node-3 ~]# firewall-cmd --list-all #查看状态FirewallD is not running</code></pre><h1><span id="四-ntp设置">四、NTP设置</span></h1><p><strong>所有节点执行：</strong></p><pre><code>[root@node-3 ~]# cd /etc/yum.repos.d/[root@node-3 yum.repos.d]# lsCentOS-Base.repo  CentOS-Debuginfo.repo  CentOS-Media.repo    CentOS-Vault.repoCentOS-CR.repo    CentOS-fasttrack.repo  CentOS-Sources.repo  CentOS-x86_64-kernel.repo[root@node-3 yum.repos.d]# rm -rf *[root@node-3 yum.repos.d]# ls[root@node-1 yum.repos.d]# curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  2523  100  2523    0     0   6577      0 --:--:-- --:--:-- --:--:--  6587[root@node-1 yum.repos.d]# [root@node-1 yum.repos.d]# lsCentOS-Base.repo[root@node-1 yum.repos.d]# yum clean all &amp;&amp; yum makecache #清除缓存[root@node-1 yum.repos.d]# yum install ntp -y #安装NTP[root@node-1 yum.repos.d]# systemctl restart ntpd #启动NTP服务[root@node-1 yum.repos.d]# systemctl enable ntpd #设置开机自启动Created symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.[root@node-1 yum.repos.d]# ntpq -pn #查看NTP状态     remote           refid      st t when poll reach   delay   offset  jitter==============================================================================+78.46.102.180   176.9.157.12     3 u   33   64    1  245.918   17.006  14.216*144.76.76.107   192.53.103.103   2 u   33   64    1  214.840    0.073   0.324 193.182.111.14  192.36.143.153   2 u   67   64    1  293.490   -4.158   1.037 [root@node-2 ~]# crontab -l #使用crontab -e 编辑，每分钟同步一次时间*/1 * * * * /usr/sbin/ntpdate node-1;/sbin/hwclock -w</code></pre><p><strong>node-2、node-3执行：</strong></p><pre><code>[root@node-3 yum.repos.d]# cat /etc/ntp.conf # For more information about this file, see the man pages# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).driftfile /var/lib/ntp/drift# Permit time synchronization with our time source, but do not# permit the source to query or modify the service on this system.restrict default nomodify notrap nopeer noquery# Permit all access over the loopback interface.  This could# be tightened as well, but to do so would effect some of# the administrative functions.restrict 127.0.0.1 restrict ::1# Hosts on local network are less restricted.#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).server node-1 iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst#broadcast 192.168.1.255 autokey    # broadcast server#broadcastclient            # broadcast client#broadcast 224.0.1.1 autokey        # multicast server#multicastclient 224.0.1.1        # multicast client#manycastserver 239.255.254.254        # manycast server#manycastclient 239.255.254.254 autokey # manycast client# Enable public key cryptography.#cryptoincludefile /etc/ntp/crypto/pw# Key file containing the keys and key identifiers used when operating# with symmetric key cryptography. keys /etc/ntp/keys# Specify the key identifiers which are trusted.#trustedkey 4 8 42# Specify the key identifier to use with the ntpdc utility.#requestkey 8# Specify the key identifier to use with the ntpq utility.#controlkey 8# Enable writing of statistics records.#statistics clockstats cryptostats loopstats peerstats# Disable the monitoring facility to prevent amplification attacks using ntpdc# monlist command when default restrict does not include the noquery flag. See# CVE-2013-5211 for more details.# Note: Monitoring will not be disabled with the limited restriction flag.disable monitor</code></pre><h1><span id="五-配置yum源">五、配置Yum源</span></h1><p><strong>所有节点执行：</strong></p><pre><code>[root@node-3 ~]# cd /etc/yum.repos.d/[root@node-3 yum.repos.d]# lsCentOS-Base.repo  CentOS-Debuginfo.repo  CentOS-Media.repo    CentOS-Vault.repoCentOS-CR.repo    CentOS-fasttrack.repo  CentOS-Sources.repo  CentOS-x86_64-kernel.repo[root@node-3 yum.repos.d]# rm -rf *[root@node-3 yum.repos.d]# ls[root@node-1 yum.repos.d]# curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  2523  100  2523    0     0   6577      0 --:--:-- --:--:-- --:--:--  6587[root@node-1 yum.repos.d]# [root@node-1 yum.repos.d]# lsCentOS-Base.repo[root@node-1 yum.repos.d]# curl -o /etc/yum.repos.d/epel.repo https://mirrors.aliyun.com/repo/epel-7.repo  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   664  100   664    0     0   2193      0 --:--:-- --:--:-- --:--:--  2191[root@node-1 yum.repos.d]# [root@node-1 yum.repos.d]# lsCentOS-Base.repo  epel.repo[root@node-1 yum.repos.d]# cat ceph.repo  #手动创建ceph.repo文件内容如下[norch]name=norchbaseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/enabled=1gpgcheck=0[x86_64]name=x86 64baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/enabled=1gpgcheck=0[root@node-1 yum.repos.d]# yum clean all &amp;&amp; yum makecache #清除缓存[root@node-1 yum.repos.d]# yum update -y</code></pre><h1><span id="六-安装ceph-deploy">六、安装Ceph-deploy</span></h1><p><strong>Node1执行</strong></p><pre><code>[root@node-1 yum.repos.d]# yum install python-setuptools ceph-deploy -y #安装核心软件[root@node-1 yum.repos.d]# ceph-deploy --version #查看版本2.0.1</code></pre><h1><span id="七-安装mon节点">七、安装Mon节点</span></h1><h2><span id="一-重置集群">一、重置集群</span></h2><p><strong>如果安装失败或者重新安装时执行：</strong></p><pre><code>ceph-deploy purge &#123;ceph-node&#125; [&#123;ceph-node&#125;]ceph-deploy purgedata  &#123;ceph-node&#125; [&#123;ceph-node&#125;]ceph-deploy forgetkeysrm ceph.*</code></pre><h2><span id="二-安装相关软件">二、安装相关软件</span></h2><p><strong>Node1执行：</strong></p><pre><code>[root@node-1 yum.repos.d]# cd /opt/[root@node-1 opt]# mkdir my-cluster[root@node-1 opt]# cd my-cluster/[root@node-1 my-cluster]# ceph-deploy new --public-network 192.168.187.0/24 --cluster-network 192.168.199.0/24 node-1</code></pre><p><strong>公共网络是外部访问集群时使用的，集群网络时内部同步使用的</strong><br><strong>所有节点执行：</strong></p><pre><code>[root@node-1 my-cluster]# yum install ceph ceph-mon ceph-mgr ceph-mds ceph-radosgw -y #安装核心软件包</code></pre><h1><span id="三-mon-初始化及安装mgr">三、Mon 初始化及安装MGR</span></h1><pre><code>[root@node-1 my-cluster]# ceph-deploy mon create-initial #初始化Mon[root@node-1 my-cluster]# ceph-deploy admin node-1 node-2 node-3 #推送最新配置到所有节点[root@node-1 my-cluster]# ceph -s  #查看集群状态  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            mon is allowing insecure global_id reclaim  #mon允许不安全的global_id回收   services:    mon: 1 daemons, quorum node-1 (age 2m)    mgr: no daemons active    osd: 0 osds: 0 up, 0 in   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   0 B used, 0 B / 0 B avail    pgs:     [root@node-1 my-cluster]# ceph config set mon auth_allow_insecure_global_id_reclaim false #取消mon允许不安全的global_id回收[root@node-1 my-cluster]# ceph -s   cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 1 daemons, quorum node-1 (age 3m)    mgr: no daemons active    osd: 0 osds: 0 up, 0 in   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   0 B used, 0 B / 0 B avail    pgs:      [root@node-1 my-cluster]# ceph-deploy mgr create node-1 #安装mgr 监控服务[root@node-1 my-cluster]#  ceph -s  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            OSD count 0 &lt; osd_pool_default_size 3   services:    mon: 1 daemons, quorum node-1 (age 6m)    mgr: node-1(active, since 55s) #查看已经成功安装    osd: 0 osds: 0 up, 0 in   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   0 B used, 0 B / 0 B avail    pgs:     </code></pre><h1><span id="八-部署osd节点">八、部署OSD节点</span></h1><p><strong>node-1 执行:</strong></p><pre><code>[root@node-1 my-cluster]# lsblk #确定硬盘NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsda               8:0    0   20G  0 disk ├─sda1            8:1    0    1G  0 part /boot└─sda2            8:2    0   19G  0 part   ├─centos-root 253:0    0   17G  0 lvm  /  └─centos-swap 253:1    0    2G  0 lvm  [SWAP]sdb               8:16   0   10G  0 disk sdc               8:32   0   10G  0 disk sr0              11:0    1 1024M  0 rom [root@node-1 my-cluster]# ceph-deploy osd create node-1 --data /dev/sdb --journal /dev/sdb # --data指定硬盘  --journal /dev/sdb 指定加速[root@node-1 my-cluster]# ceph-deploy osd create node-2 --data /dev/sdb --journal /dev/sdb # --data指定硬盘  --journal /dev/sdb 指定加速[root@node-1 my-cluster]# ceph-deploy osd create node-3 --data /dev/sdb --journal /dev/sdb # --data指定硬盘  --journal /dev/sdb 指定加速[root@node-1 my-cluster]# ceph osd tree  #查看OSD状态ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.02939 root default                            -3       0.00980     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000 -5       0.00980     host node-2                          1   hdd 0.00980         osd.1       up  1.00000 1.00000 -7       0.00980     host node-3                          2   hdd 0.00980         osd.2       up  1.00000 1.00000 </code></pre><h1><span id="九-扩展mon和mgr">九、扩展MON和MGR</span></h1><p><strong>Mon 使用Paxos算法，一般都是奇数 3、5、7</strong></p><h2><span id="一-mon">一、Mon</span></h2><pre><code>NODE-1执行：ceph-deploy mon add node-2 --address 192.168.187.202 #添加node-2成为Mon 并指定IPceph-deploy mon add node-3 --address 192.168.187.203 #添加node-3成为Mon 并指定IP[root@node-1 my-cluster]# ceph -s  #查看Mon是否添加成功  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 54s) #已经添加node-2和node-3    mgr: node-1(active, since 18m)    osd: 3 osds: 3 up (since 8m), 3 in (since 8m)   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   3.0 GiB used, 27 GiB / 30 GiB avail    pgs: [root@node-1 my-cluster]# ceph quorum_status --format json-pretty #查看Mon 仲裁情况&#123;    &quot;election_epoch&quot;: 12,    &quot;quorum&quot;: [        0,        1,        2    ],    &quot;quorum_names&quot;: [ #可以看到当前3个节点正在参加仲裁        &quot;node-1&quot;,        &quot;node-2&quot;,        &quot;node-3&quot;    ],    &quot;quorum_leader_name&quot;: &quot;node-1&quot;, #当前主Mon是Node-1    &quot;quorum_age&quot;: 234,    &quot;monmap&quot;: &#123;        &quot;epoch&quot;: 3,        &quot;fsid&quot;: &quot;e9a90625-4707-4b6b-b52f-661512ea831d&quot;,        &quot;modified&quot;: &quot;2023-03-03 12:22:43.254681&quot;,        &quot;created&quot;: &quot;2023-03-03 11:59:53.474948&quot;,        &quot;min_mon_release&quot;: 14,        &quot;min_mon_release_name&quot;: &quot;nautilus&quot;,        &quot;features&quot;: &#123;            &quot;persistent&quot;: [                &quot;kraken&quot;,                &quot;luminous&quot;,                &quot;mimic&quot;,                &quot;osdmap-prune&quot;,                &quot;nautilus&quot;            ],            &quot;optional&quot;: []        &#125;,        &quot;mons&quot;: [            &#123;                &quot;rank&quot;: 0,                &quot;name&quot;: &quot;node-1&quot;,                &quot;public_addrs&quot;: &#123;                    &quot;addrvec&quot;: [                        &#123;                            &quot;type&quot;: &quot;v2&quot;,                            &quot;addr&quot;: &quot;192.168.187.201:3300&quot;,                            &quot;nonce&quot;: 0                        &#125;,                        &#123;                            &quot;type&quot;: &quot;v1&quot;,                            &quot;addr&quot;: &quot;192.168.187.201:6789&quot;,                            &quot;nonce&quot;: 0                        &#125;                    ]                &#125;,                &quot;addr&quot;: &quot;192.168.187.201:6789/0&quot;,                &quot;public_addr&quot;: &quot;192.168.187.201:6789/0&quot;            &#125;,            &#123;                &quot;rank&quot;: 1,                &quot;name&quot;: &quot;node-2&quot;,                &quot;public_addrs&quot;: &#123;                    &quot;addrvec&quot;: [                        &#123;                            &quot;type&quot;: &quot;v2&quot;,                            &quot;addr&quot;: &quot;192.168.187.202:3300&quot;,                            &quot;nonce&quot;: 0                        &#125;,                        &#123;                            &quot;type&quot;: &quot;v1&quot;,                            &quot;addr&quot;: &quot;192.168.187.202:6789&quot;,                            &quot;nonce&quot;: 0                        &#125;                    ]                &#125;,                &quot;addr&quot;: &quot;192.168.187.202:6789/0&quot;,                &quot;public_addr&quot;: &quot;192.168.187.202:6789/0&quot;            &#125;,            &#123;                &quot;rank&quot;: 2,                &quot;name&quot;: &quot;node-3&quot;,                &quot;public_addrs&quot;: &#123;                    &quot;addrvec&quot;: [                        &#123;                            &quot;type&quot;: &quot;v2&quot;,                            &quot;addr&quot;: &quot;192.168.187.203:3300&quot;,                            &quot;nonce&quot;: 0                        &#125;,                        &#123;                            &quot;type&quot;: &quot;v1&quot;,                            &quot;addr&quot;: &quot;192.168.187.203:6789&quot;,                            &quot;nonce&quot;: 0                        &#125;                    ]                &#125;,                &quot;addr&quot;: &quot;192.168.187.203:6789/0&quot;,                &quot;public_addr&quot;: &quot;192.168.187.203:6789/0&quot;            &#125;        ]    &#125;&#125;[root@node-1 my-cluster]#  ceph mon stat #查看Mon 状态e3: 3 mons at &#123;node-1=[v2:192.168.187.201:3300/0,v1:192.168.187.201:6789/0],node-2=[v2:192.168.187.202:3300/0,v1:192.168.187.202:6789/0],node-3=[v2:192.168.187.203:3300/0,v1:192.168.187.203:6789/0]&#125;, election epoch 12, leader 0 node-1, quorum 0,1,2 node-1,node-2,node-3[root@node-1 my-cluster]# ceph mon dump #查看Mon 状态epoch 3fsid e9a90625-4707-4b6b-b52f-661512ea831dlast_changed 2023-03-03 12:22:43.254681created 2023-03-03 11:59:53.474948min_mon_release 14 (nautilus)0: [v2:192.168.187.201:3300/0,v1:192.168.187.201:6789/0] mon.node-11: [v2:192.168.187.202:3300/0,v1:192.168.187.202:6789/0] mon.node-22: [v2:192.168.187.203:3300/0,v1:192.168.187.203:6789/0] mon.node-3dumped monmap epoch 3</code></pre><h2><span id="二-mgr">二、MGR</span></h2><p><strong>MGR默认是主备模式</strong></p><p><strong>node-1：执行</strong></p><pre><code>[root@node-1 my-cluster]#  ceph-deploy mgr create node-2 node-3 #添加node-2 node-3 成为mgr[root@node-1 my-cluster]#  ceph -s  #查看MGR状态  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 11m)    mgr: node-1(active, since 28m), standbys: node-2, node-3 #可以看到备MGR为：node-2和node-3    osd: 3 osds: 3 up (since 18m), 3 in (since 18m)   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   3.0 GiB used, 27 GiB / 30 GiB avail    pgs:[root@node-1 my-cluster]# ceph mgr dump #查看具体MAP图</code></pre><h1><span id="十-创建资源池pool">十、创建资源池Pool</span></h1><pre><code>Node-1执行：[root@node-1 my-cluster]# ceph osd lspools #查看当前pool[root@node-1 my-cluster]# ceph osd pool create ceph-demo 64 64 #创建一个叫做ceph-demo的pool 并指定PG数为64（第一个数字），pgp数64（第二个数字）pool &#39;ceph-demo&#39; created[root@node-1 my-cluster]# ceph osd lspools #再次查看当前pool 新增了要给ceph-demo的pool1 ceph-demo[root@node-1 my-cluster]#  ceph osd pool get ceph-demo pg_num #查看ceph-demo的PG数量pg_num: 64[root@node-1 my-cluster]#  ceph osd pool get ceph-demo pgp_num #查看ceph-demo的PGP数量pgp_num: 64[root@node-1 my-cluster]#  ceph osd pool get ceph-demo size #查看ceph-demo的副本数size: 3[root@node-1 my-cluster]#  ceph osd pool get ceph-demo crush_rule #查看crush的调度算法crush_rule: replicated_rule #默认的复制规则[root@node-1 my-cluster]#   ceph osd pool set ceph-demo size 2  #可以通过set的方式调整副本数量为2set pool 1 size to 2[root@node-1 my-cluster]# ceph osd pool set ceph-demo pg_num 128 #可以通过set的方式调整pg数量为128set pool 1 pg_num to 128[root@node-1 my-cluster]# ceph osd pool set ceph-demo pgp_num 128 #可以通过set的方式调整pgp数量为128,PGP数量应该与PG数一致set pool 1 pgp_num to 12[root@node-1 my-cluster]# rbd pool init &lt;pool_name&gt; #初始化pool</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RBD写入流程</title>
      <link href="/2023/04/11/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B/"/>
      <url>/2023/04/11/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><!-- tocstop --><p><img src="/images/RBD%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B/1.jpg"></p><blockquote><p>块设备都是瘦分配的，意味着使用的越多，分配的空间越多</p></blockquote><pre><code>[root@node-1 ~]# rbd -p ceph-demo info rbd-demo.img #查看镜像信息rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023[root@node-1 ~]# rados  -p ceph-demo ls | grep rbd_data.11e24b04cf01 #查看objectrbd_data.11e24b04cf01.0000000000000960rbd_data.11e24b04cf01.0000000000000500rbd_data.11e24b04cf01.0000000000000780rbd_data.11e24b04cf01.0000000000000320rbd_data.11e24b04cf01.00000000000008c0rbd_data.11e24b04cf01.00000000000006e0rbd_data.11e24b04cf01.00000000000009ffrbd_data.11e24b04cf01.0000000000000640rbd_data.11e24b04cf01.0000000000000000rbd_data.11e24b04cf01.0000000000000280rbd_data.11e24b04cf01.0000000000000501rbd_data.11e24b04cf01.0000000000000001rbd_data.11e24b04cf01.00000000000003c0rbd_data.11e24b04cf01.0000000000000502rbd_data.11e24b04cf01.00000000000005a0rbd_data.11e24b04cf01.0000000000000460rbd_data.11e24b04cf01.0000000000000140rbd_data.11e24b04cf01.00000000000001e0rbd_data.11e24b04cf01.0000000000000820rbd_data.11e24b04cf01.00000000000000a0[root@node-1 data]# rados -p ceph-demo stat rbd_data.11e24b04cf01.000000000000007a#查看object 大小 size=比特，需要除以两次1024得出Mceph-demo/rbd_data.11e24b04cf01.000000000000007a mtime 2023-03-04 15:08:07.000000, size 4194304[root@node-1 ~]#  ceph osd map ceph-demo rbd_data.11e24b04cf01.000000000000007a #查看object 落到那个OSD和PG上 PG=1.20 最终落在0，1，2三个OSD上osdmap e339 pool &#39;ceph-demo&#39; (1) object &#39;rbd_data.11e24b04cf01.0000000000000960&#39; -&gt; pg 1.85b57b60 (1.20) -&gt; up ([2,1,0], p2) acting ([2,1,0], p2)[root@node-1 ~]# rados  -p ceph-demo ls |wc -l 查看当前ceph-demo的pool中一共存在25个object 每个object等于4 M，一共有25个，所以总共分配空间为100M，随着数据增加，会动态扩容分配空间。25[root@node-1 data]# dd if=/dev/zero of=test.img bs=1M count=1024  #尝试写入1G的文件到集群中1024+0 records in1024+0 records out1073741824 bytes (1.1 GB) copied, 5.97513 s, 180 MB/sYou have new mail in /var/spool/mail/root[root@node-1 data]# ls -lha total 1.1Gdrwxr-xr-x.  2 root root   34 Mar  4 15:08 .dr-xr-xr-x. 18 root root  236 Mar  4 02:27 ..-rw-r--r--.  1 root root    5 Mar  4 02:30 test-rw-r--r--.  1 root root 1.0G Mar  4 15:08 test.img[root@node-1 data]# rados  -p ceph-demo ls | grep rbd_data.11e24b04cf01 |wc -l  #可以看到object的数量由25个增加到了276个 每个object等于4 M，所以总计分配1,104M276[root@node-1 data]# df -HT #查看实际使用情况发现基本相当Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   14M  941M   2% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.7G   16G  15% //dev/sda1               xfs       1.1G  190M  874M  18% /boottmpfs                   tmpfs     191M     0  191M   0% /run/user/0tmpfs                   tmpfs     954M   54k  954M   1% /var/lib/ceph/osd/ceph-0/dev/rbd0               xfs        11G  1.2G  9.7G  11% /data</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> RBD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RBD块创建及映射</title>
      <link href="/2023/04/11/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%9D%97%E5%88%9B%E5%BB%BA%E5%8F%8A%E6%98%A0%E5%B0%84/"/>
      <url>/2023/04/11/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%9D%97%E5%88%9B%E5%BB%BA%E5%8F%8A%E6%98%A0%E5%B0%84/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#rbd%E5%9D%97%E5%88%9B%E5%BB%BA%E5%8F%8A%E6%98%A0%E5%B0%84">RBD块创建及映射</a><ul><li><a href="#%E4%B8%80-%E5%88%9B%E5%BB%BA%E5%9D%97%E8%AE%BE%E5%A4%87">一、创建块设备</a></li><li><a href="#%E4%BA%8C-%E4%BD%BF%E7%94%A8%E5%9D%97%E8%AE%BE%E5%A4%87">二、使用块设备</a></li></ul></li></ul><!-- tocstop --><h1><span id="rbd块创建及映射">RBD块创建及映射</span></h1><h2><span id="一-创建块设备">一、创建块设备</span></h2><p><strong>任意节点执行即可：</strong></p><pre><code>[root@node-1 my-cluster]#  rbd help create #查看create的帮助文档[root@node-1 my-cluster]#  rbd create -p ceph-demo --image rbd-demo.img --size 10G #在ceph-demo的pool中创建一个叫做rbd-demo.img的块设备，大小为10G[root@node-1 my-cluster]# rbd ls ceph-demo #查看创建结果rbd-demo.img[root@node-1 my-cluster]#  rbd create  ceph-demo/rbd-demo-1.img --size 10G #在ceph-demo的pool中创建一个叫做rbd-demo.img的块设备，大小为10G[root@node-1 my-cluster]# rbd ls ceph-demo  #查看创建结果rbd-demo-1.imgrbd-demo.img[root@node-1 my-cluster]# rbd -p ceph-demo ls  #与rbd ls ceph-demo 作用一致rbd-demo-1.imgrbd-demo.img[root@node-1 my-cluster]#  rbd info ceph-demo/rbd-demo.img #查看块设备的具体信息，与rbd info-p ceph-demo --image rbd-demo.img 一致rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Fri Mar  3 13:03:28 2023    access_timestamp: Fri Mar  3 13:03:28 2023    modify_timestamp: Fri Mar  3 13:03:28 2023[root@node-1 my-cluster]# rbd rm  ceph-demo/rbd-demo-1.img #删除块设备，与rbd rm -p ceph-demo --image rbd-demo-1.img用法一致Removing image: 100% complete...done.</code></pre><h2><span id="二-使用块设备">二、使用块设备</span></h2><pre><code>[root@node-1 my-cluster]#  rbd map ceph-demo/rbd-demo.img #映射块设备到本地rbd: sysfs write failedRBD image feature set mismatch. You can disable features unsupported by the kernel with &quot;rbd feature disable ceph-demo/rbd-demo.img object-map fast-diff deep-flatten&quot;.In some cases useful info is found in syslog - try &quot;dmesg | tail&quot;.rbd: map failed: (6) No such device or address</code></pre><blockquote><p>上述报错主要是由于Centos 7 不支持某些特性，需要手动禁用，具体可使用rbd info ceph-demo&#x2F;rbd-demo.img 进行查看，特性主要记录在features中</p></blockquote><p><strong>禁用相关特性：</strong></p><pre><code>[root@node-1 my-cluster]#  rbd info ceph-demo/rbd-demo.img #查看当前特性rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Fri Mar  3 13:03:28 2023    access_timestamp: Fri Mar  3 13:03:28 2023    modify_timestamp: Fri Mar  3 13:03:28 2023[root@node-1 my-cluster]# rbd help feature disable #查看使用文档usage: rbd feature disable [--pool &lt;pool&gt;] [--namespace &lt;namespace&gt;]                            [--image &lt;image&gt;]                            &lt;image-spec&gt; &lt;features&gt; [&lt;features&gt; ...]Disable the specified image feature.Positional arguments  &lt;image-spec&gt;         image specification                       (example: [&lt;pool-name&gt;/[&lt;namespace&gt;/]]&lt;image-name&gt;)  &lt;features&gt;           image features                       [exclusive-lock, object-map, journaling]Optional arguments  -p [ --pool ] arg    pool name  --namespace arg      namespace name  --image arg          image name[root@node-1 my-cluster]# rbd feature disable ceph-demo/rbd-demo.img deep-flatten #禁用rbd-demo.img块设备的deep-flatten特性[root@node-1 my-cluster]# rbd feature disable ceph-demo/rbd-demo.img fast-diff #禁用rbd-demo.img块设备的fast-diff特性[root@node-1 my-cluster]# rbd feature disable ceph-demo/rbd-demo.img exclusive-lock #禁用rbd-demo.img块设备的exclusive-lock特性[root@node-1 my-cluster]# rbd info ceph-demo/rbd-demo.img #查看相关特性已经被禁用rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering #layering不影响使用，可不禁用    op_features:     flags:     create_timestamp: Fri Mar  3 13:03:28 2023    access_timestamp: Fri Mar  3 13:03:28 2023    modify_timestamp: Fri Mar  3 13:03:28 2023 [root@node-1 my-cluster]#  rbd map ceph-demo/rbd-demo.img #挂载到本地/dev/rbd0[root@node-1 my-cluster]# lsblkNAME                                                                                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsda                                                                                                     8:0    0   20G  0 disk ├─sda1                                                                                                  8:1    0    1G  0 part /boot└─sda2                                                                                                  8:2    0   19G  0 part   ├─centos-root                                                                                       253:0    0   17G  0 lvm  /  └─centos-swap                                                                                       253:1    0    2G  0 lvm  [SWAP]sdb                                                                                                     8:16   0   10G  0 disk └─ceph--53398982--af97--4b05--9a0b--bf91741b7f6a-osd--block--68c09ae0--7d72--4984--b5c0--6f1476698c2b 253:2    0   10G  0 lvm  sdc                                                                                                     8:32   0   10G  0 disk sr0                                                                                                    11:0    1 1024M  0 rom  rbd0                                                                                                  252:0    0   10G  0 disk [root@node-1 my-cluster]# rbd device list #使用此命令进行查看id pool      namespace image        snap device    0  ceph-demo           rbd-demo.img -    /dev/rbd0[root@node-1 my-cluster]#  mkfs.xfs /dev/rbd0 #格式化为xfs格式Discarding blocks...Done.meta-data=/dev/rbd0              isize=512    agcount=16, agsize=163840 blks         =                       sectsz=512   attr=2, projid32bit=1         =                       crc=1        finobt=0, sparse=0data     =                       bsize=4096   blocks=2621440, imaxpct=25         =                       sunit=1024   swidth=1024 blksnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1log      =internal log           bsize=4096   blocks=2560, version=2         =                       sectsz=512   sunit=8 blks, lazy-count=1realtime =none                   extsz=4096   blocks=0, rtextents=0[root@node-1 my-cluster]# mkdir /data[root@node-1 my-cluster]# mount /dev/rbd0 /data #进行挂载[root@node-1 my-cluster]# df -HT #查看挂载情况Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   11M  944M   2% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.6G   16G  15% //dev/sda1               xfs       1.1G  190M  874M  18% /boottmpfs                   tmpfs     191M     0  191M   0% /run/user/0tmpfs                   tmpfs     954M   54k  954M   1% /var/lib/ceph/osd/ceph-0/dev/rbd0               xfs        11G   35M   11G   1% /data</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> RBD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RBD存储扩容/缩容</title>
      <link href="/2023/04/11/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%9D%97%E5%AD%98%E5%82%A8%E6%89%A9%E5%AE%B9%E5%8F%8A%E7%BC%A9%E5%AE%B9/"/>
      <url>/2023/04/11/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%9D%97%E5%AD%98%E5%82%A8%E6%89%A9%E5%AE%B9%E5%8F%8A%E7%BC%A9%E5%AE%B9/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#rbd%E5%AD%98%E5%82%A8%E6%89%A9%E5%AE%B9">RBD存储扩容</a></li></ul><!-- tocstop --><h1><span id="rbd存储扩容">RBD存储扩容</span></h1><p><strong>查看需要扩容的镜像</strong></p><pre><code>[root@node-1 data]# rbd ls ceph-demo rbd-demo.img</code></pre><p><strong>查看需要扩容的镜像</strong></p><pre><code>[root@node-1 data]# rbd -p ceph-demo info rbd-demo.img rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023</code></pre><p><strong>使用rbd resize进行扩容帮助</strong></p><pre><code>[root@node-1 data]# rbd help resize  #使用rbd resize进行扩容usage: rbd resize [--pool &lt;pool&gt;] [--namespace &lt;namespace&gt;]                   [--image &lt;image&gt;] --size &lt;size&gt; [--allow-shrink]                   [--no-progress]                   &lt;image-spec&gt; Resize (expand or shrink) image.Positional arguments  &lt;image-spec&gt;         image specification                       (example: [&lt;pool-name&gt;/[&lt;namespace&gt;/]]&lt;image-name&gt;)Optional arguments  -p [ --pool ] arg    pool name  --namespace arg      namespace name  --image arg          image name  -s [ --size ] arg    image size (in M/G/T) [default: M]  --allow-shrink       permit shrinking  #缩容操作，不建议，可能会导致数据丢失  --no-progress        disable progress output</code></pre><p><strong>将ceph-demo池下的rbd-demo.img镜像扩容到20G</strong></p><pre><code>[root@node-1 data]#  rbd resize ceph-demo/rbd-demo.img --size 20G Resizing image: 100% complete...done.[root@node-1 data]# rbd -p ceph-demo info rbd-demo.img #查看已经扩容到20G了rbd image &#39;rbd-demo.img&#39;:    size 20 GiB in 5120 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023</code></pre><p><strong>#将上述扩容到20G的硬盘缩容到15G</strong></p><pre><code>[root@node-1 data]#  rbd resize ceph-demo/rbd-demo.img --allow-shrink --size 15GResizing image: 100% complete...done.[root@node-1 data]# rbd -p ceph-demo info rbd-demo.imgrbd image &#39;rbd-demo.img&#39;:    size 15 GiB in 3840 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> RBD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RGW对象存储</title>
      <link href="/2023/04/11/Ceph/5.RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/"/>
      <url>/2023/04/11/Ceph/5.RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#ceph-%E5%AF%B9%E8%B1%A1%E7%BD%91%E5%85%B3%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D">CEPH 对象网关架构介绍</a></li><li><a href="#%E9%83%A8%E7%BD%B2rgw">部署RGW</a><ul><li><a href="#%E6%9F%A5%E7%9C%8B%E5%B8%AE%E5%8A%A9%E6%96%87%E6%A1%A3">查看帮助文档</a></li><li><a href="#%E4%BD%BF%E7%94%A8node-1%E4%BD%9C%E4%B8%BA%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3">使用node-1作为对象存储网关</a></li><li><a href="#%E6%A3%80%E6%9F%A5%E6%98%AF%E5%90%A6%E6%B7%BB%E5%8A%A0%E6%88%90%E5%8A%9F">检查是否添加成功</a></li><li><a href="#%E6%A3%80%E6%9F%A5%E6%9C%8D%E5%8A%A1%E8%BF%90%E8%A1%8C%E6%83%85%E5%86%B5">检查服务运行情况</a></li></ul></li><li><a href="#%E4%BF%AE%E6%94%B9rgw%E9%BB%98%E8%AE%A4%E7%AB%AF%E5%8F%A3">修改RGW默认端口</a><ul><li><a href="#%E4%BF%AE%E6%94%B9%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">修改节点配置文件</a></li><li><a href="#%E5%B0%86%E9%85%8D%E7%BD%AE%E5%A5%BD%E7%9A%84cephconf-%E6%8E%A8%E9%80%81%E5%88%B0%E6%89%80%E6%9C%89%E8%8A%82%E7%82%B9%E4%B8%AD">将配置好的ceph.conf 推送到所有节点中</a></li><li><a href="#%E9%87%8D%E5%90%AFrgw%E6%9C%8D%E5%8A%A1">重启rgw服务</a></li><li><a href="#%E6%A0%A1%E9%AA%8C%E9%85%8D%E7%BD%AE%E6%98%AF%E5%90%A6%E7%94%9F%E6%95%88-%E5%8F%AF%E4%BF%AE%E6%94%B9%E6%88%90443%E7%AB%AF%E5%8F%A3%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6%E5%8F%AF%E4%BD%BF%E7%94%A8%E7%99%BE%E5%BA%A6%E8%BF%9B%E8%A1%8C%E6%9F%A5%E8%AF%A2%E5%85%B7%E4%BD%93%E9%85%8D%E7%BD%AE">校验配置是否生效 可修改成443端口，添加证书可使用百度进行查询具体配置</a></li></ul></li><li><a href="#rgw%E4%B9%8Bs3%E6%8E%A5%E5%8F%A3%E4%BD%BF%E7%94%A8">RGW之s3接口使用</a><ul><li><a href="#%E6%9F%A5%E7%9C%8B%E5%B8%AE%E5%8A%A9%E6%96%87%E6%A1%A3%E4%BD%BF%E7%94%A8user-create%E8%BF%9B%E8%A1%8C%E5%88%9B%E5%BB%BA">查看帮助文档，使用user create进行创建</a></li><li><a href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%98%BE%E7%A4%BA%E5%90%8D%E4%B8%BAceph-s3-user-demo%E7%9A%84%E7%94%A8%E6%88%B7%E5%B9%B6%E6%8C%87%E5%AE%9Auid%E4%B8%BAceph-s3-user">创建一个显示名为Ceph S3 User Demo的用户，并指定uid为ceph-s3-user</a></li><li><a href="#%E4%BF%9D%E5%AD%98%E7%94%A8%E6%88%B7%E5%92%8C%E5%AF%86%E7%A0%81%E4%BF%A1%E6%81%AF">保存用户和密码信息</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8Dradosgw%E5%88%9B%E5%BB%BA%E7%9A%84%E7%94%A8%E6%88%B7">查看当前radosgw创建的用户</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E7%94%A8%E6%88%B7%E7%9A%84%E5%85%B7%E4%BD%93%E4%BF%A1%E6%81%AF">查看用户的具体信息</a></li><li><a href="#%E5%AE%89%E8%A3%85python-boto%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%8C%85">安装python-boto的软件包</a></li><li><a href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAs3-clientpy%E7%9A%84python%E8%84%9A%E6%9C%AC">创建一个s3-client.py的python脚本</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%9A%84pool">查看自动生成的pool</a></li><li><a href="#%E5%88%9B%E5%BB%BAceph-s3-bucket%E7%9A%84bucket">创建ceph-s3-bucket的bucket</a></li><li><a href="#%E5%86%8D%E6%AC%A1%E6%9F%A5%E7%9C%8B%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%9A%84pool">再次查看自动生成的pool</a></li><li><a href="#%E5%AE%89%E8%A3%85s3cmd%E7%9A%84%E5%B7%A5%E5%85%B7%E5%AE%9E%E7%8E%B0%E4%B8%8A%E4%BC%A0%E4%B8%8B%E8%BD%BD%E5%8A%9F%E8%83%BD">安装s3cmd的工具实现上传下载功能</a></li><li><a href="#%E9%85%8D%E7%BD%AEs3cmd%E7%9A%84%E5%B7%A5%E5%85%B7">配置s3cmd的工具</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E7%94%9F%E6%88%90%E7%9A%84%E6%96%87%E4%BB%B6">查看生成的文件</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8Dbuck%E6%83%85%E5%86%B5">查看当前buck情况</a></li><li><a href="#%E5%88%9B%E5%BB%BAbucket">创建bucket</a></li><li><a href="#%E6%9F%A5%E7%9C%8Bbucket">查看bucket</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6">上传文件</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E5%A4%B9">上传文件夹</a></li><li><a href="#%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6">下载文件</a></li><li><a href="#%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6">删除文件</a></li><li><a href="#%E5%88%A0%E9%99%A4%E7%9B%AE%E5%BD%95">删除目录</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E5%AD%98%E5%82%A8%E6%B1%A0%E5%8F%98%E5%8C%96">查看存储池变化</a></li></ul></li><li><a href="#rgw%E4%B9%8Bswift%E6%8E%A5%E5%8F%A3%E4%BD%BF%E7%94%A8">RGW之Swift接口使用</a><ul><li><a href="#%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8D%E7%94%A8%E6%88%B7">查看当前用户</a></li><li><a href="#%E6%A0%B9%E6%8D%AE%E5%BD%93%E5%89%8D%E7%94%A8%E6%88%B7%E5%88%9B%E5%BB%BAswift%E7%94%A8%E6%88%B7">根据当前用户创建swift用户</a></li><li><a href="#%E7%94%9F%E6%88%90swift%E7%9A%84key">生成Swift的key</a></li><li><a href="#%E5%AE%89%E8%A3%85swift%E5%AE%A2%E6%88%B7%E7%AB%AF">安装Swift客户端</a></li><li><a href="#%E6%9F%A5%E7%9C%8Bbucket-1">查看bucket</a></li><li><a href="#%E5%88%9B%E5%BB%BAswift%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E6%96%87%E4%BB%B6">创建Swift环境变量文件</a></li><li><a href="#%E9%AA%8C%E8%AF%81%E5%8F%98%E9%87%8F%E6%98%AF%E5%90%A6%E7%94%9F%E6%95%88">验证变量是否生效</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6-1">上传文件</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E7%9B%AE%E5%BD%95">上传目录</a></li><li><a href="#%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6-1">下载文件</a></li><li><a href="#%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6-1">删除文件</a></li></ul></li></ul><!-- tocstop --><h1><span id="ceph-对象网关架构介绍">CEPH 对象网关架构介绍</span></h1><p>Ceph 对象网关是建立在 librados. 它在应用程序和 Ceph 存储集群之间提供了一个 RESTful 网关。Ceph 对象存储支持两种接口：</p><ol><li>S3 兼容：通过与 Amazon S3 RESTful API 的大部分子集兼容的接口提供对象存储功能。</li><li>Swift 兼容：通过与 OpenStack Swift API 的大部分子集兼容的接口提供对象存储功能。</li></ol><p>Ceph 对象存储使用 Ceph 对象网关守护进程 ( radosgw)，这是一个设计用于与 Ceph 存储集群交互的 HTTP 服务器。</p><p>Ceph 对象网关提供与 Amazon S3 和 OpenStack Swift 兼容的接口，并且有自己的用户管理。Ceph 对象网关可以将数据存储在同一个 Ceph 存储集群中，其中存储了来自 Ceph 文件系统客户端和 Ceph 块设备客户端的数据。S3 API 和 Swift API 共享一个公共命名空间，这使得可以使用一个 API 将数据写入 Ceph 存储集群，然后使用另一个 API 检索该数据。</p><p><img src="/images/RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3/1.jpg"></p><blockquote><p>Ceph 对象存储不使用Ceph 元数据服务器。</p></blockquote><h1><span id="部署rgw">部署RGW</span></h1><h2><span id="查看帮助文档">查看帮助文档</span></h2><pre><code>[root@node-1 my-cluster]# ceph-deploy rgw -h  usage: ceph-deploy rgw [-h] &#123;create&#125; ...Ceph RGW daemon managementpositional arguments:  &#123;create&#125;    create    Create an RGW instanceoptional arguments:  -h, --help  show this help message and exit</code></pre><h2><span id="使用node-1作为对象存储网关">使用node-1作为对象存储网关</span></h2><pre><code>[root@node-1 my-cluster]# ceph-deploy rgw create [HOST[:NAME] ...]usage: ceph-deploy rgw create [-h] HOST[:NAME] [HOST[:NAME] ...][root@node-1 my-cluster]# ceph-deploy rgw create node-1</code></pre><h2><span id="检查是否添加成功">检查是否添加成功</span></h2><pre><code>[root@node-1 my-cluster]# ceph -s  #查看集群多了一个rgw的对象网关  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 32s)    mgr: node-1(active, since 18h), standbys: node-2, node-3    osd: 3 osds: 3 up (since 2h), 3 in (since 5h)    rgw: 1 daemon active (node-1)   task status:   data:    pools:   5 pools, 192 pgs    objects: 468 objects, 1.0 GiB    usage:   6.2 GiB used, 24 GiB / 30 GiB avail    pgs:     192 active+clean[root@node-1 my-cluster]# netstat -antupl | grep 7480 #查看监听端口tcp        0      0 0.0.0.0:7480            0.0.0.0:*               LISTEN      66157/radosgw       tcp6       0      0 :::7480                 :::*                    LISTEN      66157/radosgw [root@node-1 my-cluster]#  yum whatprovides &quot;*bin/netstat&quot; #如果netstat执行失败可查看是否没有安装net-toolsLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.aliyun.comnet-tools-2.0-0.25.20131004git.el7.x86_64 : Basic networking toolsRepo        : baseMatched from:Filename    : /bin/netstatnet-tools-2.0-0.25.20131004git.el7.x86_64 : Basic networking toolsRepo        : @baseMatched from:Filename    : /bin/netsta[root@node-1 my-cluster]#  yum install -y net-tools</code></pre><h2><span id="检查服务运行情况">检查服务运行情况</span></h2><pre><code>[root@node-1 my-cluster]# curl http://node-1:7480 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ListAllMyBucketsResult xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;</code></pre><h1><span id="修改rgw默认端口">修改RGW默认端口</span></h1><h2><span id="修改节点配置文件">修改节点配置文件</span></h2><p><strong>node-1节点执行：</strong></p><pre><code>[root@node-1 my-cluster]# cat ceph.conf [global]fsid = e9a90625-4707-4b6b-b52f-661512ea831dpublic_network = 192.168.187.0/24cluster_network = 192.168.199.0/24mon_initial_members = node-1mon_host = 192.168.187.201auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephx[client.rgw.node-1] #新增如下配置</code></pre><h2><span id="将配置好的cephconf-推送到所有节点中">将配置好的ceph.conf 推送到所有节点中</span></h2><blockquote><p>–overwrite-conf 由于其他节点已经有ceph.conf 文件，所以需要加此参数进行覆盖</p></blockquote><p>[root@node-1 my-cluster]# ceph-deploy –overwrite-conf config push node-1 node-2 node-3 </p><h2><span id="重启rgw服务">重启rgw服务</span></h2><pre><code>[root@node-1 my-cluster]#  systemctl restart ceph-radosgw.target</code></pre><h2><span id="校验配置是否生效-可修改成443端口添加证书可使用百度进行查询具体配置">校验配置是否生效 可修改成443端口，添加证书可使用百度进行查询具体配置</span></h2><pre><code>[root@node-1 my-cluster]# netstat -antupl | grep 80 tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      67161/radosgw</code></pre><h1><span id="rgw之s3接口使用">RGW之s3接口使用</span></h1><blockquote><p> <strong>使用对象存储的前提是需要创建用户，一种是S3风格的，一种是swift风格的</strong></p></blockquote><h2><span id="查看帮助文档使用user-create进行创建">查看帮助文档，使用user create进行创建</span></h2><pre><code>[root@node-1 my-cluster]# radosgw-admin -h | grep user   user create                create a new user  user modify                modify user  user info                  get user info  user rm                    remove user  user suspend               suspend a user  user enable                re-enable user after suspension  user check                 check user info  user stats                 show user stats as accounted by quota subsystem  user list                  list users  caps add                   add user capabilities  caps rm                    remove user capabilities  subuser create             create a new subuser  subuser modify             modify subuser  subuser rm                 remove subuser  bucket link                link bucket to specified user  bucket unlink              unlink bucket from specified user  usage show                 show usage (by user, by bucket, date range)  usage trim                 trim usage (by user, by bucket, date range)   --uid=&lt;id&gt;                user id   --subuser=&lt;name&gt;          subuser name   --email=&lt;email&gt;           user&#39;s email address   --access=&lt;access&gt;         Set access permissions for sub-user, should be one   --display-name=&lt;name&gt;     user&#39;s display name   --max-buckets             max number of buckets for a user   --admin                   set the admin flag on the user   --system                  set the system flag on the user   --op-mask                 set the op mask on the user   --purge-data              when specified, user removal will also purge all the                             user data   --purge-keys              when specified, subuser removal will also purge all the                             subuser keys   --sync-stats              option to &#39;user stats&#39;, update user stats with current                             stats reported by user&#39;s buckets indexes   --reset-stats             option to &#39;user stats&#39;, reset stats in accordance with user buckets   --caps=&lt;caps&gt;             list of caps (e.g., &quot;usage=read, write; user=read&quot;)   --quota-scope             scope of quota (bucket, user)  --setuser USER    set uid to user or uid (and gid to user&#39;s gid)</code></pre><h2><span id="创建一个显示名为ceph-s3-user-demo的用户并指定uid为ceph-s3-user">创建一个显示名为Ceph S3 User Demo的用户，并指定uid为ceph-s3-user</span></h2><pre><code>[root@node-1 my-cluster]# radosgw-admin user create --uid ceph-s3-user --display-name &quot;Ceph S3 User Demo&quot; &#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000, #最大只允许创建1000个buckets    &quot;subusers&quot;: [],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;, #用户ID            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;, #SSH的key            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot; #  secret的key        &#125;    ],    &quot;swift_keys&quot;: [],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123; #bucket配额信息        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123; #用户的配额信息        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="保存用户和密码信息">保存用户和密码信息</span></h2><pre><code>[root@node-1 my-cluster]# cat key.txt      &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],</code></pre><h2><span id="查看当前radosgw创建的用户">查看当前radosgw创建的用户</span></h2><pre><code>[root@node-1 my-cluster]#  radosgw-admin user list [    &quot;ceph-s3-user&quot;]</code></pre><h2><span id="查看用户的具体信息">查看用户的具体信息</span></h2><pre><code>[root@node-1 my-cluster]# radosgw-admin user info --uid ceph-s3-user &#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000,    &quot;subusers&quot;: [],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],    &quot;swift_keys&quot;: [],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="安装python-boto的软件包">安装python-boto的软件包</span></h2><pre><code>[root@node-1 my-cluster]# yum install python-boto</code></pre><h2><span id="创建一个s3-clientpy的python脚本">创建一个s3-client.py的python脚本</span></h2><pre><code>[root@node-1 my-cluster]# cat s3-client.pyimport botoimport boto.s3.connectionaccess_key = &#39;5SCX55201QUQ0FOPCPD6&#39; #上述保存的keysecret_key = &#39;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&#39; #上述保存的keyconn = boto.connect_s3(        aws_access_key_id = access_key,        aws_secret_access_key = secret_key,        host = &#39;192.168.187.201&#39;, port=80, #服务器的IP及端口        is_secure=False,calling_format=boto.s3.connection.OrdinaryCallingFormat(),        )bucket = conn.create_bucket(&#39;ceph-s3-bucket&#39;)for bucket in conn.get_all_buckets():        print(&quot;&#123;name&#125;\t&#123;created&#125;&quot;.format(                name = bucket.name,                created = bucket.creation_date,        ))</code></pre><h2><span id="查看自动生成的pool">查看自动生成的pool</span></h2><pre><code>[root@node-1 my-cluster]# ceph osd lspools1 ceph-demo2 .rgw.root3 default.rgw.control4 default.rgw.meta5 default.rgw.log</code></pre><h2><span id="创建ceph-s3-bucket的bucket">创建ceph-s3-bucket的bucket</span></h2><pre><code>[root@node-1 my-cluster]#  python  s3-client.py ceph-s3-bucket    2023-03-04T12:52:01.787Z</code></pre><h2><span id="再次查看自动生成的pool">再次查看自动生成的pool</span></h2><blockquote><p>执行后可以看到多了一个6 default.rgw.buckets.index的pool，写入数据后还会增加一个data的pool</p></blockquote><pre><code>[root@node-1 my-cluster]# ceph osd lspools1 ceph-demo2 .rgw.root #rgw的根pool3 default.rgw.control #rgw的配置pool4 default.rgw.meta #rgw的元数据pool5 default.rgw.log #rgw的日志pool 6 default.rgw.buckets.index #rgw的索引pool</code></pre><h2><span id="安装s3cmd的工具实现上传下载功能">安装s3cmd的工具实现上传下载功能</span></h2><p><code>[root@node-1 my-cluster]#  yum install s3cmd</code></p><h2><span id="配置s3cmd的工具">配置s3cmd的工具</span></h2><pre><code>[root@node-1 my-cluster]# s3cmd --configureEnter new values or accept defaults in brackets with Enter.Refer to user manual for detailed description of all options.Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.Access Key: 5SCX55201QUQ0FOPCPD6 #输入Secret Key: k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm #输入Default Region [US]: Use &quot;s3.amazonaws.com&quot; for S3 Endpoint and not modify it to the 回车target Amazon S3.S3 Endpoint [s3.amazonaws.com]: 192.168.187.201:80 #输入回车Use &quot;%(bucket)s.s3.amazonaws.com&quot; to the target Amazon S3. &quot;%(bucket)s&quot; and &quot;%(location)s&quot; vars can be usedif the target S3 system supports dns based buckets.DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: 192.168.187.201:80/%(bucket)s #输入回车Encryption password is used to protect your files from readingby unauthorized persons while in transfer to S3Encryption password: Path to GPG program [/usr/bin/gpg]: When using secure HTTPS protocol all communication with Amazon S3servers is protected from 3rd party eavesdropping. This method isslower than plain HTTP, and can only be proxied with Python 2.7 or newerUse HTTPS protocol [Yes]: no #输入回车On some networks all internet access must go through a HTTP proxy.Try setting it here if you can&#39;t connect to S3 directlyHTTP Proxy server name: New settings:  Access Key: 5SCX55201QUQ0FOPCPD6  Secret Key: k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm  Default Region: US  S3 Endpoint: 192.168.187.201:80  DNS-style bucket+hostname:port template for accessing a bucket: 192.168.187.201:80/%(bucket)s  Encryption password:   Path to GPG program: /usr/bin/gpg  Use HTTPS protocol: False  HTTP Proxy server name:   HTTP Proxy server port: 0Test access with supplied credentials? [Y/n] y #输入Please wait, attempting to list all buckets...Success. Your access key and secret key worked fine :-)Now verifying that encryption works...Not configured. Never mind.Save settings? [y/N] y #输入Configuration saved to &#39;/root/.s3cfg&#39;You have new mail in /var/spool/mail/root</code></pre><h2><span id="查看生成的文件">查看生成的文件</span></h2><pre><code>[root@node-1 my-cluster]# cat /root/.s3cfg[default]access_key = 5SCX55201QUQ0FOPCPD6access_token = add_encoding_exts = add_headers = bucket_location = USca_certs_file = cache_file = check_ssl_certificate = Truecheck_ssl_hostname = Truecloudfront_host = cloudfront.amazonaws.comconnection_max_age = 5connection_pooling = Truecontent_disposition = content_type = default_mime_type = binary/octet-streamdelay_updates = Falsedelete_after = Falsedelete_after_fetch = Falsedelete_removed = Falsedry_run = Falseenable_multipart = Trueencrypt = Falseexpiry_date = expiry_days = expiry_prefix = follow_symlinks = Falseforce = Falseget_continue = Falsegpg_command = /usr/bin/gpggpg_decrypt = %(gpg_command)s -d --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)sgpg_encrypt = %(gpg_command)s -c --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)sgpg_passphrase = guess_mime_type = Truehost_base = 192.168.187.201:80host_bucket = 192.168.187.201:80/%(bucket)shuman_readable_sizes = Falseinvalidate_default_index_on_cf = Falseinvalidate_default_index_root_on_cf = Trueinvalidate_on_cf = Falsekms_key = limit = -1limitrate = 0list_allow_unordered = Falselist_md5 = Falselog_target_prefix = long_listing = Falsemax_delete = -1mime_type = multipart_chunk_size_mb = 15multipart_copy_chunk_size_mb = 1024multipart_max_chunks = 10000preserve_attrs = Trueprogress_meter = Trueproxy_host = proxy_port = 0public_url_use_https = Falseput_continue = Falserecursive = Falserecv_chunk = 65536reduced_redundancy = Falserequester_pays = Falserestore_days = 1restore_priority = Standardsecret_key = k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldmsend_chunk = 65536server_side_encryption = Falsesignature_v2 = Falsesignurl_use_https = Falsesimpledb_host = sdb.amazonaws.comskip_existing = Falsesocket_timeout = 300ssl_client_cert_file = ssl_client_key_file = stats = Falsestop_on_error = Falsestorage_class = throttle_max = 100upload_id = urlencoding_mode = normaluse_http_expect = Falseuse_https = Falseuse_mime_magic = Trueverbosity = WARNINGwebsite_endpoint = http://%(bucket)s.s3-website-%(location)s.amazonaws.com/website_error = website_index = index.html</code></pre><h2><span id="查看当前buck情况">查看当前buck情况</span></h2><pre><code>[root@node-1 ~]# s3cmd ls 2023-03-04 12:52  s3://ceph-s3-bucket</code></pre><h2><span id="创建bucket">创建bucket</span></h2><blockquote><p>使用cmd创建一个叫做s3cmd-demo的bucket</p></blockquote><pre><code>[root@node-1 ~]# s3cmd mb s3://s3cmd-demo ERROR: S3 error: 403 (SignatureDoesNotMatch) #由于当前设置的版本不一致导致</code></pre><blockquote><p>修改版本</p></blockquote><pre><code>[root@node-1 ~]# vi /root/.s3cfg signature_v2 = True</code></pre><blockquote><p>重新创建</p></blockquote><pre><code>[root@node-1 ~]# s3cmd mb s3://s3cmd-demo Bucket &#39;s3://s3cmd-demo/&#39; created</code></pre><h2><span id="查看bucket">查看bucket</span></h2><pre><code>[root@node-1 ~]# s3cmd ls 2023-03-04 12:52  s3://ceph-s3-bucket2023-03-04 17:38  s3://s3cmd-demo</code></pre><h2><span id="上传文件">上传文件</span></h2><pre><code>[root@node-1 ~]#  s3cmd put /etc/fstab s3://s3cmd-demo/fastab-demo #报错，经查询最简单的解决办法是添加OSD或者修改ceph.conf 文件upload: &#39;/etc/fstab&#39; -&gt; &#39;s3://s3cmd-demo/fastab-demo&#39;  [1 of 1] 465 of 465   100% in    0s   866.93 B/s  doneERROR: S3 error: 416 (InvalidRange)</code></pre><blockquote><p>报错，经查询最简单的解决办法是添加OSD或者修改ceph.conf 文件,原因是当前OSD数量太少；</p></blockquote><blockquote><p>添加OSD</p></blockquote><pre><code>[root@node-1 my-cluster]# ceph-deploy osd create node-1 --data /dev/sdc --journal /dev/sdc[root@node-1 my-cluster]# ceph-deploy osd create node-2 --data /dev/sdc --journal /dev/sdc[root@node-1 my-cluster]# ceph-deploy osd create node-3 --data /dev/sdc --journal /dev/sdc</code></pre><blockquote><p>尝试重新上传</p></blockquote><pre><code>[root@node-1 my-cluster]#  s3cmd put /etc/fstab s3://s3cmd-demo/fastab-demoupload: &#39;/etc/fstab&#39; -&gt; &#39;s3://s3cmd-demo/fastab-demo&#39;  [1 of 1] 465 of 465   100% in    1s   317.00 B/s  done</code></pre><blockquote><p>查看上传的文件</p></blockquote><pre><code>[root@node-1 my-cluster]# s3cmd ls s3://s3cmd-demo 2023-03-04 17:47          465  s3://s3cmd-demo/fastab-demo</code></pre><h2><span id="上传文件夹">上传文件夹</span></h2><blockquote><p>–recursive 上传目录需要使用递归的方式</p></blockquote><p><code>[root@node-1 my-cluster]# s3cmd put /etc s3://s3cmd-demo/etc/ --recursive </code></p><blockquote><p>查看上传结果</p></blockquote><pre><code>[root@node-1 my-cluster]# s3cmd ls s3://s3cmd-demo/etc/etc/                          DIR  s3://s3cmd-demo/etc/etc/NetworkManager/                          DIR  s3://s3cmd-demo/etc/etc/X11/                          DIR  s3://s3cmd-demo/etc/etc/audisp/                          DIR  s3://s3cmd-demo/etc/etc/audit/                          DIR  s3://s3cmd-demo/etc/etc/bash_completion.d/                          DIR  s3://s3cmd-demo/etc/etc/ceph/                          DIR  s3://s3cmd-demo/etc/etc/cron.d/</code></pre><h2><span id="下载文件">下载文件</span></h2><blockquote><p>#下载文件并重命名为yum.conf-download</p></blockquote><pre><code>[root@node-1 ~]# s3cmd get s3://s3cmd-demo/etc/etc/yum.conf yum.conf-download download: &#39;s3://s3cmd-demo/etc/etc/yum.conf&#39; -&gt; &#39;yum.conf-download&#39;  [1 of 1] 970 of 970   100% in    0s    22.27 KB/s  done</code></pre><h2><span id="删除文件">删除文件</span></h2><pre><code>[root@node-1 ~]#  s3cmd rm s3://s3cmd-demo/fastab-demo #删除操作delete: &#39;s3://s3cmd-demo/fstab-demo&#39;</code></pre><h2><span id="删除目录">删除目录</span></h2><p><code>s3cmd rm s3://s3cmd-demo/etc --recursive</code></p><h2><span id="查看存储池变化">查看存储池变化</span></h2><blockquote><p>完成上述操作后ceph会多出一个default.rgw.buckets.data的pool</p></blockquote><pre><code>[root@node-1 ~]# ceph osd lspools1 ceph-demo2 .rgw.root3 default.rgw.control4 default.rgw.meta5 default.rgw.log6 default.rgw.buckets.index7 default.rgw.buckets.data #rgw的数据pool</code></pre><blockquote><p>查看内容</p></blockquote><pre><code>[root@node-1 ~]# rados -p default.rgw.buckets.data ls 3db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_23db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_1</code></pre><blockquote><p>#重新上传文件</p></blockquote><p><code>root@node-1 ~]#  s3cmd put /etc/fstab s3://s3cmd-demo/fastab-demo</code></p><blockquote><p>再次查看内容</p></blockquote><pre><code>[root@node-1 ~]# rados -p default.rgw.buckets.data ls  #可以看到多了一个3db4c650-9c23-4843-8015-97b79566b3b0.5970.2_fastab-demo的对象3db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_23db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_13db4c650-9c23-4843-8015-97b79566b3b0.5970.2_fastab-demo #可以看到这个文件多了写前缀，前缀是存放在default.rgw.buckets.index的pool中的</code></pre><blockquote><p>查看索引</p></blockquote><pre><code>[root@node-1 ~]# rados -p default.rgw.buckets.index ls .dir.3db4c650-9c23-4843-8015-97b79566b3b0.5970.2</code></pre><h1><span id="rgw之swift接口使用">RGW之Swift接口使用</span></h1><blockquote><p><strong>需要在原有用户的基础上进行添加</strong></p></blockquote><h2><span id="查看当前用户">查看当前用户</span></h2><pre><code>[root@node-1 ~]# radosgw-admin user list [    &quot;ceph-s3-user&quot;]</code></pre><h2><span id="根据当前用户创建swift用户">根据当前用户创建swift用户</span></h2><pre><code>[root@node-1 ~]# radosgw-admin subuser create --uid ceph-s3-user --subuser=ceph-s3-user:swift --access=full&#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000,    &quot;subusers&quot;: [        &#123;            &quot;id&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;permissions&quot;: &quot;full-control&quot;        &#125;    ],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],    &quot;swift_keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;secret_key&quot;: &quot;9sWyeIFQphNButuZp2zNs2t7xU9qEkgW27yHxqBL&quot;        &#125;    ],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="生成swift的key">生成Swift的key</span></h2><pre><code>[root@node-1 ~]# radosgw-admin key create --subuser=ceph-s3-user:swift --key-type=swift --gen-secret &#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000,    &quot;subusers&quot;: [        &#123;            &quot;id&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;permissions&quot;: &quot;full-control&quot;        &#125;    ],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],    &quot;swift_keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;secret_key&quot;: &quot;mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjh&quot; #提前保存        &#125;    ],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="安装swift客户端">安装Swift客户端</span></h2><blockquote><p>安装Swift客户端需要先安装pip</p></blockquote><pre><code>[root@node-1 ~]# yum install python-pip #安装pip 由于当前pip版本较老（8.1），无法通过pip install -U pip 进行升级，只能手动进行升级[root@node-1 ~]wget https://files.pythonhosted.org/packages/0b/f5/be8e741434a4bf4ce5dbc235aa28ed0666178ea8986ddc10d035023744e6/pip-20.2.4.tar.gz  #下载安装包[root@node-1 ~]tar -zxvf pip-20.2.4.tar.gz  # 解压[root@node-1 ~]cd pip-20.2.4/[root@node-1 ~]sudo python setup.py install #给予权限不然可能安装失败[root@node-1 ~]pip install -U pip #再次更新DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.Collecting pip  Downloading pip-20.3.4-py2.py3-none-any.whl (1.5 MB)     |████████████████████████████████| 1.5 MB 1.0 MB/s Installing collected packages: pip  Attempting uninstall: pip    Found existing installation: pip 20.2.4    Uninstalling pip-20.2.4:      Successfully uninstalled pip-20.2.4Successfully installed pip-20.3.4</code></pre><blockquote><p>再次安装Swift客户端</p></blockquote><pre><code>[root@node-1 pip-20.2.4]# pip install  python-swiftclient #安装成功DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.Collecting python-swiftclient  Downloading python_swiftclient-3.13.1-py2.py3-none-any.whl (87 kB)     |████████████████████████████████| 87 kB 6.4 kB/s Collecting futures&gt;=3.0.0; python_version == &quot;2.7&quot;  Downloading futures-3.4.0-py2-none-any.whl (16 kB)Requirement already satisfied: requests&gt;=1.1.0 in /usr/lib/python2.7/site-packages (from python-swiftclient) (2.6.0)Requirement already satisfied: six&gt;=1.9.0 in /usr/lib/python2.7/site-packages (from python-swiftclient) (1.9.0)Installing collected packages: futures, python-swiftclientSuccessfully installed futures-3.4.0 python-swiftclient-3.13.1</code></pre><h2><span id="查看bucket">查看bucket</span></h2><blockquote><p>查看，list可换成upload或download实现上传下载</p></blockquote><pre><code>[root@node-1 ~]# swift -A http://192.168.187.201:80/auth -U ceph-s3-user:swift -K mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjh list ceph-s3-buckets3cmd-demo</code></pre><h2><span id="创建swift环境变量文件">创建Swift环境变量文件</span></h2><pre><code>[root@node-1 ~]# cat swift-openrc.shexport ST_AUTH=http://192.168.187.201:80/authexport ST_USER=ceph-s3-user:swiftexport ST_KEY=mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjh</code></pre><h2><span id="验证变量是否生效">验证变量是否生效</span></h2><pre><code>[root@node-1 ~]# set |grep ST #查看环境变量是否生效DIRSTACK=()HISTCONTROL=ignoredupsHISTFILE=/root/.bash_historyHISTFILESIZE=1000HISTSIZE=1000HOSTNAME=node-1HOSTTYPE=x86_64OSTYPE=linux-gnuPIPESTATUS=([0]=&quot;0&quot;)SELINUX_LEVEL_REQUESTED=SELINUX_ROLE_REQUESTED=ST_AUTH=http://192.168.187.201:80/authST_KEY=mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjhexport[root@node-1 ~]# swift list  #验证变量成功ceph-s3-buckets3cmd-demo</code></pre><h2><span id="上传文件">上传文件</span></h2><p><code>[root@node-1 ~]# swift upload swift-demo /etc/shadow </code></p><h2><span id="上传目录">上传目录</span></h2><p><code>[root@node-1 ~]# swift upload swift-demo /etc/ </code></p><h2><span id="下载文件">下载文件</span></h2><pre><code>[root@node-1 ~]# swift download swift-demo etc/passwdetc/passwd [auth 0.008s, headers 0.011s, total 0.012s, 0.278 MB/s]</code></pre><p>##查看详细信息</p><pre><code>[root@node-1 ~]# swift stat swift-demo                      Account: v1                    Container: swift-demo                      Objects: 1694                        Bytes: 35940093                     Read ACL:                    Write ACL:                      Sync To:                     Sync Key:X-Container-Bytes-Used-Actual: 40927232                Accept-Ranges: bytes             X-Storage-Policy: default-placement              X-Storage-Class: STANDARD                Last-Modified: Sat, 04 Mar 2023 19:14:33 GMT                  X-Timestamp: 1677957085.09881                   X-Trans-Id: tx0000000000000000000d1-0064043339-85b3-default                 Content-Type: text/plain; charset=utf-8       X-Openstack-Request-Id: tx0000000000000000000d1-0064043339-85b3-default</code></pre><h2><span id="删除文件">删除文件</span></h2><blockquote><p>#删除文件，不要使用 swift delete -a 会删除所有bucket</p></blockquote><pre><code>[root@node-1 ~]# swift delete swift-demo etc/shadow #删除文件，不要使用 swift delete -a 会删除所有bucketetc/shadow[root@node-1 ~]# swift list swift-demo</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> RGW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph FS文件存储</title>
      <link href="/2023/04/11/Ceph/6.Ceph_%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/Ceph_%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"/>
      <url>/2023/04/11/Ceph/6.Ceph_%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/Ceph_%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E5%AE%89%E8%A3%85mds%E6%9C%8D%E5%8A%A1">安装MDS服务</a></li><li><a href="#%E5%88%9B%E5%BB%BAceph-fs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F">创建Ceph FS文件系统</a></li><li><a href="#%E6%9F%A5%E7%9C%8Bfs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F">查看fs文件系统</a></li><li><a href="#%E5%86%85%E6%A0%B8%E6%8C%82%E8%BD%BD">内核挂载</a></li><li><a href="#%E7%94%A8%E6%88%B7%E6%80%81%E6%8C%82%E8%BD%BD">用户态挂载</a></li></ul><!-- tocstop --><p>Ceph FS文件存储时由NAS衍生过来的。</p><p>Ceph 文件系统或CephFS是一个 POSIX 兼容的文件系统，构建在 Ceph 的分布式对象存储RADOS之上。CephFS致力于为各种应用程序提供最先进、多用途、高可用性和高性能的文件存储，包括共享主目录、HPC 暂存空间和分布式工作流共享存储等传统用例。</p><p>CephFS 通过使用一些新颖的架构选择来实现这些目标。值得注意的是，文件元数据存储在与文件数据不同的 RADOS 池中，并通过可调整大小的元数据服务器集群或MDS提供服务，它可以扩展以支持更高吞吐量的元数据工作负载。文件系统的客户端可以直接访问 RADOS 来读写文件数据块。因此，工作负载可能会随着底层 RADOS 对象存储的大小线性扩展；也就是说，没有网关或代理为客户端调解数据 I&#x2F;O。</p><p>对数据的访问通过 MDS 集群进行协调，MDS 集群充当由客户端和 MDS 共同维护的分布式元数据缓存状态的权威。每个 MDS 将元数据的变化聚合成一系列高效写入 RADOS 上的日志；MDS 没有在本地存储任何元数据状态。该模型允许在 POSIX 文件系统的上下文中在客户端之间进行连贯和快速的协作。</p><p><img src="/images/Ceph_FS%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/1.jpg"></p><p>CephFS 因其新颖的设计和对文件系统研究的贡献而成为众多学术论文的主题。它是 Ceph 中最古老的存储接口，曾经是 RADOS的主要用例。现在它与另外两个存储接口一起组成了一个现代统一存储系统：RBD（Ceph 块设备）和 RGW（Ceph 对象存储网关）。</p><h1><span id="安装mds服务">安装MDS服务</span></h1><p>在三个节点上同时部署MDS</p><pre><code>[root@node-1 my-cluster]# ceph-deploy mds create node-1 node-2 node-3</code></pre><p>查看当前MDS状态，因为当前没有文件系统，所以3台主机都是备状态</p><pre><code>[root@node-1 my-cluster]#  3 up:standby </code></pre><h1><span id="创建ceph-fs文件系统">创建Ceph FS文件系统</span></h1><p>创建pool</p><pre><code>[root@node-1 my-cluster]# ceph osd pool create cephfs_data 16 16[root@node-1 my-cluster]# ceph osd pool create cephfs_metadata 16 16</code></pre><p>查看使用帮助</p><pre><code>[root@node-1 my-cluster]# ceph -h|grep fs |grep new fs new &lt;fs_name&gt; &lt;metadata&gt; &lt;data&gt; &#123;--force&#125; &#123;--allow-dangerous-make new filesystem using named pools &lt;metadata&gt; and &lt;data&gt;</code></pre><p>创建一个fs文件系统</p><pre><code>[root@node-1 my-cluster]# ceph fs new cephfs-demo cephfs_metadata cephfs_data  new fs with metadata pool 9 and data pool 8</code></pre><h1><span id="查看fs文件系统">查看fs文件系统</span></h1><pre><code>[root@node-1 my-cluster]# ceph fs ls name: cephfs-demo, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</code></pre><p>查看集群状态</p><pre><code>[root@node-1 my-cluster]#  ceph -s  #可以看到MDS状态已经又一个active了  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            1 pools have too many placement groups            too many PGs per OSD (368 &gt; max 250)   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 4m)    mgr: node-2(active, since 10m), standbys: node-3, node-1    mds: cephfs-demo:1 &#123;0=node-2=up:active&#125; 2 up:standby    osd: 6 osds: 6 up (since 4m), 6 in (since 14h)    rgw: 1 daemon active (node-1)   task status:   data:    pools:   9 pools, 736 pgs    objects: 534 objects, 1.0 GiB    usage:   9.6 GiB used, 50 GiB / 60 GiB avail    pgs:     736 active+clean</code></pre><h1><span id="内核挂载">内核挂载</span></h1><p>创建一个挂载点</p><pre><code>[root@node-1 my-cluster]# mkdir /mnt/cephfs #创建一个挂载点</code></pre><p>使用命令进行挂载</p><pre><code>[root@node-1 my-cluster]# which mount.ceph #使用这个命令进行挂载/usr/sbin/mount.ceph [root@node-1 my-cluster]# rpm -qf /usr/sbin/mount.ceph #安装ceph-common的时候添加的命令ceph-common-14.2.22-0.el7.x86_64[root@node-1 my-cluster]# mount -t ceph 192.168.187.201:6789:/ /mnt/cephfs/ -o name=admin #挂载</code></pre><p>查看挂载</p><pre><code>[root@node-1 my-cluster]# df -HT # 查看挂载Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   11M  944M   2% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.6G   16G  14% //dev/sda1               xfs       1.1G  204M  860M  20% /boottmpfs                   tmpfs     954M   25k  954M   1% /var/lib/ceph/osd/ceph-0tmpfs                   tmpfs     954M   25k  954M   1% /var/lib/ceph/osd/ceph-3tmpfs                   tmpfs     191M     0  191M   0% /run/user/0/dev/rbd0               xfs        11G  1.2G  9.7G  11% /data192.168.187.201:6789:/  ceph       17G     0   17G   0% /mnt/cephfs [root@node-1 my-cluster]# lsmod | grep ceph #会自动添加ceph的模块，此方法性能比较高ceph                  363016  1 libceph               314775  2 rbd,cephdns_resolver           13140  1 libcephlibcrc32c              12644  2 xfs,libceph</code></pre><h1><span id="用户态挂载">用户态挂载</span></h1><p>安装软件包</p><pre><code>[root@node-1 my-cluster]#  yum install ceph-fuse -y</code></pre><p>查看帮助文档</p><pre><code>[root@node-1 my-cluster]# ceph-fuse -h #查看使用帮助usage: ceph-fuse [-n client.username] [-m mon-ip-addr:mon-port] &lt;mount point&gt; [OPTIONS]  --client_mountpoint/-r &lt;sub_directory&gt;                    use sub_directory as the mounted root, rather than the full Ceph tree.usage: ceph-fuse mountpoint [options]general options:    -o opt,[opt...]        mount options    -h   --help            print help    -V   --version         print versionFUSE options:    -d   -o debug          enable debug output (implies -f)    -f                     foreground operation    -s                     disable multi-threaded operation  --conf/-c FILE    read configuration from the given configuration file  --id ID           set ID portion of my name  --name/-n TYPE.ID set name  --cluster NAME    set cluster name (default: ceph)  --setuser USER    set uid to user or uid (and gid to user&#39;s gid)  --setgroup GROUP  set gid to group or gid  --version         show version and quit</code></pre><p>挂载</p><p>可以看到与内核态挂载的地点文件是共享的 </p><pre><code>[root@node-1 my-cluster]# ceph-fuse -n client.admin -m 192.168.187.201:6789,192.168.187.202:6789,192.168.187.203:6789 /mnt/ceph-fuse/ #使用ceph-fuse进行挂载，-n 指定用户名 -m 指定mon节点，可指定1个或者多个，或者不指定，最后写上挂载地点ceph-fuse[205722023-03-05 17:10:28.202 7f6b40549f80 -1 init, newargv = 0x5623b0d084e0 newargc=9]: starting ceph clientceph-fuse[20572]: starting fuse</code></pre><p>确定挂载情况</p><pre><code>[root@node-1 my-cluster]# cd /mnt/ceph-fuse/ [root@node-1 ceph-fuse]# ls[root@node-1 ceph-fuse]# lltotal 0[root@node-1 ceph-fuse]# [root@node-1 ceph-fuse]# ls[root@node-1 ceph-fuse]# echo aaa &gt; test[root@node-1 ceph-fuse]# lstest[root@node-1 ceph-fuse]# ls /mnt/cephfstest</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Ceph FS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph故障排查</title>
      <link href="/2023/04/10/Ceph/0.Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/"/>
      <url>/2023/04/10/Ceph/0.Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-application-not-enabled-on-1-pools">一、application not enabled on 1 pool(s)</a></li><li><a href="#%E4%BA%8C-%E6%B8%85%E7%90%86mgr%E5%91%8A%E8%AD%A6">二、清理MGR告警</a></li></ul><!-- tocstop --><h1><span id="一-application-not-enabled-on-1-pools">一、application not enabled on 1 pool(s)</span></h1><blockquote><p>故障现象：</p></blockquote><pre><code>[root@node-1 data]# ceph -s   cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            application not enabled on 1 pool(s) #提示有一个pool没有启用application ，是由于创建pool的时候没有使用rbd pool init &lt;pool-name&gt;进行初始化，使用此命令会默认指定成RBD格式的application   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 71m)    mgr: node-1(active, since 14h), standbys: node-2, node-3    osd: 3 osds: 3 up (since 39m), 3 in (since 65m)   data:    pools:   1 pools, 64 pgs    objects: 281 objects, 1.0 GiB    usage:   6.2 GiB used, 24 GiB / 30 GiB avail    pgs:     64 active+clean</code></pre><blockquote><p> 查看当前集群的健康状态</p></blockquote><pre><code>[root@node-1 data]# ceph health detailHEALTH_WARN application not enabled on 1 pool(s)POOL_APP_NOT_ENABLED application not enabled on 1 pool(s)    application not enabled on pool &#39;ceph-demo&#39;    use &#39;ceph osd pool application enable &lt;pool-name&gt; &lt;app-name&gt;&#39;, where &lt;app-name&gt; is &#39;cephfs&#39;, &#39;rbd&#39;, &#39;rgw&#39;, or freeform for custom applications</code></pre><blockquote><p>启用ceph-demo的资源池的application，并设置成RBD格式</p></blockquote><pre><code>[root@node-1 data]# ceph osd pool application enable ceph-demo rbd  enabled application &#39;rbd&#39; on pool &#39;ceph-demo&#39;</code></pre><blockquote><p>通过get命令可查看资源池的application设置</p></blockquote><pre><code>[root@node-1 data]#  ceph -h |grep application osd pool application disable &lt;poolname&gt; &lt;app&gt; &#123;--yes-i-really-mean-   disables use of an application &lt;app&gt; on pool &lt;poolname&gt;osd pool application enable &lt;poolname&gt; &lt;app&gt; &#123;--yes-i-really-mean-it&#125; enable use of an application &lt;app&gt; [cephfs,rbd,rgw] on pool osd pool application get &#123;&lt;poolname&gt;&#125; &#123;&lt;app&gt;&#125; &#123;&lt;key&gt;&#125;                 get value of key &lt;key&gt; of application &lt;app&gt; on pool &lt;poolname&gt;osd pool application rm &lt;poolname&gt; &lt;app&gt; &lt;key&gt;                        removes application &lt;app&gt; metadata key &lt;key&gt; on pool &lt;poolname&gt;osd pool application set &lt;poolname&gt; &lt;app&gt; &lt;key&gt; &lt;value&gt;               sets application &lt;app&gt; metadata key &lt;key&gt; to &lt;value&gt; on pool [root@node-1 data]# ceph osd pool application get ceph-demo #默认为空&#123;    &quot;rbd&quot;: &#123;&#125;&#125;</code></pre><blockquote><p>查看集群已经恢复</p></blockquote><pre><code>[root@node-1 data]# ceph -s  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK  services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 80m)    mgr: node-1(active, since 14h), standbys: node-2, node-3    osd: 3 osds: 3 up (since 48m), 3 in (since 73m)   data:    pools:   1 pools, 64 pgs    objects: 281 objects, 1.0 GiB    usage:   6.2 GiB used, 24 GiB / 30 GiB avail    pgs:     64 active+clean</code></pre><h1><span id="二-清理mgr告警">二、清理MGR告警</span></h1><blockquote><p>故障现象：</p></blockquote><pre><code>[root@node-1 data]# ceph -s    cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            2 daemons have recently crashed #有两个告警信息，可能是由于某个服务出现异常导致   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 80m)    mgr: node-1(active, since 14h), standbys: node-2, node-3    osd: 3 osds: 3 up (since 48m), 3 in (since 73m)   data:    pools:   1 pools, 64 pgs    objects: 281 objects, 1.0 GiB    usage:   6.2 GiB used, 24 GiB / 30 GiB avail    pgs:     64 active+clean[root@node-1 data]#  ceph health detailHEALTH_WARN 2 daemons have recently crashedRECETN_CRASH 2 daemons have recently crashed    mgr.node-1 crashed on host node-1 at 2023-03-04 15:42:01.891238Z    mgr.node-1 crashed on host node-1 at 2023-03-04 15:45:31.891238Z</code></pre><blockquote><p>查看crash用法</p></blockquote><pre><code>[root@node-1 data]# ceph -h | grep crashcrash archive &lt;id&gt;                                                    Acknowledge a crash and silence health warning(s)crash archive-all                                                     Acknowledge all new crashes and silence health warning(s)crash info &lt;id&gt;                                                       show crash dump metadatacrash json_report &lt;hours&gt;                                             Crashes in the last &lt;hours&gt; hourscrash ls                                                              Show new and archived crash dumpscrash ls-new                                                          Show new crash dumpscrash post                                                            Add a crash dump (use -i &lt;jsonfile&gt;)crash prune &lt;keep&gt;                                                    Remove crashes older than &lt;keep&gt; dayscrash rm &lt;id&gt;                                                         Remove a saved crash &lt;id&gt;crash stat                                                            Summarize recorded crashes</code></pre><blockquote><p>查看当前状态</p></blockquote><pre><code>[root@node-1 data]# ceph crash ls</code></pre><p><img src="/images/Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/1.jpg"></p><blockquote><p>查看该ID的具体信息</p></blockquote><pre><code>[root@node-1 data]# ceph crash info &lt;id&gt;</code></pre><blockquote><p>使用ceph crash archive <id>进行标记，视为已读，或者重启所有OSD也能解决告警问题</id></p></blockquote><p><img src="/images/Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/2.jpg"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> 故障排查 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph简介</title>
      <link href="/2023/04/10/Ceph/2.Ceph%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84/Ceph-%E7%AE%80%E4%BB%8B/"/>
      <url>/2023/04/10/Ceph/2.Ceph%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84/Ceph-%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#ceph-%E7%AE%80%E4%BB%8B">CEPH 简介</a><ul><li><a href="#%E5%BB%BA%E8%AE%AE">建议</a></li><li><a href="#%E7%A1%AC%E4%BB%B6%E6%8E%A8%E8%8D%90">硬件推荐</a><ul><li><a href="#cpu">CPU</a></li><li><a href="#%E5%86%85%E5%AD%98">内存</a></li><li><a href="#%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8">数据存储</a></li><li><a href="#%E7%BD%91%E7%BB%9C">网络</a></li><li><a href="#%E6%95%85%E9%9A%9C%E5%9F%9F">故障域</a></li></ul></li><li><a href="#%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%BB%BA%E8%AE%AE">操作系统建议</a><ul><li><a href="#ceph-%E4%BE%9D%E8%B5%96%E9%A1%B9">CEPH 依赖项</a></li><li><a href="#%E5%B9%B3%E5%8F%B0">平台</a></li></ul></li></ul></li></ul><!-- tocstop --><h1><span id="ceph-简介">CEPH 简介</span></h1><p>无论您是想为云平台提供Ceph 对象存储和&#x2F;或 Ceph 块设备服务、部署Ceph 文件系统还是将 Ceph 用于其他目的，所有 Ceph 存储集群部署都从设置每个 Ceph 节点、您的网络和 Ceph存储集群。一个 Ceph 存储集群至少需要一个 Ceph Monitor、Ceph Manager 和 Ceph OSD（Object Storage Daemon）。运行 Ceph 文件系统客户端时也需要 Ceph 元数据服务器。<br><img src="/images/Ceph%E7%AE%80%E4%BB%8B/1.jpg"></p><ul><li>Monitors：Ceph Monitor ( ceph-mon) 维护集群状态图，包括监视器图、管理器图、OSD 图、MDS 图和 CRUSH 图。这些映射是 Ceph 守护进程相互协调所需的关键集群状态。监视器还负责管理守护进程和客户端之间的身份验证。通常至少需要三个监视器才能实现冗余和高可用性。</li><li>管理器：Ceph 管理器守护进程 ( ceph-mgr) 负责跟踪运行时指标和 Ceph 集群的当前状态，包括存储利用率、当前性能指标和系统负载。Ceph Manager 守护进程还托管基于 python 的模块来管理和公开 Ceph 集群信息，包括基于 Web 的Ceph Dashboard和 REST API。高可用性通常至少需要两个管理器。</li><li>Ceph OSDs：一个对象存储守护进程（Ceph OSD， ceph-osd）存储数据，处理数据复制，恢复，重新平衡，并通过检查其他 Ceph OSD 守护进程的心跳向 Ceph Monitors 和 Managers 提供一些监控信息。通常至少需要三个 Ceph OSD 才能实现冗余和高可用性。</li><li>MDS：Ceph 元数据服务器（MDS ceph-mds）代表Ceph 文件系统存储元数据（即 Ceph 块设备和 Ceph 对象存储不使用 MDS）。Ceph 元数据服务器允许 POSIX 文件系统用户执行基本命令（如 ls、find等），而不会给 Ceph 存储集群带来巨大负担。</li></ul><p>Ceph 将数据作为对象存储在逻辑存储池中。使用 CRUSH算法，Ceph 计算出哪个归置组 (PG) 应该包含该对象，以及哪个 OSD 应该存储该归置组。CRUSH 算法使 Ceph 存储集群能够动态扩展、重新平衡和恢复。</p><h2><span id="建议">建议</span></h2><p>要开始在生产中使用 Ceph，您应该查看我们的硬件建议和操作系统建议。</p><ul><li>硬件推荐<ul><li>中央处理器</li><li>内存</li><li>记忆</li><li>数据存储</li><li>网络</li><li>故障域</li><li>最低硬件建议</li></ul></li><li>操作系统建议<ul><li>Ceph 依赖项</li><li>平台</li></ul></li></ul><h2><span id="硬件推荐">硬件推荐</span></h2><p>Ceph 被设计为在商用硬件上运行，这使得构建和维护 PB 级数据集群在经济上是可行的。在规划集群硬件时，您需要平衡许多考虑因素，包括故障域和潜在的性能问题。硬件规划应该包括在许多主机上分布 Ceph 守护进程和其他使用 Ceph 的进程。通常，我们建议在为特定类型的守护进程配置的主机上运行特定类型的 Ceph 守护进程。我们建议将其他主机用于利用您的数据集群的进程（例如，OpenStack、CloudStack 等）。<br>也可以查看<a href="https://ceph.com/community/blog/">Ceph 博客。</a></p><h3><span id="cpu">CPU</span></h3><p>CephFS 元数据服务器 (MDS) 是 CPU 密集型的。因此，CephFS 元数据服务器 (MDS) 应具有四核（或更好）CPU 和高时钟频率 (GHz)。OSD 节点需要足够的处理能力来运行 RADOS 服务、使用 CRUSH 计算数据放置、复制数据以及维护它们自己的集群映射副本。</p><blockquote><p><strong>一个 Ceph 集群的要求与另一个集群的要求不同，但这里有一些通用准则。</strong></p></blockquote><p>在 Ceph 的早期版本中，我们会根据每个 OSD 的核心数来提出硬件建议，但这个每个 OSD 的核心数指标不再像每个 IOP 的周期数和每个 OSD 的 IOP 数一样有用。例如，对于 NVMe 驱动器，Ceph 可以轻松地在真实集群上使用五个或六个内核，并在单个 OSD 上隔离使用多达大约十四个内核。因此，每个 OSD 的核心不再像以前那样紧迫。选择硬件时，选择每个内核的 IOP。</p><p>监控节点和管理器节点对 CPU 的要求不高，只需要适度的处理器。如果您的主机除了运行 Ceph 守护进程外还将运行 CPU 密集型进程，请确保您有足够的处理能力来运行 CPU 密集型进程和 Ceph 守护进程。（OpenStack Nova 是 CPU 密集型进程的一个例子。）我们建议您在单独的主机上（即，在不是您的监视器和管理器节点的主机上）运行非 Ceph CPU 密集型进程，以避免占用资源争论。</p><h3><span id="内存">内存</span></h3><p>通常，RAM 越大越好。适度集群的监视器&#x2F;管理器节点可能使用 64GB 就可以了；对于拥有数百个 OSD 的更大集群，128GB 是一个合理的目标。BlueStore OSD 的内存目标默认为 4GB。考虑到操作系统和管理任务（如监控和指标）的谨慎余量以及恢复期间增加的消耗：建议为每个 BlueStore OSD 预配 ~8GB。</p><h4><span id="监视器和管理器ceph-mon-和-ceph-mgr">监视器和管理器（CEPH-MON 和 CEPH-MGR）</span></h4><p>监视器和管理器守护进程的内存使用量通常随集群的大小而变化。请注意，在启动时以及拓扑更改和恢复期间，这些守护程序将需要比稳态操作期间更多的 RAM，因此请计划使用高峰期。对于非常小的集群，32 GB 就足够了。对于多达 300 个 OSD 的集群，需要 64GB</p><h4><span id="元数据服务器-ceph-mds">元数据服务器 (CEPH-MDS)</span></h4><p>元数据守护程序内存利用率取决于其缓存配置为消耗的内存量。对于大多数系统，我们建议至少使用 1 GB。 看mds_cache_memory。</p><h4><span id="记忆">记忆</span></h4><p>Bluestore 使用自己的内存来缓存数据，而不是依赖操作系统的页面缓存。在 Bluestore 中，您可以通过更改配置选项来调整 OSD 尝试消耗的内存量osd_memory_target 。</p><ul><li>通常不建议设置osd_memory_target低于 2GB（Ceph 可能无法将内存消耗保持在 2GB 以下，这可能会导致性能极慢）。</li><li>将内存目标设置在 2GB 和 4GB 之间通常可行，但可能会导致性能下降，因为元数据可能会在 IO 期间从磁盘读取，除非活动数据集相对较小。</li><li>4GB 是当前的默认osd_memory_target大小。此默认值是为典型用例选择的，旨在平衡内存需求和 OSD 性能。</li><li>osd_memory_target当有很多（小）对象或处理大（256GB&#x2F;OSD 或更多）数据集时，设置高于 4GB 可以提高性能。</li></ul><blockquote><p>OSD 内存自动调整是“尽力而为”。虽然 OSD 可以取消映射内存以允许内核回收它，但不能保证内核会在特定时间范围内实际回收释放的内存。这尤其适用于旧版本的 Ceph，其中透明大页面可以防止内核回收从碎片大页面中释放的内存。现代版本的 Ceph 在应用程序级别禁用透明大页面以避免这种情况，尽管这仍然不能保证内核会立即回收未映射的内存。OSD 有时仍可能超出其内存目标。我们建议在您的系统上预算大约 20% 的额外内存，以防止 OSD 在临时峰值期间或由于内核回收已释放页面的任何延迟而出现 OOM。</p></blockquote><p>使用旧版 FileStore 后端时，页面缓存用于缓存数据，因此通常不需要调整。<br>使用旧版 FileStore 后端时，OSD 内存消耗与系统中每个守护进程的 PG 数量有关。</p><h3><span id="数据存储">数据存储</span></h3><p>仔细规划您的数据存储配置。在规划数据存储时，需要考虑显着的成本和性能权衡。同时进行的操作系统操作以及多个守护进程对单个驱动器的读写操作的同时请求会大大降低性能。</p><h4><span id="硬盘驱动器">硬盘驱动器</span></h4><p>OSD 应该有足够的硬盘驱动器空间来存储对象数据。我们建议最小硬盘驱动器大小为 1 TB。考虑更大磁盘的每 GB 成本优势。我们建议将硬盘驱动器的价格除以千兆字节数，得出每千兆字节的成本，因为更大的驱动器可能会对每千兆字节的成本产生重大影响。例如，价格为 75.00 美元的 1 TB 硬盘的成本为每 GB 0.07 美元（即 75 美元 &#x2F; 1024 &#x3D; 0.0732）。相比之下，价格为 150.00 美元的 3 TB 硬盘的成本为每 GB 0.05 美元（即 150 美元 &#x2F; 3072 &#x3D; 0.0488）。在前面的示例中，使用 1 TB 的磁盘通常会使每 GB 的成本增加 40%——使您的集群的成本效率大大降低。</p><blockquote><p>在单个 SAS &#x2F; SATA 驱动器上运行多个 OSD 不是一个好主意。但是，NVMe 驱动器可以通过拆分为两个或更多 OSD 来提高性能。<br>在单个驱动器上运行 OSD 和监视器或元数据服务器也不是一个好主意。</p></blockquote><p>存储驱动器受寻道时间、访问时间、读写时间以及总吞吐量的限制。这些物理限制会影响整体系统性能，尤其是在恢复期间。我们建议为操作系统和软件使用专用（最好是镜像）驱动器，并为主机上运行的每个 Ceph OSD 守护进程使用一个驱动器（上面的模数 NVMe）。许多不是由硬件故障引起的“慢 OSD”问题是由于在同一驱动器上运行操作系统和多个 OSD 而引起的。由于在小型集群上解决性能问题的成本可能超过额外磁盘驱动器的成本，因此您可以通过避免让 OSD 存储驱动器负担过重的诱惑来优化您的集群设计规划。</p><blockquote><p>您可以在每个 SAS &#x2F; SATA 驱动器上运行多个 Ceph OSD 守护进程，但这可能会导致资源争用并降低整体吞吐量。</p></blockquote><h4><span id="态硬盘">态硬盘</span></h4><p>性能改进的一个机会是使用固态驱动器 (SSD) 来减少随机访问时间和读取延迟，同时加快吞吐量。与硬盘驱动器相比，SSD 每 GB 的成本通常是硬盘驱动器的 10 倍以上，但 SSD 的访问时间通常至少比硬盘驱动器快 100 倍。<br>SSD 没有移动机械部件，因此它们不一定受到与硬盘驱动器相同类型的限制。SSD 确实有很大的局限性。在评估 SSD 时，重要的是要考虑顺序读写的性能。</p><blockquote><p>我们建议探索使用 SSD 来提高性能。但是，在对 SSD 进行重大投资之前，我们强烈建议查看 SSD 的性能指标并在测试配置中测试 SSD 以衡量性能。</p></blockquote><p>相对便宜的 SSD 可能会吸引您的经济意识。谨慎使用。选择用于 Ceph 的 SSD 时，可接受的 IOPS 是不够的。</p><p>SSD 在历史上一直是对象存储的成本高昂，但新兴的 QLC 驱动器正在缩小差距。通过将 WAL+DB 卸载到 SSD，HDD OSD 可能会看到显着的性能提升。</p><p>Ceph 加速 CephFS 文件系统性能的一种方法是将 CephFS 元数据的存储与 CephFS 文件内容的存储分开。Ceph 为 CephFS 元数据提供了一个默认metadata池。您永远不必为 CephFS 元数据创建一个池，但您可以为您的 CephFS 元数据池创建一个仅指向主机的 SSD 存储介质的 CRUSH 映射层次结构。有关详细信息，请参阅 <a href="https://docs.ceph.com/en/quincy/rados/operations/crush-map-edits/#crush-map-device-class">CRUSH 设备类</a>。</p><h4><span id="控制器">控制器</span></h4><p>磁盘控制器 (HBA) 会对写入吞吐量产生重大影响。仔细考虑您的选择以确保它们不会造成性能瓶颈。值得注意的是，RAID 模式 (IR) HBA 可能比更简单的“JBOD”(IT) 模式 HBA 表现出更高的延迟，并且 RAID SoC、写缓存和电池备份会显着增加硬件和维护成本。某些 RAID HBA 可以配置有 IT 模式“个性”。</p><p><a href="https://ceph.com/community/blog/">Ceph 博客</a>通常是有关 Ceph 性能问题的极佳信息来源。有关更多详细信息，请参阅<a href="https://ceph.com/community/ceph-performance-part-1-disk-controller-write-throughput/">Ceph 写入吞吐量 1</a>和<a href="https://ceph.com/community/ceph-performance-part-2-write-throughput-without-ssd-journals/">Ceph 写入吞吐量 2</a>。</p><h4><span id="对标">对标</span></h4><p>BlueStore 在 O_DIRECT 中打开块设备，并频繁使用 fsync 以确保数据安全地持久化到介质中。您可以使用 评估驱动器的低级写入性能fio。例如，4kB 随机写性能测量如下：</p><pre><code>#fio --name=/dev/sdX --ioengine=libaio --direct=1 --fsync=1 --readwrite=randwrite --blocksize=4k --runtime=300</code></pre><h4><span id="写缓存">写缓存</span></h4><p>企业 SSD 和 HDD 通常包括断电保护功能，使用多级缓存来加速直接或同步写入。这些设备可以在两种缓存模式之间切换——易失性缓存通过 fsync 刷新到持久介质，或同步写入的非易失性缓存。</p><p>通过“启用”或“禁用”写入（易失性）缓存来选择这两种模式。当启用易失性缓存时，Linux 使用“回写”模式的设备，禁用时，它使用“直写”。</p><p>默认配置（通常启用缓存）可能不是最佳配置，并且 OSD 性能可能会通过禁用写缓存在增加 IOPS 和减少 commit_latency 方面得到显着提高。</p><p>因此，鼓励用户fio如前所述对他们的设备进行基准测试，并为他们的设备保留最佳缓存配置。<br>hdparm可以使用、sdparm或 smartctl读取中的值来查询缓存配置&#x2F;sys&#x2F;class&#x2F;scsi_disk&#x2F;*&#x2F;cache_type，例如：</p><pre><code>#hdparm -W /dev/sda/dev/sda: write-caching =  1 (on)# sdparm --get WCE /dev/sda    /dev/sda: ATA       TOSHIBA MG07ACA1  0101WCE           1  [cha: y]# smartctl -g wcache /dev/sdasmartctl 7.1 2020-04-05 r5049 [x86_64-linux-4.18.0-305.19.1.el8_4.x86_64] (local build)Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.orgWrite cache is:   Enabled# cat /sys/class/scsi_disk/0\:0\:0\:0/cache_typewrite back</code></pre><p>可以使用相同的工具禁用写缓存：</p><pre><code>#hdparm -W0 /dev/sda/dev/sda: setting drive write-caching to 0 (off) write-caching =  0 (off)# sdparm --clear WCE /dev/sda    /dev/sda: ATA       TOSHIBA MG07ACA1  0101# smartctl -s wcache,off /dev/sdasmartctl 7.1 2020-04-05 r5049 [x86_64-linux-4.18.0-305.19.1.el8_4.x86_64] (local build)Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.org=== START OF ENABLE/DISABLE COMMANDS SECTION ===Write cache disabled</code></pre><p>hdparm通常，使用、sdparm或禁用缓存smartctl 会导致 cache_type 自动更改为“write through”。如果不是这样，你可以尝试直接如下设置。（用户应注意，设置 cache_type 也会正确保留设备的缓存模式，直到下一次重启）：</p><pre><code>#echo &quot;write through&quot; &gt; /sys/class/scsi_disk/0\:0\:0\:0/cache_type# hdparm -W /dev/sda/dev/sda: write-caching =  0 (off)</code></pre><p>这个 udev 规则（在 CentOS 8 上测试过）会将所有 SATA&#x2F;SAS 设备 cache_types 设置为“write through”：</p><pre><code>#cat /etc/udev/rules.d/99-ceph-write-through.rulesACTION==&quot;add&quot;, SUBSYSTEM==&quot;scsi_disk&quot;, ATTR&#123;cache_type&#125;:=&quot;write through&quot;</code></pre><p>这个 udev 规则（在 CentOS 7 上测试过）会将所有 SATA&#x2F;SAS 设备 cache_types 设置为“write through”：</p><pre><code>#cat /etc/udev/rules.d/99-ceph-write-through-el7.rulesACTION==&quot;add&quot;, SUBSYSTEM==&quot;scsi_disk&quot;, RUN+=&quot;/bin/sh -c &#39;echo write through &gt; /sys/class/scsi_disk/$kernel/cache_type&#39;&quot;</code></pre><p>该sdparm实用程序可用于一次查看&#x2F;更改多个设备上的易失性写入缓存：</p><pre><code>#sdparm --get WCE /dev/sd*    /dev/sda: ATA       TOSHIBA MG07ACA1  0101WCE           0  [cha: y]    /dev/sdb: ATA       TOSHIBA MG07ACA1  0101WCE           0  [cha: y]# sdparm --clear WCE /dev/sd*    /dev/sda: ATA       TOSHIBA MG07ACA1  0101    /dev/sdb: ATA       TOSHIBA MG07ACA1  0101</code></pre><h4><span id="其他注意事项">其他注意事项</span></h4><p>您通常会在每个主机上运行多个 OSD，但您应该确保 OSD 驱动器的总吞吐量不超过满足客户端读取或写入数据需求所需的网络带宽。您还应该考虑集群在每个主机上存储的总数据的百分比。如果特定主机上的百分比很大并且该主机发生故障，则可能导致诸如超过 之类的问题，这会导致 Ceph 停止操作以作为防止数据丢失的安全预防措施。full ratio</p><p>当您在每个主机上运行多个 OSD 时，您还需要确保内核是最新的。请参阅<a href="https://docs.ceph.com/en/quincy/start/os-recommendations">OS Recommendations</a>了解有关注意事项glibc并 syncfs(2)确保您的硬件在每个主机运行多个 OSD 时按预期运行。</p><h3><span id="网络">网络</span></h3><p>在您的机架中提供至少 10 Gb&#x2F;s 的网络。</p><h4><span id="速度">速度</span></h4><p>在 1 Gb&#x2F;s 网络上复制 1 TB 数据需要三个小时，在 1 Gb&#x2F;s 网络上复制 10 TB 数据需要三十个小时。但是在 10 Gb&#x2F;s 网络上复制 1 TB 数据只需要 20 分钟，而在 10 Gb&#x2F;s 网络上复制 10 TB 数据只需要一个小时。</p><h4><span id="成本">成本</span></h4><p>Ceph 集群越大，OSD 故障就越常见。degraded归置组 (PG) 从一种状态恢复到另一种状态的速度越快越好。值得注意的是，快速恢复将可能导致数据暂时不可用甚至丢失的多重重叠故障的可能性降至最低。当然，在配置您的网络时，您必须在价格与性能之间取得平衡。active + clean</p><p>一些部署工具使用 VLAN 来使硬件和网络布线更易于管理。使用 802.1q 协议的 VLAN 需要支持 VLAN 的 NIC 和交换机。该硬件的额外费用可能会被网络设置和维护方面节省的运营成本所抵消。当使用 VLAN 处理集群和计算堆栈（例如 OpenStack、CloudStack 等）之间的 VM 流量时，使用 10 Gb&#x2F;s 以太网或更好的以太网具有额外的价值；截至 2022 年，40 Gb&#x2F;s 或25&#x2F;50&#x2F;100 Gb&#x2F;s 网络在生产集群中很常见。</p><blockquote><p>架顶式 (TOR) 交换机还需要快速和冗余的上行链路来旋转主干交换机&#x2F;路由器，通常至少为 40 Gb&#x2F;s。</p></blockquote><h4><span id="底板管理控制器-bmc">底板管理控制器 (BMC)</span></h4><p>您的服务器机箱应该有底板管理控制器 (BMC)。众所周知的例子是 iDRAC (Dell)、CIMC (Cisco UCS) 和 iLO (HPE)。管理和部署工具也可能广泛使用 BMC，尤其是通过 IPMI 或 Redfish，因此请考虑带外网络在安全性和管理方面的成本&#x2F;收益权衡。Hypervisor SSH 访问、VM 映像上传、OS 映像安装、管理套接字等会给网络带来巨大的负载。运行三个网络似乎有点矫枉过正，但每个流量路径都代表潜在的容量、吞吐量和&#x2F;或性能瓶颈，您在部署大规模数据集群之前应该仔细考虑。</p><h3><span id="故障域">故障域</span></h3><p>故障域是阻止访问一个或多个 OSD 的任何故障。那可能是主机上停止的守护进程；硬盘故障、操作系统崩溃、网卡故障、电源故障、网络中断、停电等等。在规划您的硬件需求时，您必须权衡通过将过多的责任置于过少的故障域中来降低成本的诱惑与隔离每个潜在故障域的额外成本之间的平衡。</p><h4><span id="最低硬件建议">最低硬件建议</span></h4><p>Ceph 可以在廉价的商用硬件上运行。小型生产集群和开发集群可以使用适度的硬件成功运行。</p><table>    <tr>        <td>过程</td>        <td>标准</td>        <td>最低推荐</td>    </tr>    <tr>        <td rowspan="5">ceph-osd</td>    </tr>        <tr>            <td>处理器</td>            <td>最少 1 个核心 <br>  每 200-500 MB/s 1 个核心 <br> 每 1000-3000 IOPS 1 个核心  <br>  结果在复制之前。 <br>  结果可能因不同的 CPU 型号和 Ceph 功能而异。（纠删码、压缩等）。<br> ARM 处理器可能特别需要额外的内核。 <br> 实际性能取决于许多因素，包括驱动器、网络和客户端吞吐量和延迟。强烈建议进行基准测试。 </td>        </tr>        <tr>            <td>内存</td>            <td>每个守护进程 4GB+（越多越好）<br> 2-4GB 经常运行（可能很慢） <br> 小于 2GB 不推荐 </td>        </tr>        <tr>            <td>卷存储</td>            <td>每个守护进程 1 个存储驱动器 </td>        </tr>        <tr>            <td>数据库/文件</td>            <td>每个守护进程 1 个 SSD 分区（可选） </td>        </tr>        <tr>            <td>网络</td>            <td>1x 1GbE+ NIC（推荐 10GbE+） </td>        </tr>    <tr>        <td rowspan="4">ceph-mon</td>    </tr>        <tr>            <td>处理器</td>            <td>最少 2 个核心 </td>        </tr>        <tr>            <td>内存</td>            <td>每个守护进程 2-4GB+ </td>        </tr>        <tr>        <td>磁盘空间</td>        <td>每个守护进程 60 GB </td>        </tr>        <tr>            <td>网络</td>            <td>1x 1GbE+ NIC</td>        </tr>    <tr>        <td rowspan="4">ceph-mds</td>    </tr>        <tr>            <td>处理器</td>            <td>最少 2 个核心 </td>        </tr>        <tr>            <td>内存</td>            <td>每个守护进程 2GB+ </td>        </tr>        <tr>            <td>磁盘空间</td>            <td>每个守护进程 1MB</td>        </tr>        <tr>            <td>网络</td>            <td>1x 1GbE+ NIC</td>        </tr></table><blockquote><p>如果您使用单个磁盘运行 OSD，请为您的卷存储创建一个分区，该分区与包含操作系统的分区分开。通常，我们建议操作系统和卷存储使用单独的磁盘。</p></blockquote><h2><span id="操作系统建议">操作系统建议</span></h2><h3><span id="ceph-依赖项">CEPH 依赖项</span></h3><p>作为一般规则，我们建议在较新版本的 Linux 上部署 Ceph。我们还建议在具有长期支持的版本上进行部署。</p><h4><span id="内核">内核</span></h4><blockquote><p>Ceph 内核客户端</p></blockquote><p>如果您使用内核客户端映射 RBD 块设备或挂载 CephFS，一般建议是使用 <a href="http://kernel.org/">http://kernel.org</a> 或您在任何客户端上的 Linux 发行版提供的“稳定”或“长期维护”内核系列主机。</p><p>对于 RBD，如果选择跟踪长期内核，我们目前推荐基于 4.x 的“长期维护”内核系列或更高版本：</p><p>对于 RBD，如果选择跟踪长期内核，我们目前推荐基于 4.x 的“长期维护”内核系列或更高版本：</p><ul><li>4.19.z</li><li>4.14.z</li><li>5.x</li></ul><p>对于 CephFS，请参阅有关使用内核驱动程序安装 CephFS 的部分 以获取内核版本指南。</p><p>较旧的内核客户端版本可能不支持您的CRUSH 可调配置文件或 Ceph 集群的其他较新功能，需要将存储集群配置为禁用这些功能。</p><h3><span id="平台">平台</span></h3><p>下面的图表显示了 Ceph 的需求如何映射到各种 Linux 平台。一般而言，对内核和系统初始化包（即 sysvinit、systemd）之外的特定发行版的依赖性非常小。</p><table>    <tr>        <td>Release Name</td>        <td>Tag</td>        <td>CentOS</td>        <td>Ubuntu</td>        <td>OpenSUSE C</td>        <td>Debian C</td>    </tr>        <tr>            <td>Quincy</td>            <td>17.2.z</td>            <td>8 A</td>            <td>20.04 A</td>            <td>15.3</td>            <td>11</td>        </tr>        <tr>            <td>Pacific</td>            <td>16.2.z</td>            <td>8 A</td>            <td>18.04 C,<br>20.04 A</td>            <td>15.2</td>            <td>10<br>11</td>        </tr>        <tr>            <td>Octopus</td>            <td>15.2.z</td>            <td>7 B <br> 8 A</td>            <td>18.04 C,<br>20.04 A</td>            <td>15.2</td>            <td>10</td>        </tr></table><ul><li>A : Ceph 提供软件包，并对其中的软件进行了全面的测试。</li><li>B : Ceph 提供了软件包，并对其中的软件做了基本的测试。</li><li>C : Ceph 只提供包。尚未对这些版本进行任何测试。</li></ul><blockquote><p>Centos 7 用户：Btrfs在 Octopus 版本中不再在 Centos 7 上进行测试。我们建议bluestore改用。</p></blockquote><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客中插入 Chart 动态图表</title>
      <link href="/2023/04/08/chatjs/"/>
      <url>/2023/04/08/chatjs/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>该文基本(全部)来自于chatjs中文文档</p><h1><span id="背景">背景</span></h1><!-- toc --><ul><li><a href="#hexo-%E4%B8%AD%E7%9A%84-chartjs">Hexo 中的 Chartjs</a></li><li><a href="#%E7%A4%BA%E4%BE%8B">示例</a><ul><li><a href="#%E6%8A%98%E7%BA%BF%E5%9B%BE">折线图</a></li><li><a href="#%E7%9B%B8%E5%85%B3%E5%B1%9E%E6%80%A7">相关属性</a></li></ul></li></ul><!-- tocstop --><span id="more"></span><p>Chartjs是一款简单优雅的数据可视化工具，对比其他图表库如echarts、highcharts、c3、flot、amchart等，它的画面效果、动态效果都更精致，它的 文档首页 就透出一股小清新，基于 HTML5 Canvas，它拥有更好的性能且响应式，基本满足了一般数据展示的需要，包括折线图，条形图，饼图，散点图，雷达图，极地图，甜甜圈图等</p><h1><span id="hexo-中的-chartjs">Hexo 中的 Chartjs</span></h1><p>为了方便在 Hexo 中使用这么漂亮的图表库，我自己写了一个 Hexo 的 Chartjs 插件。插件的安装和使用非常的简单，只需要进入博客目录，然后打开命令行，用npm安装一下：</p><pre><code>npm install hexo-tag-chart --save</code></pre><p>之后在文章内使用 chart 的 tag 就可以了</p><pre><code>&#123;% chart 90% 300 %&#125;\\TODO option goes here&#123;% endchart %&#125;</code></pre><p>其中 chart 是标签名，endchart 是结束标签，不需要更改，90% 是图表容器的相对宽度，默认是100%，300 是图表容器的高度，默认是按正常比例缩放的，你可以通过设置 options 里面的 aspectRatio 属性来调整宽高比例，另外还有许多属性可以自定义，你可以查看 官方文档。在标签之间的部分，都是需要自己填充的图表数据和属性。</p><p>我们来看一个样例，你可以把鼠标移上去看看动态效果。</p><div style="width: 90%;margin: 0 auto">    <canvas id="chart7468" style="height: 300px"></canvas></div><script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script><script type="text/javascript">    var ctx = document.getElementById('chart7468').getContext('2d');    var options =     {    type: 'line',    data: {    labels: ['January', 'February', 'March', 'April', 'May', 'June', 'July'],    datasets: [{        label: 'My First dataset',        backgroundColor: 'rgb(255, 99, 132)',        borderColor: 'rgb(255, 99, 132)',        data: [0, 10, 5, 2, 20, 30, 45]        }]    },    options: {        responsive: true,        title: {        display: true,        text: 'Chart.js Line Chart'        }    }};    new Chart(ctx, options);</script><p>上面这个样例可以通过以下代码来实现：</p><pre><code>&#123;% chart 90% 300 %&#125;    &#123;    type: 'line',    data: &#123;    labels: ['January', 'February', 'March', 'April', 'May', 'June', 'July'],    datasets: [&#123;        label: 'My First dataset',        backgroundColor: 'rgb(255, 99, 132)',        borderColor: 'rgb(255, 99, 132)',        data: [0, 10, 5, 2, 20, 30, 45]        &#125;]    &#125;,    options: &#123;        responsive: true,        title: &#123;        display: true,        text: 'Chart.js Line Chart'        &#125;    &#125;&#125;&#123;% endchart %&#125;</code></pre><h1><span id="示例">示例</span></h1><p>现在你已经基本学会了在Hexo中插入图表了，我再展示一些更炫酷的图表吧，你可以自己去尝试一下。</p><h2><span id="折线图">折线图</span></h2><p>效果</p><div style="width: 90%;margin: 0 auto">    <canvas id="chart197" style="height: 300px"></canvas></div><script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script><script type="text/javascript">    var ctx = document.getElementById('chart197').getContext('2d');    var options =   //其中 chart 是标签名，endchart 是结束标签，不需要更改，可以通过设置 options 里面的  aspectRatio 属性来调整图表的宽高比例，默认 width 为 100%，height 为 300px。 {    type: 'line',    data: { //  折线图数据    labels: ['8.4', '8.5', '8.6', '8.7', '8.8', '8.9', '8.10'], //  横坐标    datasets: [{ //  纵坐标变量名称        label: 'My First dataset',        lineTension: 0,        borderColor: 'rgb(255, 99, 132)', //  线条颜色        data: [0, 10, 5, 2, 20, 30, 45] //  纵坐标        }]    },    options: {        responsive: true,        title: {        display: true,        text: 'Chart.js 折线图' //  图表名称        }    }};    new Chart(ctx, options);</script><pre><code>&#123;% chart 90% 300 %&#125;  //其中 chart 是标签名，endchart 是结束标签，不需要更改，可以通过设置 options 里面的               aspectRatio 属性来调整图表的宽高比例，默认 width 为 100%，height 为 300px。 &#123;    type: 'line',    data: &#123; //  折线图数据    labels: ['8.4', '8.5', '8.6', '8.7', '8.8', '8.9', '8.10'], //  横坐标    datasets: [&#123; //  纵坐标变量名称        label: 'My First dataset',        lineTension: 0,        borderColor: 'rgb(255, 99, 132)', //  线条颜色        data: [0, 10, 5, 2, 20, 30, 45] //  纵坐标        &#125;]    &#125;,    options: &#123;        responsive: true,        title: &#123;        display: true,        text: 'Chart.js 折线图' //  图表名称        &#125;    &#125;&#125;&#123;% endchart %&#125;</code></pre><h2><span id="相关属性">相关属性</span></h2><table><thead><tr><th>名称</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>backgroundColor</td><td>Color&#x2F;Color[]</td><td>线条背景色</td></tr><tr><td></td><td></td><td></td></tr><tr><td>borderColor</td><td>Color&#x2F;Color[]</td><td>线条颜色</td></tr></tbody></table><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatjs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hello-world</title>
      <link href="/2023/04/08/hello-world/"/>
      <url>/2023/04/08/hello-world/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><span id="more"></span><h2><span id="quick-start">Quick Start</span></h2><h3><span id="create-a-new-post">Create a new post</span></h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3><span id="run-server">Run server</span></h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3><span id="generate-static-files">Generate static files</span></h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3><span id="deploy-to-remote-sites">Deploy to remote sites</span></h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>常用存储介绍</title>
      <link href="/2023/04/08/Ceph/1.%E5%AD%98%E5%82%A8%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E5%AD%98%E5%82%A8%E4%BB%8B%E7%BB%8D/"/>
      <url>/2023/04/08/Ceph/1.%E5%AD%98%E5%82%A8%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E5%AD%98%E5%82%A8%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><h1><span id="常用存储介绍">常用存储介绍</span></h1><p><img src="/images/%E5%B8%B8%E7%94%A8%E5%AD%98%E5%82%A8%E4%BB%8B%E7%BB%8D/1.jpg"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
