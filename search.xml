<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2023/04/19/Ceph/0.Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/"/>
      <url>/2023/04/19/Ceph/0.Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/Ceph%E5%91%8A%E8%AD%A6%E6%8E%92%E6%9F%A5/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>crash info <id>                                                       show crash dump metadata<br>crash json_report <hours>                                             Crashes in the last <hours> hours<br>crash ls                                                              Show new and archived crash dumps<br>crash ls-new                                                          Show new crash dumps<br>crash post                                                            Add a crash dump (use -i <jsonfile>)<br>crash prune <keep>                                                    Remove crashes older than <keep> days<br>crash rm <id>                                                         Remove a saved crash <id><br>crash stat                                                            Summarize recorded crashes</id></id></keep></keep></jsonfile></hours></hours></id></p><pre><code>&gt; 查看当前状态</code></pre><p>[root@node-1 data]# ceph crash ls</p><pre><code>![](images/Ceph/Ceph告警排查/1.jpg)&gt; 查看该ID的具体信息</code></pre><p>[root@node-1 data]# ceph crash info <id></id></p><pre><code>&gt; 使用ceph crash archive &lt;id&gt;进行标记，视为已读，或者重启所有OSD也能解决告警问题![](images/Ceph/Ceph告警排查/2.jpg)</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>RBD块存储</title>
      <link href="/2023/04/19/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%9D%97%E5%AD%98%E5%82%A8/"/>
      <url>/2023/04/19/Ceph/4.RBD%E5%9D%97%E5%AD%98%E5%82%A8/RBD%E5%9D%97%E5%AD%98%E5%82%A8/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-rbd%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B">一、RBD写入流程</a></li><li><a href="#rbd%E5%9D%97%E5%88%9B%E5%BB%BA%E5%8F%8A%E6%98%A0%E5%B0%84">RBD块创建及映射</a><ul><li><a href="#%E4%B8%80-%E5%88%9B%E5%BB%BA%E5%9D%97%E8%AE%BE%E5%A4%87">一、创建块设备</a></li><li><a href="#%E4%BA%8C-%E4%BD%BF%E7%94%A8%E5%9D%97%E8%AE%BE%E5%A4%87">二、使用块设备</a></li></ul></li></ul><!-- tocstop --><h1><span id="一-rbd写入流程">一、RBD写入流程</span></h1><p><img src="/images/Ceph/RBD%E5%9D%97%E5%AD%98%E5%82%A8/1.jpg"></p><blockquote><p>块设备都是瘦分配的，意味着使用的越多，分配的空间越多</p></blockquote><pre><code>[root@node-1 ~]# rbd -p ceph-demo info rbd-demo.img #查看镜像信息rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023[root@node-1 ~]# rados  -p ceph-demo ls | grep rbd_data.11e24b04cf01 #查看objectrbd_data.11e24b04cf01.0000000000000960rbd_data.11e24b04cf01.0000000000000500rbd_data.11e24b04cf01.0000000000000780rbd_data.11e24b04cf01.0000000000000320rbd_data.11e24b04cf01.00000000000008c0rbd_data.11e24b04cf01.00000000000006e0rbd_data.11e24b04cf01.00000000000009ffrbd_data.11e24b04cf01.0000000000000640rbd_data.11e24b04cf01.0000000000000000rbd_data.11e24b04cf01.0000000000000280rbd_data.11e24b04cf01.0000000000000501rbd_data.11e24b04cf01.0000000000000001rbd_data.11e24b04cf01.00000000000003c0rbd_data.11e24b04cf01.0000000000000502rbd_data.11e24b04cf01.00000000000005a0rbd_data.11e24b04cf01.0000000000000460rbd_data.11e24b04cf01.0000000000000140rbd_data.11e24b04cf01.00000000000001e0rbd_data.11e24b04cf01.0000000000000820rbd_data.11e24b04cf01.00000000000000a0[root@node-1 data]# rados -p ceph-demo stat rbd_data.11e24b04cf01.000000000000007a#查看object 大小 size=比特，需要除以两次1024得出Mceph-demo/rbd_data.11e24b04cf01.000000000000007a mtime 2023-03-04 15:08:07.000000, size 4194304[root@node-1 ~]#  ceph osd map ceph-demo rbd_data.11e24b04cf01.000000000000007a #查看object 落到那个OSD和PG上 PG=1.20 最终落在0，1，2三个OSD上osdmap e339 pool &#39;ceph-demo&#39; (1) object &#39;rbd_data.11e24b04cf01.0000000000000960&#39; -&gt; pg 1.85b57b60 (1.20) -&gt; up ([2,1,0], p2) acting ([2,1,0], p2)[root@node-1 ~]# rados  -p ceph-demo ls |wc -l 查看当前ceph-demo的pool中一共存在25个object 每个object等于4 M，一共有25个，所以总共分配空间为100M，随着数据增加，会动态扩容分配空间。25[root@node-1 data]# dd if=/dev/zero of=test.img bs=1M count=1024  #尝试写入1G的文件到集群中1024+0 records in1024+0 records out1073741824 bytes (1.1 GB) copied, 5.97513 s, 180 MB/sYou have new mail in /var/spool/mail/root[root@node-1 data]# ls -lha total 1.1Gdrwxr-xr-x.  2 root root   34 Mar  4 15:08 .dr-xr-xr-x. 18 root root  236 Mar  4 02:27 ..-rw-r--r--.  1 root root    5 Mar  4 02:30 test-rw-r--r--.  1 root root 1.0G Mar  4 15:08 test.img[root@node-1 data]# rados  -p ceph-demo ls | grep rbd_data.11e24b04cf01 |wc -l  #可以看到object的数量由25个增加到了276个 每个object等于4 M，所以总计分配1,104M276[root@node-1 data]# df -HT #查看实际使用情况发现基本相当Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   14M  941M   2% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.7G   16G  15% //dev/sda1               xfs       1.1G  190M  874M  18% /boottmpfs                   tmpfs     191M     0  191M   0% /run/user/0tmpfs                   tmpfs     954M   54k  954M   1% /var/lib/ceph/osd/ceph-0/dev/rbd0               xfs        11G  1.2G  9.7G  11% /data</code></pre><h1><span id="rbd块创建及映射">RBD块创建及映射</span></h1><h2><span id="一-创建块设备">一、创建块设备</span></h2><p><strong>任意节点执行即可：</strong></p><pre><code>[root@node-1 my-cluster]#  rbd help create #查看create的帮助文档[root@node-1 my-cluster]#  rbd create -p ceph-demo --image rbd-demo.img --size 10G #在ceph-demo的pool中创建一个叫做rbd-demo.img的块设备，大小为10G[root@node-1 my-cluster]# rbd ls ceph-demo #查看创建结果rbd-demo.img[root@node-1 my-cluster]#  rbd create  ceph-demo/rbd-demo-1.img --size 10G #在ceph-demo的pool中创建一个叫做rbd-demo.img的块设备，大小为10G[root@node-1 my-cluster]# rbd ls ceph-demo  #查看创建结果rbd-demo-1.imgrbd-demo.img[root@node-1 my-cluster]# rbd -p ceph-demo ls  #与rbd ls ceph-demo 作用一致rbd-demo-1.imgrbd-demo.img[root@node-1 my-cluster]#  rbd info ceph-demo/rbd-demo.img #查看块设备的具体信息，与rbd info-p ceph-demo --image rbd-demo.img 一致rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Fri Mar  3 13:03:28 2023    access_timestamp: Fri Mar  3 13:03:28 2023    modify_timestamp: Fri Mar  3 13:03:28 2023[root@node-1 my-cluster]# rbd rm  ceph-demo/rbd-demo-1.img #删除块设备，与rbd rm -p ceph-demo --image rbd-demo-1.img用法一致Removing image: 100% complete...done.</code></pre><h2><span id="二-使用块设备">二、使用块设备</span></h2><pre><code>[root@node-1 my-cluster]#  rbd map ceph-demo/rbd-demo.img #映射块设备到本地rbd: sysfs write failedRBD image feature set mismatch. You can disable features unsupported by the kernel with &quot;rbd feature disable ceph-demo/rbd-demo.img object-map fast-diff deep-flatten&quot;.In some cases useful info is found in syslog - try &quot;dmesg | tail&quot;.rbd: map failed: (6) No such device or address</code></pre><blockquote><p>上述报错主要是由于Centos 7 不支持某些特性，需要手动禁用，具体可使用rbd info ceph-demo&#x2F;rbd-demo.img 进行查看，特性主要记录在features中</p></blockquote><p><strong>禁用相关特性：</strong></p><pre><code>[root@node-1 my-cluster]#  rbd info ceph-demo/rbd-demo.img #查看当前特性rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten    op_features:     flags:     create_timestamp: Fri Mar  3 13:03:28 2023    access_timestamp: Fri Mar  3 13:03:28 2023    modify_timestamp: Fri Mar  3 13:03:28 2023[root@node-1 my-cluster]# rbd help feature disable #查看使用文档usage: rbd feature disable [--pool &lt;pool&gt;] [--namespace &lt;namespace&gt;]                            [--image &lt;image&gt;]                            &lt;image-spec&gt; &lt;features&gt; [&lt;features&gt; ...]Disable the specified image feature.Positional arguments  &lt;image-spec&gt;         image specification                       (example: [&lt;pool-name&gt;/[&lt;namespace&gt;/]]&lt;image-name&gt;)  &lt;features&gt;           image features                       [exclusive-lock, object-map, journaling]Optional arguments  -p [ --pool ] arg    pool name  --namespace arg      namespace name  --image arg          image name[root@node-1 my-cluster]# rbd feature disable ceph-demo/rbd-demo.img deep-flatten #禁用rbd-demo.img块设备的deep-flatten特性[root@node-1 my-cluster]# rbd feature disable ceph-demo/rbd-demo.img fast-diff #禁用rbd-demo.img块设备的fast-diff特性[root@node-1 my-cluster]# rbd feature disable ceph-demo/rbd-demo.img exclusive-lock #禁用rbd-demo.img块设备的exclusive-lock特性[root@node-1 my-cluster]# rbd info ceph-demo/rbd-demo.img #查看相关特性已经被禁用rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering #layering不影响使用，可不禁用    op_features:     flags:     create_timestamp: Fri Mar  3 13:03:28 2023    access_timestamp: Fri Mar  3 13:03:28 2023    modify_timestamp: Fri Mar  3 13:03:28 2023 [root@node-1 my-cluster]#  rbd map ceph-demo/rbd-demo.img #挂载到本地/dev/rbd0[root@node-1 my-cluster]# lsblkNAME                                                                                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsda                                                                                                     8:0    0   20G  0 disk ├─sda1                                                                                                  8:1    0    1G  0 part /boot└─sda2                                                                                                  8:2    0   19G  0 part   ├─centos-root                                                                                       253:0    0   17G  0 lvm  /  └─centos-swap                                                                                       253:1    0    2G  0 lvm  [SWAP]sdb                                                                                                     8:16   0   10G  0 disk └─ceph--53398982--af97--4b05--9a0b--bf91741b7f6a-osd--block--68c09ae0--7d72--4984--b5c0--6f1476698c2b 253:2    0   10G  0 lvm  sdc                                                                                                     8:32   0   10G  0 disk sr0                                                                                                    11:0    1 1024M  0 rom  rbd0                                                                                                  252:0    0   10G  0 disk [root@node-1 my-cluster]# rbd device list #使用此命令进行查看id pool      namespace image        snap device    0  ceph-demo           rbd-demo.img -    /dev/rbd0[root@node-1 my-cluster]#  mkfs.xfs /dev/rbd0 #格式化为xfs格式Discarding blocks...Done.meta-data=/dev/rbd0              isize=512    agcount=16, agsize=163840 blks         =                       sectsz=512   attr=2, projid32bit=1         =                       crc=1        finobt=0, sparse=0data     =                       bsize=4096   blocks=2621440, imaxpct=25         =                       sunit=1024   swidth=1024 blksnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1log      =internal log           bsize=4096   blocks=2560, version=2         =                       sectsz=512   sunit=8 blks, lazy-count=1realtime =none                   extsz=4096   blocks=0, rtextents=0[root@node-1 my-cluster]# mkdir /data[root@node-1 my-cluster]# mount /dev/rbd0 /data #进行挂载[root@node-1 my-cluster]# df -HT #查看挂载情况Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   11M  944M   2% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.6G   16G  15% //dev/sda1               xfs       1.1G  190M  874M  18% /boottmpfs                   tmpfs     191M     0  191M   0% /run/user/0tmpfs                   tmpfs     954M   54k  954M   1% /var/lib/ceph/osd/ceph-0/dev/rbd0               xfs        11G   35M   11G   1% /data</code></pre><p>BD存储扩容</p><p><strong>查看需要扩容的镜像</strong></p><pre><code>[root@node-1 data]# rbd ls ceph-demo rbd-demo.img</code></pre><p><strong>查看需要扩容的镜像</strong></p><pre><code>[root@node-1 data]# rbd -p ceph-demo info rbd-demo.img rbd image &#39;rbd-demo.img&#39;:    size 10 GiB in 2560 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023</code></pre><p><strong>使用rbd resize进行扩容帮助</strong></p><pre><code>[root@node-1 data]# rbd help resize  #使用rbd resize进行扩容usage: rbd resize [--pool &lt;pool&gt;] [--namespace &lt;namespace&gt;]                   [--image &lt;image&gt;] --size &lt;size&gt; [--allow-shrink]                   [--no-progress]                   &lt;image-spec&gt; Resize (expand or shrink) image.Positional arguments  &lt;image-spec&gt;         image specification                       (example: [&lt;pool-name&gt;/[&lt;namespace&gt;/]]&lt;image-name&gt;)Optional arguments  -p [ --pool ] arg    pool name  --namespace arg      namespace name  --image arg          image name  -s [ --size ] arg    image size (in M/G/T) [default: M]  --allow-shrink       permit shrinking  #缩容操作，不建议，可能会导致数据丢失  --no-progress        disable progress output</code></pre><p><strong>将ceph-demo池下的rbd-demo.img镜像扩容到20G</strong></p><pre><code>[root@node-1 data]#  rbd resize ceph-demo/rbd-demo.img --size 20G Resizing image: 100% complete...done.[root@node-1 data]# rbd -p ceph-demo info rbd-demo.img #查看已经扩容到20G了rbd image &#39;rbd-demo.img&#39;:    size 20 GiB in 5120 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023</code></pre><p><strong>#将上述扩容到20G的硬盘缩容到15G</strong></p><pre><code>[root@node-1 data]#  rbd resize ceph-demo/rbd-demo.img --allow-shrink --size 15GResizing image: 100% complete...done.[root@node-1 data]# rbd -p ceph-demo info rbd-demo.imgrbd image &#39;rbd-demo.img&#39;:    size 15 GiB in 3840 objects    order 22 (4 MiB objects)    snapshot_count: 0    id: 11e24b04cf01    block_name_prefix: rbd_data.11e24b04cf01    format: 2    features: layering    op_features:     flags:     create_timestamp: Sat Mar  4 02:03:28 2023    access_timestamp: Sat Mar  4 02:03:28 2023    modify_timestamp: Sat Mar  4 02:03:28 2023</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> RBD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OSD扩容和换盘</title>
      <link href="/2023/04/19/Ceph/7.OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/"/>
      <url>/2023/04/19/Ceph/7.OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-osd%E6%89%A9%E5%AE%B9">一、OSD扩容</a><ul><li><a href="#1-%E7%BA%B5%E5%90%91%E6%A8%AA%E5%90%91%E6%89%A9%E5%AE%B9">1、纵向&amp;横向扩容</a><ul><li><a href="#11-%E7%BA%B5%E5%90%91%E6%89%A9%E5%AE%B9">1.1、纵向扩容</a></li></ul></li></ul></li><li><a href="#%E4%BA%8C-%E6%95%B0%E6%8D%AE%E9%87%8D%E5%88%86%E5%B8%83">二、数据重分布</a></li><li><a href="#%E4%B8%89-%E5%9D%8F%E7%9B%98%E6%9B%B4%E6%8D%A2">三、坏盘更换</a><ul><li><a href="#%E6%A8%A1%E6%8B%9F%E5%88%A0%E9%99%A4osd">模拟删除OSD</a></li></ul></li><li><a href="#%E5%9B%9B-%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7">四、数据一致性</a></li></ul><!-- tocstop --><h1><span id="一-osd扩容">一、OSD扩容</span></h1><p>OSD扩容大体上可分为两种：</p><ol><li>横向扩容: scale out 通过增加节点来完成扩容。</li><li>纵向扩容: csale up 通过在现有节点下增加硬盘来完成扩容。</li></ol><p>详情查看<a href="https://docs.ceph.com/en/nautilus/rados/deployment/ceph-deploy-osd/#zap-disks">官方文档</a>。</p><h2><span id="1-纵向amp横向扩容">1、纵向&amp;横向扩容</span></h2><p><strong>横向扩容时请先安装ceph的软件包，ntp，防火墙等基础功能</strong></p><h3><span id="11-纵向扩容">1.1、纵向扩容</span></h3><pre><code>ceph-deploy osd create --data &#123;data-disk&#125; &#123;node-name&#125; ceph-deploy disk zap &lt;node-name&gt; &lt;data-disk&gt; #清除要增加硬盘的分区    </code></pre><h1><span id="二-数据重分布">二、数据重分布</span></h1><p>当您将 Ceph OSD 守护进程添加到 Ceph 存储集群时，集群映射会使用新的 OSD 进行更新。回头参考 <a href="https://docs.ceph.com/en/nautilus/architecture/#calculating-pg-ids">计算PG ID</a>，这会更改集群映射。因此，它会更改对象放置，因为它会更改计算的输入。下图描述了重新平衡过程（虽然相当粗略，因为它对大型集群的影响要小得多），其中一些但不是所有 PG 从现有 OSD（OSD 1 和 OSD 2）迁移到新的 OSD（OSD 3） ). 即使在重新平衡时，CR​​USH 也很稳定。许多归置组仍保留其原始配置，并且每个 OSD 都获得了一些额外的容量，因此在重新平衡完成后新 OSD 上不会出现负载峰值。</p><p><img src="/images/Ceph/OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/1.jpg"></p><p><strong>写入一个2G的文件</strong></p><p><code>[root@node-1 /]#dd if=/dev/zero of=rebalancing-file.img bs=1M count=2048</code></p><p><strong>上传到data中</strong></p><p><code>[root@node-1 /]# mv rebalancing-file.img /data</code></p><p><strong>data是ceph中的块设备</strong></p><pre><code>[root@node-1 /]# df -HTFilesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   11M  944M   2% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  4.8G   14G  26% //dev/sda1               xfs       1.1G  204M  860M  20% /boottmpfs                   tmpfs     954M   25k  954M   1% /var/lib/ceph/osd/ceph-0tmpfs                   tmpfs     954M   25k  954M   1% /var/lib/ceph/osd/ceph-3tmpfs                   tmpfs     191M     0  191M   0% /run/user/0/dev/rbd0               xfs        11G  2.2G  8.6G  21% /data</code></pre><p><strong>可以看到ceph-demo中有6G的数据，一共写入了2G的数据，3副本，所以共使用6G</strong></p><pre><code>[root@node-1 /]# ceph df RAW STORAGE:    CLASS     SIZE       AVAIL      USED       RAW USED     %RAW USED     hdd       60 GiB     24 GiB     30 GiB       36 GiB         60.75     TOTAL     60 GiB     24 GiB     30 GiB       36 GiB         60.75  POOLS:    POOL                          ID     PGS     STORED      OBJECTS     USED        %USED     MAX AVAIL     ceph-demo                      1     512     2.0 GiB         537     6.0 GiB     25.56       5.9 GiB     .rgw.root                      2      32     1.2 KiB           4     768 KiB         0       5.9 GiB     default.rgw.control            3      32         0 B           8         0 B         0       5.9 GiB     default.rgw.meta               4      32     1.5 KiB           8     1.3 MiB         0       5.9 GiB     default.rgw.log                5      32         0 B         207         0 B         0       5.9 GiB     default.rgw.buckets.index      6      32         0 B           2         0 B         0       5.9 GiB     default.rgw.buckets.data       7      32         0 B           0         0 B         0       5.9 GiB     cephfs_data                    8      16       8 GiB       2.05k      24 GiB     57.73       5.9 GiB     cephfs_metadata                9      16     361 KiB          23     2.6 MiB      0.01       5.9 GiB </code></pre><p><strong>PG数据重分布状态</strong></p><p><img src="/images/Ceph/OSD%E6%89%A9%E5%AE%B9%E5%92%8C%E6%8D%A2%E7%9B%98/2.jpg"></p><p><strong>查看OSD线程</strong></p><pre><code>[root@node-1 /]# ceph --admin-daemon /var/run/ceph/ceph-mon.node-1.asok config show | grep max_b    &quot;bluestore_compression_max_blob_size&quot;: &quot;0&quot;,    &quot;bluestore_compression_max_blob_size_hdd&quot;: &quot;524288&quot;,    &quot;bluestore_compression_max_blob_size_ssd&quot;: &quot;65536&quot;,    &quot;bluestore_max_blob_size&quot;: &quot;0&quot;,    &quot;bluestore_max_blob_size_hdd&quot;: &quot;524288&quot;,    &quot;bluestore_max_blob_size_ssd&quot;: &quot;65536&quot;,    &quot;bluestore_rocksdb_options&quot;: &quot;compression=kNoCompression,max_write_buffer_number=4,min_write_buffer_number_to_merge=1,recycle_log_file_num=4,write_buffer_size=268435456,writable_file_max_buffer_size=0,compaction_readahead_size=2097152,max_background_compactions=2&quot;,    &quot;client_readahead_max_bytes&quot;: &quot;0&quot;,    &quot;filestore_queue_max_bytes&quot;: &quot;104857600&quot;,    &quot;filestore_rocksdb_options&quot;: &quot;max_background_jobs=10,compaction_readahead_size=2097152,compression=kNoCompression&quot;,    &quot;kstore_max_bytes&quot;: &quot;67108864&quot;,    &quot;ms_max_backoff&quot;: &quot;15.000000&quot;,    &quot;osd_bench_max_block_size&quot;: &quot;67108864&quot;,    &quot;osd_map_message_max_bytes&quot;: &quot;10485760&quot;,    &quot;osd_max_backfills&quot;: &quot;1&quot;, #每个OSD最多有一个线程    &quot;osd_tier_promote_max_bytes_sec&quot;: &quot;5242880&quot;,    &quot;rbd_readahead_max_bytes&quot;: &quot;524288&quot;,    &quot;rgw_sync_log_trim_max_buckets&quot;: &quot;16&quot;,    &quot;rgw_user_max_buckets&quot;: &quot;1000&quot;,</code></pre><p><strong>数据同步的时候使用cluster_network网络，客户端连接的时候使用pubic_network网络 生产环境中最好分开</strong></p><pre><code>[root@node-1 /]# cat /etc/ceph/ceph.conf #数据同步的时候使用cluster_network网络，客户端连接的时候使用pubic_network网络 生产环境中最好分开[global]fsid = e9a90625-4707-4b6b-b52f-661512ea831dpublic_network = 192.168.187.0/24cluster_network = 192.168.199.0/24mon_initial_members = node-1mon_host = 192.168.187.201auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxosd pool default pg num = 32osd pool default pgp num = 32mon_pg_warn_max_per_osd = 512mon_max_pg_per_osd=3000[client.rgw.node-1]rgw_frontends = &quot;civetweb port=80&quot;</code></pre><p><strong>如果是生产环境中需要扩容OSD，但是业务收到影响后可以使用 norebalance 停止</strong><br><strong>查看帮助</strong></p><pre><code>[root@node-1 /]# ceph --help |grep reba osd set full|pause|noup|nodown|noout|noin|nobackfill|norebalance|     set &lt;key&gt;osd unset full|pause|noup|nodown|noout|noin|nobackfill|norebalance|   unset &lt;key&gt;</code></pre><p><strong>暂停同步</strong></p><pre><code>[root@node-1 /]# ceph osd set norebalance  #暂停同步norebalance is set[root@node-1 /]# ceph osd set nobackfill #同时需要暂停这个，这个也会进行数据填充的nobackfill is set</code></pre><p><strong>查看集群状态</strong></p><pre><code>[root@node-1 /]# ceph -s   cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            norebalance flag(s) set #可以看到已经暂停同步了            1 pools have too few placement groups            1 pools have too many placement groups            clock skew detected on mon.node-2, mon.node-3   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 2m)    mgr: node-3(active, since 56m), standbys: node-2, node-1    mds: cephfs-demo:1 &#123;0=node-3=up:active&#125; 2 up:standby    osd: 6 osds: 6 up (since 20m), 6 in (since 19h)         flags norebalance    rgw: 1 daemon active (node-1)   task status:   data:    pools:   9 pools, 736 pgs    objects: 2.84k objects, 10 GiB    usage:   36 GiB used, 24 GiB / 60 GiB avail    pgs:     736 active+clean</code></pre><p><strong>启动同步</strong></p><pre><code>[root@node-1 /]# ceph osd unset norebalance  #启动同步norebalance is unset[root@node-1 /]# ceph osd unset nobackfill #启动同步</code></pre><h1><span id="三-坏盘更换">三、坏盘更换</span></h1><p>当你想减少集群的大小或更换硬件时，你可以在运行时删除一个 OSD。对于 Ceph，一个 OSD 通常是ceph-osd 主机中一个存储驱动器的一个 Ceph 守护进程。如果您的主机有多个存储驱动器，您可能需要ceph-osd为每个驱动器删除一个守护进程。通常，最好检查集群的容量以查看是否已达到其容量的上限。确保当您删除 OSD 时，您的集群未达到其比率。near full</p><p><strong>可通过这个命令查看到坏盘的信息</strong></p><p><code>[root@node-3 ~]# dmesg</code></p><p><strong>可以通过这个命令看到那个OSD的延迟比较大， 来判断是否存在坏道情况</strong></p><pre><code>[root@node-1 /]# ceph osd perf #可以通过这个命令看到那个OSD的延迟比较大， 来判断是否存在坏道情况osd commit_latency(ms) apply_latency(ms)   5                  0                 0   4                  0                 0   0                  0                 0   1                  0                 0   2                  0                 0   3                  0                 0 </code></pre><h2><span id="模拟删除osd">模拟删除OSD</span></h2><pre><code>[root@node-1 /]# ceph osd tree  #模拟删除osd.5ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.05878 root default                            -3       0.01959     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000  3   hdd 0.00980         osd.3       up  1.00000 1.00000 -5       0.01959     host node-2                          1   hdd 0.00980         osd.1       up  1.00000 1.00000  4   hdd 0.00980         osd.4       up  1.00000 1.00000 -7       0.01959     host node-3                          2   hdd 0.00980         osd.2       up  1.00000 1.00000  5   hdd 0.00980         osd.5       up  1.00000 1.00000  [root@node-1 /]# ssh node-3 #远程到node-3服务器Last login: Sun Mar  5 19:50:53 2023 from 192.168.187.1[root@node-3 ~]# systemctl stop ceph-osd@5 #停止osd5服务[root@node-3 ~]# systemctl status ceph-osd@5 #查看状态● ceph-osd@5.service - Ceph object storage daemon osd.5   Loaded: loaded (/usr/lib/systemd/system/ceph-osd@.service; enabled-runtime; vendor preset: disabled)   Active: inactive (dead) since Sun 2023-03-05 20:45:15 CST; 8s ago  Process: 1729 ExecStart=/usr/bin/ceph-osd -f --cluster $&#123;CLUSTER&#125; --id %i --setuser ceph --setgroup ceph (code=exited, status=0/SUCCESS)  Process: 1724 ExecStartPre=/usr/lib/ceph/ceph-osd-prestart.sh --cluster $&#123;CLUSTER&#125; --id %i (code=exited, status=0/SUCCESS) Main PID: 1729 (code=exited, status=0/SUCCESS)Mar 05 19:49:50 node-3 systemd[1]: Starting Ceph object storage daemon osd.5...Mar 05 19:49:50 node-3 systemd[1]: Started Ceph object storage daemon osd.5.Mar 05 19:49:56 node-3 ceph-osd[1729]: 2023-03-05 19:49:56.053 7f3fa59a6a80 -1 osd.5 1454 log_to_monitors &#123;default=true&#125;Mar 05 19:50:11 node-3 ceph-osd[1729]: 2023-03-05 19:50:11.748 7f3f97fc5700 -1 osd.5 1454 set_numa_affinity unable to identify...rectoryMar 05 20:45:15 node-3 systemd[1]: Stopping Ceph object storage daemon osd.5...Mar 05 20:45:15 node-3 ceph-osd[1729]: 2023-03-05 20:45:15.800 7f3f9b84b700 -1 received  signal: Terminated from /usr/lib/syst... UID: 0Mar 05 20:45:15 node-3 ceph-osd[1729]: 2023-03-05 20:45:15.800 7f3f9b84b700 -1 osd.5 1479 *** Got signal Terminated ***Mar 05 20:45:15 node-3 ceph-osd[1729]: 2023-03-05 20:45:15.800 7f3f9b84b700 -1 osd.5 1479 *** Immediate shutdown (osd_fast_shu...ue) ***Mar 05 20:45:15 node-3 systemd[1]: Stopped Ceph object storage daemon osd.5.Hint: Some lines were ellipsized, use -l to show in full.[root@node-3 ~]# ceph osd tree #在集群中查看状态ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.05878 root default                            -3       0.01959     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000  3   hdd 0.00980         osd.3       up  1.00000 1.00000 -5       0.01959     host node-2                          1   hdd 0.00980         osd.1       up  1.00000 1.00000  4   hdd 0.00980         osd.4       up  1.00000 1.00000 -7       0.01959     host node-3                          2   hdd 0.00980         osd.2       up  1.00000 1.00000  5   hdd 0.00980         osd.5     down  1.00000 1.00000 [root@node-3 ~]# ceph -s  #大概需要10分钟的时候才会触发rebalance的操作  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            1 osds down            Degraded data redundancy: 1272/8514 objects degraded (14.940%), 199 pgs degraded            1 pools have too few placement groups            1 pools have too many placement groups            clock skew detected on mon.node-2, mon.node-3   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 25m)    mgr: node-3(active, since 53m), standbys: node-2, node-1    mds: cephfs-demo:1 &#123;0=node-3=up:active&#125; 2 up:standby    osd: 6 osds: 5 up (since 136y), 6 in (since 19h)    rgw: 1 daemon active (node-1)   task status:   data:    pools:   9 pools, 736 pgs    objects: 2.84k objects, 10 GiB    usage:   36 GiB used, 24 GiB / 60 GiB avail    pgs:     1272/8514 objects degraded (14.940%)             365 active+clean             199 active+undersized+degraded             172 active+undersized[root@node-3 ~]# ceph osd out osd.5 #踢出集群marked out osd.5. [root@node-3 ~]# ceph osd tree  #可以看到权重变小，从1变成了0ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.05878 root default                            -3       0.01959     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000  3   hdd 0.00980         osd.3       up  1.00000 1.00000 -5       0.01959     host node-2                          1   hdd 0.00980         osd.1       up  1.00000 1.00000  4   hdd 0.00980         osd.4       up  1.00000 1.00000 -7       0.01959     host node-3                          2   hdd 0.00980         osd.2       up  1.00000 1.00000  5   hdd 0.00980         osd.5     down        0 1.00000 [root@node-3 ~]# ceph osd crush dump |grep osd.5 #查看crush            &quot;name&quot;: &quot;osd.5&quot;,[root@node-3 ~]# ceph osd crush rm osd.5  #删除crushremoved item id 5 name &#39;osd.5&#39; from crush map[root@node-3 ~]# ceph osd tree #查看已经删除crushID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.04898 root default                            -3       0.01959     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000  3   hdd 0.00980         osd.3       up  1.00000 1.00000 -5       0.01959     host node-2                          1   hdd 0.00980         osd.1       up  1.00000 1.00000  4   hdd 0.00980         osd.4       up  1.00000 1.00000 -7       0.00980     host node-3                          2   hdd 0.00980         osd.2       up  1.00000 1.00000  5             0 osd.5             down        0 1.00000 [root@node-3 ~]# ceph osd rm osd.5 #删除osd.5removed osd.5[root@node-3 ~]# ceph osd tree #查看已经删除ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.04898 root default                            -3       0.01959     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000  3   hdd 0.00980         osd.3       up  1.00000 1.00000 -5       0.01959     host node-2                          1   hdd 0.00980         osd.1       up  1.00000 1.00000  4   hdd 0.00980         osd.4       up  1.00000 1.00000 -7       0.00980     host node-3                          2   hdd 0.00980         osd.2       up  1.00000 1.00000 [root@node-3 ~]#  ceph auth list #查看认证信息mds.node-1    key: AQBLPwRkgcRTARAAJD+hzTIC+jSlAqsrLqsd4w==    caps: [mds] allow    caps: [mon] allow profile mds    caps: [osd] allow rwxmds.node-2    key: AQBLPwRk6Z6QOBAAbcoYG/A7WVQ6Xq26QngXcw==    caps: [mds] allow    caps: [mon] allow profile mds    caps: [osd] allow rwxmds.node-3    key: AQBMPwRk2fIcNhAAtP/K0mVMYRJfN8h5Jlbyig==    caps: [mds] allow    caps: [mon] allow profile mds    caps: [osd] allow rwxosd.0    key: AQCSKgJkOuSDBBAAN3OVzTiJK9rNWEePf/cHZQ==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *osd.1    key: AQAMKwJkXYOpChAA8Q/pNsKr9BVz+LG5qKLpGA==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *osd.2    key: AQAbKwJkIEoeFRAAVns6sv+UQc85fP4MjoB89w==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *osd.3    key: AQBCgwNkuu0HOhAAz/pnHg4xHpusO6/exoee9Q==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *osd.4    key: AQBSgwNkQp6uJRAAtS5oUJjJsnyWwtruAy9q4Q==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *osd.5    key: AQBigwNkaWJAJRAAUNcvokMMax4UGTgVXSRB4w==    caps: [mgr] allow profile osd    caps: [mon] allow profile osd    caps: [osd] allow *client.admin    key: AQCJJwJk4RpHLRAAQWMtAAO/4EXn2qXm35LzKw==    caps: [mds] allow *    caps: [mgr] allow *    caps: [mon] allow *    caps: [osd] allow *client.bootstrap-mds    key: AQCJJwJkHStHLRAAj1Nc65WqUG2R+YNC/ewQVQ==    caps: [mon] allow profile bootstrap-mdsclient.bootstrap-mgr    key: AQCJJwJkyTVHLRAA9eMnrbNwGYgbmietKQ33Kw==    caps: [mon] allow profile bootstrap-mgrclient.bootstrap-osd    key: AQCJJwJkrj5HLRAA3n8qXNp0Z7k4zPFtb2UXQg==    caps: [mon] allow profile bootstrap-osdclient.bootstrap-rbd    key: AQCJJwJk4UhHLRAAb0x5K5jM/YqcJNpP0aDBQg==    caps: [mon] allow profile bootstrap-rbdclient.bootstrap-rbd-mirror    key: AQCJJwJkfVFHLRAA/vh27nUJAeA2aSquy6oBug==    caps: [mon] allow profile bootstrap-rbd-mirrorclient.bootstrap-rgw    key: AQCJJwJk1VlHLRAALUO430jjgsp+TZKY151KDw==    caps: [mon] allow profile bootstrap-rgwclient.rgw.node-1    key: AQDKMANkby1/GxAAfeOIDDwPxjrwS0pibixXtg==    caps: [mon] allow rw    caps: [osd] allow rwxmgr.node-1    key: AQDIKAJkngmTBhAAGSBO+ELb4O93uqGQyhtfRA==    caps: [mds] allow *    caps: [mon] allow profile mgr    caps: [osd] allow *mgr.node-2    key: AQBvLwJkDDPYAhAAkmvWkFb/rdiggG/f1n18dA==    caps: [mds] allow *    caps: [mon] allow profile mgr    caps: [osd] allow *mgr.node-3    key: AQBwLwJkrN9IJRAA9VAsfNeDV14AOf7uaiqRTQ==    caps: [mds] allow *    caps: [mon] allow profile mgr    caps: [osd] allow *installed auth entries:[root@node-3 ~]# ceph auth rm osd.5 #删除关于osd5的认证updated需要重新添加上述硬盘时需要格式化硬盘，在node-3执行  清除逻辑卷的DM映射，操作如下：dmsetup info -Cdmsetup remove [dm_map_name]格式化磁盘mkfs.xfs -f /dev/vdbceph-deploy disk zap node-3 /dev/sdcceph-deploy osd create node-3 --data /dev/sdc --journal /dev/sdc</code></pre><h1><span id="四-数据一致性">四、数据一致性</span></h1><p>作为维护数据一致性和清洁度的一部分，Ceph OSD 还可以清理归置组中的对象。也就是说，Ceph OSD 可以将一个归置组中的对象元数据与其存储在其他 OSD 中的归置组中的副本进行比较。清理（通常每天执行）会捕获 OSD 错误或文件系统错误。OSD 还可以通过逐位比较对象中的数据来执行更深入的清理。深度清理（通常每周执行一次）会发现磁盘上的坏扇区，而这些坏扇区在轻度清理中并不明显。<br>有关配置清理的详细信息，请参阅<a href="https://docs.ceph.com/en/nautilus/rados/configuration/osd-config-ref#scrubbing">数据清理</a>。</p><p>除了制作对象的多个副本外，Ceph 还通过清理归置组来确保数据完整性。Ceph 清理类似于fsck对象存储层。对于每个归置组，Ceph 生成所有对象的目录并比较每个主对象及其副本以确保没有对象丢失或不匹配。轻度擦洗（每天）检查对象大小和属性。深度清理（每周一次）读取数据并使用校验和确保数据完整性。</p><p><strong>查看帮助</strong></p><pre><code>[root@node-3 ~]# ceph --help | grep scrub   --block               block until completion (scrub and deep-scrub only)mon scrub                                                             scrub the monitor storesosd deep-scrub &lt;who&gt;                                                  initiate deep scrub on osd &lt;who&gt;, or use &lt;all|any&gt; to deep scrub allosd pool deep-scrub &lt;poolname&gt; [&lt;poolname&gt;...]                        initiate deep-scrub on pool &lt;who&gt; noscrub|nodeep-scrub|hit_set_type|hit_set_period|hit_set_count|hit_   read|hit_set_grade_decay_rate|hit_set_search_last_n|scrub_min_        interval|scrub_max_interval|deep_scrub_interval|recovery_priority|    recovery_op_priority|scrub_priority|compression_mode|compression_    osd pool scrub &lt;poolname&gt; [&lt;poolname&gt;...]                             initiate scrub on pool &lt;who&gt; fadvise_dontneed|noscrub|nodeep-scrub|hit_set_type|hit_set_period|    hit_set_grade_decay_rate|hit_set_search_last_n|scrub_min_interval|    scrub_max_interval|deep_scrub_interval|recovery_priority|recovery_    op_priority|scrub_priority|compression_mode|compression_algorithm|   osd scrub &lt;who&gt;                                                       initiate scrub on osd &lt;who&gt;, or use &lt;all|any&gt; to scrub all norecover|noscrub|nodeep-scrub|notieragent|nosnaptrim|pglog_          norecover|noscrub|nodeep-scrub|notieragent|nosnaptrim                pg deep-scrub &lt;pgid&gt;                                                  start deep-scrub on &lt;pgid&gt; #深度清理pg scrub &lt;pgid&gt;                                                       start scrub on &lt;pgid&gt; #轻量清理</code></pre><p><strong>清理PG</strong></p><pre><code>[root@node-3 ~]# ceph pg dump #先拿到PG的ID[root@node-3 ~]# ceph pg scrub 1.197 #进行对比instructing pg 1.197 on osd.1 to scrub</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> OSD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KVM虚拟化进阶之基于iSCSI的KVM群集构建</title>
      <link href="/2023/04/19/KVM/6.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA.md/6.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/"/>
      <url>/2023/04/19/KVM/6.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA.md/6.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#1-%E8%A7%84%E5%88%92%E8%AE%BE%E8%AE%A1">1、规划设计</a></li><li><a href="#2-%E8%8A%82%E7%82%B9%E5%87%86%E5%A4%87">2、节点准备</a><ul><li><a href="#1-%E9%98%B6%E6%AE%B51%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85">1、阶段1：操作系统安装</a></li><li><a href="#2-%E9%98%B6%E6%AE%B52%E7%BE%A4%E9%9B%86%E7%BB%84%E4%BB%B6%E5%AE%89%E8%A3%85">2、阶段2：群集组件安装</a></li><li><a href="#3-%E9%98%B6%E6%AE%B53%E7%BE%A4%E9%9B%86%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85">3、阶段3：群集节点安装</a></li></ul></li><li><a href="#3-%E9%85%8D%E7%BD%AEiscsi-target">3、配置iSCSI Target</a></li><li><a href="#4-%E9%85%8D%E7%BD%AEstonithdisk">4、配置STONITH（DISK）</a></li><li><a href="#5-%E9%85%8D%E7%BD%AEdlm">5、配置DLM</a></li><li><a href="#6-%E9%85%8D%E7%BD%AEclvm">6、配置CLVM</a></li><li><a href="#7-%E9%85%8D%E7%BD%AEgfs2">7、配置GFS2</a></li><li><a href="#8-%E5%90%91%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%B7%BB%E5%8A%A0%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%B5%84%E6%BA%90">8、向集群中添加虚拟机资源</a><ul><li><a href="#1-%E5%B0%86%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E5%88%B0iscsi%E7%9A%84%E5%85%B1%E4%BA%AB%E7%9B%AE%E5%BD%95">1、将虚拟机数据迁移到iSCSI的共享目录</a></li><li><a href="#2-%E6%B5%8B%E8%AF%95%E5%8A%A8%E6%80%81%E8%BF%81%E7%A7%BB">2、测试动态迁移</a></li></ul></li></ul><!-- tocstop --><p>基于iSCSI的KVM群集构建</p><blockquote><p>群集节点约束：DLM &gt; CLVM &gt; File System &gt; Virtual Domain</p></blockquote><h1><span id="1-规划设计">1、规划设计</span></h1><table><thead><tr><th>主机名称</th><th>管理地址</th><th>心跳地址</th><th>存储地址</th></tr></thead><tbody><tr><td>node1</td><td>192.168.200.200</td><td>192.168.0.200</td><td>192.168.1.200</td></tr><tr><td>node2</td><td>192.168.200.201</td><td>192.168.0.201</td><td>192.168.1.201</td></tr><tr><td>stor1</td><td>192.168.200.202</td><td></td><td>192.168.1.202</td></tr></tbody></table><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/80.jpg"></p><h1><span id="2-节点准备">2、节点准备</span></h1><h2><span id="1-阶段1操作系统安装">1、阶段1：操作系统安装</span></h2><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/81.jpg"></p><h2><span id="2-阶段2群集组件安装">2、阶段2：群集组件安装</span></h2><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/82.jpg"></p><h2><span id="3-阶段3群集节点安装">3、阶段3：群集节点安装</span></h2><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/83.jpg"></p><blockquote><p>需要注意各个节点的防火墙配置！</p></blockquote><h1><span id="3-配置iscsi-target">3、配置iSCSI Target</span></h1><p><strong>在stor1服务器中安装targetcli</strong></p><pre><code>[root@stor1 ~]# yum install -y targetcli[root@stor1 ~]# targetcliWarning: Could not load preferences file /root/.targetcli/prefs.bin.targetcli shell version 2.1.53Copyright 2011-2013 by Datera, Inc and others.For help on commands, type &#39;help&#39;./&gt; /&gt; /&gt; lso- / ......................................................................................................................... [...]  o- backstores .............................................................................................................. [...]  | o- block .................................................................................................. [Storage Objects: 0]  | o- fileio ................................................................................................. [Storage Objects: 0]  | o- pscsi .................................................................................................. [Storage Objects: 0]  | o- ramdisk ................................................................................................ [Storage Objects: 0]  o- iscsi ............................................................................................................ [Targets: 0]  o- loopback ......................................................................................................... [Targets: 0]/&gt; [root@stor1 ~]# firewall-cmd --add-service=iscsi-target --permanentsuccess[root@stor1 ~]# firewall-cmd --reloadsuccess[root@stor1 ~]# fdisk -l Disk /dev/sdb: 42.9 GB, 42949672960 bytes, 83886080 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes[root@stor1 ~]# fdisk /dev/sdb Welcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Device does not contain a recognized partition tableBuilding a new DOS disklabel with disk identifier 0x923327a1.Command (m for help): pDisk /dev/sdb: 42.9 GB, 42949672960 bytes, 83886080 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x923327a1   Device Boot      Start         End      Blocks   Id  SystemCommand (m for help): nPartition type:   p   primary (0 primary, 0 extended, 4 free)   e   extendedSelect (default p): pPartition number (1-4, default 1): First sector (2048-83886079, default 2048): Using default value 2048Last sector, +sectors or +size&#123;K,M,G&#125; (2048-83886079, default 83886079): Using default value 83886079Partition 1 of type Linux and of size 40 GiB is setCommand (m for help): pDisk /dev/sdb: 42.9 GB, 42949672960 bytes, 83886080 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x923327a1   Device Boot      Start         End      Blocks   Id  System/dev/sdb1            2048    83886079    41942016   83  LinuxCommand (m for help): tSelected partition 1Hex code (type L to list all codes): 8eChanged type of partition &#39;Linux&#39; to &#39;Linux LVM&#39;Command (m for help): pDisk /dev/sdb: 42.9 GB, 42949672960 bytes, 83886080 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x923327a1   Device Boot      Start         End      Blocks   Id  System/dev/sdb1            2048    83886079    41942016   8e  Linux LVMCommand (m for help): wThe partition table has been altered!Calling ioctl() to re-read partition table.Syncing disks.[root@stor1 ~]# pvcreate /dev/sbd1  Device /dev/sbd1 not found.[root@stor1 ~]# pvcreate /dev/sdb1  Physical volume &quot;/dev/sdb1&quot; successfully created.[root@stor1 ~]# vgcreate stor1 /dev/sdb1  Volume group &quot;stor1&quot; successfully created[root@stor1 ~]# lvcreate -l 100%FREE -n lvstor1 stor1  Logical volume &quot;lvstor1&quot; created.[root@stor1 ~]# lvscan   ACTIVE            &#39;/dev/stor1/lvstor1&#39; [&lt;40.00 GiB] inherit  ACTIVE            &#39;/dev/centos/swap&#39; [&lt;3.88 GiB] inherit  ACTIVE            &#39;/dev/centos/root&#39; [&lt;35.12 GiB] inherit[root@stor1 ~]# mkfs.xfs /dev/stor1/lvstor1 meta-data=/dev/stor1/lvstor1     isize=512    agcount=4, agsize=2621184 blks         =                       sectsz=512   attr=2, projid32bit=1         =                       crc=1        finobt=0, sparse=0data     =                       bsize=4096   blocks=10484736, imaxpct=25         =                       sunit=0      swidth=0 blksnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1log      =internal log           bsize=4096   blocks=5119, version=2         =                       sectsz=512   sunit=0 blks, lazy-count=1realtime =none                   extsz=4096   blocks=0, rtextents=0[root@stor1 ~]# mkdir /stor1[root@stor1 ~]# mount /dev/stor1/lvstor1 /stor1/[root@stor1 ~]# targetclitargetcli shell version 2.1.53Copyright 2011-2013 by Datera, Inc and others.For help on commands, type &#39;help&#39;./&gt; lso- / ......................................................................................................................... [...]  o- backstores .............................................................................................................. [...]  | o- block .................................................................................................. [Storage Objects: 0]  | o- fileio ................................................................................................. [Storage Objects: 0]  | o- pscsi .................................................................................................. [Storage Objects: 0]  | o- ramdisk ................................................................................................ [Storage Objects: 0]  o- iscsi ............................................................................................................ [Targets: 0]  o- loopback ......................................................................................................... [Targets: 0]/&gt; /&gt; cd backstores/fileio/backstores/fileio&gt; create disk01 /stor1/disk01.img 1GCreated fileio disk01 with size 1073741824/backstores/fileio&gt; lso- fileio ..................................................................................................... [Storage Objects: 1]  o- disk01 .................................................................... [/stor1/disk01.img (1.0GiB) write-back deactivated]    o- alua ....................................................................................................... [ALUA Groups: 1]      o- default_tg_pt_gp ........................................................................... [ALUA state: Active/optimized]/backstores/fileio&gt; create disk02 /stor1/disk02.img 20GCreated fileio disk02 with size 21474836480/backstores/fileio&gt; lso- fileio ..................................................................................................... [Storage Objects: 2]  o- disk01 .................................................................... [/stor1/disk01.img (1.0GiB) write-back deactivated]  | o- alua ....................................................................................................... [ALUA Groups: 1]  |   o- default_tg_pt_gp ........................................................................... [ALUA state: Active/optimized]  o- disk02 ................................................................... [/stor1/disk02.img (20.0GiB) write-back deactivated]    o- alua ....................................................................................................... [ALUA Groups: 1]      o- default_tg_pt_gp ........................................................................... [ALUA state: Active/optimized]/iscsi&gt; cd /iscsi//iscsi&gt; create iqn.2016-10.org.linuxplus.srv:target00Created target iqn.2016-10.org.linuxplus.srv:target00.Created TPG 1.Global pref auto_add_default_portal=trueCreated default portal listening on all IPs (0.0.0.0), port 3260./iscsi&gt; cd iqn.2016-10.org.linuxplus.srv:target00/tpg1/luns/iscsi/iqn.20...t00/tpg1/luns&gt;  create /backstores/fileio/disk0/backstores/fileio/disk01  /backstores/fileio/disk02  /iscsi/iqn.20...t00/tpg1/luns&gt;  create /backstores/fileio/disk01Created LUN 0./iscsi/iqn.20...t00/tpg1/luns&gt;  create /backstores/fileio/disk02Created LUN 1./iscsi/iqn.20...t00/tpg1/luns&gt; lso- luns .................................................................................................................. [LUNs: 2]  o- lun0 ................................................................... [fileio/disk01 (/stor1/disk01.img) (default_tg_pt_gp)]  o- lun1 ................................................................... [fileio/disk02 (/stor1/disk02.img) (default_tg_pt_gp)]/iscsi/iqn.20...t00/tpg1/luns&gt; cd ../acls [root@node1 ~]# yum install icssi-initiarot-utils #在node1和node2节点执行#在node1执行[root@node1 ~]# cat /etc/iscsi/initiatorname.iscsi InitiatorName=iqn.1994-05.com.redhat:50adb21c54[root@node1 ~]# vim /etc/iscsi/initiatorname.iscsi [root@node1 ~]# cat /etc/iscsi/initiatorname.iscsi InitiatorName=iqn.1994-05.com.redhat:node1#在node2执行[root@node2 ~]# cat /etc/iscsi/initiatorname.iscsi InitiatorName=iqn.1994-05.com.redhat:6b7ee6fb2748[root@node2 ~]# vim /etc/iscsi/initiatorname.iscsi [root@node2 ~]# cat /etc/iscsi/initiatorname.iscsiInitiatorName=iqn.1994-05.com.redhat:node2回到stor1服务器中执行：/iscsi/iqn.20...t00/tpg1/acls&gt; create iqn.1994-05.com.redhat:node1Created Node ACL for iqn.1994-05.com.redhat:node1Created mapped LUN 1.Created mapped LUN 0./iscsi/iqn.20...t00/tpg1/acls&gt; create iqn.1994-05.com.redhat:node2Created Node ACL for iqn.1994-05.com.redhat:node2Created mapped LUN 1.Created mapped LUN 0./iscsi/iqn.20...t00/tpg1/acls&gt; ls /o- / ......................................................................................................................... [...]  o- backstores .............................................................................................................. [...]  | o- block .................................................................................................. [Storage Objects: 0]  | o- fileio ................................................................................................. [Storage Objects: 2]  | | o- disk01 .................................................................. [/stor1/disk01.img (1.0GiB) write-back activated]  | | | o- alua ................................................................................................... [ALUA Groups: 1]  | | |   o- default_tg_pt_gp ....................................................................... [ALUA state: Active/optimized]  | | o- disk02 ................................................................. [/stor1/disk02.img (20.0GiB) write-back activated]  | |   o- alua ................................................................................................... [ALUA Groups: 1]  | |     o- default_tg_pt_gp ....................................................................... [ALUA state: Active/optimized]  | o- pscsi .................................................................................................. [Storage Objects: 0]  | o- ramdisk ................................................................................................ [Storage Objects: 0]  o- iscsi ............................................................................................................ [Targets: 1]  | o- iqn.2016-10.org.linuxplus.srv:target00 ............................................................................ [TPGs: 1]  |   o- tpg1 ............................................................................................... [no-gen-acls, no-auth]  |     o- acls .......................................................................................................... [ACLs: 2]  |     | o- iqn.1994-05.com.redhat:node1 ......................................................................... [Mapped LUNs: 2]  |     | | o- mapped_lun0 ............................................................................... [lun0 fileio/disk01 (rw)]  |     | | o- mapped_lun1 ............................................................................... [lun1 fileio/disk02 (rw)]  |     | o- iqn.1994-05.com.redhat:node2 ......................................................................... [Mapped LUNs: 2]  |     |   o- mapped_lun0 ............................................................................... [lun0 fileio/disk01 (rw)]  |     |   o- mapped_lun1 ............................................................................... [lun1 fileio/disk02 (rw)]  |     o- luns .......................................................................................................... [LUNs: 2]  |     | o- lun0 ........................................................... [fileio/disk01 (/stor1/disk01.img) (default_tg_pt_gp)]  |     | o- lun1 ........................................................... [fileio/disk02 (/stor1/disk02.img) (default_tg_pt_gp)]  |     o- portals .................................................................................................... [Portals: 1]  |       o- 0.0.0.0:3260 ..................................................................................................... [OK]  o- loopback ......................................................................................................... [Targets: 0]node1执行：[root@node1 ~]# iscsiadm --mode discovery --type sendtargets --portal 192.168.1.202 #扫描后端存储192.168.1.202:3260,1 iqn.2016-10.org.linuxplus.srv:target00[root@node1 ~]# iscsiadm -m node --login d 2 Logging in to [iface: default, target: iqn.2016-10.org.linuxplus.srv:target00, portal: 192.168.1.202,3260] (multiple)Login to [iface: default, target: iqn.2016-10.org.linuxplus.srv:target00, portal: 192.168.1.202,3260] successful.[root@node1 ~]# fdisk -l |grep sdDisk /dev/sda: 42.9 GB, 42949672960 bytes, 83886080 sectors/dev/sda1   *        2048     2099199     1048576   83  Linux/dev/sda2         2099200    83886079    40893440   8e  Linux LVMDisk /dev/sdb: 1073 MB, 1073741824 bytes, 2097152 sectorsDisk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectorsnode2执行：[root@node2 ~]#  iscsiadm --mode discovery --type sendtargets --portal 192.168.1.202192.168.1.202:3260,1 iqn.2016-10.org.linuxplus.srv:target00[root@node2 ~]# iscsiadm -m node --login d 2 Logging in to [iface: default, target: iqn.2016-10.org.linuxplus.srv:target00, portal: 192.168.1.202,3260] (multiple)Login to [iface: default, target: iqn.2016-10.org.linuxplus.srv:target00, portal: 192.168.1.202,3260] successful.[root@node2 ~]# fdisk -l | grep sdDisk /dev/sda: 42.9 GB, 42949672960 bytes, 83886080 sectors/dev/sda1   *        2048     2099199     1048576   83  Linux/dev/sda2         2099200    83886079    40893440   8e  Linux LVMDisk /dev/sdb: 1073 MB, 1073741824 bytes, 2097152 sectorsDisk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectors</code></pre><h1><span id="4-配置stonithdisk">4、配置STONITH（DISK）</span></h1><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/93.jpg"></p><p>node1执行：</p><pre><code>[root@node1 ~]# pcs stonith describe fence_scsifence_scsi - Fence agent for SCSI persistent reservationfence_scsi is an I/O fencing agent that uses SCSI-3 persistent reservations to control access to shared storage devices. These devices must support SCSI-3 persistent reservations (SPC-3 or greater) as well as the &quot;preempt-and-abort&quot; subcommand.The fence_scsi agent works by having each node in the cluster register a unique key with the SCSI device(s). Once registered, a single node will become the reservation holder by creating a &quot;write exclusive, registrants only&quot; reservation on the device(s). The result is that only registered nodes may write to the device(s). When a node failure occurs, the fence_scsi agent will remove the key belonging to the failed node from the device(s). The failed node will no longer be able to write to the device(s). A manual reboot is required.When used as a watchdog device you can define e.g. retry=1, retry-sleep=2 and verbose=yes parameters in /etc/sysconfigtonith if you have issues with it failing.Stonith options:  aptpl: Use the APTPL flag for registrations. This option is only used for the &#39;on&#39; action.  devices: List of devices to use for current operation. Devices can be comma-separated list of raw devices (eg. /dev/sdc). Each device must           support SCSI-3 persistent reservations.  key: Key to use for the current operation. This key should be unique to a node. For the &quot;on&quot; action, the key specifies the key use to       register the local node. For the &quot;off&quot; action, this key specifies the key to be removed from the device(s).  port: Name of the node to be fenced. The node name is used to generate the key value used for the current operation. This option will be        ignored when used with the -k option.  logfile: Log output (stdout and stderr) to file  quiet: Disable logging to stderr. Does not affect --verbose or --debug-file or logging to syslog.  verbose: Verbose mode  debug: Write debug information to given file  delay: Wait X seconds before fencing is started  login_timeout: Wait X seconds for cmd prompt after login  power_timeout: Test X seconds for status change after ON/OFF  power_wait: Wait X seconds after issuing ON/OFF  shell_timeout: Wait X seconds for cmd prompt after issuing command  retry_on: Count of attempts to retry power on  corosync_cmap_path: Path to corosync-cmapctl binary  sg_persist_path: Path to sg_persist binary  sg_turs_path: Path to sg_turs binary  vgs_path: Path to vgs binary  pcmk_host_map: A mapping of host names to ports numbers for devices that do not support host names. Eg. node1:1;node2:2,3 would tell the                 cluster to use port 1 for node1 and ports 2 and 3 for node2  pcmk_host_list: A list of machines controlled by this device (Optional unless pcmk_host_check=static-list).  pcmk_host_check: How to determine which machines are controlled by the device. Allowed values: dynamic-list (query the device via the                   &#39;list&#39; command), static-list (check the pcmk_host_list attribute), status (query the device via the &#39;status&#39; command),                   none (assume every device can fence every machine)  pcmk_delay_max: Enable a random delay for stonith actions and specify the maximum of random delay. This prevents double fencing when using                  slow devices such as sbd. Use this to enable a random delay for stonith actions. The overall delay is derived from this                  random delay value adding a static delay so that the sum is kept below the maximum delay.  pcmk_delay_base: Enable a base delay for stonith actions and specify base delay value. This prevents double fencing when different delays                   are configured on the nodes. Use this to enable a static delay for stonith actions. The overall delay is derived from a                   random delay value adding this static delay so that the sum is kept below the maximum delay.  pcmk_action_limit: The maximum number of actions can be performed in parallel on this device Pengine property concurrent-fencing=true needs                     to be configured first. Then use this to specify the maximum number of actions can be performed in parallel on this                     device. -1 is unlimited.Default operations:  monitor: interval=60s[root@node1 ~]# ll /dev/disk/by-id/|grep sdblrwxrwxrwx. 1 root root  9 Apr 15 19:36 scsi-36001405ac1f6f3fd0704cfab66c86851 -&gt; ../../sdblrwxrwxrwx. 1 root root  9 Apr 15 19:36 wwn-0x6001405ac1f6f3fd0704cfab66c86851 -&gt; ../../sdb[root@node1 ~]# pcs stonith create scsi-shooter fence_scsi \pcmk_host_list=&quot;node1-heartbeat node2-heartbeat&quot; \devices=&quot;/dev/disk/by-id/wwn-0x6001405ac1f6f3fd0704cfab66c86851&quot; \meta provides=unfencing[root@node1 ~]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node1-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 20:48:15 2023Last change: Sat Apr 15 20:46:56 2023 by root via cibadmin on node1-heartbeat2 nodes configured1 resource instance configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: scsi-shooter   (stonith:fence_scsi):   Started node1-heartbeatDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled</code></pre><h1><span id="5-配置dlm">5、配置DLM</span></h1><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/94.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# yum install -y gfs2-utils dlm[root@node1 ~]# rpm -qi gfs2-utilsName        : gfs2-utilsVersion     : 3.1.10Release     : 11.el7_9.1Architecture: x86_64Install Date: Sat 15 Apr 2023 08:52:22 PM CSTGroup       : System Environment/KernelSize        : 1006104License     : GPLv2+ and LGPLv2+Signature   : RSA/SHA256, Wed 18 Nov 2020 10:17:32 PM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : gfs2-utils-3.1.10-11.el7_9.1.src.rpmBuild Date  : Tue 17 Nov 2020 12:16:58 AM CSTBuild Host  : x86-01.bsys.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : https://pagure.io/gfs2-utilsSummary     : Utilities for managing the global file system (GFS2)Description :The gfs2-utils package contains a number of utilities for creating,checking, modifying, and correcting any inconsistencies in GFS2file systems.[root@node1 ~]# rpm -qi dlmName        : dlmVersion     : 4.0.7Release     : 1.el7Architecture: x86_64Install Date: Sat 15 Apr 2023 08:52:22 PM CSTGroup       : System Environment/KernelSize        : 176589License     : GPLv2 and GPLv2+ and LGPLv2+Signature   : RSA/SHA256, Thu 10 Aug 2017 11:37:37 PM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : dlm-4.0.7-1.el7.src.rpmBuild Date  : Fri 04 Aug 2017 01:33:37 AM CSTBuild Host  : c1bm.rdu2.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : https://fedorahosted.org/clusterSummary     : dlm control daemon and toolDescription :The kernel dlm requires a user daemon to control membership.</code></pre><p>node2:</p><pre><code>[root@node2 ~]# yum install -y gfs2-utils dlm</code></pre><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/95.jpg"></p><blockquote><p>clone-max&#x3D;2是指最多两个dlm，clone-host-max&#x3D;1 是指每个节点最多启动一个dlm</p></blockquote><p>node1:</p><pre><code>[root@node1 ~]# pcs resource create dlm ocf:pacemaker:controld \op monitor interval=30s on-fail=fence \clone interleave=true ordered=true[root@node1 ~]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node1-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 21:04:42 2023Last change: Sat Apr 15 21:02:15 2023 by root via cibadmin on node1-heartbeat2 nodes configured3 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: scsi-shooter   (stonith:fence_scsi):   Started node1-heartbeat Clone Set: dlm-clone [dlm]     Started: [ node1-heartbeat node2-heartbeat ]Daemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled</code></pre><h1><span id="6-配置clvm">6、配置CLVM</span></h1><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/96.jpg"><br><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/97.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# yum install -y lvm2-cluster.x86_64[root@node1 ~]# cat /etc/lvm/lvm.conf |grep -v &quot;#&quot; | grep -v ^$ | grep -A 25 globalglobal &#123;    umask = 077    test = 0    units = &quot;r&quot;    si_unit_consistency = 1    suffix = 1    activation = 1    proc = &quot;/proc&quot;    etc = &quot;/etc&quot;    locking_type = 1     wait_for_locks = 1    fallback_to_clustered_locking = 1    fallback_to_local_locking = 1    locking_dir = &quot;/run/lock/lvm&quot;    prioritise_write_locks = 1    abort_on_internal_errors = 0    metadata_read_only = 0    mirror_segtype_default = &quot;raid1&quot;    raid10_segtype_default = &quot;raid10&quot;    sparse_segtype_default = &quot;thin&quot;    use_lvmetad = 1    use_lvmlockd = 0    system_id_source = &quot;none&quot;    use_lvmpolld = 1    notify_dbus = 1&#125;[root@node1 ~]# whatis lvmconflvmconf (8)          - LVM configuration modifier[root@node1 ~]# lvmconf --help Usage: /usr/sbin/lvmconf &lt;command&gt;Commands:Enable clvm:  --enable-cluster [--lockinglibdir &lt;dir&gt;] [--lockinglib &lt;lib&gt;]Disable clvm: --disable-clusterEnable halvm: --enable-halvmDisable halvm: --disable-halvmSet locking library: --lockinglibdir &lt;dir&gt; [--lockinglib &lt;lib&gt;]Global options:Config file location: --file &lt;configfile&gt;Set services: --services [--mirrorservice] [--startstopservices]Use the separate command &#39;lvmconfig&#39; to display configuration information[root@node1 ~]# lvmconf --enable-clusterglobal &#123;    umask = 077    test = 0    units = &quot;r&quot;    si_unit_consistency = 1    suffix = 1    activation = 1    proc = &quot;/proc&quot;    etc = &quot;/etc&quot;    locking_type = 3 #通过上述命令，这个配置由1改成了3    wait_for_locks = 1    fallback_to_clustered_locking = 1    fallback_to_local_locking = 1    locking_dir = &quot;/run/lock/lvm&quot;    prioritise_write_locks = 1    abort_on_internal_errors = 0    metadata_read_only = 0    mirror_segtype_default = &quot;raid1&quot;    raid10_segtype_default = &quot;raid10&quot;    sparse_segtype_default = &quot;thin&quot;    use_lvmetad = 0    use_lvmlockd = 0    system_id_source = &quot;none&quot;    use_lvmpolld = 1    notify_dbus = 1&#125;[root@node1 ~]# reboot</code></pre><p>node2:</p><pre><code>[root@node2 ~]# yum install -y lvm2-cluster.x86_64[root@node2 ~]# lvmconf --enable-cluster[root@node2 ~]# reboot[root@node1 ~]# pcs cluster start --all node1-heartbeat: Starting Cluster (corosync)...node2-heartbeat: Starting Cluster (corosync)...node2-heartbeat: Starting Cluster (pacemaker)...node1-heartbeat: Starting Cluster (pacemaker)...[root@node1 ~]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 21:21:05 2023Last change: Sat Apr 15 21:02:15 2023 by root via cibadmin on node1-heartbeat2 nodes configured3 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: scsi-shooter   (stonith:fence_scsi):   Stopped Clone Set: dlm-clone [dlm]     Stopped: [ node1-heartbeat node2-heartbeat ]Daemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled</code></pre><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/98.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# pcs resource describe clvmAssumed agent name &#39;ocf:heartbeat:clvm&#39; (deduced from &#39;clvm&#39;)ocf:heartbeat:clvm - clvmdThis agent manages the clvmd daemon.Resource options:  with_cmirrord: Start with cmirrord (cluster mirror log daemon).  daemon_options: Options to clvmd. Refer to clvmd.8 for detailed descriptions.  activate_vgs: Whether or not to activate all cluster volume groups after starting the clvmd or not. Note that clustered volume groups will always be                deactivated before the clvmd stops regardless of what this option is set to.  exclusive: If set, only exclusive volume groups will be monitored.Default operations:  start: interval=0s timeout=90s  stop: interval=0s timeout=90s  monitor: interval=30s timeout=90s[root@node1 ~]# pcs resource create clvmd ocf:heartbeat:clvm op monitor interval=30s \on-fail=fence clone interleave=true ordered=true[root@node1 ~]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 21:25:06 2023Last change: Sat Apr 15 21:24:29 2023 by root via cibadmin on node1-heartbeat2 nodes configured5 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: scsi-shooter   (stonith:fence_scsi):   Started node1-heartbeat Clone Set: dlm-clone [dlm]     Started: [ node1-heartbeat node2-heartbeat ] Clone Set: clvmd-clone [clvmd]     Started: [ node1-heartbeat node2-heartbeat ]Daemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled[root@node1 ~]# systemctl status pacemaker.service ● pacemaker.service - Pacemaker High Availability Cluster Manager   Loaded: loaded (/usr/lib/systemd/system/pacemaker.service; disabled; vendor preset: disabled)   Active: active (running) since Sat 2023-04-15 21:20:42 CST; 6min ago     Docs: man:pacemakerd           https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/1.1/html-single/Pacemaker_Explained/index.html Main PID: 2074 (pacemakerd)   CGroup: /system.slice/pacemaker.service           ├─2074 /usr/sbin/pacemakerd -f           ├─2075 /usr/libexec/pacemaker/cib           ├─2076 /usr/libexec/pacemaker/stonithd           ├─2077 /usr/libexec/pacemaker/lrmd           ├─2078 /usr/libexec/pacemaker/attrd           ├─2079 /usr/libexec/pacemaker/pengine           ├─2080 /usr/libexec/pacemaker/crmd           ├─2231 dlm_controld -s 0           └─2563 /usr/sbin/clvmd -T90 -d0 #可以看到多了clvmd的线程，依赖于dlmApr 15 21:21:05 node1 stonith-ng[2076]:   notice: Operation &#39;on&#39; targeting node1-heartbeat on node1-heartbeat for crmd.2056@node2-heartbeat.f507774c: OKApr 15 21:21:05 node1 crmd[2080]:   notice: node1-heartbeat was successfully unfenced by node1-heartbeat (at the request of node2-heartbeat)Apr 15 21:21:06 node1 crmd[2080]:   notice: Result of probe operation for scsi-shooter on node1-heartbeat: 7 (not running)Apr 15 21:21:06 node1 crmd[2080]:   notice: Result of probe operation for dlm on node1-heartbeat: 7 (not running)Apr 15 21:21:06 node1 crmd[2080]:   notice: Result of start operation for scsi-shooter on node1-heartbeat: 0 (ok)Apr 15 21:21:07 node1 dlm_controld[2231]: 150 dlm_controld 4.0.7 startedApr 15 21:21:08 node1 crmd[2080]:   notice: Result of start operation for dlm on node1-heartbeat: 0 (ok)Apr 15 21:24:29 node1 crmd[2080]:   notice: Result of probe operation for clvmd on node1-heartbeat: 7 (not running)Apr 15 21:24:32 node1 clvmd[2563]: Cluster LVM daemon started - connected to CorosyncApr 15 21:24:33 node1 crmd[2080]:   notice: Result of start operation for clvmd on node1-heartbeat: 0 (ok)</code></pre><p>配置约束：Clmvd必须在dlm启动后启动，而且必须在同一节点运行</p><pre><code>[root@node1 ~]#  pcs constraint order start dlm-clone then clvmd-clone  #次序约束，先启动dlm，然后在启动clvmdAdding dlm-clone clvmd-clone (kind: Mandatory) (Options: first-action=start then-action=start)[root@node1 ~]# pcs constraint colocation add clvmd-clone with dlm-clone #位置约束，dlm和clvmd要同时在一个节点上运行[root@node1 ~]#   pcs constraint  #查看约束Location Constraints:Ordering Constraints:  start dlm-clone then start clvmd-clone (kind:Mandatory)Colocation Constraints:  clvmd-clone with dlm-clone (score:INFINITY)Ticket Constraints:</code></pre><p>创建lv</p><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/99.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# fdisk -l | grep sdcDisk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectors[root@node1 ~]# fdisk /dev/sdcWelcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Device does not contain a recognized partition tableBuilding a new DOS disklabel with disk identifier 0x760ad300.Command (m for help): pDisk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 4194304 bytesDisk label type: dosDisk identifier: 0x760ad300   Device Boot      Start         End      Blocks   Id  SystemCommand (m for help): nPartition type:   p   primary (0 primary, 0 extended, 4 free)   e   extendedSelect (default p): pPartition number (1-4, default 1): First sector (8192-41943039, default 8192): Using default value 8192Last sector, +sectors or +size&#123;K,M,G&#125; (8192-41943039, default 41943039): Using default value 41943039Partition 1 of type Linux and of size 20 GiB is setCommand (m for help): pDisk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 4194304 bytesDisk label type: dosDisk identifier: 0x760ad300   Device Boot      Start         End      Blocks   Id  System/dev/sdc1            8192    41943039    20967424   83  LinuxCommand (m for help): tSelected partition 1Hex code (type L to list all codes): 8eChanged type of partition &#39;Linux&#39; to &#39;Linux LVM&#39;Command (m for help): pDisk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 4194304 bytesDisk label type: dosDisk identifier: 0x760ad300   Device Boot      Start         End      Blocks   Id  System/dev/sdc1            8192    41943039    20967424   8e  Linux LVMCommand (m for help): wThe partition table has been altered!Calling ioctl() to re-read partition table.Syncing disks.[root@node1 ~]# vgscan   Reading all physical volumes.  This may take a while...  Found volume group &quot;centos&quot; using metadata type lvm2  [root@node1 ~]# vgcreate vmvg0 /dev/sdc1  Physical volume &quot;/dev/sdc1&quot; successfully created.  Clustered volume group &quot;vmvg0&quot; successfully created[root@node1 ~]# vgdisplay vmvg0  --- Volume group ---  VG Name               vmvg0  System ID               Format                lvm2  Metadata Areas        1  Metadata Sequence No  1  VG Access             read/write  VG Status             resizable  Clustered             yes #这是一个集群的lvm  Shared                no  MAX LV                0  Cur LV                0  Open LV               0  Max PV                0  Cur PV                1  Act PV                1  VG Size               19.99 GiB  PE Size               4.00 MiB  Total PE              5118  Alloc PE / Size       0 / 0     Free  PE / Size       5118 / 19.99 GiB  VG UUID               mbCxgE-6pfx-HKLT-5f4Z-vcRm-TKSo-3dEnNp[root@node1 ~]# vgs #Attr下的wz--nc中的c表示这是一个集群的  VG     #PV #LV #SN Attr   VSize   VFree   centos   1   2   0 wz--n- &lt;39.00g  4.00m  vmvg0    1   0   0 wz--nc  19.99g 19.99g[root@node1 ~]#  lvcreate -n  lvvm0 -l 100%FREE vmvg0  #创建lv  Logical volume &quot;lvvm0&quot; created.[root@node1 ~]# lvscan   ACTIVE            &#39;/dev/vmvg0/lvvm0&#39; [19.99 GiB] inherit  ACTIVE            &#39;/dev/centos/swap&#39; [&lt;3.88 GiB] inherit  ACTIVE            &#39;/dev/centos/root&#39; [&lt;35.12 GiB] inherit</code></pre><p>node2:</p><pre><code>[root@node2 ~]# partprobe Warning: Unable to open /dev/sr0 read-write (Read-only file system).  /dev/sr0 has been opened read-only.[root@node2 ~]# multipath -rApr 15 21:39:08 | DM multipath kernel driver not loadedApr 15 21:39:08 | /etc/multipath.conf does not exist, blacklisting all devices.Apr 15 21:39:08 | A default multipath.conf file is located atApr 15 21:39:08 | /usr/share/doc/device-mapper-multipath-0.4.9/multipath.confApr 15 21:39:08 | You can run /sbin/mpathconf --enable to createApr 15 21:39:08 | /etc/multipath.conf. See man mpathconf(8) for more detailsApr 15 21:39:08 | DM multipath kernel driver not loaded[root@node2 ~]# fdisk -l | grep sdcDisk /dev/sdc: 21.5 GB, 21474836480 bytes, 41943040 sectors/dev/sdc1            8192    41943039    20967424   8e  Linux LVM[root@node2 ~]# lvscan  #可以看到/dev/vmvg0/lvvm0  ACTIVE            &#39;/dev/vmvg0/lvvm0&#39; [19.99 GiB] inherit  ACTIVE            &#39;/dev/centos/swap&#39; [&lt;3.88 GiB] inherit  ACTIVE            &#39;/dev/centos/root&#39; [&lt;35.12 GiB] inherit</code></pre><h1><span id="7-配置gfs2">7、配置GFS2</span></h1><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/100.jpg"><br><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/101.jpg"></p><pre><code>[root@node1 ~]# lvscan   ACTIVE            &#39;/dev/vmvg0/lvvm0&#39; [19.99 GiB] inherit  ACTIVE            &#39;/dev/centos/swap&#39; [&lt;3.88 GiB] inherit  ACTIVE            &#39;/dev/centos/root&#39; [&lt;35.12 GiB] inherit[root@node1 ~]# mkfs.gfs2 -p lock_dlm -j 2 -t cluster1:node1 /dev/vmvg0/lvvm0-p=锁定的协议-j=保存两个文件系统的日志，因为现在是双节点-t=dlm里面使用的表cluster1:node1=前面是集群的名称，后面是自定义名称/dev/vmvg0/lvvm0 is a symbolic link to /dev/dm-2This will destroy any data on /dev/dm-2Are you sure you want to proceed? [y/n] yDiscarding device contents (may take a while on large devices): DoneAdding journals: Done Building resource groups: Done   Creating quota file: DoneWriting superblock and syncing: DoneDevice:                    /dev/vmvg0/lvvm0Block size:                4096Device size:               19.99 GB (5240832 blocks)Filesystem size:           19.99 GB (5240829 blocks)Journals:                  2Journal size:              64MBResource groups:           82Locking protocol:          &quot;lock_dlm&quot;Lock table:                &quot;cluster1:node1&quot;UUID:                      4a42f87d-29ff-44c5-af0b-1d1576d7df46[root@node1 ~]# pcs resource create VMFS Filesystem \ #VMFS资源的名称device=&quot;/dev/vmvg0/lvvm0&quot; \ #存储路径directory=&quot;/vm&quot; \ #挂载点 fstype=&quot;gfs2&quot; \ #文件格式类型clone #克隆Assumed agent name &#39;ocf:heartbeat:Filesystem&#39; (deduced from &#39;Filesystem&#39;)[root@node1 vm]# df -HTFilesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  2.0G     0  2.0G   0% /devtmpfs                   tmpfs     2.0G   63M  2.0G   4% /dev/shmtmpfs                   tmpfs     2.0G   13M  2.0G   1% /runtmpfs                   tmpfs     2.0G     0  2.0G   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        38G  4.3G   34G  12% //dev/sda1               xfs       1.1G  206M  858M  20% /boottmpfs                   tmpfs     396M     0  396M   0% /run/user/0/dev/mapper/vmvg0-lvvm0 gfs2       22G  137M   22G   1% /vm #已经挂载上了</code></pre><p><strong>测试</strong></p><p>node1:</p><pre><code>[root@node1 ~]# df -HT Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  2.0G     0  2.0G   0% /devtmpfs                   tmpfs     2.0G   63M  2.0G   4% /dev/shmtmpfs                   tmpfs     2.0G   13M  2.0G   1% /runtmpfs                   tmpfs     2.0G     0  2.0G   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        38G  4.3G   34G  12% //dev/sda1               xfs       1.1G  206M  858M  20% /boottmpfs                   tmpfs     396M     0  396M   0% /run/user/0/dev/mapper/vmvg0-lvvm0 gfs2       22G  137M   22G   1% /vm[root@node1 ~]# cd /vm[root@node1 vm]# touch test.txt[root@node1 vm]# echo test &gt; test.txt [root@node1 vm]# ls -ltotal 8-rw-r--r--. 1 root root 5 Apr 16 20:57 test.txt</code></pre><p>node2:</p><pre><code>[root@node2 ~]# cd /vm[root@node2 vm]# lstest.txt[root@node2 vm]# cat test.txt test[root@node2 vm]# cp test.txt test2.txt  #测试节点2是否可以读写[root@node2 vm]# cat test2.txt test</code></pre><p><strong>配置约束</strong></p><pre><code>[root@node1 ~]# pcs constraint order clvmd-clone then VMFS-cloneAdding clvmd-clone VMFS-clone (kind: Mandatory) (Options: first-action=start then-action=start)[root@node1 ~]# pcs constraint colocation add VMFS-clone with clvmd-clone[root@node1 ~]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sun Apr 16 20:55:31 2023Last change: Sun Apr 16 20:51:30 2023 by root via cibadmin on node1-heartbeat2 nodes configured7 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: scsi-shooter   (stonith:fence_scsi):   Started node1-heartbeat Clone Set: dlm-clone [dlm]     Started: [ node1-heartbeat node2-heartbeat ] Clone Set: clvmd-clone [clvmd]     Started: [ node1-heartbeat node2-heartbeat ] Clone Set: VMFS-clone [VMFS]     Started: [ node1-heartbeat node2-heartbeat ]Daemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled  [root@node2 vm]# pcs constraint #查看约束Location Constraints:Ordering Constraints:  start dlm-clone then start clvmd-clone (kind:Mandatory)  start clvmd-clone then start VMFS-clone (kind:Mandatory)Colocation Constraints:  clvmd-clone with dlm-clone (score:INFINITY)  VMFS-clone with clvmd-clone (score:INFINITY)Ticket Constraints:</code></pre><p><strong>配置Selinux</strong></p><p>node1:</p><pre><code>[root@node1 vm]# yum install policycoreutils-python[root@node1 vm]# semanage fcontext -a -t virt_image_t &quot;/vm(/.*)?&quot;[root@node1 vm]# restorecon -R -v /vmrestorecon reset /vm context system_u:object_r:unlabeled_t:s0-&gt;system_u:object_r:virt_image_t:s0restorecon reset /vm/test.txt context unconfined_u:object_r:unlabeled_t:s0-&gt;unconfined_u:object_r:virt_image_t:s0restorecon reset /vm/test2.txt context unconfined_u:object_r:unlabeled_t:s0-&gt;unconfined_u:object_r:virt_image_t:s0</code></pre><p>node2:</p><pre><code>[root@node1 vm]# yum install policycoreutils-python[root@node2 vm]# semanage fcontext -a -t virt_image_t &quot;/vm(/.*)?&quot;[root@node2 vm]# restorecon -R -v /vm #很重要！</code></pre><h1><span id="8-向集群中添加虚拟机资源">8、向集群中添加虚拟机资源</span></h1><h2><span id="1-将虚拟机数据迁移到iscsi的共享目录">1、将虚拟机数据迁移到iSCSI的共享目录</span></h2><pre><code>[root@node1 vmdata]# virsh list --all  #查看关机的虚拟机 Id    Name                           State---------------------------------------------------- -     Centos7.9                      shut off[root@node1 vmdata]# virsh domblklist --domain Centos7.9  #获取硬盘路径Target     Source------------------------------------------------vda        /vmdata/Centos7.9.qcow2hda        -[root@node1 vmdata]# df -HT #查看Iscsi的挂载点在/vmFilesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  2.0G     0  2.0G   0% /devtmpfs                   tmpfs     2.0G   63M  2.0G   4% /dev/shmtmpfs                   tmpfs     2.0G   13M  2.0G   1% /runtmpfs                   tmpfs     2.0G     0  2.0G   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        38G   13G   25G  35% //dev/sda1               xfs       1.1G  206M  858M  20% /boottmpfs                   tmpfs     396M     0  396M   0% /run/user/0/dev/mapper/vmvg0-lvvm0 gfs2       22G  137M   22G   1% /vm[root@node1 vmdata]# mv /vmdata/Centos7.9.qcow2 /vm #将数据盘迁移到/vm[root@node1 vmdata]# sar -n DEV 1 4 #期间可以多开窗口查看网络流量[root@node1 vm]# mkdir -p /vm/qemu_config[root@node1 vm]# virsh dumpxml --domain Centos7.9 &gt; /vm/qemu_config/Centos7.9.xml #将配置文件输出到iSCSI的挂载点[root@node1 vm]# virsh undefine --domain Centos7.9  #取消原主机定义Domain Centos7.9 has been undefined[root@node1 vm]# vim /vm/qemu_config/Centos7.9.xml #修改硬盘路径为iSCSI的挂载点      &lt;source file=&#39;/vm/Centos7.9.qcow2&#39;/&gt;[root@node1 vm]# firewall-cmd --add-port=16509/tcp --permanent #别忘记添加防火墙规则success[root@node1 vm]# firewall-cmd --add-port=49152-49251/tcp --permanent success[root@node1 vm]# firewall-cmd --reload success</code></pre><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8EiSCSI%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/102.jpg"></p><h2><span id="2-测试动态迁移">2、测试动态迁移</span></h2><p>node1:</p><pre><code>[root@node1 vm]# virsh define --file /vm/qemu_config/Centos7.9.xml Domain Centos7.9 defined from /vm/qemu_config/Centos7.9.xml[root@node1 vm]# virsh start --domain Centos7.9 Domain Centos7.9 started[root@node1 vm]# virsh migrate --domain Centos7.9 --desturi qemu+ssh://root@node2-storge/system --live  --persistent --verbose #热迁移到node2[100%][root@node1 vm]# virsh shutdown --domain Centos7.9 #关机[root@node1 ~]# virsh undefine --domain Centos7.9 #取消定义[root@node1 ~]# pcs resource create Centos7.9 VirtualDomain hypervisor=&quot;qemu:///system&quot; config=&quot;/vm/qemu_config/Centos7.9.xml&quot; migration_transport=ssh meta allow-migrate=&quot;true&quot; #在集群中定义主机[root@node1 ~]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Wed Apr 19 10:09:00 2023Last change: Wed Apr 19 10:08:55 2023 by root via crm_resource on node2-heartbeat2 nodes configured8 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: scsi-shooter   (stonith:fence_scsi):   Started node1-heartbeat Clone Set: dlm-clone [dlm]     Started: [ node1-heartbeat node2-heartbeat ] Clone Set: clvmd-clone [clvmd]     Started: [ node1-heartbeat node2-heartbeat ] Clone Set: VMFS-clone [VMFS]     Started: [ node1-heartbeat node2-heartbeat ] Centos7.9  (ocf::heartbeat:VirtualDomain): Started node2-heartbeatDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled[root@node1 ~]# pcs constraint order start VMFS-clone then Centos7.9 #配置顺序约束Adding VMFS-clone Centos7.9 (kind: Mandatory) (Options: first-action=start then-action=start)[root@node1 ~]# pcs constraint Location Constraints:Ordering Constraints:  start dlm-clone then start clvmd-clone (kind:Mandatory)  start clvmd-clone then start VMFS-clone (kind:Mandatory)  start VMFS-clone then start Centos7.9 (kind:Mandatory)Colocation Constraints:  clvmd-clone with dlm-clone (score:INFINITY)  VMFS-clone with clvmd-clone (score:INFINITY)Ticket Constraints:[root@node1 ~]# pcs resource move Centos7.9 node1-heartbeat #热迁移</code></pre><blockquote><p>经过多次测试，由于semanage fcontext -a -t virt_image_t “&#x2F;vm(&#x2F;.*)?”配置未生效，导致热迁移提示权限错误，原因就是因为selinux禁止了，关闭selinux后热迁移成功</p></blockquote><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KVM </tag>
            
            <tag> 基于iSCSI的KVM群集构建 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KVM虚拟化进阶之虚拟网络高级配置</title>
      <link href="/2023/04/12/KVM/1.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/"/>
      <url>/2023/04/12/KVM/1.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#1-%E9%85%8D%E7%BD%AE%E7%BD%91%E5%8D%A1%E7%BB%91%E5%AE%9Abond">1、配置网卡绑定（bond）</a><ul><li><a href="#1-%E7%BB%91%E5%AE%9A%E7%BD%91%E5%8D%A1">1、绑定网卡</a><ul><li><a href="#1-bond%E7%9A%84%E6%A8%A1%E5%BC%8F">1、Bond的模式</a></li></ul></li><li><a href="#2-%E9%85%8D%E7%BD%AE%E7%BD%91%E6%A1%A5">2、配置网桥</a></li></ul></li><li><a href="#2-%E9%85%8D%E7%BD%AEvlan">2、配置Vlan</a><ul><li><a href="#1-nmcli%E6%96%B9%E6%B3%95">1、nmcli方法</a></li><li><a href="#2-%E9%80%9A%E8%BF%87%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%88%9B%E5%BB%BA">2、通过命令行创建</a></li><li><a href="#3-%E9%80%9A%E8%BF%87virt-manager%E5%88%9B%E5%BB%BA">3、通过virt-manager创建</a></li><li><a href="#4-%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%BD%BF%E7%94%A8vlan">4、虚拟机使用vlan</a></li></ul></li><li><a href="#3-%E9%85%8D%E7%BD%AE%E7%BD%91%E7%BB%9C%E8%BF%87%E6%BB%A4">3、配置网络过滤</a></li></ul><!-- tocstop --><h1><span id="1-配置网卡绑定bond">1、配置网卡绑定（bond）</span></h1><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/1.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/2.jpg"></p><h2><span id="1-绑定网卡">1、绑定网卡</span></h2><pre><code>[root@kvm ~]# ip a  #查看需要绑定的网卡，ens33和ens361: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet 192.168.187.134/24 brd 192.168.187.255 scope global noprefixroute dynamic ens33       valid_lft 1066sec preferred_lft 1066sec    inet6 fe80::1fc1:66d:29af:eabe/64 scope link noprefixroute        valid_lft forever preferred_lft forever3: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0       valid_lft forever preferred_lft forever4: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff5: ens36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:e6 brd ff:ff:ff:ff:ff:ff    inet 192.168.187.135/24 brd 192.168.187.255 scope global noprefixroute dynamic ens36       valid_lft 1783sec preferred_lft 1783sec    inet6 fe80::84be:5ac8:542c:705f/64 scope link noprefixroute        valid_lft forever preferred_lft forever[root@kvm ~]# virsh net-list #查看当前kvm网络情况 名称               状态     自动开始  持久---------------------------------------------------------- default              活动     是           是[root@kvm ~]# brctl show #查看当前宿主机网桥情况bridge name    bridge id        STP enabled    interfacesvirbr0        8000.525400ccc5d5    yes        virbr0-nic[root@kvm ~]# virsh net-dumpxml --network default  #查看default的xml文件&lt;network&gt;  &lt;name&gt;default&lt;/name&gt;  &lt;uuid&gt;8e682a63-b77a-4aad-bc25-5d4d63a2a66e&lt;/uuid&gt;  &lt;forward mode=&#39;nat&#39;&gt;    &lt;nat&gt;      &lt;port start=&#39;1024&#39; end=&#39;65535&#39;/&gt;    &lt;/nat&gt;  &lt;/forward&gt;  &lt;bridge name=&#39;virbr0&#39; stp=&#39;on&#39; delay=&#39;0&#39;/&gt;  &lt;mac address=&#39;52:54:00:cc:c5:d5&#39;/&gt;  &lt;ip address=&#39;192.168.122.1&#39; netmask=&#39;255.255.255.0&#39;&gt;    &lt;dhcp&gt;      &lt;range start=&#39;192.168.122.2&#39; end=&#39;192.168.122.254&#39;/&gt;    &lt;/dhcp&gt;  &lt;/ip&gt;&lt;/network&gt;[root@kvm ~]# virsh iface-list #查看宿主机网卡情况，没有认到新增加的网卡（ens36），需要增加ifcfg-ens36的配置文件，可以通过virt-manager进行添加 名称               状态     MAC 地址--------------------------------------------------- ens33                活动     00:0c:29:0b:57:dc lo                   活动     00:00:00:00:00:00</code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/3.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/4.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/5.jpg"></p><pre><code>[root@kvm ~]# cat /etc/sysconfig/network-scripts/ifcfg-ens36  #查看通过virt-nanager自动生成的配置文件DEVICE=&quot;ens36&quot;HWADDR=&quot;00:0c:29:0b:57:e6&quot;ONBOOT=&quot;no&quot;BOOTPROTO=&quot;dhcp&quot;[root@kvm ~]# virsh iface-list --all #可以识别到ens36网卡了 名称               状态     MAC 地址--------------------------------------------------- ens33                活动     00:0c:29:0b:57:dc ens36                活动     00:0c:29:0b:57:e6 lo                   活动     00:00:00:00:00:00 [root@kvm ~]# modprobe --first-time bonding #查看系统是否安装了网桥的模块，如果没有就自动加载，这里没有报错，意思是当前系统没有加载，但是已经通过--first-time进行加载了[root@kvm ~]# lsmod | grep bonding #查看已经正常加载bonding               157075  0 [root@kvm network-scripts]# pwd/etc/sysconfig/network-scripts[root@kvm network-scripts]# ls -lha ifcfg-ens* #查看当前网卡的配置文件-rw-r--r--. 1 root root 280 4月   2 15:41 ifcfg-ens33-rw-r--r--. 1 root root  71 4月   2 19:37 ifcfg-ens36[root@kvm network-scripts]# cp ifcfg-ens33 ifcfg-ens33-bak #备份文件[root@kvm network-scripts]# cp ifcfg-ens36 ifcfg-ens36-bak [root@kvm network-scripts]# vim ifcfg-ens33 #编辑网卡文件TYPE=EthernetBOOTPROTO=noneNAME=ens33DEVICE=ens33ONBOOT=yesMASTER=bond0SLAVE=yesNM_CONTROLLERD=no #非必须USERCTL=no #非必须[root@kvm network-scripts]# vim ifcfg-ens36 #编辑网卡文件TYPE=EthernetBOOTPROTO=noneNAME=ens36DEVICE=ens36ONBOOT=yesMASTER=bond0SLAVE=yesNM_CONTROLLERD=noUSERCTL=no [root@kvm network-scripts]# vim ifcfg-bond0 #新建bond0文件DEVICE=bond0ONBOOT=yesNM_CONTROLLERD=noUSERCTL=noBONDING_OPTS=&quot;mode=1 miimon=100 fail_over_mac=1&quot;BOOTPROTO=staticIPADDR=192.168.187.134NETMASK=255.255.255.0GATEWAY=192.168.187.1DNS1=223.5.5.5[root@kvm network-scripts]# systemctl restart network #重启网络服务，远程时谨慎！连接已成功激活（master waiting for slaves）（D-Bus 活动路径：/org/freedesktop/NetworkManager/ActiveConnection/5）[root@kvm network-scripts]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff3: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0       valid_lft forever preferred_lft forever4: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff5: ens36: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond0 state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff6: bond0: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet 192.168.187.134/24 brd 192.168.187.255 scope global noprefixroute bond0       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fe0b:57dc/64 scope link        valid_lft forever preferred_lft forever[root@kvm network-scripts]# dmesg |grep bond0 #查看日志[ 2586.969003] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2586.969152] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2586.971585] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2586.979903] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2587.011775] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2635.311460] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2635.457459] IPv6: ADDRCONF(NETDEV_UP): bond0: link is not ready[ 2635.590868] bond0: making interface ens33 the new active one[ 2635.596124] bond0: Enslaving ens33 as an active interface with an up link[ 2635.596466] IPv6: ADDRCONF(NETDEV_CHANGE): bond0: link becomes ready[ 2635.700504] bond0: Releasing backup interface ens33[ 2637.395032] bond0: Enslaving ens33 as a backup interface with a down link[ 2637.483876] bond0: link status definitely up for interface ens33, 1000 Mbps full duplex[ 2637.483882] bond0: making interface ens33 the new active one[ 2637.491125] bond0: first active interface up![ 2637.764573] bond0: Enslaving ens36 as a backup interface with a down link[ 2637.793398] bond0: link status definitely up for interface ens36, 1000 Mbps full duplex[root@kvm network-scripts]# cat /proc/net/bonding/bond0  #查看内存中的文件Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)Bonding Mode: fault-tolerance (active-backup) #bond模式Primary Slave: NoneCurrently Active Slave: ens33 #当前的主设备是ens33MII Status: up #主设备的当前状态MII Polling Interval (ms): 100Up Delay (ms): 0Down Delay (ms): 0Slave Interface: ens33MII Status: upSpeed: 1000 MbpsDuplex: fullLink Failure Count: 0Permanent HW addr: 00:0c:29:0b:57:dcSlave queue ID: 0Slave Interface: ens36MII Status: upSpeed: 1000 MbpsDuplex: fullLink Failure Count: 0Permanent HW addr: 00:0c:29:0b:57:e6Slave queue ID: 0</code></pre><blockquote><p>与真机环境有所不同，vmware虚拟机下给linux系统做bond0网卡配置，照这样做完后，测试发现down掉eth0后，bond0网卡ping不通，无法起到网卡备份效果。BONDING_OPTS&#x3D;“fail_over_mac&#x3D;1”配置解释: 默认fail_over_mac&#x3D;0，当发生错误时，只改slave的mac不改bond；fail_over_mac&#x3D;1时，只改blave。</p></blockquote><h3><span id="1-bond的模式">1、Bond的模式</span></h3><blockquote><p><a href="https://www.likecs.com/show-308299629.html">网卡的7种bond模式</a><br><a href="https://blog.csdn.net/weixin_45548465/article/details/122625777">Linux网卡bond的七种模式详解，⽹卡绑定(bond)怎么实现？有哪些绑定⽅式</a>：</p></blockquote><table><thead><tr><th>模式</th><th>名称</th><th>特点</th><th>负载均衡</th><th>交换机配置</th><th>条件</th></tr></thead><tbody><tr><td>0</td><td>(balance-rr) Round-robin policy（平衡抡循环策略）</td><td>输数据包顺序是依次传输（即：第1个包走eth0，下一个包就走eth1….一直循环下去，直到最后一个传 输完毕），此模式提供负载平衡和容错能力。</td><td>是</td><td>静态聚合</td><td></td></tr><tr><td>1</td><td>(active-backup) Active-backup policy（主-备份策略）</td><td>只有一个设备处于活动状态，当一个宕掉另一个马上由备份转换为主设备。mac地址是外部可见得，从外面看来，bond的MAC地址是唯一的，以避免switch(交换机)发生混乱。此模式只提供了容错能力；由此可见此算法的优点是可以提供高网络连接的可用性，但是它的资源利用率较低，只有一个接口处于工作状态，在有 N 个网络接口的情况下，资源利用率为1&#x2F;N。</td><td>否</td><td>否</td><td></td></tr><tr><td>2</td><td>(balance-xor) XOR policy（平衡策略）</td><td>基于指定的传输HASH策略传输数据包。缺省的策略是：(源MAC地址 XOR 目标MAC地址) % slave数量。</td><td>是</td><td>静态聚合</td><td></td></tr><tr><td>3</td><td>broadcast（广播策略</td><td>每个slave接口上传输每个数据包，此模式提供了容错能力。</td><td>否</td><td>静态聚合</td><td></td></tr><tr><td>4</td><td>(802.3ad) IEEE 802.3adDynamic link aggregation（IEEE 802.3ad 动态链接聚合）</td><td>创建一个聚合组，它们共享同样的速率和双工设定。根据802.3ad规范将多个slave工作在同一个激活的聚合体下。</td><td>是</td><td>支持IEEE 802.3ad动态聚合</td><td>条件1：ethtool支持获取每个slave的速率和双工设定。<br>条件2：switch(交换机)支持IEEE 802.3ad Dynamic link aggregation。<br>条件3：大多数switch(交换机)需要经过特定配置才能支持802.3ad模式。<br></td></tr><tr><td>5</td><td>(balance-tlb) Adaptive transmit load balancing（适配器传输负载均衡）</td><td>不需要任何特别的switch(交换机)支持的通道bonding。在每个slave上根据当前的负载（根据速度计算）分配外出流量。如果正在接受数据的slave出故障了，另一个slave接管失败的slave的MAC地址。</td><td>是</td><td>否</td><td></td></tr><tr><td>6</td><td>(balance-alb) Adaptive load balancing（适配器适应性负载均衡）</td><td>该模式包含了balance-tlb模式，同时加上针对IPV4流量的接收负载均衡(receive load balance,rlb)，而且不需要任何switch(交换机)的支持。</td><td>是</td><td>否</td><td></td></tr></tbody></table><h2><span id="2-配置网桥">2、配置网桥</span></h2><pre><code>[root@kvm ~]# vim /etc/sysconfig/network-scripts/ifcfg-virbr1 #新建网桥virbr1的配置文件DEVICE=virbr1ONBOOT=yesTYPE=BridgeNM_CONTROLLER=noUSERCTL=noBOOTPROTO=staticIPADDR=192.168.187.134NETMASK=255.255.255.0GATEWAY=192.168.187.1[root@kvm ~]# vim /etc/sysconfig/network-scripts/ifcfg-bond0  #修改bond0的配置文件DEVICE=bond0ONBOOT=yesNM_CONTROLLERD=noUSERCTL=noBONDING_OPTS=&quot;mode=1 miimon=100 fail_over_mac=1&quot;BOOTPROTO=noneBRIDGE=virbr1[root@kvm ~]# systemctl restart network #重启网络服务[root@kvm ~]# brctl show  #查看当前系统网桥bridge name    bridge id        STP enabled    interfacesvirbr0        8000.525400ccc5d5    yes        virbr0-nicvirbr1        8000.000c290b57dc    no        bond0[root@kvm ~]# ip addr show virbr1 #查看IP已经到网桥1中了18: virbr1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet 192.168.187.134/24 brd 192.168.187.255 scope global noprefixroute virbr1       valid_lft forever preferred_lft forever    inet6 fe80::5c15:77ff:fece:bc31/64 scope link        valid_lft forever preferred_lft forever[root@kvm qemu]# cat /etc/libvirt/qemu/generic.xml #查看连接的虚拟机接口配置    &lt;interface type=&#39;bridge&#39;&gt;      &lt;mac address=&#39;52:54:00:8e:67:0a&#39;/&gt;      &lt;source bridge=&#39;virbr1&#39;/&gt;      &lt;model type=&#39;rtl8139&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x03&#39; function=&#39;0x0&#39;/&gt;    &lt;/interface&gt;[root@kvm qemu]# brctl show #查看多了个net0bridge name    bridge id        STP enabled    interfacesvirbr0        8000.525400ccc5d5    yes        virbr0-nicvirbr1        8000.000c290b57dc    no        bond0                            vnet0[root@kvm qemu]# virsh domiflist --domain generic  #查看接口信息，模式是8139，可以修改成virto优化性能Interface  Type       Source     Model       MAC-------------------------------------------------------vnet0      bridge     virbr1     rtl8139     52:54:00:8e:67:0a</code></pre><h1><span id="2-配置vlan">2、配置Vlan</span></h1><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/6.jpg"></p><p><strong>不建议使用nmcli创建</strong></p><table><thead><tr><th>网络</th><th>地址段</th><th>Vlan-id</th></tr></thead><tbody><tr><td>管理网络</td><td>192.168.200.0&#x2F;24</td><td>&#x2F;</td></tr><tr><td>生产网络</td><td>172.16.11.0&#x2F;24 172.16.11.11</td><td>11</td></tr><tr><td>生产网络</td><td>172.16.12.0&#x2F;24 172.16.12.12</td><td>12</td></tr></tbody></table><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/7.jpg"></p><h2><span id="1-nmcli方法">1、nmcli方法</span></h2><pre><code>[root@kvm ~]# nmcli --help #查看帮助文档Usage: nmcli [OPTIONS] OBJECT &#123; COMMAND | help &#125;OPTIONS  -a, --ask                                ask for missing parameters  -c, --colors auto|yes|no                 whether to use colors in output  -e, --escape yes|no                      escape columns separators in values  -f, --fields &lt;field,...&gt;|all|common      specify fields to output  -g, --get-values &lt;field,...&gt;|all|common  shortcut for -m tabular -t -f  -h, --help                               print this help  -m, --mode tabular|multiline             output mode  -o, --overview                           overview mode  -p, --pretty                             pretty output  -s, --show-secrets                       allow displaying passwords  -t, --terse                              terse output  -v, --version                            show program version  -w, --wait &lt;seconds&gt;                     set timeout waiting for finishing operationsOBJECT  g[eneral]       NetworkManager&#39;s general status and operations  n[etworking]    overall networking control  r[adio]         NetworkManager radio switches  c[onnection]    NetworkManager&#39;s connections  d[evice]        devices managed by NetworkManager  a[gent]         NetworkManager secret agent or polkit agent  m[onitor]       monitor NetworkManager changes[root@kvm network-scripts]# nmcli connection show  #查看当前网络信息NAME          UUID                                  TYPE      DEVICE System ens33  c96bc909-188e-ec64-3a96-6a90982b08ad  ethernet  ens33  System ens36  418da202-9a8c-b73c-e8a1-397e00f3c6b2  ethernet  ens36  virbr0        41366568-f66f-4c8b-a2f6-cb4df1714e06  bridge    virbr0 有线连接 1    52522e57-4b72-3d75-a602-ce47588f745d  ethernet  --     [root@kvm network-scripts]# nmcli connection add type vlan ifname VLAN11 dev ens33 id 11 #基于ens33创建vlan11Connection &#39;vlan-VLAN11&#39; (193348c2-73b4-4962-981c-4fdd7a966926) successfully added.[root@kvm network-scripts]# nmcli connection add type vlan ifname VLAN12 dev ens33 id 12 #基于ens33创建vlan12    Connection &#39;vlan-VLAN12&#39; (60496196-b1ee-4ff6-93fc-e167f25e63c7) successfully added.[root@kvm network-scripts]# pwd /etc/sysconfig/network-scripts[root@kvm network-scripts]# ls -lha | grep vlan  #查看自动创建的文件-rw-r--r--. 1 root root  357 Apr  8 05:08 ifcfg-vlan-VLAN11-rw-r--r--. 1 root root  357 Apr  8 05:08 ifcfg-vlan-VLAN12[root@kvm network-scripts]# cat ifcfg-vlan-VLAN11 #查看具体信息VLAN=yesTYPE=VlanPHYSDEV=ens33VLAN_ID=11REORDER_HDR=yesGVRP=noMVRP=noHWADDR=PROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=dhcpDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=vlan-VLAN11UUID=193348c2-73b4-4962-981c-4fdd7a966926DEVICE=VLAN11ONBOOT=yes[root@kvm network-scripts]# nmcli connection show  #查看创建的链接NAME          UUID                                  TYPE      DEVICE System ens33  c96bc909-188e-ec64-3a96-6a90982b08ad  ethernet  ens33  System ens36  418da202-9a8c-b73c-e8a1-397e00f3c6b2  ethernet  ens36  virbr0        41366568-f66f-4c8b-a2f6-cb4df1714e06  bridge    virbr0 vlan-VLAN11   193348c2-73b4-4962-981c-4fdd7a966926  vlan      --     vlan-VLAN12   60496196-b1ee-4ff6-93fc-e167f25e63c7  vlan      --     有线连接 1    52522e57-4b72-3d75-a602-ce47588f745d  ethernet  --  [root@kvm network-scripts]# ip a #查看创建结果1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet 192.168.200.128/24 brd 192.168.200.255 scope global noprefixroute ens33       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fe0b:57dc/64 scope link        valid_lft forever preferred_lft forever3: ens36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:e6 brd ff:ff:ff:ff:ff:ff    inet 192.168.200.135/24 brd 192.168.200.255 scope global noprefixroute ens36       valid_lft forever preferred_lft forever6: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0       valid_lft forever preferred_lft forever7: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff13: vnet0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 1000    link/ether fe:54:00:8e:67:0a brd ff:ff:ff:ff:ff:ff    inet6 fe80::fc54:ff:fe8e:670a/64 scope link        valid_lft forever preferred_lft forever25: VLAN12@ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet6 fe80::eb51:86e8:ea1f:f322/64 scope link noprefixroute        valid_lft forever preferred_lft forever26: VLAN11@ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet6 fe80::a47d:f2c7:75fc:7e54/64 scope link tentative noprefixroute        valid_lft forever preferred_lft forever[root@kvm network-scripts]# tail -n 20 /var/log/messages #查看日志Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5135] dhcp4 (VLAN12): canceled DHCP transaction, DHCP client pid 15817Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5136] dhcp4 (VLAN12): state changed timeout -&gt; doneApr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5141] device (VLAN12): state change: ip-config -&gt; failed (reason &#39;ip-config-unavailable&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;warn&gt;  [1680902267.5150] device (VLAN12): Activation: failed for connection &#39;vlan-VLAN12&#39;Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5152] device (VLAN12): state change: failed -&gt; disconnected (reason &#39;none&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5602] device (VLAN12): state change: disconnected -&gt; unmanaged (reason &#39;user-requested&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5609] policy: auto-activating connection &#39;vlan-VLAN12&#39; (60496196-b1ee-4ff6-93fc-e167f25e63c7)Apr  8 05:17:47 kvm kernel: IPv6: ADDRCONF(NETDEV_UP): VLAN12: link is not readyApr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5637] device (VLAN12): state change: unmanaged -&gt; unavailable (reason &#39;managed&#39;, sys-iface-state: &#39;external&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5862] device (VLAN12): carrier: link connectedApr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.5881] device (VLAN12): state change: unavailable -&gt; disconnected (reason &#39;user-requested&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6013] device (VLAN12): Activation: starting connection &#39;vlan-VLAN12&#39; (60496196-b1ee-4ff6-93fc-e167f25e63c7)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6023] device (VLAN12): state change: disconnected -&gt; prepare (reason &#39;none&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6033] device (VLAN12): state change: prepare -&gt; config (reason &#39;none&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6366] device (VLAN12): state change: config -&gt; ip-config (reason &#39;none&#39;, sys-iface-state: &#39;managed&#39;)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6373] dhcp4 (VLAN12): activation: beginning transaction (timeout in 45 seconds)Apr  8 05:17:47 kvm NetworkManager[786]: &lt;info&gt;  [1680902267.6465] dhcp4 (VLAN12): dhclient started with pid 15847Apr  8 05:17:47 kvm dhclient[15847]: DHCPDISCOVER on VLAN12 to 255.255.255.255 port 67 interval 7 (xid=0x1a408bc7)Apr  8 05:17:52 kvm dhclient[15834]: DHCPDISCOVER on VLAN11 to 255.255.255.255 port 67 interval 6 (xid=0x6c741821)Apr  8 05:17:54 kvm dhclient[15847]: DHCPDISCOVER on VLAN12 to 255.255.255.255 port 67 interval 20 (xid=0x1a408bc7)[root@kvm network-scripts]# virsh iface-list #在kvm上看 Name                 State      MAC Address--------------------------------------------------- ens33                active     00:0c:29:0b:57:dc ens36                active     00:0c:29:0b:57:e6 lo                   active     00:00:00:00:00:00 VLAN11               active     00:0c:29:0b:57:dc VLAN12               active     00:0c:29:0b:57:dc[root@kvm network-scripts]# rm -rf ifcfg-vlan-VLAN1* #恢复设置[root@kvm network-scripts]# systemctl restart network #重启[root@kvm network-scripts]# nmtui #图形化界面，不好用</code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/8.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/9.jpg"></p><h2><span id="2-通过命令行创建">2、通过命令行创建</span></h2><pre><code>[root@kvm network-scripts]# vim ifcfg-ens36.11 #创建vlan11配置文件DEVICE=&quot;ens36.11&quot;VLAN=&quot;yes&quot;ONBOOT=&quot;yes&quot;BOOTPROTO=&quot;none&quot;IPADDR=&quot;172.16.11.11&quot;NETMASK=&quot;255.255.255.0&quot;[root@kvm network-scripts]# cp ifcfg-ens36.11 ifcfg-ens36.12 #创建vlan11配置文件并修改[root@kvm network-scripts]# vim ifcfg-ens36.12 #配置DEVICE=&quot;ens36.12&quot;VLAN=&quot;yes&quot;ONBOOT=&quot;yes&quot;BOOTPROTO=&quot;none&quot;IPADDR=&quot;172.16.12.12&quot;NETMASK=&quot;255.255.255.0&quot;[root@kvm network-scripts]# systemctl restart network #重启网络配置[root@kvm network-scripts]# nmcli connection show  #查看配置是否生效NAME           UUID                                  TYPE      DEVICE   System ens33   c96bc909-188e-ec64-3a96-6a90982b08ad  ethernet  ens33    Vlan ens36.11  5aaffcd1-84a5-3046-15e7-adb62160402b  vlan      ens36.11 Vlan ens36.12  d100f2b5-85f7-4961-2105-8874b119178a  vlan      ens36.12 System ens36   418da202-9a8c-b73c-e8a1-397e00f3c6b2  ethernet  ens36    virbr0         41366568-f66f-4c8b-a2f6-cb4df1714e06  bridge    virbr0   有线连接 1     52522e57-4b72-3d75-a602-ce47588f745d  ethernet  --      [root@kvm network-scripts]# ip addr #查看IP1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:dc brd ff:ff:ff:ff:ff:ff    inet 192.168.200.128/24 brd 192.168.200.255 scope global noprefixroute ens33       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fe0b:57dc/64 scope link        valid_lft forever preferred_lft forever3: ens36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 00:0c:29:0b:57:e6 brd ff:ff:ff:ff:ff:ff    inet 192.168.200.135/24 brd 192.168.200.255 scope global noprefixroute ens36       valid_lft forever preferred_lft forever6: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0       valid_lft forever preferred_lft forever7: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000    link/ether 52:54:00:cc:c5:d5 brd ff:ff:ff:ff:ff:ff13: vnet0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 1000    link/ether fe:54:00:8e:67:0a brd ff:ff:ff:ff:ff:ff    inet6 fe80::fc54:ff:fe8e:670a/64 scope link        valid_lft forever preferred_lft forever32: ens36.12@ens36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether 00:0c:29:0b:57:e6 brd ff:ff:ff:ff:ff:ff    inet 172.16.12.12/24 brd 172.16.12.255 scope global noprefixroute ens36.12       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fe0b:57e6/64 scope link        valid_lft forever preferred_lft forever33: ens36.11@ens36: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000    link/ether 00:0c:29:0b:57:e6 brd ff:ff:ff:ff:ff:ff    inet 172.16.11.11/24 brd 172.16.11.255 scope global noprefixroute ens36.11       valid_lft forever preferred_lft forever    inet6 fe80::20c:29ff:fe0b:57e6/64 scope link        valid_lft forever preferred_lft forever[root@kvm network-scripts]# virsh iface-list --all  #查看 Name                 State      MAC Address--------------------------------------------------- ens33                active     00:0c:29:0b:57:dc ens36                active     00:0c:29:0b:57:e6 ens36.11             active     00:0c:29:0b:57:e6 ens36.12             active     00:0c:29:0b:57:e6 lo                   active     00:00:00:00:00:00</code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/10.jpg"></p><h2><span id="3-通过virt-manager创建">3、通过virt-manager创建</span></h2><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/11.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/12.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/13.jpg"></p><pre><code>[root@kvm network-scripts]# cat ifcfg-ens36.11 DEVICE=&quot;ens36.11&quot;VLAN=&quot;yes&quot;ONBOOT=&quot;yes&quot;BOOTPROTO=&quot;none&quot;IPADDR=&quot;192.168.200.135&quot;NETMASK=&quot;255.255.255.0&quot;GATEWAY=&quot;192.168.200.2&quot;</code></pre><h2><span id="4-虚拟机使用vlan">4、虚拟机使用vlan</span></h2><p><strong>一个vlan使用一个网桥，调用即可</strong><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/14.jpg"></p><h1><span id="3-配置网络过滤">3、配置网络过滤</span></h1><p><strong>通过libvirtd使用ebtailes创建的规则</strong><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/15.jpg"></p><pre><code>[root@kvm network-scripts]# virsh --help  Network Filter (help keyword &#39;filter&#39;)    nwfilter-define                define or update a network filter from an XML file    nwfilter-dumpxml               network filter information in XML    nwfilter-edit                  edit XML configuration for a network filter    nwfilter-list                  list network filters    nwfilter-undefine              undefine a network filter        nwfilter-binding-create        create a network filter binding from an XML file    nwfilter-binding-delete        delete a network filter binding    nwfilter-binding-dumpxml       network filter information in XML    nwfilter-binding-list          list network filter bindings[root@kvm network-scripts]#  virsh nwfilter-list  #查看网络过滤列表 UUID                                  Name                 ------------------------------------------------------------------ 3ce9cf03-2589-4b6a-8c1b-6e8b24e38f8e  allow-arp            a803defa-b342-47b4-af29-613250ff5281  allow-dhcp           5deae120-0ca4-4451-b8d4-92b6cd165962  allow-dhcp-server    49bdc080-9c7a-4278-9d83-59371bc9b981  allow-incoming-ipv4  64495a87-4efc-44f3-9aa2-1ef8e0a08600  allow-ipv4           cbd5d978-e475-4859-863d-1395d03da480  clean-traffic        #干净的流量 14ed106c-6baa-4ffd-8f28-d5038fea01ef  clean-traffic-gateway f88bd332-81af-480f-af36-57a7cd35104e  no-arp-ip-spoofing   a7611c9f-2a3a-4f62-9039-4e5be77881b3  no-arp-mac-spoofing  92dfc42c-0ed9-4482-a8f4-eee46f168cb8  no-arp-spoofing      20951a13-4570-4a87-9aa5-587d128c899e  no-ip-multicast      be8c0f6f-c811-451d-ae18-7cef13c56380  no-ip-spoofing       d85ecc2e-5573-43a7-85c1-e88b4daa6fdd  no-mac-broadcast     108c0c1a-8471-48b5-998a-6d92226f084e  no-mac-spoofing      7101e610-2e76-495f-976a-e4a37dd3f6ef  no-other-l2-traffic  60911d33-fb36-46bf-8eec-eeacc2f0fe62  no-other-rarp-traffic 40be9024-b924-4df0-b4c4-839e1026873f  qemu-announce-self   8bea3031-76ab-4077-9712-04d5b850e5fa  qemu-announce-self-rarp[root@kvm network-scripts]# virsh nwfilter-dumpxml clean-traffic #查看详细信息&lt;filter name=&#39;clean-traffic&#39; chain=&#39;root&#39;&gt; #默认规则及规则名称  &lt;uuid&gt;cbd5d978-e475-4859-863d-1395d03da480&lt;/uuid&gt; #规则UUID  &lt;filterref filter=&#39;no-mac-spoofing&#39;/&gt; #禁止MAC地址七篇  &lt;filterref filter=&#39;no-ip-spoofing&#39;/&gt; #禁止IP地址欺骗  &lt;rule action=&#39;accept&#39; direction=&#39;out&#39; priority=&#39;-650&#39;&gt; #accept代表动作，direction代表方向，in，out，inout，prioiy优先级，优先级值越低，优先级越高    &lt;mac protocolid=&#39;ipv4&#39;/&gt; #ipv4的mac地址  &lt;/rule&gt;  &lt;filterref filter=&#39;allow-incoming-ipv4&#39;/&gt; #调用其他规则  &lt;filterref filter=&#39;no-arp-spoofing&#39;/&gt;  &lt;rule action=&#39;accept&#39; direction=&#39;inout&#39; priority=&#39;-500&#39;&gt;    &lt;mac protocolid=&#39;arp&#39;/&gt;  &lt;/rule&gt;  &lt;filterref filter=&#39;no-other-l2-traffic&#39;/&gt; #调用其他规则  &lt;filterref filter=&#39;qemu-announce-self&#39;/&gt;&lt;/filter&gt;[root@kvm network-scripts]# ebtables -t nat -L #查看ebtables的NAT表规则Bridge table: natBridge chain: PREROUTING, entries: 1, policy: ACCEPT-j PREROUTING_directBridge chain: OUTPUT, entries: 1, policy: ACCEPT-j OUTPUT_directBridge chain: POSTROUTING, entries: 1, policy: ACCEPT-j POSTROUTING_directBridge chain: PREROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: POSTROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: OUTPUT_direct, entries: 1, policy: ACCEPT-j RETURN [root@kvm network-scripts]# virsh list --all  #查看虚拟机，准备于过滤规则绑定 Id    Name                           State---------------------------------------------------- 5     generic                        running -     centos7.0                      shut off[root@kvm network-scripts]# virsh dumpxml --domain generic    &lt;interface type=&#39;bridge&#39;&gt;      &lt;mac address=&#39;52:54:00:8e:67:0a&#39;/&gt;      &lt;source bridge=&#39;virbr1&#39;/&gt;      &lt;target dev=&#39;vnet0&#39;/&gt;      &lt;model type=&#39;rtl8139&#39;/&gt;      &lt;alias name=&#39;net0&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x03&#39; function=&#39;0x0&#39;/&gt;    &lt;/interface&gt;[root@kvm ~]# vim nwtest.xml    &lt;interface type=&#39;bridge&#39;&gt;      &lt;mac address=&#39;52:54:00:8e:67:0a&#39;/&gt;      &lt;source bridge=&#39;virbr1&#39;/&gt;      &lt;target dev=&#39;vnet0&#39;/&gt;      &lt;model type=&#39;rtl8139&#39;/&gt;      &lt;alias name=&#39;net0&#39;/&gt;      &lt;address type=&#39;pci&#39; domain=&#39;0x0000&#39; bus=&#39;0x00&#39; slot=&#39;0x03&#39; function=&#39;0x0&#39;/&gt;      &lt;filterref filter=&#39;clean-traffic&#39;/&gt; #增加过滤规则，调用clean-traffic    &lt;/interface&gt;[root@kvm ~]# virsh update-device --domain generic --file /root/nwtest.xml --persistent --live #更新generic主机，--file选择创建的文件，--persistent 代表永久生效，--live代表当前主机正在运行Device updated successfully[root@kvm ~]#  ebtables -t nat -L  #查看规则生效情况Bridge table: natBridge chain: PREROUTING, entries: 2, policy: ACCEPT-j PREROUTING_direct-i vnet0 -j libvirt-I-vnet0Bridge chain: OUTPUT, entries: 1, policy: ACCEPT-j OUTPUT_directBridge chain: POSTROUTING, entries: 1, policy: ACCEPT-j POSTROUTING_directBridge chain: PREROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: POSTROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: OUTPUT_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: libvirt-I-vnet0, entries: 4, policy: ACCEPT-s ! 52:54:0:8e:67:a -j DROP -p IPv4 -j ACCEPT -p ARP -j ACCEPT -j DROP [root@kvm ~]# virsh shutdown --domain generic  #关闭虚拟机Domain generic is being shutdown[root@kvm ~]# virsh list --all  #查看主机是否正常关闭 Id    Name                           State---------------------------------------------------- -     centos7.0                      shut off -     generic                        shut off[root@kvm ~]#  ebtables -t nat -L  #查看规则清空Bridge table: natBridge chain: PREROUTING, entries: 1, policy: ACCEPT-j PREROUTING_directBridge chain: OUTPUT, entries: 1, policy: ACCEPT-j OUTPUT_directBridge chain: POSTROUTING, entries: 1, policy: ACCEPT-j POSTROUTING_directBridge chain: PREROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: POSTROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: OUTPUT_direct, entries: 1, policy: ACCEPT-j RETURN [root@kvm ~]# virsh start --domain generic  #重新启动主机Domain generic started[root@kvm ~]#  ebtables -t nat -L  #再次查看规则Bridge table: natBridge chain: PREROUTING, entries: 2, policy: ACCEPT-j PREROUTING_direct-i vnet0 -j libvirt-I-vnet0Bridge chain: OUTPUT, entries: 1, policy: ACCEPT-j OUTPUT_directBridge chain: POSTROUTING, entries: 1, policy: ACCEPT-j POSTROUTING_directBridge chain: PREROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: POSTROUTING_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: OUTPUT_direct, entries: 1, policy: ACCEPT-j RETURN Bridge chain: libvirt-I-vnet0, entries: 4, policy: ACCEPT-s ! 52:54:0:8e:67:a -j DROP -p IPv4 -j ACCEPT -p ARP -j ACCEPT -j DROP </code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/16.jpg"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KVM </tag>
            
            <tag> KVM虚拟网络高级配置 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KVM虚拟化进阶之虚拟机迁移</title>
      <link href="/2023/04/12/KVM/2.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/"/>
      <url>/2023/04/12/KVM/2.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#1-%E9%80%9A%E8%BF%87uri%E8%BF%9E%E6%8E%A5%E8%BF%9E%E6%8E%A5kvm%E4%B8%BB%E6%9C%BA">1、通过URI连接连接KVM主机</a></li><li><a href="#2-%E9%9D%99%E6%80%81%E8%BF%81%E7%A7%BB">2、静态迁移</a><ul><li><a href="#1-%E5%90%8C%E4%B8%80%E5%AE%BF%E4%B8%BB%E6%9C%BA%E5%86%85%E8%BF%81%E7%A7%BB">1、同一宿主机内迁移</a></li><li><a href="#2-%E4%B8%8D%E5%90%8C%E5%AE%BF%E4%B8%BB%E6%9C%BA%E8%BF%81%E7%A7%BB">2、不同宿主机迁移</a><ul><li><a href="#kvm">kvm：</a></li><li><a href="#kvm2">kvm2：</a></li></ul></li><li><a href="#3-%E4%B8%8D%E5%90%8C%E5%AE%BF%E4%B8%BB%E6%9C%BA%E8%BF%81%E7%A7%BB%E4%BD%BF%E7%94%A8virsh-migrate%E5%91%BD%E4%BB%A4">3、不同宿主机迁移使用virsh migrate命令</a></li></ul></li><li><a href="#3-%E5%9F%BA%E4%BA%8E%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8%E7%9A%84%E5%8A%A8%E6%80%81%E8%BF%81%E7%A7%BB">3、基于共享存储的动态迁移</a><ul><li><a href="#1-%E5%AE%89%E8%A3%85nfs%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BB%84%E4%BB%B6">1、安装NFS服务器组件</a></li><li><a href="#2-%E5%88%9B%E5%BB%BAnfs%E5%85%B1%E4%BA%AB%E6%96%87%E4%BB%B6%E5%A4%B9">2、创建NFS共享文件夹</a></li><li><a href="#3-%E5%B0%86kvm1%E4%B8%AD%E7%9A%84%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%A1%AC%E7%9B%98%E8%BF%81%E7%A7%BB%E5%88%B0nfs%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8%E4%B8%8A">3、将KVM1中的虚拟机硬盘迁移到NFS服务器的共享存储上</a></li><li><a href="#4-%E4%BD%BF%E7%94%A8virt-manager%E8%BF%9B%E8%A1%8C%E8%BF%81%E7%A7%BB">4、使用virt-manager进行迁移</a></li><li><a href="#5-%E4%BD%BF%E7%94%A8virsh%E5%91%BD%E4%BB%A4%E8%BF%9B%E8%A1%8C%E8%BF%81%E7%A7%BB">5、使用virsh命令进行迁移</a><ul><li><a href="#1-%E6%9F%A5%E7%9C%8B%E5%B8%AE%E5%8A%A9">1、查看帮助</a></li><li><a href="#2-%E8%BF%81%E7%A7%BB">2、迁移</a></li></ul></li></ul></li><li><a href="#4-%E5%9F%BA%E4%BA%8E%E6%9C%AC%E5%9C%B0%E5%AD%98%E5%82%A8%E7%9A%84%E5%8A%A8%E6%80%81%E8%BF%81%E7%A7%BB">4、基于本地存储的动态迁移</a></li></ul><!-- tocstop --><p><strong>静态迁移中可以通过将虚拟机暂停，迁移完成后重置虚拟机状态，完成迁移，但是也会影响业务</strong></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/17.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/18.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/19.jpg"></p><h1><span id="1-通过uri连接连接kvm主机">1、通过URI连接连接KVM主机</span></h1><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/20.jpg"></p><pre><code>[root@kvm ~]# virsh --helpvirsh [options]... [&lt;command_string&gt;]virsh [options]... &lt;command&gt; [args...]  options:    -c | --connect=URI      hypervisor connection URI[root@kvm ~]# virsh -c qemu+ssh://root@122.200.93.9/system #远程连接其他KVM主机root@122.200.93.9&#39;s password: Welcome to virsh, the virtualization interactive terminal.Type:  &#39;help&#39; for help with commands       &#39;quit&#39; to quitvirsh # list --all Id    Name                           State---------------------------------------------------- -     centos7.0                      shut off -     centos7.0-2                    shut off -     centos7.0-3                    shut off -     generic          virsh # hostname kvm.novalocal</code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/21.jpg"></p><h1><span id="2-静态迁移">2、静态迁移</span></h1><h2><span id="1-同一宿主机内迁移">1、同一宿主机内迁移</span></h2><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/22.jpg"></p><pre><code>[root@kvm ~]# virsh list --all  #查看虚拟机状态 Id    Name                           State---------------------------------------------------- 7     generic                        running -     centos7.0                      shut off[root@kvm ~]# virsh domblklist --domain generic  #查看硬盘路径Target     Source------------------------------------------------hda        /vm/centos7.6.rawhdb        -[root@kvm ~]# virsh shutdown --domain generic #关闭虚拟机Domain generic is being shutdown[root@kvm ~]# virsh list --all #再次查看状态 Id    Name                           State---------------------------------------------------- -     centos7.0                      shut off -     generic                        shut off[root@kvm ~]# mkdir /vm1 #创建迁移后的文件夹[root@kvm ~]# mv /vm/centos7.6.raw /vm1/ #迁移硬盘[root@kvm ~]# ls -lha /vm1 #查看迁移结果total 21Gdrwxr-xr-x.  2 root root  27 Apr  8 06:41 .dr-xr-xr-x. 22 root root 279 Apr  8 06:41 ..-rw-------.  1 root root 20G Apr  8 06:40 centos7.6.raw[root@kvm ~]# virsh start --domain generic  #尝试启动虚拟机，报错，需要修改配置文件error: Failed to start domain genericerror: Cannot access storage file &#39;/vm/centos7.6.raw&#39;: 没有那个文件或目录[root@kvm ~]# virsh edit --domain generic    &lt;disk type=&#39;file&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;source file=&#39;/vm1/centos7.6.raw&#39;/&gt; #修改源文件路径      &lt;target dev=&#39;hda&#39; bus=&#39;ide&#39;/&gt;      &lt;address type=&#39;drive&#39; controller=&#39;0&#39; bus=&#39;0&#39; target=&#39;0&#39; unit=&#39;0&#39;/&gt;    &lt;/disk&gt;[root@kvm ~]# virsh start --domain generic #启动成功，迁移完毕Domain generic started</code></pre><h2><span id="2-不同宿主机迁移">2、不同宿主机迁移</span></h2><pre><code>![](images/KVM/虚拟机迁移/23.jpg)![](images/KVM/虚拟机迁移/24.jpg)| 序号 | 主机名称 | 主机IP || --- | --- | --- || 1 | kvm | 192.168.200.128 |</code></pre><p>| 2 | kvm2 | 192.168.200.129 |</p><h3><span id="kvm">kvm：</span></h3><p><strong>静态迁移要确保虚拟机处于关闭状态</strong></p><blockquote><p>迁移可以使用scp或者rsync -avSHP进行数据传输</p></blockquote><pre><code>[root@kvm ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.128 kvm192.168.200.129 kvm2[root@kvm ~]# virsh list --all  Id    Name                           State---------------------------------------------------- -     centos7.0                      shut off -     generic                        shut off #迁移此台主机 [root@kvm ~]# scp /etc/libvirt/qemu/generic.xml root@192.168.200.129:/etc/libvirt/qemu/ #将配置文件传送过去，确保主机当前状态是关机The authenticity of host &#39;192.168.200.129 (192.168.200.129)&#39; can&#39;t be established.ECDSA key fingerprint is SHA256:s2qiffBOYjuMJQOHEI9r87NTNjd0XyoEf81myM78kbQ.ECDSA key fingerprint is MD5:a5:8f:de:d4:07:9f:b7:b3:6f:7c:b0:c3:55:eb:bb:b0.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &#39;192.168.200.129&#39; (ECDSA) to the list of known hosts.root@192.168.200.129&#39;s password: generic.xml                                                                                                                                                                                                                                                                               100% 4329     5.0MB/s   00:00    [root@kvm ~]# virsh domblklist --domain generic #查看硬盘文件Target     Source------------------------------------------------hda        /vm1/centos7.6.rawhdb        -[root@kvm ~]# scp /vm1/centos7.6.raw root@kvm2:/vm1/ #传输硬盘文件到目标主机上，路径最好保持一致，否则需要修改XML文件</code></pre><h3><span id="kvm2">kvm2：</span></h3><pre><code>[root@kvm2 ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.128 kvm192.168.200.129 kvm2[root@kvm2 ~]# virsh list --all  Id    Name                           State----------------------------------------------------[root@kvm2 ~]# mkdir /vm1 #需要提前创建出目录，否则上述传输的时候会报错[root@kvm2 vm1]# ls -lha /etc/libvirt/qemu/generic.xml  #查看传输过来的xml配置文件-rw-------. 1 root root 4.3K Apr 10 16:02 /etc/libvirt/qemu/generic.xml[root@kvm2 vm1]# ls /vm1/centos7.6.raw #查看硬盘文件 /vm1/centos7.6.raw[root@kvm2 vm1]# virsh define --file /etc/libvirt/qemu/generic.xml  #定义硬盘文件Domain generic defined from /etc/libvirt/qemu/generic.xml[root@kvm2 vm1]# virsh list --all #查看识别到此主机，准备启动 Id    Name                           State---------------------------------------------------- -     generic                        shut off[root@kvm2 vm1]# virsh start generic #启动成功Domain generic started</code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/25.jpg"></p><p><strong>尝试本地远程成功，迁移完毕</strong></p><pre><code>[root@kvm2 vm1]# ssh root@192.168.122.149root@192.168.122.149&#39;s password: Last login: Mon Apr 10 04:35:12 2023[root@localhost ~]# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host        valid_lft forever preferred_lft forever2: ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000    link/ether 52:54:00:8e:67:0a brd ff:ff:ff:ff:ff:ff    inet 192.168.122.149/24 brd 192.168.122.255 scope global noprefixroute dynamic ens3       valid_lft 3503sec preferred_lft 3503sec    inet6 fe80::aaa:8f11:d0df:a3a7/64 scope link noprefixroute        valid_lft forever preferred_lft forever</code></pre><blockquote><p><strong>确保正常启动且无问题后删除源宿主机的相关配置，防止IP、Mac等冲突，引起不必要的问题</strong></p></blockquote><h2><span id="3-不同宿主机迁移使用virsh-migrate命令">3、不同宿主机迁移使用virsh migrate命令</span></h2><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/26.jpg"></p><p><strong>如果做了上述动作，需要先还原环境，本方法只作为了解，实际和scp效果一样</strong></p><p>1、在KVM2主机中查看主机列表</p><pre><code>[root@kvm2 vm1]# virsh list --all  Id    Name                           State---------------------------------------------------- 1     generic                        running</code></pre><p>2、在KVM2主机中关闭启动的generic主机</p><pre><code>[root@kvm2 vm1]# virsh shutdown --domain generic Domain generic is being shutdown</code></pre><p>3、在KVM2主机中取消generic的xml文件定义</p><pre><code>[root@kvm2 vm1]# virsh undefine --domain genericDomain generic has been undefined</code></pre><p>4、在KVM2主机中删除generic的硬盘文件</p><pre><code>[root@kvm2 vm1]# cd /vm1[root@kvm2 vm1]# ls -lha total 20Gdrwxr-xr-x.  2 root root  27 Apr 10 16:03 .dr-xr-xr-x. 22 root root 279 Apr  8 06:41 ..-rw-------.  1 root root 20G Apr 10 16:46 centos7.6.raw[root@kvm2 vm1]# rm -rf centos7.6.raw </code></pre><p>5、在KVM1主机中将generic主机关机<br><code>[root@kvm ~]# virsh shutdown --domain generic</code></p><p>6、在KVM1主机中查看migrate帮助</p><pre><code>[root@kvm ~]# virsh migrate --help   NAME    migrate - 将域迁移到另一个主机中  SYNOPSIS    migrate &lt;domain&gt; &lt;desturi&gt; [--live] [--offline] [--p2p] [--direct] [--tunnelled] [--persistent] [--undefinesource] [--suspend] [--copy-storage-all] [--copy-storage-inc] [--change-protection] [--unsafe] [--verbose] [--compressed] [--auto-converge] [--rdma-pin-all] [--abort-on-error] [--postcopy] [--postcopy-after-precopy] [--migrateuri &lt;string&gt;] [--graphicsuri &lt;string&gt;] [--listen-address &lt;string&gt;] [--dname &lt;string&gt;] [--timeout &lt;number&gt;] [--timeout-suspend] [--timeout-postcopy] [--xml &lt;string&gt;] [--migrate-disks &lt;string&gt;] [--disks-port &lt;number&gt;] [--comp-methods &lt;string&gt;] [--comp-mt-level &lt;number&gt;] [--comp-mt-threads &lt;number&gt;] [--comp-mt-dthreads &lt;number&gt;] [--comp-xbzrle-cache &lt;number&gt;] [--auto-converge-initial &lt;number&gt;] [--auto-converge-increment &lt;number&gt;] [--persistent-xml &lt;string&gt;] [--tls]  DESCRIPTION    将域迁移到另一个主机中。热迁移时添加 --live。  OPTIONS    [--domain] &lt;string&gt;  domain name, id or uuid    [--desturi] &lt;string&gt;  客户端（常规迁移）或者源（p2p 迁移）中看到到目的地主机连接 URI    --live           热迁移    --offline        离线迁移    --p2p            点对点迁移    --direct         直接迁移    --tunnelled      管道迁移    --persistent     目的地中的持久 VM    --undefinesource  在源中取消定义 VM    --suspend        部启用目的地主机中的域    --copy-storage-all  使用全磁盘复制的非共享存储进行迁移    --copy-storage-inc  使用增值复制（源和目的地共享同一基础映像）的非共享存储进行迁移    --change-protection  迁移结束前不得对域进行任何配置更改    --unsafe         即使不安全也要强制迁移    --verbose        显示迁移进程    --compressed     实时迁移过程中压缩重复的页    --auto-converge  force convergence during live migration    --rdma-pin-all   pin all memory before starting RDMA live migration    --abort-on-error  在迁移过程中忽略软错误    --postcopy       enable post-copy migration; switch to it using migrate-postcopy command    --postcopy-after-precopy  automatically switch to post-copy migration after one pass of pre-copy    --migrateuri &lt;string&gt;  迁移 URI， 通常可省略    --graphicsuri &lt;string&gt;  无空隙图形迁移中使用的图形 URI    --listen-address &lt;string&gt;  listen address that destination should bind to for incoming migration    --dname &lt;string&gt;  在迁移过长中重新命名为一个新名称（如果支持）    --timeout &lt;number&gt;  run action specified by --timeout-* option (suspend by default) if live migration exceeds timeout (in seconds)    --timeout-suspend  suspend the guest after timeout    --timeout-postcopy  switch to post-copy after timeout    --xml &lt;string&gt;   包含为目标更新的 XML 的文件名    --migrate-disks &lt;string&gt;  comma separated list of disks to be migrated    --disks-port &lt;number&gt;  port to use by target server for incoming disks migration    --comp-methods &lt;string&gt;  comma separated list of compression methods to be used    --comp-mt-level &lt;number&gt;  compress level for multithread compression    --comp-mt-threads &lt;number&gt;  number of compression threads for multithread compression    --comp-mt-dthreads &lt;number&gt;  number of decompression threads for multithread compression    --comp-xbzrle-cache &lt;number&gt;  page cache size for xbzrle compression    --auto-converge-initial &lt;number&gt;  initial CPU throttling rate for auto-convergence    --auto-converge-increment &lt;number&gt;  CPU throttling rate increment for auto-convergence    --persistent-xml &lt;string&gt;  filename containing updated persistent XML for the target    --tls            use TLS for migration</code></pre><p>7、在KVM1主机中使用migrate命令迁移</p><pre><code>[root@kvm ~]# virsh migrate --domain generic --desturi qemu+ssh://root@kvm2/system --offline --persistent root@kvm2&#39;s password: </code></pre><p>8、在KVM2主机中查看主机是否迁移并尝试启动主机</p><pre><code>[root@kvm2 vm1]# virsh list --all  Id    Name                           State---------------------------------------------------- -     generic                        shut off[root@kvm2 vm1]# virsh start --domain generic error: Failed to start domain genericerror: Cannot access storage file &#39;/vm1/centos7.6.raw&#39;: 没有那个文件或目录</code></pre><p>9、在KVM1主机中将所需的硬盘文件复制到KVM2主机的vm1目录下</p><pre><code>[root@kvm ~]# rsync -avSHP /vm1/centos7.6.raw root@kvm2:/vm1root@kvm2&#39;s password: sending incremental file listcentos7.6.raw 21,474,836,480 100%   57.68MB/s    0:05:55 (xfr#1, to-chk=0/1)sent 21,480,079,452 bytes  received 35 bytes  59,749,873.40 bytes/sectotal size is 21,474,836,480  speedup is 1.00</code></pre><p>10、在KVM2主机中再次尝试启动主机</p><pre><code>[root@kvm2 vm1]# virsh start --domain generic Domain generic started</code></pre><h1><span id="3-基于共享存储的动态迁移">3、基于共享存储的动态迁移</span></h1><blockquote><p><strong>确保防火墙等安全防护工具处于关闭状态或者端口放行状态</strong></p></blockquote><p><strong>需要安装NFS服务器</strong></p><table><thead><tr><th>主机</th><th>Lan</th><th>Private</th><th>Stroage</th></tr></thead><tbody><tr><td>kvm1</td><td>192.168.200.135</td><td>172.16.1.135</td><td>10.0.1.231</td></tr><tr><td>kvm2</td><td>192.168.200.136</td><td>172.16.1.136</td><td>10.0.1.232</td></tr><tr><td>NFS</td><td>192.168.200.137</td><td></td><td>10.0.1.230</td></tr></tbody></table><h2><span id="1-安装nfs服务器组件">1、安装NFS服务器组件</span></h2><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/27.jpg"></p><pre><code>[root@nfs ~]# yum -y install nfs-utils[root@nfs ~]# systemctl start rpcbind[root@nfs ~]# systemctl start nfs-server[root@nfs ~]# systemctl enable rpcbind[root@nfs ~]# systemctl enable nfs-serverCreated symlink from /etc/systemd/system/multi-user.target.wants/nfs-server.service to /usr/lib/systemd/system/nfs-server.service.</code></pre><h2><span id="2-创建nfs共享文件夹">2、创建NFS共享文件夹</span></h2><pre><code>[root@nfs ~]# mkdir /vm1[root@nfs ~]# touch /vm1/testfile1.txt[root@nfs ~]# vim /etc/exports  #修改export文件/vm1 *(rw,no_root_squash,sync)[root@nfs ~]# systemctl restart nfs-server #重启服务[root@nfs ~]# showmount -e localhost #查看对外提供的共享文件夹Export list for localhost:/vm1 *</code></pre><h2><span id="3-将kvm1中的虚拟机硬盘迁移到nfs服务器的共享存储上">3、将KVM1中的虚拟机硬盘迁移到NFS服务器的共享存储上</span></h2><pre><code>[root@kvm ~]# mkdir /vmdata #在KVM1服务器中创建挂载点[root@kvm ~]# mount 192.168.200.137:/vm1 /vmdata/ #挂载NFS服务器中的vm1共享文件夹到本地的/vmdata。这里的IP建议使用专有的隧道网络，如万兆光等[root@kvm ~]# df -HT  #查看挂载情况文件系统                类型      容量  已用  可用 已用% 挂载点devtmpfs                devtmpfs  2.0G     0  2.0G    0% /devtmpfs                   tmpfs     2.0G     0  2.0G    0% /dev/shmtmpfs                   tmpfs     2.0G   13M  2.0G    1% /runtmpfs                   tmpfs     2.0G     0  2.0G    0% /sys/fs/cgroup/dev/mapper/centos-root xfs        38G   35G  3.7G   91% //dev/sda1               xfs       1.1G  206M  859M   20% /boottmpfs                   tmpfs     396M     0  396M    0% /run/user/0192.168.200.137:/vm1    nfs4       38G   15G   24G   39% /vmdata[root@kvm ~]# virsh list --all #查看迁移的主机 Id    名称                         状态---------------------------------------------------- -     generic                        关闭[root@kvm ~]# virsh edit --domain generic #编辑xml文件中的disk，如果直接修改xml文件，源硬盘会被直接迁移到nfs的共享目录中，老版本可能需要手动复制到挂载点    &lt;disk type=&#39;file&#39; device=&#39;disk&#39;&gt;      &lt;driver name=&#39;qemu&#39; type=&#39;raw&#39;/&gt;      &lt;source file=&#39;/vmdata/centos7.6.raw&#39;/&gt; #修改      &lt;target dev=&#39;hda&#39; bus=&#39;ide&#39;/&gt;      &lt;address type=&#39;drive&#39; controller=&#39;0&#39; bus=&#39;0&#39; target=&#39;0&#39; unit=&#39;0&#39;/&gt;    &lt;/disk&gt;[root@kvm vmdata]# ls -lh /vmdata/ #查看共享文件中已经存在相关硬盘文件总用量 1.8G-rw-------. 1 root root 20G 4月  10 17:10 centos7.6.raw-rw-r--r--. 1 root root   0 4月  12 2023 testfile1.txt[root@kvm vmdata]# virsh start --domain generic  #尝试启动错误：开始域 generic 失败错误：内部错误：process exited while connecting to monitor: 2023-04-10T09:59:16.927297Z qemu-kvm: -drive file=/vmdata/centos7.6.raw,format=raw,if=none,id=drive-ide0-0-0: could not open disk image /vmdata/centos7.6.raw: Could not open &#39;/vmdata/centos7.6.raw&#39;: Permission denied[root@kvm vmdata]# sestatus #查看selinux启动，导致无法启动虚拟机SELinux status:                 enabledSELinuxfs mount:                /sys/fs/selinuxSELinux root directory:         /etc/selinuxLoaded policy name:             targetedCurrent mode:                   enforcingMode from config file:          enforcingPolicy MLS status:              enabledPolicy deny_unknown status:     allowedMax kernel policy version:      31[root@kvm vmdata]# setenforce usage:  setenforce [ Enforcing | Permissive | 1 | 0 ][root@kvm vmdata]# setenforce 0 #临时关闭selinux[root@kvm vmdata]# virsh start --domain generic  #正常启动域 generic 已开始</code></pre><blockquote><p>如果修改完后虚拟机无法正常启动，提示权限错误，可能是由于Selinux的问题</p></blockquote><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/28.jpg"></p><blockquote><p>建议不要关闭selinux，可以放行nfs的相关服务</p></blockquote><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/29.png"></p><pre><code>[root@kvm vmdata]# virsh shutdown --domain generic #关闭虚拟机域 generic 被关闭[root@kvm vmdata]# sestatus #查看当前Selinux状态为临时关闭SELinux status:                 enabledSELinuxfs mount:                /sys/fs/selinuxSELinux root directory:         /etc/selinuxLoaded policy name:             targetedCurrent mode:                   permissiveMode from config file:          enforcingPolicy MLS status:              enabledPolicy deny_unknown status:     allowedMax kernel policy version:      31[root@kvm vmdata]# setenforce 1 #启用Selinux[root@kvm vmdata]# sestatus #再次查看为启动状态SELinux status:                 enabledSELinuxfs mount:                /sys/fs/selinuxSELinux root directory:         /etc/selinuxLoaded policy name:             targetedCurrent mode:                   enforcingMode from config file:          enforcingPolicy MLS status:              enabledPolicy deny_unknown status:     allowedMax kernel policy version:      31[root@kvm vmdata]# setsebool -P virt_use_nfs 1 #需要在所有的计算节点开启[root@kvm vmdata]# virsh start --domain generic  #尝试可以正常启动域 generic 已开始[root@kvm vmdata]# virsh shutdown --domain generic #关机，准备迁移域 generic 被关闭</code></pre><h2><span id="4-使用virt-manager进行迁移">4、使用virt-manager进行迁移</span></h2><p>KVM2：</p><pre><code>[root@kvm2 ~]# mkdir /vmdata[root@kvm2 ~]# ls -lh /vmdatatotal 0[root@kvm2 ~]# mount 192.168.200.137:/vm1 /vmdata/[root@kvm2 ~]# ls -lh /vmdatatotal 1.8G-rw-------. 1 root root 20G Apr 12 17:40 centos7.6.raw-rw-r--r--. 1 root root   0 Apr 12 16:57 testfile1.txt</code></pre><p>KVM1:</p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/30.png"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/31.jpg"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/32.jpg"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/33.jpg"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/34.png"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/35.png"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/36.jpg"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/37.jpg"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/38.png"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/39.jpg"></p><pre><code>[root@kvm2 ~]# setsebool -P virt_use_nfs 1[root@kvm2 ~]# iptables -I INPUT -p tcp --dport 49152:49215 -j ACCEPT[root@kvm2 ~]# iptables -I INPUT -p tcp --dport 16509 -j ACCEPT</code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/40.jpg"></p><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/41.jpg"></p><p>Allow unsafe：CPU缓存的模式，默认使用none<br>Temporary move：临时移动，移动后不删除源主机的相关配置</p><h2><span id="5-使用virsh命令进行迁移">5、使用virsh命令进行迁移</span></h2><h3><span id="1-查看帮助">1、查看帮助</span></h3><pre><code>[root@kvm ~]# virsh  migrate --help   名称：    migrate-将域迁移到另一台主机  简介：    migrate &lt;domain&gt; &lt;desturi&gt; [--live] [--offline] [--p2p] [--direct] [--tunnelled] [--persistent] [--undefinesource] [--suspend] [--copy-storage-all] [--copy-storage-inc] [--change-protection] [--unsafe] [--verbose] [--compressed] [--auto-converge] [--rdma-pin-all] [--abort-on-error] [--postcopy] [--postcopy-after-precopy] [--migrateuri &lt;string&gt;] [--graphicsuri &lt;string&gt;] [--listen-address &lt;string&gt;] [--dname &lt;string&gt;] [--timeout &lt;number&gt;] [--timeout-suspend] [--timeout-postcopy] [--xml &lt;string&gt;] [--migrate-disks &lt;string&gt;] [--disks-port &lt;number&gt;] [--comp-methods &lt;string&gt;] [--comp-mt-level &lt;number&gt;] [--comp-mt-threads &lt;number&gt;] [--comp-mt-dthreads &lt;number&gt;] [--comp-xbzrle-cache &lt;number&gt;] [--auto-converge-initial &lt;number&gt;] [--auto-converge-increment &lt;number&gt;] [--persistent-xml &lt;string&gt;] [--tls]  说明：    将域迁移到另一台主机。添加--实时迁移。  选项：    [--domain] &lt;string&gt;  域名、id或uuid    [--desturi] &lt;string&gt;  从客户端（正常迁移）或源（p2p迁移）看到的目标主机的连接URI    --live           热迁移    --offline        离线迁移    --p2p            对等迁移    --direct         直接迁移    --tunnelled      隧道迁移    --persistent     在目标上持久化VM    --undefinesource  源上未定义VM    --suspend        不要在目标主机上重新启动域    --copy-storage-all  使用带有完整磁盘拷贝的非共享存储进行迁移    --copy-storage-inc  使用带有增量拷贝的非共享存储进行迁移（源和目标之间共享相同的基本映像）    --change-protection  在迁移结束之前，禁止对域进行任何配置更改    --unsafe         强制迁移，即使可能不安全    --verbose        显示迁移进度    --compressed     在实时迁移过程中压缩重复的页面    --auto-converge  现场迁移过程中的力收敛    --rdma-pin-all   在开始RDMA实时迁移之前固定所有内存    --abort-on-error  迁移过程中出现软错误时中止    --postcopy       启用复制后迁移；使用“移植”后复制命令切换到它    --postcopy-after-precopy  经过一次预复制后自动切换到复制后迁移    --migrateuri &lt;string&gt;  迁移URI，通常可以省略    --graphicsuri &lt;string&gt;  用于无缝图形迁移的图形URI    --listen-address &lt;string&gt;  目标应绑定到的侦听地址以进行传入迁移    --dname &lt;string&gt;  迁移期间重命名为新名称（如果支持）    --timeout &lt;number&gt;  如果实时迁移超过超时（以秒为单位），则运行由--timeout-*选项指定的操作（默认情况下为挂起）    --timeout-suspend  超时后暂停来宾    --timeout-postcopy  超时后切换到后复制    --xml &lt;string&gt;   切换到超时后发布复制包含目标的更新XML的文件名    --migrate-disks &lt;string&gt;  要迁移的磁盘的逗号分隔列表    --disks-port &lt;number&gt;  目标服务器用于传入磁盘迁移的端口    --comp-methods &lt;string&gt;  要使用的压缩方法的逗号分隔列表    --comp-mt-level &lt;number&gt;  多线程压缩的压缩级别    --comp-mt-threads &lt;number&gt;  用于多线程压缩的压缩线程数    --comp-mt-dthreads &lt;number&gt;  用于多线程压缩的解压缩线程数    --comp-xbzrle-cache &lt;number&gt;  xbzrle压缩的页面缓存大小    --auto-converge-initial &lt;number&gt;  用于自动收敛的初始CPU节流速率    --auto-converge-increment &lt;number&gt;  用于自动收敛的CPU节流速率增量    --persistent-xml &lt;string&gt;  包含目标的更新的持久XML的文件名    --tls            使用TLS进行迁移</code></pre><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/42.jpg"></p><h3><span id="2-迁移">2、迁移</span></h3><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/43.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/44.jpg"></p><p><strong>使用TCP指定迁移时使用的网络</strong></p><p><code> virsh migrate centos64a gemu+ssh://root@labkvml/system--migrateuri tcp://172.16.1.231 --live--persistent --undefinesource</code></p><p><strong>查看kvm2主机上的xml文件</strong></p><pre><code>[root@kvm2 qemu]# lsgeneric.xml  networks[root@kvm2 qemu]# pwd/etc/libvirt/qemu</code></pre><p><strong>迁移</strong></p><pre><code>[root@kvm2 qemu]# virsh list  Id    Name                           State---------------------------------------------------- 3     generic                        running[root@kvm2 qemu]# virsh migrate --domain generic --desturi qemu+ssh://root@kvm/system --live --unsafe --persistent --undefinesource root@kvm&#39;s password: error: 无法在 &#39;kvm :49152&#39; 连接到服务器: 没有到主机的路由</code></pre><p><strong>上述报错是由于目标防火墙没开通49152端口导致的，开放相关端口即可</strong></p><pre><code>[root@kvm qemu]# iptables -I INPUT -p tcp --dport 49152 -j ACCEPT</code></pre><p><strong>重新尝试迁移</strong></p><pre><code>[root@kvm2 qemu]# virsh migrate --domain generic --desturi qemu+ssh://root@kvm/system --live --unsafe --persistent --undefinesource root@kvm&#39;s password: [root@kvm2 qemu]# ls #迁移成功，且配置文件被删除networks</code></pre><p><strong>在目标端查看迁移结果</strong></p><pre><code>[root@kvm qemu]# virsh list  Id    Name                           State---------------------------------------------------- 9     generic                        running</code></pre><h1><span id="4-基于本地存储的动态迁移">4、基于本地存储的动态迁移</span></h1><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/45.png"></p><p><strong>查看主机状态及硬盘路径</strong></p><pre><code>[root@kvm qemu]# virsh list --all Id    Name                           State---------------------------------------------------- 10    generic                        running -     centos7.0                      shut off[root@kvm qemu]# virsh domblklist --domain generic Target     Source------------------------------------------------hda        /vm/centos7.6.rawhdb        -</code></pre><p><strong>迁移</strong></p><pre><code>[root@kvm qemu]# virsh migrate --domain generic --desturi qemu+ssh://kvm2/system --live --persistent --undefinesource root@kvm2&#39;s password: error: 不安全的迁移:Migration without shared storage is unsafe[root@kvm qemu]# virsh migrate --domain generic --desturi qemu+ssh://kvm2/system --live --persistent --undefinesource --unsaferoot@kvm2&#39;s password: error: Cannot access storage file &#39;/vm/centos7.6.raw&#39;: 没有那个文件或目录[root@kvm qemu]# virsh migrate --domain generic --desturi qemu+ssh://kvm2/system --live --persistent --undefinesource --unsafe --copy-storage-all root@kvm2&#39;s password: error: Cannot access storage file &#39;/vm/centos7.6.raw&#39;: 没有那个文件或目录[root@kvm qemu]# yum install centos-release-qemu-ev qemu-kvm-ev -y #目标宿主机和本机都需要执行[root@kvm qemu]# reboot #目标宿主机和本机都需要执行</code></pre><blockquote><p>centos-release-qemu-ev qemu-kvm-ev 这两个安装包会在yum的配置文件下生成相关配置文件，并且会对现有的kvm和libvirt版本进行升级和替换，生产环境中谨慎操作，建议升级后重启宿主机。目前Centos 7版本有这个问题，Ubuntu没有测试</p></blockquote><p><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/46.jpg"><br><img src="/images/KVM/%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%BF%81%E7%A7%BB/47.jpg"></p><pre><code>[root@kvm ~]# virsh migrate --domain generic --desturi qemu+ssh://kvm2/system --live --persistent --undefinesource --unsafe --copy-storage-all root@kvm2&#39;s password: error: 不支持的配置：Hypervisor 不支持 CPU 型号 Broadwell-noTSX-IBRS</code></pre><blockquote><p>上述报错是由于KVM2主机上的kvm版本不一致导致的，发现qemu-kvm-ev的软件包没有没安装成功，重新安装后解决此问题；</p></blockquote><pre><code>[root@kvm ~]# virsh migrate --domain generic --desturi qemu+ssh://kvm2/system --live --persistent --undefinesource --unsafe --copy-storage-all root@kvm2&#39;s password: error: 内部错误：无法执行 QEMU 命令 &#39;drive-mirror&#39;：Failed to connect socket: No route to host</code></pre><blockquote><p>上述报错是由于KVM2主机的防火墙没有放行相关端口，执行iptables -I INPUT -p tcp -s 192.168.200.128 -j ACCEPT 即可解决；</p></blockquote><pre><code>[root@kvm ~]# virsh migrate --domain generic --desturi qemu+ssh://kvm2/system --live --persistent --undefinesource --unsafe --copy-storage-all --verboseroot@kvm2&#39;s password: [root@kvm2 ~]# virsh list --all  Id    Name                           State---------------------------------------------------- 5     generic                        running</code></pre><blockquote><p>迁移成功，需要保证主机两端的存储池都是一致的,迁移过程中可以使用sar -n DEV 1 4 （间隔一秒，采样四次），进行流量观察，也可以使用iostat等命令观察硬盘IO的情况</p></blockquote><blockquote><p>在基于本地硬盘的迁移中，如果一个虚拟机由A宿主机迁移到B宿主机正常迁移后，想再次从B宿主机迁移回A宿主机，那么此时就需要重启B宿主机的libvirtd服务，否则会报下列错误</p></blockquote><pre><code>[root@node2 /]# virsh migrate --domain Centos7.9 --desturi qemu+ssh://root@node1/system --live --persistent --undefinesource --unsafe --copy-storage-all --verbose --error: Cannot access storage file &#39;/vm/Centos7.9.qcow2&#39; (as uid:107, gid:107): 没有那个文件或目录[root@node2 /]# qemu-img info /vm/Centos7.9.qcow2 qemu-img: Could not open &#39;/vm/Centos7.9.qcow2&#39;: Failed to get shared &quot;write&quot; lockIs another process using the image [/vm/Centos7.9.qcow2]?</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KVM </tag>
            
            <tag> KVM虚拟机迁移 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KVM虚拟化进阶之Linux HA群集体系架构</title>
      <link href="/2023/04/12/KVM/3.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8BLinux_HA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/"/>
      <url>/2023/04/12/KVM/3.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8BLinux_HA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E4%B8%80-linux-ha%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84">一、Linux HA群集体系架构</a></li></ul><!-- tocstop --><h1><span id="一-linux-ha群集体系架构">一、Linux HA群集体系架构</span></h1><p><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/48.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/49.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/50.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/51.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/52.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/53.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/54.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/55.png"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/56.png"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/58.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/58.jpg"><br><img src="/images/KVM/LinuxHA%E7%BE%A4%E9%9B%86%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84/59.png"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KVM </tag>
            
            <tag> Linux HA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KVM虚拟化进阶之Linux群集安装与配置</title>
      <link href="/2023/04/12/KVM/4.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8BLinux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"/>
      <url>/2023/04/12/KVM/4.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8BLinux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#1-%E8%A7%84%E5%88%92%E8%AE%BE%E8%AE%A1">1、规划设计</a></li><li><a href="#2-%E7%BE%A4%E9%9B%86%E7%BB%84%E4%BB%B6%E5%AE%89%E8%A3%85">2、群集组件安装</a></li><li><a href="#3-%E7%BE%A4%E9%9B%86%E8%8A%82%E7%82%B9%E5%87%86%E5%A4%87">3、群集节点准备</a><ul><li><a href="#1-%E9%85%8D%E7%BD%AE%E4%B8%BB%E6%9C%BA%E5%90%8D%E5%8F%8A%E8%A7%A3%E6%9E%90">1、配置主机名及解析</a></li><li><a href="#2-%E9%85%8D%E7%BD%AEssh-key%E4%BA%92%E4%BF%A1%E5%8F%AF%E9%80%89">2、配置SSH Key互信（可选）</a></li><li><a href="#3-%E9%85%8D%E7%BD%AE%E6%97%B6%E9%92%9F">3、配置时钟</a></li><li><a href="#4-%E9%85%8D%E7%BD%AEiptables%E9%98%B2%E7%81%AB%E5%A2%99%E5%85%81%E8%AE%B8%E9%9B%86%E7%BE%A4%E7%BB%84%E4%BB%B6%E8%BF%90%E8%A1%8C">4、配置iptables防火墙允许集群组件运行</a></li><li><a href="#5-%E9%85%8D%E7%BD%AEpcs%E5%AE%88%E6%8A%A4%E7%A8%8B%E5%BA%8F">5、配置pcs守护程序</a></li><li><a href="#6-%E9%85%8D%E7%BD%AEhacluster%E8%B4%A6%E6%88%B7%E5%AF%86%E7%A0%81">6、配置hacluster账户密码</a></li><li><a href="#7-%E7%BC%96%E8%BE%91%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">7、编辑配置文件</a></li></ul></li><li><a href="#4-%E7%BE%A4%E9%9B%86%E7%9A%84%E5%88%9B%E5%BB%BA">4、群集的创建</a></li></ul><!-- tocstop --><h1><span id="1-规划设计">1、规划设计</span></h1><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/60.jpg"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/61.jpg"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/62.png"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/63.png"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/64.png"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/65.png"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/65.png"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/66.png"></p><table><thead><tr><th>主机名称</th><th>管理地址</th><th>心跳地址</th><th>存储地址</th></tr></thead><tbody><tr><td>KVM</td><td>192.168.200.200</td><td>192.168.0.200</td><td>192.168.1.200</td></tr><tr><td>KVM2</td><td>192.168.200.201</td><td>192.168.0.201</td><td>192.168.1.201</td></tr></tbody></table><h1><span id="2-群集组件安装">2、群集组件安装</span></h1><p><strong>1、查看需要安装的软件包</strong></p><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/67.jpg"></p><pre><code>[root@kvm ~]# yum list  | grep pacemakerdrbd-pacemaker.x86_64                    9.17.0-1.el7                  epel   pacemaker.x86_64                         1.1.23-1.el7_9.1              updatespacemaker-cli.x86_64                     1.1.23-1.el7_9.1              updatespacemaker-cluster-libs.i686              1.1.23-1.el7_9.1              updatespacemaker-cluster-libs.x86_64            1.1.23-1.el7_9.1              updatespacemaker-cts.x86_64                     1.1.23-1.el7_9.1              updatespacemaker-doc.x86_64                     1.1.23-1.el7_9.1              updatespacemaker-libs.i686                      1.1.23-1.el7_9.1              updatespacemaker-libs.x86_64                    1.1.23-1.el7_9.1              updatespacemaker-libs-devel.i686                1.1.23-1.el7_9.1              updatespacemaker-libs-devel.x86_64              1.1.23-1.el7_9.1              updatespacemaker-nagios-plugins-metadata.x86_64 1.1.23-1.el7_9.1              updatespacemaker-remote.x86_64                  1.1.23-1.el7_9.1              updates</code></pre><p><strong>2、安装相关软件</strong></p><pre><code>[root@kvm ~]# yum install -y pacemaker corosync pcs psmisc policycoreutils-python fence-agents-all #所有集群节点都需要安装Installed:  corosync.x86_64 0:2.4.5-7.el7_9.2                         fence-agents-all.x86_64 0:4.2.1-41.el7_9.6                         pacemaker.x86_64 0:1.1.23-1.el7_9.1                         pcs.x86_64 0:0.9.169-3.el7.centos.3                         policycoreutils-python.x86_64 0:2.5-34.el7                        Dependency Installed:  OpenIPMI.x86_64 0:2.0.27-1.el7                      OpenIPMI-libs.x86_64 0:2.0.27-1.el7                  OpenIPMI-modalias.x86_64 0:2.0.27-1.el7            audit-libs-python.x86_64 0:2.8.5-4.el7               checkpolicy.x86_64 0:2.5-8.el7                    cifs-utils.x86_64 0:6.2-10.el7                           clufter-bin.x86_64 0:0.77.1-1.el7                   clufter-common.noarch 0:0.77.1-1.el7                 corosynclib.x86_64 0:2.4.5-7.el7_9.2               device-mapper-multipath.x86_64 0:0.4.9-136.el7_9     fence-agents-amt-ws.x86_64 0:4.2.1-41.el7_9.6     fence-agents-apc.x86_64 0:4.2.1-41.el7_9.6               fence-agents-apc-snmp.x86_64 0:4.2.1-41.el7_9.6     fence-agents-bladecenter.x86_64 0:4.2.1-41.el7_9.6   fence-agents-brocade.x86_64 0:4.2.1-41.el7_9.6     fence-agents-cisco-mds.x86_64 0:4.2.1-41.el7_9.6     fence-agents-cisco-ucs.x86_64 0:4.2.1-41.el7_9.6  fence-agents-common.x86_64 0:4.2.1-41.el7_9.6            fence-agents-compute.x86_64 0:4.2.1-41.el7_9.6      fence-agents-drac5.x86_64 0:4.2.1-41.el7_9.6         fence-agents-eaton-snmp.x86_64 0:4.2.1-41.el7_9.6  fence-agents-emerson.x86_64 0:4.2.1-41.el7_9.6       fence-agents-eps.x86_64 0:4.2.1-41.el7_9.6        fence-agents-heuristics-ping.x86_64 0:4.2.1-41.el7_9.6   fence-agents-hpblade.x86_64 0:4.2.1-41.el7_9.6      fence-agents-ibmblade.x86_64 0:4.2.1-41.el7_9.6      fence-agents-ifmib.x86_64 0:4.2.1-41.el7_9.6       fence-agents-ilo-moonshot.x86_64 0:4.2.1-41.el7_9.6  fence-agents-ilo-mp.x86_64 0:4.2.1-41.el7_9.6     fence-agents-ilo-ssh.x86_64 0:4.2.1-41.el7_9.6           fence-agents-ilo2.x86_64 0:4.2.1-41.el7_9.6         fence-agents-intelmodular.x86_64 0:4.2.1-41.el7_9.6  fence-agents-ipdu.x86_64 0:4.2.1-41.el7_9.6        fence-agents-ipmilan.x86_64 0:4.2.1-41.el7_9.6       fence-agents-kdump.x86_64 0:4.2.1-41.el7_9.6      fence-agents-mpath.x86_64 0:4.2.1-41.el7_9.6             fence-agents-redfish.x86_64 0:4.2.1-41.el7_9.6      fence-agents-rhevm.x86_64 0:4.2.1-41.el7_9.6         fence-agents-rsa.x86_64 0:4.2.1-41.el7_9.6         fence-agents-rsb.x86_64 0:4.2.1-41.el7_9.6           fence-agents-sbd.x86_64 0:4.2.1-41.el7_9.6        fence-agents-scsi.x86_64 0:4.2.1-41.el7_9.6              fence-agents-vmware-rest.x86_64 0:4.2.1-41.el7_9.6  fence-agents-vmware-soap.x86_64 0:4.2.1-41.el7_9.6   fence-agents-wti.x86_64 0:4.2.1-41.el7_9.6         fence-virt.x86_64 0:0.3.2-16.el7                     ipmitool.x86_64 0:1.8.18-10.el7_9                 liberation-fonts-common.noarch 1:1.07.2-16.el7           liberation-sans-fonts.noarch 1:1.07.2-16.el7        libldb.x86_64 0:1.5.4-2.el7                          libqb.x86_64 0:1.0.1-9.el7                         libsemanage-python.x86_64 0:2.5-14.el7               libtalloc.x86_64 0:2.1.16-1.el7                   libtdb.x86_64 0:1.3.18-1.el7                             libtevent.x86_64 0:0.9.39-1.el7                     libwbclient.x86_64 0:4.10.16-24.el7_9                libwsman1.x86_64 0:2.6.3-7.git4391e5c.el7          net-snmp-libs.x86_64 1:5.7.2-49.el7_9.2              net-snmp-utils.x86_64 1:5.7.2-49.el7_9.2          openwsman-python.x86_64 0:2.6.3-7.git4391e5c.el7         overpass-fonts.noarch 0:2.1-1.el7                   pacemaker-cli.x86_64 0:1.1.23-1.el7_9.1              pacemaker-cluster-libs.x86_64 0:1.1.23-1.el7_9.1   pacemaker-libs.x86_64 0:1.1.23-1.el7_9.1             perl-TimeDate.noarch 1:2.30-2.el7                 pexpect.noarch 0:2.3-11.el7                              python-IPy.noarch 0:0.75-6.el7                      python-clufter.noarch 0:0.77.1-1.el7                 python-lxml.x86_64 0:3.2.1-4.el7                   python2-subprocess32.x86_64 0:3.2.6-14.el7           resource-agents.x86_64 0:4.1.1-61.el7_9.18        ruby.x86_64 0:2.0.0.648-39.el7_9                         ruby-irb.noarch 0:2.0.0.648-39.el7_9                ruby-libs.x86_64 0:2.0.0.648-39.el7_9                rubygem-bigdecimal.x86_64 0:1.2.0-39.el7_9         rubygem-io-console.x86_64 0:0.4.2-39.el7_9           rubygem-json.x86_64 0:1.7.7-39.el7_9              rubygem-psych.x86_64 0:2.0.0-39.el7_9                    rubygem-rdoc.noarch 0:4.0.0-39.el7_9                rubygems.noarch 0:2.0.14.1-39.el7_9                  samba-client-libs.x86_64 0:4.10.16-24.el7_9        samba-common.noarch 0:4.10.16-24.el7_9               samba-common-libs.x86_64 0:4.10.16-24.el7_9       setools-libs.x86_64 0:3.3.8-4.el7                        telnet.x86_64 1:0.17-66.el7                        [root@kvm ~]# yum update -y</code></pre><p><strong>3、查看相关软件描述</strong></p><p><strong>pacemaker</strong></p><pre><code>[root@kvm ~]# rpm -qi pacemakerName        : pacemakerVersion     : 1.1.23Release     : 1.el7_9.1Architecture: x86_64Install Date: Thu 13 Apr 2023 03:21:10 PM CSTGroup       : System Environment/DaemonsSize        : 1285198License     : GPLv2+ and LGPLv2+Signature   : RSA/SHA256, Fri 18 Dec 2020 04:35:18 AM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : pacemaker-1.1.23-1.el7_9.1.src.rpmBuild Date  : Wed 16 Dec 2020 12:40:01 AM CSTBuild Host  : x86-02.bsys.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : http://www.clusterlabs.orgSummary     : Scalable High-Availability cluster resource managerDescription :Pacemaker是一种先进的、可扩展的高可用性集群资源Corosync、CMAN和/或Linux HA的管理器。它支持超过16个具有重要功能的节点群集用于管理资源和依赖关系。它将在初始化时运行脚本，当机器上升或下降时，当相关资源出现故障时，可以配置为定期检查资源健康。可用的重建重建选项：-with(out) :cman覆盖范围文档stonithd硬化预释放分析</code></pre><p><strong>corosync</strong></p><pre><code>[root@kvm ~]# rpm -qi corosyncName        : corosyncVersion     : 2.4.5Release     : 7.el7_9.2Architecture: x86_64Install Date: Thu 13 Apr 2023 03:21:06 PM CSTGroup       : System Environment/BaseSize        : 485644License     : BSDSignature   : RSA/SHA256, Wed 01 Dec 2021 10:14:09 PM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : corosync-2.4.5-7.el7_9.2.src.rpmBuild Date  : Thu 25 Nov 2021 12:33:10 AM CSTBuild Host  : x86-02.bsys.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : http://corosync.github.io/corosync/Summary     : The Corosync Cluster Engine and Application Programming InterfacesDescription :此软件包包含Corosync Cluster Engine Executive，几个默认配置API和库、默认配置文件以及init脚本。</code></pre><p><strong>pcs</strong></p><pre><code>[root@kvm ~]# rpm -qi pcsName        : pcsVersion     : 0.9.169Release     : 3.el7.centos.3Architecture: x86_64Install Date: Thu 13 Apr 2023 03:21:17 PM CSTGroup       : System Environment/BaseSize        : 11483086License     : GPLv2Signature   : RSA/SHA256, Sat 12 Nov 2022 12:15:48 AM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : pcs-0.9.169-3.el7.centos.3.src.rpmBuild Date  : Thu 10 Nov 2022 09:56:56 PM CSTBuild Host  : x86-02.bsys.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : https://github.com/ClusterLabs/pcsSummary     : Pacemaker Configuration SystemDescription :pcs是一种同步器和起搏器配置工具。它允许用户轻松查看、修改和创建基于起搏器的集群。</code></pre><p><strong>psmisc</strong></p><pre><code>[root@kvm ~]# rpm -qi psmiscName        : psmiscVersion     : 22.20Release     : 17.el7Architecture: x86_64Install Date: Sun 02 Apr 2023 03:58:57 PM CSTGroup       : Applications/SystemSize        : 486607License     : GPLv2+Signature   : RSA/SHA256, Thu 15 Oct 2020 02:59:05 AM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : psmisc-22.20-17.el7.src.rpmBuild Date  : Thu 01 Oct 2020 01:20:29 AM CSTBuild Host  : x86-01.bsys.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : http://sourceforge.net/projects/psmiscSummary     : Utilities for managing processes on your systemDescription :psmisc包包含用于管理系统：pstree、killall和fuser。pstree命令显示一个树系统上所有正在运行的进程的结构。杀手命令将指定的信号（如果未指定任何内容，则为SIGTERM）发送到通过名称标识的进程。热熔器命令识别PID使用指定文件或文件系统的进程。</code></pre><p><strong>policycoreutils-python</strong></p><pre><code>[root@kvm ~]# rpm -qi policycoreutils-python Name        : policycoreutils-pythonVersion     : 2.5Release     : 34.el7Architecture: x86_64Install Date: Thu 13 Apr 2023 03:21:11 PM CSTGroup       : System Environment/BaseSize        : 1304826License     : GPLv2Signature   : RSA/SHA256, Sat 04 Apr 2020 05:05:35 AM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : policycoreutils-2.5-34.el7.src.rpmBuild Date  : Wed 01 Apr 2020 12:04:58 PM CSTBuild Host  : x86-02.bsys.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : http://www.selinuxproject.orgSummary     : SELinux policy core python utilitiesDescription :policycoreutils-python包包含用于管理的管理工具SELinux环境。</code></pre><p><strong>fence-agents-all</strong></p><pre><code>[root@kvm ~]# rpm -qi fence-agents-all Name        : fence-agents-allVersion     : 4.2.1Release     : 41.el7_9.6Architecture: x86_64Install Date: Thu 13 Apr 2023 03:21:15 PM CSTGroup       : System Environment/BaseSize        : 0License     : GPLv2+ and LGPLv2+ and ASL 2.0Signature   : RSA/SHA256, Thu 07 Apr 2022 01:04:19 AM CST, Key ID 24c6a8a7f4a80eb5Source RPM  : fence-agents-4.2.1-41.el7_9.6.src.rpmBuild Date  : Wed 06 Apr 2022 12:26:21 AM CSTBuild Host  : x86-01.bsys.centos.orgRelocations : (not relocatable)Packager    : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor      : CentOSURL         : https://github.com/ClusterLabs/fence-agentsSummary     : Fence agentsDescription :Red Hat围栏代理是所有受支持的围栏代理的集合。</code></pre><h1><span id="3-群集节点准备">3、群集节点准备</span></h1><h2><span id="1-配置主机名及解析">1、配置主机名及解析</span></h2><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/67.jpg"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/68.jpg"><br>node1:</p><pre><code>[root@kvm ~]# hostnamectl set-hostname node1[root@kvm ~]# bash[root@node1 ~]# vim /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.200 node1-manage192.168.200.201 node2-manage192.168.0.200 node1-heartbeat192.168.0.201 node2-heartbeat192.168.1.200 node1-storage192.168.1.201 node2-storage[root@node1 ~]# ping node2-manage PING node2-manage (192.168.200.201) 56(84) bytes of data.64 bytes from node2-manage (192.168.200.201): icmp_seq=1 ttl=64 time=1.26 ms--- node2-manage ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 1.268/1.268/1.268/0.000 ms[root@node1 ~]# ping node2-heartbeat PING node2-heartbeat (192.168.0.201) 56(84) bytes of data.64 bytes from node2-heartbeat (192.168.0.201): icmp_seq=1 ttl=64 time=9.27 ms--- node2-heartbeat ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 9.276/9.276/9.276/0.000 ms[root@node1 ~]# ping node2-storage PING node2-storage (192.168.1.201) 56(84) bytes of data.64 bytes from node2-storage (192.168.1.201): icmp_seq=1 ttl=64 time=0.700 ms--- node2-storage ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.700/0.700/0.700/0.000 ms</code></pre><p>node2:</p><pre><code>[root@kvm2 ~]# hostnamectl set-hostname node2[root@kvm2 ~]# bash[root@node2 ~]# vim /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.200 node1-manage192.168.200.201 node2-manage192.168.0.200 node1-heartbeat192.168.0.201 node2-heartbeat192.168.1.200 node1-storage192.168.1.201 node2-storage[root@node2 ~]# ping node1-manage PING node1-manage (192.168.200.200) 56(84) bytes of data.64 bytes from node1-manage (192.168.200.200): icmp_seq=1 ttl=64 time=1.06 ms--- node1-manage ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 1.063/1.063/1.063/0.000 ms[root@node2 ~]# ping node1-heartbeat PING node1-heartbeat (192.168.0.200) 56(84) bytes of data.64 bytes from node1-heartbeat (192.168.0.200): icmp_seq=1 ttl=64 time=10.6 ms--- node1-heartbeat ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 10.616/10.616/10.616/0.000 ms[root@node2 ~]# ping node1-storage PING node1-storage (192.168.1.200) 56(84) bytes of data.64 bytes from node1-storage (192.168.1.200): icmp_seq=1 ttl=64 time=0.269 ms--- node1-storage ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 0.269/0.269/0.269/0.000 ms</code></pre><h2><span id="2-配置ssh-key互信可选">2、配置SSH Key互信（可选）</span></h2><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/69.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# ssh-keygen -t rsa -P &#39;&#39;Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:OGWrktFMBJXAHiwVbzg5pX9ZTg5Do99QQyCJWxjlUaI root@node1The key&#39;s randomart image is:+---[RSA 2048]----+|   +*XO+=o+      ||  . *X+* o .     ||   oE+* * o      ||    oO = %       ||    . * S +      ||     o +         ||    o .          ||     .           ||                 |+----[SHA256]-----+[root@node1 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@node2/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: ERROR: ssh: Could not resolve hostname node2: Name or service not known[root@node1 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@node2-manage/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;The authenticity of host &#39;node2-manage (192.168.200.201)&#39; can&#39;t be established.ECDSA key fingerprint is SHA256:s2qiffBOYjuMJQOHEI9r87NTNjd0XyoEf81myM78kbQ.ECDSA key fingerprint is MD5:a5:8f:de:d4:07:9f:b7:b3:6f:7c:b0:c3:55:eb:bb:b0.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@node2-manage&#39;s password: Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#39;root@node2-manage&#39;&quot;and check to make sure that only the key(s) you wanted were added.[root@node1 ~]# ssh root@node2-manageLast login: Thu Apr 13 15:16:28 2023 from 192.168.200.1[root@node2 ~]# exit logoutConnection to node2-manage closed.</code></pre><p>node2:</p><pre><code>[root@node2 ~]# ssh-keygen -t rsa -P &#39;&#39;Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:E9njGHnhnwZtQGy5CmomfuxGQENWeVNRWHe9W0q9QRw root@node2The key&#39;s randomart image is:+---[RSA 2048]----+| .o... .oB*.. oE.|| .o . o .=+= . o.|| . . . .=.*.o ...||  .   .  *.= ..oo||   . . .S.. +. .=||  . =   .. .  .o || . *             ||  . +            ||   +.            |+----[SHA256]-----+[root@node2 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@node1-manage/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;The authenticity of host &#39;node1-manage (192.168.200.200)&#39; can&#39;t be established.ECDSA key fingerprint is SHA256:s2qiffBOYjuMJQOHEI9r87NTNjd0XyoEf81myM78kbQ.ECDSA key fingerprint is MD5:a5:8f:de:d4:07:9f:b7:b3:6f:7c:b0:c3:55:eb:bb:b0.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@node1-manage&#39;s password: Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#39;root@node1-manage&#39;&quot;and check to make sure that only the key(s) you wanted were added.[root@node2 ~]# ssh root@node1-manageLast login: Thu Apr 13 15:16:27 2023 from 192.168.200.1[root@node1 ~]# exit logoutConnection to node1-manage closed.</code></pre><h2><span id="3-配置时钟">3、配置时钟</span></h2><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/70.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# ntpdate time.windows.com13 Apr 16:20:11 ntpdate[6059]: adjust time server 52.231.114.183 offset 0.021235 sec[root@node1 ~]# crontab -eno crontab for root - using an empty onecrontab: installing new crontab[root@node1 ~]# crontab -l*/30 * * * * /sbin/ntpdate ntp1.aliyun.com &amp;&gt; /dev/null</code></pre><p>node2:</p><pre><code>[root@node1 ~]# ntpdate time.windows.com13 Apr 16:20:11 ntpdate[6059]: adjust time server 52.231.114.183 offset 0.021235 sec[root@node1 ~]# crontab -eno crontab for root - using an empty onecrontab: installing new crontab[root@node1 ~]# crontab -l*/30 * * * * /sbin/ntpdate ntp1.aliyun.com &amp;&gt; /dev/null</code></pre><h2><span id="4-配置iptables防火墙允许集群组件运行">4、配置iptables防火墙允许集群组件运行</span></h2><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/71.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# firewall-cmd --permanent --add-service=high-availabilitysuccess[root@node1 ~]# firewall-cmd --add-service=high-availabilitysuccess[root@node1 ~]# firewall-cmd --reloadsuccess[root@node1 ~]# firewall-cmd --get-serviceRH-Satellite-6 RH-Satellite-6-capsule amanda-client amanda-k5-client amqp amqps apcupsd audit bacula bacula-client bgp bitcoin bitcoin-rpc bitcoin-testnet bitcoin-testnet-rpc ceph ceph-mon cfengine condor-collector ctdb dhcp dhcpv6 dhcpv6-client distcc dns docker-registry docker-swarm dropbox-lansync elasticsearch etcd-client etcd-server finger freeipa-ldap freeipa-ldaps freeipa-replication freeipa-trust ftp ganglia-client ganglia-master git gre high-availability http https imap imaps ipp ipp-client ipsec irc ircs iscsi-target isns jenkins kadmin kerberos kibana klogin kpasswd kprop kshell ldap ldaps libvirt libvirt-tls lightning-network llmnr managesieve matrix mdns minidlna mongodb mosh mountd mqtt mqtt-tls ms-wbt mssql murmur mysql nfs nfs3 nmea-0183 nrpe ntp nut openvpn ovirt-imageio ovirt-storageconsole ovirt-vmconsole plex pmcd pmproxy pmwebapi pmwebapis pop3 pop3s postgresql privoxy proxy-dhcp ptp pulseaudio puppetmaster quassel radius redis rpc-bind rsh rsyncd rtsp salt-master samba samba-client samba-dc sane sip sips slp smtp smtp-submission smtps snmp snmptrap spideroak-lansync squid ssh steam-streaming svdrp svn syncthing syncthing-gui synergy syslog syslog-tls telnet tftp tftp-client tinc tor-socks transmission-client upnp-client vdsm vnc-server wbem-http wbem-https wsman wsmans xdmcp xmpp-bosh xmpp-client xmpp-local xmpp-server zabbix-agent zabbix-server</code></pre><p>node2:</p><pre><code>[root@node1 ~]# firewall-cmd --permanent --add-service=high-availabilitysuccess[root@node1 ~]# firewall-cmd --add-service=high-availabilitysuccess[root@node1 ~]# firewall-cmd --reloadsuccess</code></pre><h2><span id="5-配置pcs守护程序">5、配置pcs守护程序</span></h2><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/72.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# systemctl start pcsd[root@node1 ~]# systemctl enable pcsdCreated symlink from /etc/systemd/system/multi-user.target.wants/pcsd.service to /usr/lib/systemd/system/pcsd.service.[root@node1 ~]# systemctl status pcsd● pcsd.service - PCS GUI and remote configuration interface   Loaded: loaded (/usr/lib/systemd/system/pcsd.service; enabled; vendor preset: disabled)   Active: active (running) since Thu 2023-04-13 16:36:13 CST; 8s ago     Docs: man:pcsd(8)           man:pcs(8) Main PID: 6161 (pcsd)   CGroup: /system.slice/pcsd.service           └─6161 /usr/bin/ruby /usr/lib/pcsd/pcsdApr 13 16:36:13 node1 systemd[1]: Starting PCS GUI and remote configuration interface...Apr 13 16:36:13 node1 systemd[1]: Started PCS GUI and remote configuration interface.</code></pre><p>node2:</p><pre><code>[root@node2 ~]# systemctl start pcsd[root@node2 ~]# systemctl enable pcsdCreated symlink from /etc/systemd/system/multi-user.target.wants/pcsd.service to /usr/lib/systemd/system/pcsd.service.[root@node2 ~]# systemctl status pcsd● pcsd.service - PCS GUI and remote configuration interface   Loaded: loaded (/usr/lib/systemd/system/pcsd.service; enabled; vendor preset: disabled)   Active: active (running) since Thu 2023-04-13 16:35:20 CST; 19s ago     Docs: man:pcsd(8)           man:pcs(8) Main PID: 6483 (pcsd)   CGroup: /system.slice/pcsd.service           └─6483 /usr/bin/ruby /usr/lib/pcsd/pcsdApr 13 16:35:20 node2 systemd[1]: Starting PCS GUI and remote configuration interface...Apr 13 16:35:20 node2 systemd[1]: Started PCS GUI and remote configuration interface.</code></pre><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/73.jpg"></p><h2><span id="6-配置hacluster账户密码">6、配置hacluster账户密码</span></h2><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/74.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# echo &quot;password&quot; | passwd --stdin hacluster Changing password for user hacluster.passwd: all authentication tokens updated successfully.</code></pre><p>node2:</p><pre><code>[root@node2 ~]# echo &quot;password&quot; | passwd --stdin hacluster Changing password for user hacluster.passwd: all authentication tokens updated successfully.</code></pre><h2><span id="7-编辑配置文件">7、编辑配置文件</span></h2><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/75.jpg"></p><h1><span id="4-群集的创建">4、群集的创建</span></h1><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/76.jpg"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/77.jpg"></p><p>node1:</p><pre><code>[root@node1 ~]# pcs cluster auth node1-heartbeat node2-heartbeatUsername: haclusterPassword: passwordnode1-heartbeat: Authorizednode2-heartbeat: Authorized[root@node1 corosync]# pcs cluster setup --name cluster1 node1-heartbeat node2-heartbeat[root@node1 corosync]# pwd/etc/corosync[root@node1 corosync]# ls -lhtotal 16K-rw-r--r--. 1 root root  397 Apr 13 16:56 corosync.conf-rw-r--r--. 1 root root 2.9K Nov 25  2021 corosync.conf.example-rw-r--r--. 1 root root  767 Nov 25  2021 corosync.conf.example.udpu-rw-r--r--. 1 root root 3.3K Nov 25  2021 corosync.xml.exampledrwxr-xr-x. 2 root root    6 Nov 25  2021 uidgid.d[root@node1 corosync]# pcs cluster status Error: cluster is not currently running on this node[root@node1 corosync]# cat corosync.conftotem &#123;    version: 2 #集群版本    cluster_name: cluster1 #集群名称    secauth: off #安全身份验证协议    transport: udpu #节点之间的传输协议&#125;nodelist &#123; #节点的列表，可以使用Ip或者定义的主机名称    node &#123;        ring0_addr: node1-heartbeat #心跳网络        nodeid: 1    &#125;    node &#123;        ring0_addr: node1-heartbeat #心跳网络        nodeid: 2    &#125;&#125;quorum &#123;     provider: corosync_votequorum     two_node: 1  #告诉集群当前是双节点的，只剩一个节点时不要将集群停止&#125;logging &#123; #日志相关    to_logfile: yes    logfile: /var/log/cluster/corosync.log    to_syslog: yes&#125;[root@node1 corosync]# pcs cluster start --help  #查看启动帮助Usage: pcs cluster start...    start [--all | &lt;node&gt;... ] [--wait[=&lt;n&gt;]] [--request-timeout=&lt;seconds&gt;]        Start a cluster on specified node(s). If no nodes are specified then        start a cluster on the local node. If --all is specified then start        a cluster on all nodes. If the cluster has many nodes then the start        request may time out. In that case you should consider setting        --request-timeout to a suitable value. If --wait is specified, pcs        waits up to &#39;n&#39; seconds for the cluster to get ready to provide        services after the cluster has successfully started.[root@node1 corosync]# pcs cluster start  --all  #启动所有节点，不使用--all的时候默认启动当前节点，此时可以在其他主机的syslog中查看相关日志信息node1-heartbeat: Starting Cluster (corosync)...node2-heartbeat: Starting Cluster (corosync)...node1-heartbeat: Starting Cluster (pacemaker)...node2-heartbeat: Starting Cluster (pacemaker)...[root@node1 corosync]# pcs status #查看pcs状态Cluster name: cluster1 #集群的名称WARNINGS:No stonith devices and stonith-enabled is not false #目前是正常现象，还没有配置stonithStack: corosync #使用corosync作为心跳Current DC: node1-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition WITHOUT quorumLast updated: Fri Apr 14 09:25:54 2023 #获取信息时间Last change: Fri Apr 14 09:24:26 2023 by hacluster via crmd on node1-heartbeat #获取信息时间2 nodes configured #双节点的集群0 resource instances configured  #目前没有配置资源Online: [ node1-heartbeat node2-heartbeat ] #当前集群运行的主机No resourcesDaemon Status: #响应的守护程序  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled[root@node1 corosync]# pcs cluster status #查看集群当前状态Cluster Status: Stack: corosync Current DC: node1-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition WITHOUT quorum #当前的DC Last updated: Fri Apr 14 09:25:00 2023 Last change: Fri Apr 14 09:24:26 2023 by hacluster via crmd on node1-heartbeat 2 nodes configured 0 resource instances configuredPCSD Status: #当前集群两个节点的状态都是在线的  node1-heartbeat: Online  node2-heartbeat: Online  [root@node1 corosync]# systemctl status pacemaker.service -l #查看响应的服务状态● pacemaker.service - Pacemaker High Availability Cluster Manager   Loaded: loaded (/usr/lib/systemd/system/pacemaker.service; disabled; vendor preset: disabled)   Active: active (running) since Fri 2023-04-14 09:32:57 CST; 4min 10s ago     Docs: man:pacemakerd           https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/1.1/html-single/Pacemaker_Explained/index.html Main PID: 12054 (pacemakerd)    Tasks: 7   CGroup: /system.slice/pacemaker.service           ├─12054 /usr/sbin/pacemakerd -f           ├─12055 /usr/libexec/pacemaker/cib           ├─12056 /usr/libexec/pacemaker/stonithd           ├─12057 /usr/libexec/pacemaker/lrmd           ├─12058 /usr/libexec/pacemaker/attrd           ├─12059 /usr/libexec/pacemaker/pengine           └─12060 /usr/libexec/pacemaker/crmdApr 14 09:32:58 node1 crmd[12060]:   notice: Quorum acquiredApr 14 09:32:58 node1 crmd[12060]:   notice: Node node1-heartbeat state is now memberApr 14 09:32:58 node1 crmd[12060]:   notice: Node node2-heartbeat state is now memberApr 14 09:32:58 node1 crmd[12060]:   notice: The local CRM is operationalApr 14 09:32:58 node1 crmd[12060]:   notice: State transition S_STARTING -&gt; S_PENDINGApr 14 09:33:00 node1 crmd[12060]:   notice: Fencer successfully connectedApr 14 09:33:19 node1 crmd[12060]:  warning: Input I_DC_TIMEOUT received in state S_PENDING from crm_timer_poppedApr 14 09:33:19 node1 crmd[12060]:   notice: State transition S_ELECTION -&gt; S_PENDINGApr 14 09:33:19 node1 crmd[12060]:   notice: State transition S_PENDING -&gt; S_NOT_DCApr 14 09:33:19 node1 attrd[12058]:   notice: Updating all attributes after cib_refresh_notify event[root@node1 corosync]# systemctl status corosync -l ● corosync.service - Corosync Cluster Engine   Loaded: loaded (/usr/lib/systemd/system/corosync.service; disabled; vendor preset: disabled)   Active: active (running) since Fri 2023-04-14 09:32:57 CST; 4min 39s ago     Docs: man:corosync           man:corosync.conf           man:corosync_overview  Process: 12025 ExecStart=/usr/share/corosync/corosync start (code=exited, status=0/SUCCESS) Main PID: 12034 (corosync)    Tasks: 2   CGroup: /system.slice/corosync.service           └─12034 corosyncApr 14 09:32:56 node1 corosync[12034]:  [QUORUM] Members[1]: 1Apr 14 09:32:56 node1 corosync[12034]:  [MAIN  ] Completed service synchronization, ready to provide service.Apr 14 09:32:56 node1 corosync[12034]:  [TOTEM ] A new membership (192.168.0.200:14) was formed. Members joined: 2Apr 14 09:32:56 node1 corosync[12034]:  [CPG   ] downlist left_list: 0 receivedApr 14 09:32:56 node1 corosync[12034]:  [CPG   ] downlist left_list: 0 receivedApr 14 09:32:56 node1 corosync[12034]:  [QUORUM] This node is within the primary component and will provide service.Apr 14 09:32:56 node1 corosync[12034]:  [QUORUM] Members[2]: 1 2Apr 14 09:32:56 node1 corosync[12034]:  [MAIN  ] Completed service synchronization, ready to provide service.Apr 14 09:32:57 node1 corosync[12025]: Starting Corosync Cluster Engine (corosync): [  确定  ]Apr 14 09:32:57 node1 systemd[1]: Started Corosync Cluster Engine.[root@node1 corosync]# systemctl status pcsd -l● pcsd.service - PCS GUI and remote configuration interface   Loaded: loaded (/usr/lib/systemd/system/pcsd.service; enabled; vendor preset: disabled)   Active: active (running) since Thu 2023-04-13 16:56:20 CST; 16h ago     Docs: man:pcsd(8)           man:pcs(8) Main PID: 6555 (pcsd)    Tasks: 6   CGroup: /system.slice/pcsd.service           └─6555 /usr/bin/ruby /usr/lib/pcsd/pcsdApr 13 16:56:19 node1 systemd[1]: Stopped PCS GUI and remote configuration interface.Apr 13 16:56:19 node1 systemd[1]: Starting PCS GUI and remote configuration interface...Apr 13 16:56:20 node1 systemd[1]: Started PCS GUI and remote configuration interface.[root@node1 corosync]# netstat -ntlp | grep 2224 #集群安装并启动后会运行一个GUI的服务，默认端口使用的是2224，使用https的方式来进行访问，用户名密码使用上述创建的，hacluster/passwordtcp6       0      0 :::2224                 :::*                    LISTEN      6555/ruby</code></pre><p><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/78.jpg"><br><img src="/images/KVM/Linux%E7%BE%A4%E9%9B%86%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/79.png"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KVM </tag>
            
            <tag> Linux HA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KVM虚拟化进阶之基于NFS的KVM群集构建</title>
      <link href="/2023/04/12/KVM/5.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%B6%E6%9E%84/"/>
      <url>/2023/04/12/KVM/5.KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BF%9B%E9%98%B6%E4%B8%8E%E6%8F%90%E9%AB%98%E8%A7%86%E9%A2%91%E8%AF%BE%E7%A8%8B%E4%B9%8B%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#1-%E8%A7%84%E5%88%92%E8%AE%BE%E8%AE%A1">1、规划设计</a></li><li><a href="#2-%E8%8A%82%E7%82%B9%E5%87%86%E5%A4%87">2、节点准备</a><ul><li><a href="#1-%E9%98%B6%E6%AE%B51%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85">1、阶段1：操作系统安装</a></li><li><a href="#2-%E9%98%B6%E6%AE%B52%E7%BE%A4%E9%9B%86%E7%BB%84%E4%BB%B6%E5%AE%89%E8%A3%85">2、阶段2：群集组件安装</a></li><li><a href="#3-%E9%98%B6%E6%AE%B53%E7%BE%A4%E9%9B%86%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85">3、阶段3：群集节点安装</a></li></ul></li><li><a href="#3-%E9%85%8D%E7%BD%AEnfs%E8%B5%84%E6%BA%90">3、配置NFS资源</a></li><li><a href="#4-%E5%87%86%E5%A4%87%E8%99%9A%E6%8B%9F%E6%9C%BA">4、准备虚拟机</a></li><li><a href="#5-%E5%90%91%E7%BE%A4%E9%9B%86%E6%B7%BB%E5%8A%A0%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%B5%84%E6%BA%90">5、向群集添加虚拟机资源</a></li><li><a href="#6-%E7%BE%A4%E9%9B%86%E6%B5%8B%E8%AF%95">6、群集测试</a></li><li><a href="#7-%E9%85%8D%E7%BD%AEstonthipmi">7、配置STONTH（IPMI）</a></li></ul><!-- tocstop --><h1><span id="1-规划设计">1、规划设计</span></h1><table><thead><tr><th>主机名称</th><th>管理地址</th><th>心跳地址</th><th>存储地址</th></tr></thead><tbody><tr><td>node1</td><td>192.168.200.200</td><td>192.168.0.200</td><td>192.168.1.200</td></tr><tr><td>node2</td><td>192.168.200.201</td><td>192.168.0.201</td><td>192.168.1.201</td></tr><tr><td>stor1</td><td>192.168.200.202</td><td></td><td>192.168.1.202</td></tr></tbody></table><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/80.jpg"></p><h1><span id="2-节点准备">2、节点准备</span></h1><h2><span id="1-阶段1操作系统安装">1、阶段1：操作系统安装</span></h2><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/81.jpg"></p><h2><span id="2-阶段2群集组件安装">2、阶段2：群集组件安装</span></h2><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/82.jpg"></p><h2><span id="3-阶段3群集节点安装">3、阶段3：群集节点安装</span></h2><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/83.jpg"></p><blockquote><p>需要注意各个节点的防火墙配置！</p></blockquote><h1><span id="3-配置nfs资源">3、配置NFS资源</span></h1><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/84.jpg"></p><blockquote><p>防火墙是指NFS服务器防火墙</p></blockquote><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/85.jpg"></p><h1><span id="4-准备虚拟机">4、准备虚拟机</span></h1><p><strong>所有节点的host应一致</strong></p><pre><code>[root@node1 corosync]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.200 node1192.168.200.201 node2192.168.0.200 node1-heartbeat192.168.0.201 node2-heartbeat192.168.1.200 node1-storage192.168.1.201 node2-storage192.168.1.202 nfs</code></pre><p><strong>所有主机都应该测试能否访问NFS服务器</strong></p><pre><code>[root@node1 vmdata]# showmount -e 192.168.1.202Export list for 192.168.1.202:/vmdata *[root@node1 vmdata]# mount nfs:/vmdata /vmdata #将NFS服务器的共享目录挂载到本地的/vmdata中，所有主机都执行[root@node1 vmdata]# df -HT  #查看挂载情况Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  2.0G     0  2.0G   0% /devtmpfs                   tmpfs     2.0G   40M  2.0G   3% /dev/shmtmpfs                   tmpfs     2.0G   13M  2.0G   1% /runtmpfs                   tmpfs     2.0G     0  2.0G   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        38G   37G  1.1G  98% //dev/sda1               xfs       1.1G  206M  859M  20% /boottmpfs                   tmpfs     396M     0  396M   0% /run/user/0nfs:/vmdata             nfs4       38G   13G   26G  34% /vmdata[root@node1 ~]# cat /etc/fstab ## /etc/fstab# Created by anaconda on Sun Apr  2 23:28:50 2023## Accessible filesystems, by reference, are maintained under &#39;/dev/disk&#39;# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#/dev/mapper/centos-root /                       xfs     defaults        0 0UUID=b283ae1c-89a3-40a1-b8f9-21699c857148 /boot                   xfs     defaults        0 0/dev/mapper/centos-swap swap                    swap    defaults        0 0nfs:/vmdata /vmdata nfs4    defaults 0 0 [root@node1 ~]# setsebool -P virt_use_nfs 1</code></pre><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/86.jpg"><br><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/87.jpg"></p><p><strong>配置虚拟机使用NFS作为后端存储</strong></p><pre><code>[root@node1 vmdata]# virsh list --all  Id    Name                           State---------------------------------------------------- 8     Centos7.9                      running[root@node1 vmdata]# virsh shutdown --domain Centos7.9 Domain Centos7.9 is being shutdown[root@node1 vmdata]# virsh edit --domain Centos7.9       &lt;source file=&#39;/vmdata/Centos7.9.qcow2&#39;/&gt;      Domain Centos7.9 XML configuration edited.[root@node1 vmdata]# virsh start --domain Centos7.9 Domain Centos7.9 started</code></pre><p><strong>测试基于NFS的动态迁移</strong></p><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/88.jpg"></p><p>node1 &gt; node2</p><pre><code>[root@node1 vmdata]# virsh migrate --domain Centos7.9 --desturi qemu+ssh://root@node2/system --live --persistent --undefinesource --unsafe  --verboseMigration: [100 %][root@node1 vmdata]# iptables -I INPUT -p tcp -s 192.168.1.0/24 -j ACCEPT</code></pre><p>node2 &gt; node1</p><pre><code>[root@node2 vmdata]# virsh migrate --domain Centos7.9 --desturi qemu+ssh://root@node1/system --live --persistent --undefinesource --unsafe --verbose --migrateuri tcp://192.168.1.200Migration: [100 %]</code></pre><h1><span id="5-向群集添加虚拟机资源">5、向群集添加虚拟机资源</span></h1><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/89.jpg"></p><p><strong>迁移虚拟机的配置文件到共享存储中</strong></p><pre><code>[root@node1 ~]# df -HT Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  2.0G     0  2.0G   0% /devtmpfs                   tmpfs     2.0G   40M  2.0G   3% /dev/shmtmpfs                   tmpfs     2.0G   13M  2.0G   1% /runtmpfs                   tmpfs     2.0G     0  2.0G   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        38G  3.8G   34G  10% //dev/sda1               xfs       1.1G  206M  858M  20% /bootnfs:/vmdata             nfs4       38G   22G   17G  57% /vmdatatmpfs                   tmpfs     396M     0  396M   0% /run/user/0[root@node1 ~]# cd /vmdata/[root@node1 vmdata]# lsCentos7.9.qcow2[root@node1 vmdata]# mkdir qemu_config[root@node1 vmdata]# virsh list  Id    Name                           State---------------------------------------------------- 15    Centos7.9                      running[root@node1 vmdata]# virsh shutdown --domain Centos7.9 Domain Centos7.9 is being shutdown[root@node1 vmdata]# cp /etc/libvirt/qemu/Centos7.9.xml /vmdata/qemu_config/[root@node1 vmdata]# virsh undefine --domain Centos7.9 Domain Centos7.9 has been undefined  取消libvirtd的控制权</code></pre><p><strong>启动虚拟机</strong></p><pre><code>[root@node1 qemu_config]# pcs status Cluster name: cluster1WARNINGS:No stonith devices and stonith-enabled is not falseStack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 14:18:02 2023Last change: Fri Apr 14 15:20:02 2023 by hacluster via crmd on node1-heartbeat2 nodes configured0 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]No resourcesDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled [root@node1 qemu_config]# pcs resource list |grep -i &quot;virt&quot; #查看支持的列表ocf:heartbeat:IPaddr - Manages virtual IPv4 and IPv6 addresses (Linux specificocf:heartbeat:IPaddr2 - Manages virtual IPv4 and IPv6 addresses (Linux specificocf:heartbeat:VirtualDomain - Manages virtual domains through the libvirt                              virtualization frameworkservice:libvirt-guests - systemd unit file for libvirt-guestsservice:libvirtd - systemd unit file for libvirtdservice:virtlockd - systemd unit file for virtlockdservice:virtlockd-admin.socket - systemd unit file for virtlockd-admin.socketservice:virtlockd.socket - systemd unit file for virtlockd.socketservice:virtlogd - systemd unit file for virtlogdservice:virtlogd-admin.socket - systemd unit file for virtlogd-admin.socketservice:virtlogd.socket - systemd unit file for virtlogd.socketsystemd:libvirt-guests - systemd unit file for libvirt-guestssystemd:libvirtd - systemd unit file for libvirtdsystemd:virtlockd - systemd unit file for virtlockdsystemd:virtlockd-admin.socket - systemd unit file for virtlockd-admin.socketsystemd:virtlockd.socket - systemd unit file for virtlockd.socketsystemd:virtlogd - systemd unit file for virtlogdsystemd:virtlogd-admin.socket - systemd unit file for virtlogd-admin.socketsystemd:virtlogd.socket - systemd unit file for virtlogd.socket[root@node1 qemu_config]# pcs resource describe  ocf:heartbeat:VirtualDomain #查看使用帮助ocf:heartbeat:VirtualDomain - Manages virtual domains through the libvirt virtualization frameworkResource agent for a virtual domain (a.k.a. domU, virtual machine,virtual environment etc., depending on context) managed by libvirtd.Resource options:  config (required) (unique): Absolute path to the libvirt configuration file, for this virtual domain.  hypervisor: Hypervisor URI to connect to. See the libvirt documentation for details on supported URI formats. The default is system dependent. Determine              the system&#39;s default uri by running &#39;virsh --quiet uri&#39;.  force_stop: Always forcefully shut down (&quot;destroy&quot;) the domain on stop. The default behavior is to resort to a forceful shutdown only after a graceful              shutdown attempt has failed. You should only set this to true if your virtual domain (or your virtualization backend) does not support graceful              shutdown.  migration_transport: Transport used to connect to the remote hypervisor while migrating. Please refer to the libvirt documentation for details on                       transports available. If this parameter is omitted, the resource will use libvirt&#39;s default transport to connect to the remote                       hypervisor.  migration_user: The username will be used in the remote libvirt remoteuri/migrateuri. No user will be given (which means root) in the username if omitted                  If remoteuri is set, migration_user will be ignored.  migration_downtime: Define max downtime during live migration in milliseconds  migration_speed: Define live migration speed per resource in MiB/s  migration_network_suffix: Use a dedicated migration network. The migration URI is composed by adding this parameters value to the end of the node name. If                            the node name happens to be an FQDN (as opposed to an unqualified host name), insert the suffix immediately prior to the first                            period (.) in the FQDN. At the moment Qemu/KVM and Xen migration via a dedicated network is supported. Note: Be sure this                            composed host name is locally resolveable and the associated IP is reachable through the favored network. This suffix will be                            added to the remoteuri and migrateuri parameters. See also the migrate_options parameter below.  migrateuri: You can also specify here if the calculated migrate URI is unsuitable for your environment. If migrateuri is set then migration_network_suffix,              migrateport and --migrateuri in migrate_options are effectively ignored. Use &quot;%n&quot; as the placeholder for the target node name. Please refer to              the libvirt documentation for details on guest migration.  migrate_options: Extra virsh options for the guest live migration. You can also specify here --migrateuri if the calculated migrate URI is unsuitable for                   your environment. If --migrateuri is set then migration_network_suffix and migrateport are effectively ignored. Use &quot;%n&quot; as the                   placeholder for the target node name. Please refer to the libvirt documentation for details on guest migration.  monitor_scripts: To additionally monitor services within the virtual domain, add this parameter with a list of scripts to monitor. Note: when monitor                   scripts are used, the start and migrate_from operations will complete only when all monitor scripts have completed successfully. Be sure                   to set the timeout of these operations to accommodate this delay.  autoset_utilization_cpu: If set true, the agent will detect the number of domainU&#39;s vCPUs from virsh, and put it into the CPU utilization of the resource                           when the monitor is executed.  autoset_utilization_hv_memory: If set true, the agent will detect the number of *Max memory* from virsh, and put it into the hv_memory utilization of the                                 resource when the monitor is executed.  migrateport: This port will be used in the qemu migrateuri. If unset, the port will be a random highport.  remoteuri: Use this URI as virsh connection URI to commuicate with a remote hypervisor. If remoteuri is set then migration_user and             migration_network_suffix are effectively ignored. Use &quot;%n&quot; as the placeholder for the target node name. Please refer to the libvirt             documentation for details on guest migration.  save_config_on_stop: Changes to a running VM&#39;s config are normally lost on stop. This parameter instructs the RA to save the configuration back to the xml                       file provided in the &quot;config&quot; parameter.  sync_config_on_stop: Setting this automatically enables save_config_on_stop. When enabled this parameter instructs the RA to call csync2 -x to synchronize                       the file to all nodes. csync2 must be properly set up for this to work.  snapshot: Path to the snapshot directory where the virtual machine image will be stored. When this parameter is set, the virtual machine&#39;s RAM state will            be saved to a file in the snapshot directory when stopped. If on start a state file is present for the domain, the domain will be restored to the            same state it was in right before it stopped last. This option is incompatible with the &#39;force_stop&#39; option.  backingfile: When the VM is used in Copy-On-Write mode, this is the backing file to use (with its full path). The VMs image will be created based on this               backing file. This backing file will never be changed during the life of the VM.  stateless: If set to true and backingfile is defined, the start of the VM will systematically create a new qcow2 based on the backing file, therefore the             VM will always be stateless. If set to false, the start of the VM will use the COW (&lt;vmname&gt;.qcow2) file if it exists, otherwise the first start             will create a new qcow2 based on the backing file given as backingfile.  copyindirs: List of directories for the virt-copy-in before booting the VM. Used only in stateless mode.  shutdown_mode: virsh shutdown method to use. Please verify that it is supported by your virsh toolsed with &#39;virsh help shutdown&#39; When this parameter is set                 --mode shutdown_mode is passed as an additional argument to the &#39;virsh shutdown&#39; command. One can use this option in case default acpi                 method does not work. Verify that this mode is supported by your VM. By default --mode is not passed.Default operations:  start: interval=0s timeout=90s  stop: interval=0s timeout=90s  monitor: interval=10s timeout=30s  migrate_from: interval=0s timeout=60s  migrate_to: interval=0s timeout=120s[root@node1 qemu_config]# pcs cluster statusCluster Status: Stack: corosync Current DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorum Last updated: Sat Apr 15 14:18:17 2023 Last change: Fri Apr 14 15:20:02 2023 by hacluster via crmd on node1-heartbeat 2 nodes configured 0 resource instances configuredPCSD Status:  node1-heartbeat: Online  node2-heartbeat: Online[root@node1 vmdata]# pcs resource create Centos7.9_res VirtualDomain \ #Centos7.9是资源名称，VirtualDomain是简化的写法，OCF自带的脚本或资源类型hypervisor=&quot;qemu:///system&quot; \ #固定的写法，虚拟化是kvm的虚拟化config=&quot;/vmdata/qemu_config/Centos7.9.xml&quot; \ #配置文件的路径migration_transport=ssh \ #migrate使用的协议meta allow-migrate=&quot;true&quot; priority=&quot;100&quot; \#上述是必须项，从这里开始是可选项，meta是元数据，允许migrate，priority是优先级op start timeout=&quot;120s&quot; \ #虚拟机启动超时的时间op stop timeout=&quot;120s&quot; \ #虚拟机关闭超时的时间op monitor timeout=&quot;30s&quot; interval=&quot;10&quot; \ #设置一些超时的时间op migrate_from interval=&quot;0&quot; timeout=&quot;120s&quot; \ #设置一些超时的时间op migrate_to interval=&quot;0&quot; timeout=&quot;120s&quot; #设置一些超时的时间[root@node1 qemu_config]# pcs status #查看虚拟机启动情况Cluster name: cluster1WARNINGS:No stonith devices and stonith-enabled is not falseStack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 14:42:15 2023Last change: Sat Apr 15 14:42:03 2023 by root via cibadmin on node1-heartbeat2 nodes configured1 resource instance configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): Stopped #因为没有设置stonith devices and stonith-enabled is not false，导致虚拟机无法启动Daemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled删除虚拟机：[root@node1 qemu_config]# pcs resource  Centos7.9_res  (ocf::heartbeat:VirtualDomain): Stopped[root@node1 qemu_config]# pcs resource delete Centos7.9_resDeleting Resource - Centos7.9_res[root@node1 qemu_config]# pcs status Cluster name: cluster1WARNINGS:No stonith devices and stonith-enabled is not falseStack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 14:43:22 2023Last change: Sat Apr 15 14:43:19 2023 by root via cibadmin on node1-heartbeat2 nodes configured0 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]No resourcesDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled关闭stonith：[root@node1 qemu_config]# pcs property list Cluster Properties: cluster-infrastructure: corosync cluster-name: cluster1 dc-version: 1.1.23-1.el7_9.1-9acf116022 have-watchdog: false[root@node1 qemu_config]# pcs property set stonith-enabled=false[root@node1 qemu_config]# pcs property list Cluster Properties: cluster-infrastructure: corosync cluster-name: cluster1 dc-version: 1.1.23-1.el7_9.1-9acf116022 have-watchdog: false stonith-enabled: false重新启动虚拟机：[root@node1 qemu_config]# pcs resource create Centos7.9_res VirtualDomain \&gt; hypervisor=&quot;qemu:///system&quot; \&gt; config=&quot;/vmdata/qemu_config/Centos7.9.xml&quot; \&gt; migration_transport=&quot;ssh&quot; \&gt; meta allow-migrate=&quot;true&quot; priority=&quot;100&quot; \&gt; op start timeout=&quot;120s&quot; \&gt; op stop timeout=&quot;120s&quot; \&gt; op monitor timeout=&quot;30s&quot; interval=&quot;10&quot; \&gt; op migrate_from interval=&quot;0&quot; timeout=&quot;120s&quot; \&gt; op migrate_to interval=&quot;0&quot; timeout=&quot;120s&quot;Assumed agent name &#39;ocf:heartbeat:VirtualDomain&#39; (deduced from &#39;VirtualDomain&#39;)[root@node1 qemu_config]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 14:45:02 2023Last change: Sat Apr 15 14:44:59 2023 by root via crm_resource on node1-heartbeat2 nodes configured1 resource instance configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): Started node1-heartbeatDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled[root@node1 qemu_config]# virsh list --all  Id    Name                           State---------------------------------------------------- 16    Centos7.9                      running</code></pre><h1><span id="6-群集测试">6、群集测试</span></h1><p><strong>迁移测试</strong></p><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/90.jpg"></p><pre><code>[root@node1 qemu_config]# pcs resource  Centos7.9_res  (ocf::heartbeat:VirtualDomain): Started node1-heartbeat[root@node1 qemu_config]# pcs resource move Centos7.9_res #提示创建了一些规则，尽量不要再从另外一个主机迁移回来Warning: Creating location constraint cli-ban-Centos7.9_res-on-node1-heartbeat with a score of -INFINITY for resource Centos7.9_res on node node1-heartbeat.This will prevent Centos7.9_res from running on node1-heartbeat until the constraint is removed. This will be the case even if node1-heartbeat is the last node in the cluster.[root@node1 qemu_config]# pcs constraint --full #可以看到创建了一个cli-ban-Centos7.9_res-on-node1-heartbeat的规则，如果想再迁移到node1上可以将这个规则进行删除Location Constraints:  Resource: Centos7.9_res    Disabled on: node1-heartbeat (score:-INFINITY) (role: Started) (id:cli-ban-Centos7.9_res-on-node1-heartbeat)Ordering Constraints:Colocation Constraints:Ticket Constraints:[root@node1 qemu_config]# pcs constraint remove  cli-ban-Centos7.9_res-on-node1-heartbeat #删除这个规则[root@node1 qemu_config]# pcs constraint --full #查看已经没有规则了Location Constraints:Ordering Constraints:Colocation Constraints:Ticket Constraints:[root@node1 qemu_config]# pcs status Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 15:01:15 2023Last change: Sat Apr 15 14:58:36 2023 by root via crm_resource on node1-heartbeat2 nodes configured1 resource instance configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): Started node2-heartbeatDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled</code></pre><p><strong>尝试再次迁移到node1上</strong></p><pre><code>[root@node1 qemu_config]# pcs cluster standby node2-heartbeat #将节点2暂停[root@node1 qemu_config]# pcs status  #查看主机已经迁移到node1上了Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 15:10:55 2023Last change: Sat Apr 15 15:10:40 2023 by root via cibadmin on node1-heartbeat2 nodes configured1 resource instance configuredNode node2-heartbeat: standbyOnline: [ node1-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): Started node1-heartbeatDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled[root@node1 qemu_config]# pcs constraint --full #查看没有创建规则Location Constraints:Ordering Constraints:Colocation Constraints:Ticket Constraints:[root@node1 qemu_config]# pcs cluster unstandby node2-heartbeat #取消节点2stanbdy[root@node2 .ssh]# pcs cluster stop node1-heartbeat #在节点2服务器中将节点1的服务停止node1-heartbeat: Stopping Cluster (pacemaker)...node1-heartbeat: Stopping Cluster (corosync)...[root@node2 .ssh]# pcs status  #查看主机迁移到node2中，实现高可用Cluster name: cluster1Stack: corosyncCurrent DC: node2-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 17:37:47 2023Last change: Sat Apr 15 17:35:52 2023 by root via cibadmin on node1-heartbeat2 nodes configured1 resource instance configuredOnline: [ node2-heartbeat ]OFFLINE: [ node1-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): Started node2-heartbeatDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled测试节点2直接关机，模拟中断，节点是否迁移，通过vmware直接关机：[root@node1 qemu_config]# pcs status  #在节点1中查看，主机不会热迁移，断电仍受影响Cluster name: cluster1Stack: corosyncCurrent DC: node1-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 17:40:05 2023Last change: Sat Apr 15 17:35:52 2023 by root via cibadmin on node1-heartbeat2 nodes configured1 resource instance configuredOnline: [ node1-heartbeat ]OFFLINE: [ node2-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): StoppedFailed Resource Actions:* Centos7.9_res_start_0 on node1-heartbeat &#39;unknown error&#39; (1): call=6, status=complete, exitreason=&#39;Failed to start virtual domain Centos7.9.&#39;,    last-rc-change=&#39;Sat Apr 15 17:39:50 2023&#39;, queued=0ms, exec=1310msDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled</code></pre><p><strong>报错处理</strong></p><p>1、</p><pre><code>Apr 15 16:25:23 node1 libvirtd: 2023-04-15 08:25:23.523+0000: 9724: error : qemuMigrationSrcIsSafe:1224 : Unsafe migration: Migration may lead to data corruption if disks use cache != none or cache != directsync</code></pre><p>由于xml文件的硬盘高级选项使用的不是none，导致这个问题，需要修改</p><pre><code>[root@node1 vmdata]# virsh dumpxml --domain Centos7.9 | grep none      &lt;driver name=&#39;qemu&#39; type=&#39;qcow2&#39; cache=&#39;none&#39;/&gt;</code></pre><p>2、</p><pre><code>Apr 15 17:23:55 node1 VirtualDomain(Centos7.9_res)[18439]: INFO: Centos7.9: Starting live migration to node2-heartbeat (using: virsh --connect=qemu:///system --quiet migrate --live  Centos7.9 qemu://node2-heartbeat/system ).Apr 15 17:23:55 node1 VirtualDomain(Centos7.9_res)[18439]: ERROR: Centos7.9: live migration to node2-heartbeat failed: 1</code></pre><pre><code>由于创建的时候ssh加了双引号导致的无法识别ssh，体现在qemu://node2-heartbeat/system这个位置，没有变成qemu+ssh</code></pre><p>3、</p><pre><code>[root@node2 .ssh]# virsh --connect=qemu:///system --quiet migrate --live  Centos7.9 qemu+ssh://node1-heartbeat/systemerror: unable to connect to server at &#39;node1:49152&#39;: No route to host</code></pre><p>经测试，由于主机重启后防火墙端口未被放开，导致无法建立连接，添加相关端口即可</p><pre><code>[root@kvm2 ~]# setsebool -P virt_use_nfs 1[root@kvm2 ~]# iptables -I INPUT -p tcp --dport 49152:49215 -j ACCEPT[root@kvm2 ~]# iptables -I INPUT -p tcp --dport 16509 -j ACCEPT</code></pre><h1><span id="7-配置stonthipmi">7、配置STONTH（IPMI）</span></h1><p><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/91.jpg"><br><img src="/images/KVM/%E5%9F%BA%E4%BA%8ENFS%E7%9A%84KVM%E7%BE%A4%E9%9B%86%E6%9E%84%E5%BB%BA/92.jpg"></p><blockquote><p>！ 由于当前宿主机使用的是vmware中的虚拟机，所以配置完成后是无法使用的，但是实体服务器可以进行相关配置</p></blockquote><pre><code>[root@node1 qemu_config]# yum -y install fence-agents-ipmilan #所有节点均要执行此命令[root@node1 qemu_config]# pcs stonith list  #查看列表fence_amt_ws - Fence agent for AMT (WS)fence_apc - Fence agent for APC over telnet/sshfence_apc_snmp - Fence agent for APC, Tripplite PDU over SNMPfence_bladecenter - Fence agent for IBM BladeCenterfence_brocade - Fence agent for HP Brocade over telnet/sshfence_cisco_mds - Fence agent for Cisco MDSfence_cisco_ucs - Fence agent for Cisco UCSfence_compute - Fence agent for the automatic resurrection of OpenStack compute instancesfence_drac5 - Fence agent for Dell DRAC CMC/5fence_eaton_snmp - Fence agent for Eaton over SNMPfence_emerson - Fence agent for Emerson over SNMPfence_eps - Fence agent for ePowerSwitchfence_evacuate - Fence agent for the automatic resurrection of OpenStack compute instancesfence_heuristics_ping - Fence agent for ping-heuristic based fencingfence_hpblade - Fence agent for HP BladeSystemfence_ibmblade - Fence agent for IBM BladeCenter over SNMPfence_idrac - Fence agent for IPMIfence_ifmib - Fence agent for IF MIBfence_ilo - Fence agent for HP iLOfence_ilo2 - Fence agent for HP iLOfence_ilo3 - Fence agent for IPMIfence_ilo3_ssh - Fence agent for HP iLO over SSHfence_ilo4 - Fence agent for IPMIfence_ilo4_ssh - Fence agent for HP iLO over SSHfence_ilo5 - Fence agent for IPMIfence_ilo5_ssh - Fence agent for HP iLO over SSHfence_ilo_moonshot - Fence agent for HP Moonshot iLOfence_ilo_mp - Fence agent for HP iLO MPfence_ilo_ssh - Fence agent for HP iLO over SSHfence_imm - Fence agent for IPMIfence_intelmodular - Fence agent for Intel Modularfence_ipdu - Fence agent for iPDU over SNMPfence_ipmilan - Fence agent for IPMIfence_kdump - fencing agent for use with kdump crash recovery servicefence_mpath - Fence agent for multipath persistent reservationfence_redfish - I/O Fencing agent for Redfishfence_rhevm - Fence agent for RHEV-M REST APIfence_rsa - Fence agent for IBM RSAfence_rsb - I/O Fencing agent for Fujitsu-Siemens RSBfence_sbd - Fence agent for sbdfence_scsi - Fence agent for SCSI persistent reservationfence_virt - Fence agent for virtual machinesfence_vmware_rest - Fence agent for VMware REST APIfence_vmware_soap - Fence agent for VMWare over SOAP APIfence_wti - Fence agent for WTIfence_xvm - Fence agent for virtual machines[root@node1 qemu_config]# pcs stonith describe fence_ipmilan #查看使用帮助fence_ipmilan - Fence agent for IPMIfence_ipmilan is an I/O Fencing agentwhich can be used with machines controlled by IPMI.This agent calls support software ipmitool (http://ipmitool.sf.net/). WARNING! This fence agent might report success before the node is powered off. You should use -m/method onoff if your fence device works correctly with that option.Stonith options:  auth: IPMI Lan Auth type.  cipher: Ciphersuite to use (same as ipmitool -C parameter)  hexadecimal_kg: Hexadecimal-encoded Kg key for IPMIv2 authentication  ipaddr: IP address or hostname of fencing device  ipport: TCP/UDP port to use for connection with device  lanplus: Use Lanplus to improve security of connection  login: Login name  method: Method to fence  passwd: Login password or passphrase  passwd_script: Script to run to retrieve password  port: IP address or hostname of fencing device (together with --port-as-ip)  privlvl: Privilege level on IPMI device  target: Bridge IPMI requests to the remote target address  quiet: Disable logging to stderr. Does not affect --verbose or --debug-file or logging to syslog.  verbose: Verbose mode  debug: Write debug information to given file  delay: Wait X seconds before fencing is started  ipmitool_path: Path to ipmitool binary  login_timeout: Wait X seconds for cmd prompt after login  port_as_ip: Make &quot;port/plug&quot; to be an alias to IP address  power_timeout: Test X seconds for status change after ON/OFF  power_wait: Wait X seconds after issuing ON/OFF  shell_timeout: Wait X seconds for cmd prompt after issuing command  retry_on: Count of attempts to retry power on  sudo: Use sudo (without password) when calling 3rd party software  sudo_path: Path to sudo binary  pcmk_host_map: A mapping of host names to ports numbers for devices that do not support host names. Eg. node1:1;node2:2,3 would tell the                 cluster to use port 1 for node1 and ports 2 and 3 for node2  pcmk_host_list: A list of machines controlled by this device (Optional unless pcmk_host_check=static-list).  pcmk_host_check: How to determine which machines are controlled by the device. Allowed values: dynamic-list (query the device via the                   &#39;list&#39; command), static-list (check the pcmk_host_list attribute), status (query the device via the &#39;status&#39; command),                   none (assume every device can fence every machine)  pcmk_delay_max: Enable a random delay for stonith actions and specify the maximum of random delay. This prevents double fencing when using                  slow devices such as sbd. Use this to enable a random delay for stonith actions. The overall delay is derived from this                  random delay value adding a static delay so that the sum is kept below the maximum delay.  pcmk_delay_base: Enable a base delay for stonith actions and specify base delay value. This prevents double fencing when different delays                   are configured on the nodes. Use this to enable a static delay for stonith actions. The overall delay is derived from a                   random delay value adding this static delay so that the sum is kept below the maximum delay.  pcmk_action_limit: The maximum number of actions can be performed in parallel on this device Pengine property concurrent-fencing=true needs                     to be configured first. Then use this to specify the maximum number of actions can be performed in parallel on this                     device. -1 is unlimited.Default operations:  monitor: interval=60s[root@node1 ~]# pcs cluster cib s_cfg #导出配置文件到s_cfg通过下列命令修改导出的配置文件[root@node1 ~]# pcs -f s_cfg stonith create ipmi-fencing fence_ipmilan \pcmk_host_list=&quot;node1-heartbeat node2-heartbeat&quot; \ipaddr=10.0.1.1 \login=testuser \passwd=123456 \op monitor interval=60s[root@node1 ~]# pcs -f s_cfg status #查看修改后的配置文件Stack: corosyncCurrent DC: node1-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 18:05:54 2023Last change: Sat Apr 15 17:48:01 2023 by root via crm_resource on node1-heartbeat2 nodes configured2 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): Started node1-heartbeat ipmi-fencing   (stonith:fence_ipmilan):    Stopped[root@node1 ~]# pcs -f a_cfg property set stonith-enabled=true #修改 stonith-enabled=true[root@node1 ~]# pcs cluster cib-push s_cfg  #上传到运行中的集群CIB updated[root@node1 ~]# pcs status  #查看集群状态Cluster name: cluster1Stack: corosyncCurrent DC: node1-heartbeat (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorumLast updated: Sat Apr 15 18:21:56 2023Last change: Sat Apr 15 18:21:49 2023 by root via cibadmin on node1-heartbeat2 nodes configured2 resource instances configuredOnline: [ node1-heartbeat node2-heartbeat ]Full list of resources: Centos7.9_res  (ocf::heartbeat:VirtualDomain): Started node1-heartbeat ipmi-fencing   (stonith:fence_ipmilan):    Starting node2-heartbeatDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KVM </tag>
            
            <tag> Linux HA </tag>
            
            <tag> NFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph-安装</title>
      <link href="/2023/04/11/Ceph/3.Ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/Ceph-%E5%AE%89%E8%A3%85/"/>
      <url>/2023/04/11/Ceph/3.Ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/Ceph-%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E7%8E%AF%E5%A2%83%E6%83%85%E5%86%B5">环境情况</a></li><li><a href="#%E4%B8%80-%E4%BF%AE%E6%94%B9hosts%E6%96%87%E4%BB%B6">一、修改HOSTS文件</a></li><li><a href="#%E4%BA%8C-%E9%85%8D%E7%BD%AE%E6%97%A0%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95">二、配置无密码登录</a></li><li><a href="#%E4%B8%89-%E5%AE%89%E5%85%A8%E8%AE%BE%E7%BD%AE">三、安全设置</a><ul><li><a href="#%E4%B8%80-%E5%85%B3%E9%97%ADselinux">一、关闭Selinux</a></li><li><a href="#%E4%BA%8C-%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99">二、关闭防火墙</a></li></ul></li><li><a href="#%E5%9B%9B-ntp%E8%AE%BE%E7%BD%AE">四、NTP设置</a></li><li><a href="#%E4%BA%94-%E9%85%8D%E7%BD%AEyum%E6%BA%90">五、配置Yum源</a></li><li><a href="#%E5%85%AD-%E5%AE%89%E8%A3%85ceph-deploy">六、安装Ceph-deploy</a></li><li><a href="#%E4%B8%83-%E5%AE%89%E8%A3%85mon%E8%8A%82%E7%82%B9">七、安装Mon节点</a><ul><li><a href="#%E4%B8%80-%E9%87%8D%E7%BD%AE%E9%9B%86%E7%BE%A4">一、重置集群</a></li><li><a href="#%E4%BA%8C-%E5%AE%89%E8%A3%85%E7%9B%B8%E5%85%B3%E8%BD%AF%E4%BB%B6">二、安装相关软件</a></li></ul></li><li><a href="#%E4%B8%89-mon-%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%8A%E5%AE%89%E8%A3%85mgr">三、Mon 初始化及安装MGR</a></li><li><a href="#%E5%85%AB-%E9%83%A8%E7%BD%B2osd%E8%8A%82%E7%82%B9">八、部署OSD节点</a></li><li><a href="#%E4%B9%9D-%E6%89%A9%E5%B1%95mon%E5%92%8Cmgr">九、扩展MON和MGR</a><ul><li><a href="#%E4%B8%80-mon">一、Mon</a></li><li><a href="#%E4%BA%8C-mgr">二、MGR</a></li></ul></li><li><a href="#%E5%8D%81-%E5%88%9B%E5%BB%BA%E8%B5%84%E6%BA%90%E6%B1%A0pool">十、创建资源池Pool</a></li></ul><!-- tocstop --><h1><span id="环境情况">环境情况</span></h1><table>    <tr>        <td>操作系统</td>        <td>公共网络</td>        <td>集群网络</td>        <td>节点名称</td>    </tr>    <tr>        <td rowspan="4">Centos 7.9</td>    </tr>        <tr>            <td>192.168.187.201</td>            <td>192.168.199.201</td>            <td>node-1</td>        </tr>        <tr>            <td>192.168.187.202</td>            <td>192.168.199.202</td>            <td>node-2</td>        </tr>        <tr>            <td>192.168.187.203</td>            <td>192.168.199.203</td>            <td>node-2</td>        </tr></table><h1><span id="一-修改hosts文件">一、修改HOSTS文件</span></h1><p><strong>所有主机均要修改</strong></p><pre><code>[root@node-1 ~]# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.199.201 node-1192.168.199.202 node-2192.168.199.203 node-3</code></pre><h1><span id="二-配置无密码登录">二、配置无密码登录</span></h1><p><strong>Node-1：</strong></p><pre><code>[root@node-1 ~]# ssh-keygen Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Created directory &#39;/root/.ssh&#39;.Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:RNd/EfeRh3t7C73PD//lsUVTQpf84hKM5Dy3CdWj6iQ root@node-1The keys randomart image is:+---[RSA 2048]----+|        . .. .o+*||       . .. o.+*=||        .+ + o.o*||       .  * = +.=||        S  = =.=o||        E o +..+o||         +   .o.*||          .    B=||               .@|+----[SHA256]-----+[root@node-1 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub node-2/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;The authenticity of host &#39;node-2 (192.168.199.202)&#39; can t be established.ECDSA key fingerprint is SHA256:ELWxoLpZlehIUk5OpR5CO/OmzpHyNco8PkV4ztDCV7w.ECDSA key fingerprint is MD5:62:87:b6:99:64:b1:b2:d3:40:f1:d5:e8:d3:a1:8a:18.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@node-2 s password: Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#39;node-2&#39;&quot;and check to make sure that only the key(s) you wanted were added.[root@node-1 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub node-3/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;The authenticity of host &#39;node-3 (192.168.199.203)&#39; can t be established.ECDSA key fingerprint is SHA256:ELWxoLpZlehIUk5OpR5CO/OmzpHyNco8PkV4ztDCV7w.ECDSA key fingerprint is MD5:62:87:b6:99:64:b1:b2:d3:40:f1:d5:e8:d3:a1:8a:18.Are you sure you want to continue connecting (yes/no)? yes/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@node-3 s password: Number of key(s) added: 1Now try logging into the machine, with:   &quot;ssh &#39;node-3&#39;&quot;and check to make sure that only the key(s) you wanted were added.</code></pre><h1><span id="三-安全设置">三、安全设置</span></h1><h2><span id="一-关闭selinux">一、关闭Selinux</span></h2><p><strong>所有节点执行，将enforcing修改成disabled：</strong></p><pre><code>[root@node-1 ~]# cat /etc/selinux/config # This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:#     enforcing - SELinux security policy is enforced.#     permissive - SELinux prints warnings instead of enforcing.#     disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of three values:#     targeted - Targeted processes are protected,#     minimum - Modification of targeted policy. Only selected processes are protected. #     mls - Multi Level Security protection.SELINUXTYPE=targeted[root@node-1 ~]# setenforce 0 #临时关闭[root@node-1 ~]# getenforce #查看状态Disabled</code></pre><h2><span id="二-关闭防火墙">二、关闭防火墙</span></h2><p><strong>所有节点均执行：</strong></p><pre><code>[root@node-1 ~]# firewall-cmd --list-all #查看防火墙状态public (active)  target: default  icmp-block-inversion: no  interfaces: ens33 ens36  sources:   services: dhcpv6-client ssh  ports:   protocols:   masquerade: no  forward-ports:   source-ports:   icmp-blocks:   rich rules: [root@node-1 ~]# systemctl disable firewalld #禁止开机自启Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.[root@node-1 ~]# systemctl stop firewalld #关闭防火墙[root@node-3 ~]# firewall-cmd --list-all #查看状态FirewallD is not running</code></pre><h1><span id="四-ntp设置">四、NTP设置</span></h1><p><strong>所有节点执行：</strong></p><pre><code>[root@node-3 ~]# cd /etc/yum.repos.d/[root@node-3 yum.repos.d]# lsCentOS-Base.repo  CentOS-Debuginfo.repo  CentOS-Media.repo    CentOS-Vault.repoCentOS-CR.repo    CentOS-fasttrack.repo  CentOS-Sources.repo  CentOS-x86_64-kernel.repo[root@node-3 yum.repos.d]# rm -rf *[root@node-3 yum.repos.d]# ls[root@node-1 yum.repos.d]# curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  2523  100  2523    0     0   6577      0 --:--:-- --:--:-- --:--:--  6587[root@node-1 yum.repos.d]# [root@node-1 yum.repos.d]# lsCentOS-Base.repo[root@node-1 yum.repos.d]# yum clean all &amp;&amp; yum makecache #清除缓存[root@node-1 yum.repos.d]# yum install ntp -y #安装NTP[root@node-1 yum.repos.d]# systemctl restart ntpd #启动NTP服务[root@node-1 yum.repos.d]# systemctl enable ntpd #设置开机自启动Created symlink from /etc/systemd/system/multi-user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.[root@node-1 yum.repos.d]# ntpq -pn #查看NTP状态     remote           refid      st t when poll reach   delay   offset  jitter==============================================================================+78.46.102.180   176.9.157.12     3 u   33   64    1  245.918   17.006  14.216*144.76.76.107   192.53.103.103   2 u   33   64    1  214.840    0.073   0.324 193.182.111.14  192.36.143.153   2 u   67   64    1  293.490   -4.158   1.037 [root@node-2 ~]# crontab -l #使用crontab -e 编辑，每分钟同步一次时间*/1 * * * * /usr/sbin/ntpdate node-1;/sbin/hwclock -w</code></pre><p><strong>node-2、node-3执行：</strong></p><pre><code>[root@node-3 yum.repos.d]# cat /etc/ntp.conf # For more information about this file, see the man pages# ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).driftfile /var/lib/ntp/drift# Permit time synchronization with our time source, but do not# permit the source to query or modify the service on this system.restrict default nomodify notrap nopeer noquery# Permit all access over the loopback interface.  This could# be tightened as well, but to do so would effect some of# the administrative functions.restrict 127.0.0.1 restrict ::1# Hosts on local network are less restricted.#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http://www.pool.ntp.org/join.html).server node-1 iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst#broadcast 192.168.1.255 autokey    # broadcast server#broadcastclient            # broadcast client#broadcast 224.0.1.1 autokey        # multicast server#multicastclient 224.0.1.1        # multicast client#manycastserver 239.255.254.254        # manycast server#manycastclient 239.255.254.254 autokey # manycast client# Enable public key cryptography.#cryptoincludefile /etc/ntp/crypto/pw# Key file containing the keys and key identifiers used when operating# with symmetric key cryptography. keys /etc/ntp/keys# Specify the key identifiers which are trusted.#trustedkey 4 8 42# Specify the key identifier to use with the ntpdc utility.#requestkey 8# Specify the key identifier to use with the ntpq utility.#controlkey 8# Enable writing of statistics records.#statistics clockstats cryptostats loopstats peerstats# Disable the monitoring facility to prevent amplification attacks using ntpdc# monlist command when default restrict does not include the noquery flag. See# CVE-2013-5211 for more details.# Note: Monitoring will not be disabled with the limited restriction flag.disable monitor</code></pre><h1><span id="五-配置yum源">五、配置Yum源</span></h1><p><strong>所有节点执行：</strong></p><pre><code>[root@node-3 ~]# cd /etc/yum.repos.d/[root@node-3 yum.repos.d]# lsCentOS-Base.repo  CentOS-Debuginfo.repo  CentOS-Media.repo    CentOS-Vault.repoCentOS-CR.repo    CentOS-fasttrack.repo  CentOS-Sources.repo  CentOS-x86_64-kernel.repo[root@node-3 yum.repos.d]# rm -rf *[root@node-3 yum.repos.d]# ls[root@node-1 yum.repos.d]# curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  2523  100  2523    0     0   6577      0 --:--:-- --:--:-- --:--:--  6587[root@node-1 yum.repos.d]# [root@node-1 yum.repos.d]# lsCentOS-Base.repo[root@node-1 yum.repos.d]# curl -o /etc/yum.repos.d/epel.repo https://mirrors.aliyun.com/repo/epel-7.repo  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   664  100   664    0     0   2193      0 --:--:-- --:--:-- --:--:--  2191[root@node-1 yum.repos.d]# [root@node-1 yum.repos.d]# lsCentOS-Base.repo  epel.repo[root@node-1 yum.repos.d]# cat ceph.repo  #手动创建ceph.repo文件内容如下[norch]name=norchbaseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch/enabled=1gpgcheck=0[x86_64]name=x86 64baseurl=https://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/enabled=1gpgcheck=0[root@node-1 yum.repos.d]# yum clean all &amp;&amp; yum makecache #清除缓存[root@node-1 yum.repos.d]# yum update -y</code></pre><h1><span id="六-安装ceph-deploy">六、安装Ceph-deploy</span></h1><p><strong>Node1执行</strong></p><pre><code>[root@node-1 yum.repos.d]# yum install python-setuptools ceph-deploy -y #安装核心软件[root@node-1 yum.repos.d]# ceph-deploy --version #查看版本2.0.1</code></pre><h1><span id="七-安装mon节点">七、安装Mon节点</span></h1><h2><span id="一-重置集群">一、重置集群</span></h2><p><strong>如果安装失败或者重新安装时执行：</strong></p><pre><code>ceph-deploy purge &#123;ceph-node&#125; [&#123;ceph-node&#125;]ceph-deploy purgedata  &#123;ceph-node&#125; [&#123;ceph-node&#125;]ceph-deploy forgetkeysrm ceph.*</code></pre><h2><span id="二-安装相关软件">二、安装相关软件</span></h2><p><strong>Node1执行：</strong></p><pre><code>[root@node-1 yum.repos.d]# cd /opt/[root@node-1 opt]# mkdir my-cluster[root@node-1 opt]# cd my-cluster/[root@node-1 my-cluster]# ceph-deploy new --public-network 192.168.187.0/24 --cluster-network 192.168.199.0/24 node-1</code></pre><p><strong>公共网络是外部访问集群时使用的，集群网络时内部同步使用的</strong><br><strong>所有节点执行：</strong></p><pre><code>[root@node-1 my-cluster]# yum install ceph ceph-mon ceph-mgr ceph-mds ceph-radosgw -y #安装核心软件包</code></pre><h1><span id="三-mon-初始化及安装mgr">三、Mon 初始化及安装MGR</span></h1><pre><code>[root@node-1 my-cluster]# ceph-deploy mon create-initial #初始化Mon[root@node-1 my-cluster]# ceph-deploy admin node-1 node-2 node-3 #推送最新配置到所有节点[root@node-1 my-cluster]# ceph -s  #查看集群状态  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            mon is allowing insecure global_id reclaim  #mon允许不安全的global_id回收   services:    mon: 1 daemons, quorum node-1 (age 2m)    mgr: no daemons active    osd: 0 osds: 0 up, 0 in   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   0 B used, 0 B / 0 B avail    pgs:     [root@node-1 my-cluster]# ceph config set mon auth_allow_insecure_global_id_reclaim false #取消mon允许不安全的global_id回收[root@node-1 my-cluster]# ceph -s   cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 1 daemons, quorum node-1 (age 3m)    mgr: no daemons active    osd: 0 osds: 0 up, 0 in   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   0 B used, 0 B / 0 B avail    pgs:      [root@node-1 my-cluster]# ceph-deploy mgr create node-1 #安装mgr 监控服务[root@node-1 my-cluster]#  ceph -s  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            OSD count 0 &lt; osd_pool_default_size 3   services:    mon: 1 daemons, quorum node-1 (age 6m)    mgr: node-1(active, since 55s) #查看已经成功安装    osd: 0 osds: 0 up, 0 in   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   0 B used, 0 B / 0 B avail    pgs:     </code></pre><h1><span id="八-部署osd节点">八、部署OSD节点</span></h1><p><strong>node-1 执行:</strong></p><pre><code>[root@node-1 my-cluster]# lsblk #确定硬盘NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTsda               8:0    0   20G  0 disk ├─sda1            8:1    0    1G  0 part /boot└─sda2            8:2    0   19G  0 part   ├─centos-root 253:0    0   17G  0 lvm  /  └─centos-swap 253:1    0    2G  0 lvm  [SWAP]sdb               8:16   0   10G  0 disk sdc               8:32   0   10G  0 disk sr0              11:0    1 1024M  0 rom [root@node-1 my-cluster]# ceph-deploy osd create node-1 --data /dev/sdb --journal /dev/sdb # --data指定硬盘  --journal /dev/sdb 指定加速[root@node-1 my-cluster]# ceph-deploy osd create node-2 --data /dev/sdb --journal /dev/sdb # --data指定硬盘  --journal /dev/sdb 指定加速[root@node-1 my-cluster]# ceph-deploy osd create node-3 --data /dev/sdb --journal /dev/sdb # --data指定硬盘  --journal /dev/sdb 指定加速[root@node-1 my-cluster]# ceph osd tree  #查看OSD状态ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF -1       0.02939 root default                            -3       0.00980     host node-1                          0   hdd 0.00980         osd.0       up  1.00000 1.00000 -5       0.00980     host node-2                          1   hdd 0.00980         osd.1       up  1.00000 1.00000 -7       0.00980     host node-3                          2   hdd 0.00980         osd.2       up  1.00000 1.00000 </code></pre><h1><span id="九-扩展mon和mgr">九、扩展MON和MGR</span></h1><p><strong>Mon 使用Paxos算法，一般都是奇数 3、5、7</strong></p><h2><span id="一-mon">一、Mon</span></h2><pre><code>NODE-1执行：ceph-deploy mon add node-2 --address 192.168.187.202 #添加node-2成为Mon 并指定IPceph-deploy mon add node-3 --address 192.168.187.203 #添加node-3成为Mon 并指定IP[root@node-1 my-cluster]# ceph -s  #查看Mon是否添加成功  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 54s) #已经添加node-2和node-3    mgr: node-1(active, since 18m)    osd: 3 osds: 3 up (since 8m), 3 in (since 8m)   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   3.0 GiB used, 27 GiB / 30 GiB avail    pgs: [root@node-1 my-cluster]# ceph quorum_status --format json-pretty #查看Mon 仲裁情况&#123;    &quot;election_epoch&quot;: 12,    &quot;quorum&quot;: [        0,        1,        2    ],    &quot;quorum_names&quot;: [ #可以看到当前3个节点正在参加仲裁        &quot;node-1&quot;,        &quot;node-2&quot;,        &quot;node-3&quot;    ],    &quot;quorum_leader_name&quot;: &quot;node-1&quot;, #当前主Mon是Node-1    &quot;quorum_age&quot;: 234,    &quot;monmap&quot;: &#123;        &quot;epoch&quot;: 3,        &quot;fsid&quot;: &quot;e9a90625-4707-4b6b-b52f-661512ea831d&quot;,        &quot;modified&quot;: &quot;2023-03-03 12:22:43.254681&quot;,        &quot;created&quot;: &quot;2023-03-03 11:59:53.474948&quot;,        &quot;min_mon_release&quot;: 14,        &quot;min_mon_release_name&quot;: &quot;nautilus&quot;,        &quot;features&quot;: &#123;            &quot;persistent&quot;: [                &quot;kraken&quot;,                &quot;luminous&quot;,                &quot;mimic&quot;,                &quot;osdmap-prune&quot;,                &quot;nautilus&quot;            ],            &quot;optional&quot;: []        &#125;,        &quot;mons&quot;: [            &#123;                &quot;rank&quot;: 0,                &quot;name&quot;: &quot;node-1&quot;,                &quot;public_addrs&quot;: &#123;                    &quot;addrvec&quot;: [                        &#123;                            &quot;type&quot;: &quot;v2&quot;,                            &quot;addr&quot;: &quot;192.168.187.201:3300&quot;,                            &quot;nonce&quot;: 0                        &#125;,                        &#123;                            &quot;type&quot;: &quot;v1&quot;,                            &quot;addr&quot;: &quot;192.168.187.201:6789&quot;,                            &quot;nonce&quot;: 0                        &#125;                    ]                &#125;,                &quot;addr&quot;: &quot;192.168.187.201:6789/0&quot;,                &quot;public_addr&quot;: &quot;192.168.187.201:6789/0&quot;            &#125;,            &#123;                &quot;rank&quot;: 1,                &quot;name&quot;: &quot;node-2&quot;,                &quot;public_addrs&quot;: &#123;                    &quot;addrvec&quot;: [                        &#123;                            &quot;type&quot;: &quot;v2&quot;,                            &quot;addr&quot;: &quot;192.168.187.202:3300&quot;,                            &quot;nonce&quot;: 0                        &#125;,                        &#123;                            &quot;type&quot;: &quot;v1&quot;,                            &quot;addr&quot;: &quot;192.168.187.202:6789&quot;,                            &quot;nonce&quot;: 0                        &#125;                    ]                &#125;,                &quot;addr&quot;: &quot;192.168.187.202:6789/0&quot;,                &quot;public_addr&quot;: &quot;192.168.187.202:6789/0&quot;            &#125;,            &#123;                &quot;rank&quot;: 2,                &quot;name&quot;: &quot;node-3&quot;,                &quot;public_addrs&quot;: &#123;                    &quot;addrvec&quot;: [                        &#123;                            &quot;type&quot;: &quot;v2&quot;,                            &quot;addr&quot;: &quot;192.168.187.203:3300&quot;,                            &quot;nonce&quot;: 0                        &#125;,                        &#123;                            &quot;type&quot;: &quot;v1&quot;,                            &quot;addr&quot;: &quot;192.168.187.203:6789&quot;,                            &quot;nonce&quot;: 0                        &#125;                    ]                &#125;,                &quot;addr&quot;: &quot;192.168.187.203:6789/0&quot;,                &quot;public_addr&quot;: &quot;192.168.187.203:6789/0&quot;            &#125;        ]    &#125;&#125;[root@node-1 my-cluster]#  ceph mon stat #查看Mon 状态e3: 3 mons at &#123;node-1=[v2:192.168.187.201:3300/0,v1:192.168.187.201:6789/0],node-2=[v2:192.168.187.202:3300/0,v1:192.168.187.202:6789/0],node-3=[v2:192.168.187.203:3300/0,v1:192.168.187.203:6789/0]&#125;, election epoch 12, leader 0 node-1, quorum 0,1,2 node-1,node-2,node-3[root@node-1 my-cluster]# ceph mon dump #查看Mon 状态epoch 3fsid e9a90625-4707-4b6b-b52f-661512ea831dlast_changed 2023-03-03 12:22:43.254681created 2023-03-03 11:59:53.474948min_mon_release 14 (nautilus)0: [v2:192.168.187.201:3300/0,v1:192.168.187.201:6789/0] mon.node-11: [v2:192.168.187.202:3300/0,v1:192.168.187.202:6789/0] mon.node-22: [v2:192.168.187.203:3300/0,v1:192.168.187.203:6789/0] mon.node-3dumped monmap epoch 3</code></pre><h2><span id="二-mgr">二、MGR</span></h2><p><strong>MGR默认是主备模式</strong></p><p><strong>node-1：执行</strong></p><pre><code>[root@node-1 my-cluster]#  ceph-deploy mgr create node-2 node-3 #添加node-2 node-3 成为mgr[root@node-1 my-cluster]#  ceph -s  #查看MGR状态  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 11m)    mgr: node-1(active, since 28m), standbys: node-2, node-3 #可以看到备MGR为：node-2和node-3    osd: 3 osds: 3 up (since 18m), 3 in (since 18m)   data:    pools:   0 pools, 0 pgs    objects: 0 objects, 0 B    usage:   3.0 GiB used, 27 GiB / 30 GiB avail    pgs:[root@node-1 my-cluster]# ceph mgr dump #查看具体MAP图</code></pre><h1><span id="十-创建资源池pool">十、创建资源池Pool</span></h1><pre><code>Node-1执行：[root@node-1 my-cluster]# ceph osd lspools #查看当前pool[root@node-1 my-cluster]# ceph osd pool create ceph-demo 64 64 #创建一个叫做ceph-demo的pool 并指定PG数为64（第一个数字），pgp数64（第二个数字）pool &#39;ceph-demo&#39; created[root@node-1 my-cluster]# ceph osd lspools #再次查看当前pool 新增了要给ceph-demo的pool1 ceph-demo[root@node-1 my-cluster]#  ceph osd pool get ceph-demo pg_num #查看ceph-demo的PG数量pg_num: 64[root@node-1 my-cluster]#  ceph osd pool get ceph-demo pgp_num #查看ceph-demo的PGP数量pgp_num: 64[root@node-1 my-cluster]#  ceph osd pool get ceph-demo size #查看ceph-demo的副本数size: 3[root@node-1 my-cluster]#  ceph osd pool get ceph-demo crush_rule #查看crush的调度算法crush_rule: replicated_rule #默认的复制规则[root@node-1 my-cluster]#   ceph osd pool set ceph-demo size 2  #可以通过set的方式调整副本数量为2set pool 1 size to 2[root@node-1 my-cluster]# ceph osd pool set ceph-demo pg_num 128 #可以通过set的方式调整pg数量为128set pool 1 pg_num to 128[root@node-1 my-cluster]# ceph osd pool set ceph-demo pgp_num 128 #可以通过set的方式调整pgp数量为128,PGP数量应该与PG数一致set pool 1 pgp_num to 12[root@node-1 my-cluster]# rbd pool init &lt;pool_name&gt; #初始化pool</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RGW对象存储</title>
      <link href="/2023/04/11/Ceph/5.RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/"/>
      <url>/2023/04/11/Ceph/5.RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#ceph-%E5%AF%B9%E8%B1%A1%E7%BD%91%E5%85%B3%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D">CEPH 对象网关架构介绍</a></li><li><a href="#%E9%83%A8%E7%BD%B2rgw">部署RGW</a><ul><li><a href="#%E6%9F%A5%E7%9C%8B%E5%B8%AE%E5%8A%A9%E6%96%87%E6%A1%A3">查看帮助文档</a></li><li><a href="#%E4%BD%BF%E7%94%A8node-1%E4%BD%9C%E4%B8%BA%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3">使用node-1作为对象存储网关</a></li><li><a href="#%E6%A3%80%E6%9F%A5%E6%98%AF%E5%90%A6%E6%B7%BB%E5%8A%A0%E6%88%90%E5%8A%9F">检查是否添加成功</a></li><li><a href="#%E6%A3%80%E6%9F%A5%E6%9C%8D%E5%8A%A1%E8%BF%90%E8%A1%8C%E6%83%85%E5%86%B5">检查服务运行情况</a></li></ul></li><li><a href="#%E4%BF%AE%E6%94%B9rgw%E9%BB%98%E8%AE%A4%E7%AB%AF%E5%8F%A3">修改RGW默认端口</a><ul><li><a href="#%E4%BF%AE%E6%94%B9%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">修改节点配置文件</a></li><li><a href="#%E5%B0%86%E9%85%8D%E7%BD%AE%E5%A5%BD%E7%9A%84cephconf-%E6%8E%A8%E9%80%81%E5%88%B0%E6%89%80%E6%9C%89%E8%8A%82%E7%82%B9%E4%B8%AD">将配置好的ceph.conf 推送到所有节点中</a></li><li><a href="#%E9%87%8D%E5%90%AFrgw%E6%9C%8D%E5%8A%A1">重启rgw服务</a></li><li><a href="#%E6%A0%A1%E9%AA%8C%E9%85%8D%E7%BD%AE%E6%98%AF%E5%90%A6%E7%94%9F%E6%95%88-%E5%8F%AF%E4%BF%AE%E6%94%B9%E6%88%90443%E7%AB%AF%E5%8F%A3%E6%B7%BB%E5%8A%A0%E8%AF%81%E4%B9%A6%E5%8F%AF%E4%BD%BF%E7%94%A8%E7%99%BE%E5%BA%A6%E8%BF%9B%E8%A1%8C%E6%9F%A5%E8%AF%A2%E5%85%B7%E4%BD%93%E9%85%8D%E7%BD%AE">校验配置是否生效 可修改成443端口，添加证书可使用百度进行查询具体配置</a></li></ul></li><li><a href="#rgw%E4%B9%8Bs3%E6%8E%A5%E5%8F%A3%E4%BD%BF%E7%94%A8">RGW之s3接口使用</a><ul><li><a href="#%E6%9F%A5%E7%9C%8B%E5%B8%AE%E5%8A%A9%E6%96%87%E6%A1%A3%E4%BD%BF%E7%94%A8user-create%E8%BF%9B%E8%A1%8C%E5%88%9B%E5%BB%BA">查看帮助文档，使用user create进行创建</a></li><li><a href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%98%BE%E7%A4%BA%E5%90%8D%E4%B8%BAceph-s3-user-demo%E7%9A%84%E7%94%A8%E6%88%B7%E5%B9%B6%E6%8C%87%E5%AE%9Auid%E4%B8%BAceph-s3-user">创建一个显示名为Ceph S3 User Demo的用户，并指定uid为ceph-s3-user</a></li><li><a href="#%E4%BF%9D%E5%AD%98%E7%94%A8%E6%88%B7%E5%92%8C%E5%AF%86%E7%A0%81%E4%BF%A1%E6%81%AF">保存用户和密码信息</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8Dradosgw%E5%88%9B%E5%BB%BA%E7%9A%84%E7%94%A8%E6%88%B7">查看当前radosgw创建的用户</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E7%94%A8%E6%88%B7%E7%9A%84%E5%85%B7%E4%BD%93%E4%BF%A1%E6%81%AF">查看用户的具体信息</a></li><li><a href="#%E5%AE%89%E8%A3%85python-boto%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%8C%85">安装python-boto的软件包</a></li><li><a href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAs3-clientpy%E7%9A%84python%E8%84%9A%E6%9C%AC">创建一个s3-client.py的python脚本</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%9A%84pool">查看自动生成的pool</a></li><li><a href="#%E5%88%9B%E5%BB%BAceph-s3-bucket%E7%9A%84bucket">创建ceph-s3-bucket的bucket</a></li><li><a href="#%E5%86%8D%E6%AC%A1%E6%9F%A5%E7%9C%8B%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%9A%84pool">再次查看自动生成的pool</a></li><li><a href="#%E5%AE%89%E8%A3%85s3cmd%E7%9A%84%E5%B7%A5%E5%85%B7%E5%AE%9E%E7%8E%B0%E4%B8%8A%E4%BC%A0%E4%B8%8B%E8%BD%BD%E5%8A%9F%E8%83%BD">安装s3cmd的工具实现上传下载功能</a></li><li><a href="#%E9%85%8D%E7%BD%AEs3cmd%E7%9A%84%E5%B7%A5%E5%85%B7">配置s3cmd的工具</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E7%94%9F%E6%88%90%E7%9A%84%E6%96%87%E4%BB%B6">查看生成的文件</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8Dbuck%E6%83%85%E5%86%B5">查看当前buck情况</a></li><li><a href="#%E5%88%9B%E5%BB%BAbucket">创建bucket</a></li><li><a href="#%E6%9F%A5%E7%9C%8Bbucket">查看bucket</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6">上传文件</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E5%A4%B9">上传文件夹</a></li><li><a href="#%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6">下载文件</a></li><li><a href="#%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6">删除文件</a></li><li><a href="#%E5%88%A0%E9%99%A4%E7%9B%AE%E5%BD%95">删除目录</a></li><li><a href="#%E6%9F%A5%E7%9C%8B%E5%AD%98%E5%82%A8%E6%B1%A0%E5%8F%98%E5%8C%96">查看存储池变化</a></li></ul></li><li><a href="#rgw%E4%B9%8Bswift%E6%8E%A5%E5%8F%A3%E4%BD%BF%E7%94%A8">RGW之Swift接口使用</a><ul><li><a href="#%E6%9F%A5%E7%9C%8B%E5%BD%93%E5%89%8D%E7%94%A8%E6%88%B7">查看当前用户</a></li><li><a href="#%E6%A0%B9%E6%8D%AE%E5%BD%93%E5%89%8D%E7%94%A8%E6%88%B7%E5%88%9B%E5%BB%BAswift%E7%94%A8%E6%88%B7">根据当前用户创建swift用户</a></li><li><a href="#%E7%94%9F%E6%88%90swift%E7%9A%84key">生成Swift的key</a></li><li><a href="#%E5%AE%89%E8%A3%85swift%E5%AE%A2%E6%88%B7%E7%AB%AF">安装Swift客户端</a></li><li><a href="#%E6%9F%A5%E7%9C%8Bbucket-1">查看bucket</a></li><li><a href="#%E5%88%9B%E5%BB%BAswift%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E6%96%87%E4%BB%B6">创建Swift环境变量文件</a></li><li><a href="#%E9%AA%8C%E8%AF%81%E5%8F%98%E9%87%8F%E6%98%AF%E5%90%A6%E7%94%9F%E6%95%88">验证变量是否生效</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6-1">上传文件</a></li><li><a href="#%E4%B8%8A%E4%BC%A0%E7%9B%AE%E5%BD%95">上传目录</a></li><li><a href="#%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6-1">下载文件</a></li><li><a href="#%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6-1">删除文件</a></li></ul></li></ul><!-- tocstop --><h1><span id="ceph-对象网关架构介绍">CEPH 对象网关架构介绍</span></h1><p>Ceph 对象网关是建立在 librados. 它在应用程序和 Ceph 存储集群之间提供了一个 RESTful 网关。Ceph 对象存储支持两种接口：</p><ol><li>S3 兼容：通过与 Amazon S3 RESTful API 的大部分子集兼容的接口提供对象存储功能。</li><li>Swift 兼容：通过与 OpenStack Swift API 的大部分子集兼容的接口提供对象存储功能。</li></ol><p>Ceph 对象存储使用 Ceph 对象网关守护进程 ( radosgw)，这是一个设计用于与 Ceph 存储集群交互的 HTTP 服务器。</p><p>Ceph 对象网关提供与 Amazon S3 和 OpenStack Swift 兼容的接口，并且有自己的用户管理。Ceph 对象网关可以将数据存储在同一个 Ceph 存储集群中，其中存储了来自 Ceph 文件系统客户端和 Ceph 块设备客户端的数据。S3 API 和 Swift API 共享一个公共命名空间，这使得可以使用一个 API 将数据写入 Ceph 存储集群，然后使用另一个 API 检索该数据。</p><p><img src="/images/Ceph/RGW%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3/1.jpg"></p><blockquote><p>Ceph 对象存储不使用Ceph 元数据服务器。</p></blockquote><h1><span id="部署rgw">部署RGW</span></h1><h2><span id="查看帮助文档">查看帮助文档</span></h2><pre><code>[root@node-1 my-cluster]# ceph-deploy rgw -h  usage: ceph-deploy rgw [-h] &#123;create&#125; ...Ceph RGW daemon managementpositional arguments:  &#123;create&#125;    create    Create an RGW instanceoptional arguments:  -h, --help  show this help message and exit</code></pre><h2><span id="使用node-1作为对象存储网关">使用node-1作为对象存储网关</span></h2><pre><code>[root@node-1 my-cluster]# ceph-deploy rgw create [HOST[:NAME] ...]usage: ceph-deploy rgw create [-h] HOST[:NAME] [HOST[:NAME] ...][root@node-1 my-cluster]# ceph-deploy rgw create node-1</code></pre><h2><span id="检查是否添加成功">检查是否添加成功</span></h2><pre><code>[root@node-1 my-cluster]# ceph -s  #查看集群多了一个rgw的对象网关  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_OK   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 32s)    mgr: node-1(active, since 18h), standbys: node-2, node-3    osd: 3 osds: 3 up (since 2h), 3 in (since 5h)    rgw: 1 daemon active (node-1)   task status:   data:    pools:   5 pools, 192 pgs    objects: 468 objects, 1.0 GiB    usage:   6.2 GiB used, 24 GiB / 30 GiB avail    pgs:     192 active+clean[root@node-1 my-cluster]# netstat -antupl | grep 7480 #查看监听端口tcp        0      0 0.0.0.0:7480            0.0.0.0:*               LISTEN      66157/radosgw       tcp6       0      0 :::7480                 :::*                    LISTEN      66157/radosgw [root@node-1 my-cluster]#  yum whatprovides &quot;*bin/netstat&quot; #如果netstat执行失败可查看是否没有安装net-toolsLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile * base: mirrors.aliyun.com * extras: mirrors.aliyun.com * updates: mirrors.aliyun.comnet-tools-2.0-0.25.20131004git.el7.x86_64 : Basic networking toolsRepo        : baseMatched from:Filename    : /bin/netstatnet-tools-2.0-0.25.20131004git.el7.x86_64 : Basic networking toolsRepo        : @baseMatched from:Filename    : /bin/netsta[root@node-1 my-cluster]#  yum install -y net-tools</code></pre><h2><span id="检查服务运行情况">检查服务运行情况</span></h2><pre><code>[root@node-1 my-cluster]# curl http://node-1:7480 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ListAllMyBucketsResult xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;</code></pre><h1><span id="修改rgw默认端口">修改RGW默认端口</span></h1><h2><span id="修改节点配置文件">修改节点配置文件</span></h2><p><strong>node-1节点执行：</strong></p><pre><code>[root@node-1 my-cluster]# cat ceph.conf [global]fsid = e9a90625-4707-4b6b-b52f-661512ea831dpublic_network = 192.168.187.0/24cluster_network = 192.168.199.0/24mon_initial_members = node-1mon_host = 192.168.187.201auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephx[client.rgw.node-1] #新增如下配置</code></pre><h2><span id="将配置好的cephconf-推送到所有节点中">将配置好的ceph.conf 推送到所有节点中</span></h2><blockquote><p>–overwrite-conf 由于其他节点已经有ceph.conf 文件，所以需要加此参数进行覆盖</p></blockquote><p>[root@node-1 my-cluster]# ceph-deploy –overwrite-conf config push node-1 node-2 node-3 </p><h2><span id="重启rgw服务">重启rgw服务</span></h2><pre><code>[root@node-1 my-cluster]#  systemctl restart ceph-radosgw.target</code></pre><h2><span id="校验配置是否生效-可修改成443端口添加证书可使用百度进行查询具体配置">校验配置是否生效 可修改成443端口，添加证书可使用百度进行查询具体配置</span></h2><pre><code>[root@node-1 my-cluster]# netstat -antupl | grep 80 tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      67161/radosgw</code></pre><h1><span id="rgw之s3接口使用">RGW之s3接口使用</span></h1><blockquote><p> <strong>使用对象存储的前提是需要创建用户，一种是S3风格的，一种是swift风格的</strong></p></blockquote><h2><span id="查看帮助文档使用user-create进行创建">查看帮助文档，使用user create进行创建</span></h2><pre><code>[root@node-1 my-cluster]# radosgw-admin -h | grep user   user create                create a new user  user modify                modify user  user info                  get user info  user rm                    remove user  user suspend               suspend a user  user enable                re-enable user after suspension  user check                 check user info  user stats                 show user stats as accounted by quota subsystem  user list                  list users  caps add                   add user capabilities  caps rm                    remove user capabilities  subuser create             create a new subuser  subuser modify             modify subuser  subuser rm                 remove subuser  bucket link                link bucket to specified user  bucket unlink              unlink bucket from specified user  usage show                 show usage (by user, by bucket, date range)  usage trim                 trim usage (by user, by bucket, date range)   --uid=&lt;id&gt;                user id   --subuser=&lt;name&gt;          subuser name   --email=&lt;email&gt;           user&#39;s email address   --access=&lt;access&gt;         Set access permissions for sub-user, should be one   --display-name=&lt;name&gt;     user&#39;s display name   --max-buckets             max number of buckets for a user   --admin                   set the admin flag on the user   --system                  set the system flag on the user   --op-mask                 set the op mask on the user   --purge-data              when specified, user removal will also purge all the                             user data   --purge-keys              when specified, subuser removal will also purge all the                             subuser keys   --sync-stats              option to &#39;user stats&#39;, update user stats with current                             stats reported by user&#39;s buckets indexes   --reset-stats             option to &#39;user stats&#39;, reset stats in accordance with user buckets   --caps=&lt;caps&gt;             list of caps (e.g., &quot;usage=read, write; user=read&quot;)   --quota-scope             scope of quota (bucket, user)  --setuser USER    set uid to user or uid (and gid to user&#39;s gid)</code></pre><h2><span id="创建一个显示名为ceph-s3-user-demo的用户并指定uid为ceph-s3-user">创建一个显示名为Ceph S3 User Demo的用户，并指定uid为ceph-s3-user</span></h2><pre><code>[root@node-1 my-cluster]# radosgw-admin user create --uid ceph-s3-user --display-name &quot;Ceph S3 User Demo&quot; &#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000, #最大只允许创建1000个buckets    &quot;subusers&quot;: [],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;, #用户ID            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;, #SSH的key            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot; #  secret的key        &#125;    ],    &quot;swift_keys&quot;: [],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123; #bucket配额信息        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123; #用户的配额信息        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="保存用户和密码信息">保存用户和密码信息</span></h2><pre><code>[root@node-1 my-cluster]# cat key.txt      &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],</code></pre><h2><span id="查看当前radosgw创建的用户">查看当前radosgw创建的用户</span></h2><pre><code>[root@node-1 my-cluster]#  radosgw-admin user list [    &quot;ceph-s3-user&quot;]</code></pre><h2><span id="查看用户的具体信息">查看用户的具体信息</span></h2><pre><code>[root@node-1 my-cluster]# radosgw-admin user info --uid ceph-s3-user &#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000,    &quot;subusers&quot;: [],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],    &quot;swift_keys&quot;: [],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="安装python-boto的软件包">安装python-boto的软件包</span></h2><pre><code>[root@node-1 my-cluster]# yum install python-boto</code></pre><h2><span id="创建一个s3-clientpy的python脚本">创建一个s3-client.py的python脚本</span></h2><pre><code>[root@node-1 my-cluster]# cat s3-client.pyimport botoimport boto.s3.connectionaccess_key = &#39;5SCX55201QUQ0FOPCPD6&#39; #上述保存的keysecret_key = &#39;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&#39; #上述保存的keyconn = boto.connect_s3(        aws_access_key_id = access_key,        aws_secret_access_key = secret_key,        host = &#39;192.168.187.201&#39;, port=80, #服务器的IP及端口        is_secure=False,calling_format=boto.s3.connection.OrdinaryCallingFormat(),        )bucket = conn.create_bucket(&#39;ceph-s3-bucket&#39;)for bucket in conn.get_all_buckets():        print(&quot;&#123;name&#125;\t&#123;created&#125;&quot;.format(                name = bucket.name,                created = bucket.creation_date,        ))</code></pre><h2><span id="查看自动生成的pool">查看自动生成的pool</span></h2><pre><code>[root@node-1 my-cluster]# ceph osd lspools1 ceph-demo2 .rgw.root3 default.rgw.control4 default.rgw.meta5 default.rgw.log</code></pre><h2><span id="创建ceph-s3-bucket的bucket">创建ceph-s3-bucket的bucket</span></h2><pre><code>[root@node-1 my-cluster]#  python  s3-client.py ceph-s3-bucket    2023-03-04T12:52:01.787Z</code></pre><h2><span id="再次查看自动生成的pool">再次查看自动生成的pool</span></h2><blockquote><p>执行后可以看到多了一个6 default.rgw.buckets.index的pool，写入数据后还会增加一个data的pool</p></blockquote><pre><code>[root@node-1 my-cluster]# ceph osd lspools1 ceph-demo2 .rgw.root #rgw的根pool3 default.rgw.control #rgw的配置pool4 default.rgw.meta #rgw的元数据pool5 default.rgw.log #rgw的日志pool 6 default.rgw.buckets.index #rgw的索引pool</code></pre><h2><span id="安装s3cmd的工具实现上传下载功能">安装s3cmd的工具实现上传下载功能</span></h2><p><code>[root@node-1 my-cluster]#  yum install s3cmd</code></p><h2><span id="配置s3cmd的工具">配置s3cmd的工具</span></h2><pre><code>[root@node-1 my-cluster]# s3cmd --configureEnter new values or accept defaults in brackets with Enter.Refer to user manual for detailed description of all options.Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.Access Key: 5SCX55201QUQ0FOPCPD6 #输入Secret Key: k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm #输入Default Region [US]: Use &quot;s3.amazonaws.com&quot; for S3 Endpoint and not modify it to the 回车target Amazon S3.S3 Endpoint [s3.amazonaws.com]: 192.168.187.201:80 #输入回车Use &quot;%(bucket)s.s3.amazonaws.com&quot; to the target Amazon S3. &quot;%(bucket)s&quot; and &quot;%(location)s&quot; vars can be usedif the target S3 system supports dns based buckets.DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: 192.168.187.201:80/%(bucket)s #输入回车Encryption password is used to protect your files from readingby unauthorized persons while in transfer to S3Encryption password: Path to GPG program [/usr/bin/gpg]: When using secure HTTPS protocol all communication with Amazon S3servers is protected from 3rd party eavesdropping. This method isslower than plain HTTP, and can only be proxied with Python 2.7 or newerUse HTTPS protocol [Yes]: no #输入回车On some networks all internet access must go through a HTTP proxy.Try setting it here if you can&#39;t connect to S3 directlyHTTP Proxy server name: New settings:  Access Key: 5SCX55201QUQ0FOPCPD6  Secret Key: k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm  Default Region: US  S3 Endpoint: 192.168.187.201:80  DNS-style bucket+hostname:port template for accessing a bucket: 192.168.187.201:80/%(bucket)s  Encryption password:   Path to GPG program: /usr/bin/gpg  Use HTTPS protocol: False  HTTP Proxy server name:   HTTP Proxy server port: 0Test access with supplied credentials? [Y/n] y #输入Please wait, attempting to list all buckets...Success. Your access key and secret key worked fine :-)Now verifying that encryption works...Not configured. Never mind.Save settings? [y/N] y #输入Configuration saved to &#39;/root/.s3cfg&#39;You have new mail in /var/spool/mail/root</code></pre><h2><span id="查看生成的文件">查看生成的文件</span></h2><pre><code>[root@node-1 my-cluster]# cat /root/.s3cfg[default]access_key = 5SCX55201QUQ0FOPCPD6access_token = add_encoding_exts = add_headers = bucket_location = USca_certs_file = cache_file = check_ssl_certificate = Truecheck_ssl_hostname = Truecloudfront_host = cloudfront.amazonaws.comconnection_max_age = 5connection_pooling = Truecontent_disposition = content_type = default_mime_type = binary/octet-streamdelay_updates = Falsedelete_after = Falsedelete_after_fetch = Falsedelete_removed = Falsedry_run = Falseenable_multipart = Trueencrypt = Falseexpiry_date = expiry_days = expiry_prefix = follow_symlinks = Falseforce = Falseget_continue = Falsegpg_command = /usr/bin/gpggpg_decrypt = %(gpg_command)s -d --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)sgpg_encrypt = %(gpg_command)s -c --verbose --no-use-agent --batch --yes --passphrase-fd %(passphrase_fd)s -o %(output_file)s %(input_file)sgpg_passphrase = guess_mime_type = Truehost_base = 192.168.187.201:80host_bucket = 192.168.187.201:80/%(bucket)shuman_readable_sizes = Falseinvalidate_default_index_on_cf = Falseinvalidate_default_index_root_on_cf = Trueinvalidate_on_cf = Falsekms_key = limit = -1limitrate = 0list_allow_unordered = Falselist_md5 = Falselog_target_prefix = long_listing = Falsemax_delete = -1mime_type = multipart_chunk_size_mb = 15multipart_copy_chunk_size_mb = 1024multipart_max_chunks = 10000preserve_attrs = Trueprogress_meter = Trueproxy_host = proxy_port = 0public_url_use_https = Falseput_continue = Falserecursive = Falserecv_chunk = 65536reduced_redundancy = Falserequester_pays = Falserestore_days = 1restore_priority = Standardsecret_key = k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldmsend_chunk = 65536server_side_encryption = Falsesignature_v2 = Falsesignurl_use_https = Falsesimpledb_host = sdb.amazonaws.comskip_existing = Falsesocket_timeout = 300ssl_client_cert_file = ssl_client_key_file = stats = Falsestop_on_error = Falsestorage_class = throttle_max = 100upload_id = urlencoding_mode = normaluse_http_expect = Falseuse_https = Falseuse_mime_magic = Trueverbosity = WARNINGwebsite_endpoint = http://%(bucket)s.s3-website-%(location)s.amazonaws.com/website_error = website_index = index.html</code></pre><h2><span id="查看当前buck情况">查看当前buck情况</span></h2><pre><code>[root@node-1 ~]# s3cmd ls 2023-03-04 12:52  s3://ceph-s3-bucket</code></pre><h2><span id="创建bucket">创建bucket</span></h2><blockquote><p>使用cmd创建一个叫做s3cmd-demo的bucket</p></blockquote><pre><code>[root@node-1 ~]# s3cmd mb s3://s3cmd-demo ERROR: S3 error: 403 (SignatureDoesNotMatch) #由于当前设置的版本不一致导致</code></pre><blockquote><p>修改版本</p></blockquote><pre><code>[root@node-1 ~]# vi /root/.s3cfg signature_v2 = True</code></pre><blockquote><p>重新创建</p></blockquote><pre><code>[root@node-1 ~]# s3cmd mb s3://s3cmd-demo Bucket &#39;s3://s3cmd-demo/&#39; created</code></pre><h2><span id="查看bucket">查看bucket</span></h2><pre><code>[root@node-1 ~]# s3cmd ls 2023-03-04 12:52  s3://ceph-s3-bucket2023-03-04 17:38  s3://s3cmd-demo</code></pre><h2><span id="上传文件">上传文件</span></h2><pre><code>[root@node-1 ~]#  s3cmd put /etc/fstab s3://s3cmd-demo/fastab-demo #报错，经查询最简单的解决办法是添加OSD或者修改ceph.conf 文件upload: &#39;/etc/fstab&#39; -&gt; &#39;s3://s3cmd-demo/fastab-demo&#39;  [1 of 1] 465 of 465   100% in    0s   866.93 B/s  doneERROR: S3 error: 416 (InvalidRange)</code></pre><blockquote><p>报错，经查询最简单的解决办法是添加OSD或者修改ceph.conf 文件,原因是当前OSD数量太少；</p></blockquote><blockquote><p>添加OSD</p></blockquote><pre><code>[root@node-1 my-cluster]# ceph-deploy osd create node-1 --data /dev/sdc --journal /dev/sdc[root@node-1 my-cluster]# ceph-deploy osd create node-2 --data /dev/sdc --journal /dev/sdc[root@node-1 my-cluster]# ceph-deploy osd create node-3 --data /dev/sdc --journal /dev/sdc</code></pre><blockquote><p>尝试重新上传</p></blockquote><pre><code>[root@node-1 my-cluster]#  s3cmd put /etc/fstab s3://s3cmd-demo/fastab-demoupload: &#39;/etc/fstab&#39; -&gt; &#39;s3://s3cmd-demo/fastab-demo&#39;  [1 of 1] 465 of 465   100% in    1s   317.00 B/s  done</code></pre><blockquote><p>查看上传的文件</p></blockquote><pre><code>[root@node-1 my-cluster]# s3cmd ls s3://s3cmd-demo 2023-03-04 17:47          465  s3://s3cmd-demo/fastab-demo</code></pre><h2><span id="上传文件夹">上传文件夹</span></h2><blockquote><p>–recursive 上传目录需要使用递归的方式</p></blockquote><p><code>[root@node-1 my-cluster]# s3cmd put /etc s3://s3cmd-demo/etc/ --recursive </code></p><blockquote><p>查看上传结果</p></blockquote><pre><code>[root@node-1 my-cluster]# s3cmd ls s3://s3cmd-demo/etc/etc/                          DIR  s3://s3cmd-demo/etc/etc/NetworkManager/                          DIR  s3://s3cmd-demo/etc/etc/X11/                          DIR  s3://s3cmd-demo/etc/etc/audisp/                          DIR  s3://s3cmd-demo/etc/etc/audit/                          DIR  s3://s3cmd-demo/etc/etc/bash_completion.d/                          DIR  s3://s3cmd-demo/etc/etc/ceph/                          DIR  s3://s3cmd-demo/etc/etc/cron.d/</code></pre><h2><span id="下载文件">下载文件</span></h2><blockquote><p>#下载文件并重命名为yum.conf-download</p></blockquote><pre><code>[root@node-1 ~]# s3cmd get s3://s3cmd-demo/etc/etc/yum.conf yum.conf-download download: &#39;s3://s3cmd-demo/etc/etc/yum.conf&#39; -&gt; &#39;yum.conf-download&#39;  [1 of 1] 970 of 970   100% in    0s    22.27 KB/s  done</code></pre><h2><span id="删除文件">删除文件</span></h2><pre><code>[root@node-1 ~]#  s3cmd rm s3://s3cmd-demo/fastab-demo #删除操作delete: &#39;s3://s3cmd-demo/fstab-demo&#39;</code></pre><h2><span id="删除目录">删除目录</span></h2><p><code>s3cmd rm s3://s3cmd-demo/etc --recursive</code></p><h2><span id="查看存储池变化">查看存储池变化</span></h2><blockquote><p>完成上述操作后ceph会多出一个default.rgw.buckets.data的pool</p></blockquote><pre><code>[root@node-1 ~]# ceph osd lspools1 ceph-demo2 .rgw.root3 default.rgw.control4 default.rgw.meta5 default.rgw.log6 default.rgw.buckets.index7 default.rgw.buckets.data #rgw的数据pool</code></pre><blockquote><p>查看内容</p></blockquote><pre><code>[root@node-1 ~]# rados -p default.rgw.buckets.data ls 3db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_23db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_1</code></pre><blockquote><p>#重新上传文件</p></blockquote><p><code>root@node-1 ~]#  s3cmd put /etc/fstab s3://s3cmd-demo/fastab-demo</code></p><blockquote><p>再次查看内容</p></blockquote><pre><code>[root@node-1 ~]# rados -p default.rgw.buckets.data ls  #可以看到多了一个3db4c650-9c23-4843-8015-97b79566b3b0.5970.2_fastab-demo的对象3db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_23db4c650-9c23-4843-8015-97b79566b3b0.5970.2__shadow_.qHY1cLeCzRSWHFoEoAqc9Nmg5S2K4qJ_13db4c650-9c23-4843-8015-97b79566b3b0.5970.2_fastab-demo #可以看到这个文件多了写前缀，前缀是存放在default.rgw.buckets.index的pool中的</code></pre><blockquote><p>查看索引</p></blockquote><pre><code>[root@node-1 ~]# rados -p default.rgw.buckets.index ls .dir.3db4c650-9c23-4843-8015-97b79566b3b0.5970.2</code></pre><h1><span id="rgw之swift接口使用">RGW之Swift接口使用</span></h1><blockquote><p><strong>需要在原有用户的基础上进行添加</strong></p></blockquote><h2><span id="查看当前用户">查看当前用户</span></h2><pre><code>[root@node-1 ~]# radosgw-admin user list [    &quot;ceph-s3-user&quot;]</code></pre><h2><span id="根据当前用户创建swift用户">根据当前用户创建swift用户</span></h2><pre><code>[root@node-1 ~]# radosgw-admin subuser create --uid ceph-s3-user --subuser=ceph-s3-user:swift --access=full&#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000,    &quot;subusers&quot;: [        &#123;            &quot;id&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;permissions&quot;: &quot;full-control&quot;        &#125;    ],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],    &quot;swift_keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;secret_key&quot;: &quot;9sWyeIFQphNButuZp2zNs2t7xU9qEkgW27yHxqBL&quot;        &#125;    ],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="生成swift的key">生成Swift的key</span></h2><pre><code>[root@node-1 ~]# radosgw-admin key create --subuser=ceph-s3-user:swift --key-type=swift --gen-secret &#123;    &quot;user_id&quot;: &quot;ceph-s3-user&quot;,    &quot;display_name&quot;: &quot;Ceph S3 User Demo&quot;,    &quot;email&quot;: &quot;&quot;,    &quot;suspended&quot;: 0,    &quot;max_buckets&quot;: 1000,    &quot;subusers&quot;: [        &#123;            &quot;id&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;permissions&quot;: &quot;full-control&quot;        &#125;    ],    &quot;keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user&quot;,            &quot;access_key&quot;: &quot;5SCX55201QUQ0FOPCPD6&quot;,            &quot;secret_key&quot;: &quot;k3zKfKPRCYCDSuDMoMYJZx9Lk6mLaFD9YZXs8ldm&quot;        &#125;    ],    &quot;swift_keys&quot;: [        &#123;            &quot;user&quot;: &quot;ceph-s3-user:swift&quot;,            &quot;secret_key&quot;: &quot;mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjh&quot; #提前保存        &#125;    ],    &quot;caps&quot;: [],    &quot;op_mask&quot;: &quot;read, write, delete&quot;,    &quot;default_placement&quot;: &quot;&quot;,    &quot;default_storage_class&quot;: &quot;&quot;,    &quot;placement_tags&quot;: [],    &quot;bucket_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;user_quota&quot;: &#123;        &quot;enabled&quot;: false,        &quot;check_on_raw&quot;: false,        &quot;max_size&quot;: -1,        &quot;max_size_kb&quot;: 0,        &quot;max_objects&quot;: -1    &#125;,    &quot;temp_url_keys&quot;: [],    &quot;type&quot;: &quot;rgw&quot;,    &quot;mfa_ids&quot;: []&#125;</code></pre><h2><span id="安装swift客户端">安装Swift客户端</span></h2><blockquote><p>安装Swift客户端需要先安装pip</p></blockquote><pre><code>[root@node-1 ~]# yum install python-pip #安装pip 由于当前pip版本较老（8.1），无法通过pip install -U pip 进行升级，只能手动进行升级[root@node-1 ~]wget https://files.pythonhosted.org/packages/0b/f5/be8e741434a4bf4ce5dbc235aa28ed0666178ea8986ddc10d035023744e6/pip-20.2.4.tar.gz  #下载安装包[root@node-1 ~]tar -zxvf pip-20.2.4.tar.gz  # 解压[root@node-1 ~]cd pip-20.2.4/[root@node-1 ~]sudo python setup.py install #给予权限不然可能安装失败[root@node-1 ~]pip install -U pip #再次更新DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.Collecting pip  Downloading pip-20.3.4-py2.py3-none-any.whl (1.5 MB)     |████████████████████████████████| 1.5 MB 1.0 MB/s Installing collected packages: pip  Attempting uninstall: pip    Found existing installation: pip 20.2.4    Uninstalling pip-20.2.4:      Successfully uninstalled pip-20.2.4Successfully installed pip-20.3.4</code></pre><blockquote><p>再次安装Swift客户端</p></blockquote><pre><code>[root@node-1 pip-20.2.4]# pip install  python-swiftclient #安装成功DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.Collecting python-swiftclient  Downloading python_swiftclient-3.13.1-py2.py3-none-any.whl (87 kB)     |████████████████████████████████| 87 kB 6.4 kB/s Collecting futures&gt;=3.0.0; python_version == &quot;2.7&quot;  Downloading futures-3.4.0-py2-none-any.whl (16 kB)Requirement already satisfied: requests&gt;=1.1.0 in /usr/lib/python2.7/site-packages (from python-swiftclient) (2.6.0)Requirement already satisfied: six&gt;=1.9.0 in /usr/lib/python2.7/site-packages (from python-swiftclient) (1.9.0)Installing collected packages: futures, python-swiftclientSuccessfully installed futures-3.4.0 python-swiftclient-3.13.1</code></pre><h2><span id="查看bucket">查看bucket</span></h2><blockquote><p>查看，list可换成upload或download实现上传下载</p></blockquote><pre><code>[root@node-1 ~]# swift -A http://192.168.187.201:80/auth -U ceph-s3-user:swift -K mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjh list ceph-s3-buckets3cmd-demo</code></pre><h2><span id="创建swift环境变量文件">创建Swift环境变量文件</span></h2><pre><code>[root@node-1 ~]# cat swift-openrc.shexport ST_AUTH=http://192.168.187.201:80/authexport ST_USER=ceph-s3-user:swiftexport ST_KEY=mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjh</code></pre><h2><span id="验证变量是否生效">验证变量是否生效</span></h2><pre><code>[root@node-1 ~]# set |grep ST #查看环境变量是否生效DIRSTACK=()HISTCONTROL=ignoredupsHISTFILE=/root/.bash_historyHISTFILESIZE=1000HISTSIZE=1000HOSTNAME=node-1HOSTTYPE=x86_64OSTYPE=linux-gnuPIPESTATUS=([0]=&quot;0&quot;)SELINUX_LEVEL_REQUESTED=SELINUX_ROLE_REQUESTED=ST_AUTH=http://192.168.187.201:80/authST_KEY=mqMdD6H5efLKDET8N7SmF1PfI6vXQE59i0EEoLjhexport[root@node-1 ~]# swift list  #验证变量成功ceph-s3-buckets3cmd-demo</code></pre><h2><span id="上传文件">上传文件</span></h2><p><code>[root@node-1 ~]# swift upload swift-demo /etc/shadow </code></p><h2><span id="上传目录">上传目录</span></h2><p><code>[root@node-1 ~]# swift upload swift-demo /etc/ </code></p><h2><span id="下载文件">下载文件</span></h2><pre><code>[root@node-1 ~]# swift download swift-demo etc/passwdetc/passwd [auth 0.008s, headers 0.011s, total 0.012s, 0.278 MB/s]</code></pre><p>##查看详细信息</p><pre><code>[root@node-1 ~]# swift stat swift-demo                      Account: v1                    Container: swift-demo                      Objects: 1694                        Bytes: 35940093                     Read ACL:                    Write ACL:                      Sync To:                     Sync Key:X-Container-Bytes-Used-Actual: 40927232                Accept-Ranges: bytes             X-Storage-Policy: default-placement              X-Storage-Class: STANDARD                Last-Modified: Sat, 04 Mar 2023 19:14:33 GMT                  X-Timestamp: 1677957085.09881                   X-Trans-Id: tx0000000000000000000d1-0064043339-85b3-default                 Content-Type: text/plain; charset=utf-8       X-Openstack-Request-Id: tx0000000000000000000d1-0064043339-85b3-default</code></pre><h2><span id="删除文件">删除文件</span></h2><blockquote><p>#删除文件，不要使用 swift delete -a 会删除所有bucket</p></blockquote><pre><code>[root@node-1 ~]# swift delete swift-demo etc/shadow #删除文件，不要使用 swift delete -a 会删除所有bucketetc/shadow[root@node-1 ~]# swift list swift-demo</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> RGW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CephFS文件存储</title>
      <link href="/2023/04/11/Ceph/6.Ceph%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/Ceph%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"/>
      <url>/2023/04/11/Ceph/6.Ceph%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/Ceph%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#%E5%AE%89%E8%A3%85mds%E6%9C%8D%E5%8A%A1">安装MDS服务</a></li><li><a href="#%E5%88%9B%E5%BB%BAceph-fs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F">创建Ceph FS文件系统</a></li><li><a href="#%E6%9F%A5%E7%9C%8Bfs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F">查看fs文件系统</a></li><li><a href="#%E5%86%85%E6%A0%B8%E6%8C%82%E8%BD%BD">内核挂载</a></li><li><a href="#%E7%94%A8%E6%88%B7%E6%80%81%E6%8C%82%E8%BD%BD">用户态挂载</a></li></ul><!-- tocstop --><p>Ceph FS文件存储时由NAS衍生过来的。</p><p>Ceph 文件系统或CephFS是一个 POSIX 兼容的文件系统，构建在 Ceph 的分布式对象存储RADOS之上。CephFS致力于为各种应用程序提供最先进、多用途、高可用性和高性能的文件存储，包括共享主目录、HPC 暂存空间和分布式工作流共享存储等传统用例。</p><p>CephFS 通过使用一些新颖的架构选择来实现这些目标。值得注意的是，文件元数据存储在与文件数据不同的 RADOS 池中，并通过可调整大小的元数据服务器集群或MDS提供服务，它可以扩展以支持更高吞吐量的元数据工作负载。文件系统的客户端可以直接访问 RADOS 来读写文件数据块。因此，工作负载可能会随着底层 RADOS 对象存储的大小线性扩展；也就是说，没有网关或代理为客户端调解数据 I&#x2F;O。</p><p>对数据的访问通过 MDS 集群进行协调，MDS 集群充当由客户端和 MDS 共同维护的分布式元数据缓存状态的权威。每个 MDS 将元数据的变化聚合成一系列高效写入 RADOS 上的日志；MDS 没有在本地存储任何元数据状态。该模型允许在 POSIX 文件系统的上下文中在客户端之间进行连贯和快速的协作。</p><p><img src="/images/Ceph/Ceph_FS%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/1.jpg"></p><p>CephFS 因其新颖的设计和对文件系统研究的贡献而成为众多学术论文的主题。它是 Ceph 中最古老的存储接口，曾经是 RADOS的主要用例。现在它与另外两个存储接口一起组成了一个现代统一存储系统：RBD（Ceph 块设备）和 RGW（Ceph 对象存储网关）。</p><h1><span id="安装mds服务">安装MDS服务</span></h1><p>在三个节点上同时部署MDS</p><pre><code>[root@node-1 my-cluster]# ceph-deploy mds create node-1 node-2 node-3</code></pre><p>查看当前MDS状态，因为当前没有文件系统，所以3台主机都是备状态</p><pre><code>[root@node-1 my-cluster]#  3 up:standby </code></pre><h1><span id="创建ceph-fs文件系统">创建Ceph FS文件系统</span></h1><p>创建pool</p><pre><code>[root@node-1 my-cluster]# ceph osd pool create cephfs_data 16 16[root@node-1 my-cluster]# ceph osd pool create cephfs_metadata 16 16</code></pre><p>查看使用帮助</p><pre><code>[root@node-1 my-cluster]# ceph -h|grep fs |grep new fs new &lt;fs_name&gt; &lt;metadata&gt; &lt;data&gt; &#123;--force&#125; &#123;--allow-dangerous-make new filesystem using named pools &lt;metadata&gt; and &lt;data&gt;</code></pre><p>创建一个fs文件系统</p><pre><code>[root@node-1 my-cluster]# ceph fs new cephfs-demo cephfs_metadata cephfs_data  new fs with metadata pool 9 and data pool 8</code></pre><h1><span id="查看fs文件系统">查看fs文件系统</span></h1><pre><code>[root@node-1 my-cluster]# ceph fs ls name: cephfs-demo, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</code></pre><p>查看集群状态</p><pre><code>[root@node-1 my-cluster]#  ceph -s  #可以看到MDS状态已经又一个active了  cluster:    id:     e9a90625-4707-4b6b-b52f-661512ea831d    health: HEALTH_WARN            1 pools have too many placement groups            too many PGs per OSD (368 &gt; max 250)   services:    mon: 3 daemons, quorum node-1,node-2,node-3 (age 4m)    mgr: node-2(active, since 10m), standbys: node-3, node-1    mds: cephfs-demo:1 &#123;0=node-2=up:active&#125; 2 up:standby    osd: 6 osds: 6 up (since 4m), 6 in (since 14h)    rgw: 1 daemon active (node-1)   task status:   data:    pools:   9 pools, 736 pgs    objects: 534 objects, 1.0 GiB    usage:   9.6 GiB used, 50 GiB / 60 GiB avail    pgs:     736 active+clean</code></pre><h1><span id="内核挂载">内核挂载</span></h1><p>创建一个挂载点</p><pre><code>[root@node-1 my-cluster]# mkdir /mnt/cephfs #创建一个挂载点</code></pre><p>使用命令进行挂载</p><pre><code>[root@node-1 my-cluster]# which mount.ceph #使用这个命令进行挂载/usr/sbin/mount.ceph [root@node-1 my-cluster]# rpm -qf /usr/sbin/mount.ceph #安装ceph-common的时候添加的命令ceph-common-14.2.22-0.el7.x86_64[root@node-1 my-cluster]# mount -t ceph 192.168.187.201:6789:/ /mnt/cephfs/ -o name=admin #挂载</code></pre><p>查看挂载</p><pre><code>[root@node-1 my-cluster]# df -HT # 查看挂载Filesystem              Type      Size  Used Avail Use% Mounted ondevtmpfs                devtmpfs  942M     0  942M   0% /devtmpfs                   tmpfs     954M     0  954M   0% /dev/shmtmpfs                   tmpfs     954M   11M  944M   2% /runtmpfs                   tmpfs     954M     0  954M   0% /sys/fs/cgroup/dev/mapper/centos-root xfs        19G  2.6G   16G  14% //dev/sda1               xfs       1.1G  204M  860M  20% /boottmpfs                   tmpfs     954M   25k  954M   1% /var/lib/ceph/osd/ceph-0tmpfs                   tmpfs     954M   25k  954M   1% /var/lib/ceph/osd/ceph-3tmpfs                   tmpfs     191M     0  191M   0% /run/user/0/dev/rbd0               xfs        11G  1.2G  9.7G  11% /data192.168.187.201:6789:/  ceph       17G     0   17G   0% /mnt/cephfs [root@node-1 my-cluster]# lsmod | grep ceph #会自动添加ceph的模块，此方法性能比较高ceph                  363016  1 libceph               314775  2 rbd,cephdns_resolver           13140  1 libcephlibcrc32c              12644  2 xfs,libceph</code></pre><h1><span id="用户态挂载">用户态挂载</span></h1><p>安装软件包</p><pre><code>[root@node-1 my-cluster]#  yum install ceph-fuse -y</code></pre><p>查看帮助文档</p><pre><code>[root@node-1 my-cluster]# ceph-fuse -h #查看使用帮助usage: ceph-fuse [-n client.username] [-m mon-ip-addr:mon-port] &lt;mount point&gt; [OPTIONS]  --client_mountpoint/-r &lt;sub_directory&gt;                    use sub_directory as the mounted root, rather than the full Ceph tree.usage: ceph-fuse mountpoint [options]general options:    -o opt,[opt...]        mount options    -h   --help            print help    -V   --version         print versionFUSE options:    -d   -o debug          enable debug output (implies -f)    -f                     foreground operation    -s                     disable multi-threaded operation  --conf/-c FILE    read configuration from the given configuration file  --id ID           set ID portion of my name  --name/-n TYPE.ID set name  --cluster NAME    set cluster name (default: ceph)  --setuser USER    set uid to user or uid (and gid to user&#39;s gid)  --setgroup GROUP  set gid to group or gid  --version         show version and quit</code></pre><p>挂载</p><p>可以看到与内核态挂载的地点文件是共享的 </p><pre><code>[root@node-1 my-cluster]# ceph-fuse -n client.admin -m 192.168.187.201:6789,192.168.187.202:6789,192.168.187.203:6789 /mnt/ceph-fuse/ #使用ceph-fuse进行挂载，-n 指定用户名 -m 指定mon节点，可指定1个或者多个，或者不指定，最后写上挂载地点ceph-fuse[205722023-03-05 17:10:28.202 7f6b40549f80 -1 init, newargv = 0x5623b0d084e0 newargc=9]: starting ceph clientceph-fuse[20572]: starting fuse</code></pre><p>确定挂载情况</p><pre><code>[root@node-1 my-cluster]# cd /mnt/ceph-fuse/ [root@node-1 ceph-fuse]# ls[root@node-1 ceph-fuse]# lltotal 0[root@node-1 ceph-fuse]# [root@node-1 ceph-fuse]# ls[root@node-1 ceph-fuse]# echo aaa &gt; test[root@node-1 ceph-fuse]# lstest[root@node-1 ceph-fuse]# ls /mnt/cephfstest</code></pre><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> CephFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph简介</title>
      <link href="/2023/04/10/Ceph/2.Ceph%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84/Ceph-%E7%AE%80%E4%BB%8B/"/>
      <url>/2023/04/10/Ceph/2.Ceph%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84/Ceph-%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><!-- toc --><ul><li><a href="#ceph-%E7%AE%80%E4%BB%8B">CEPH 简介</a><ul><li><a href="#%E5%BB%BA%E8%AE%AE">建议</a></li><li><a href="#%E7%A1%AC%E4%BB%B6%E6%8E%A8%E8%8D%90">硬件推荐</a><ul><li><a href="#cpu">CPU</a></li><li><a href="#%E5%86%85%E5%AD%98">内存</a></li><li><a href="#%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8">数据存储</a></li><li><a href="#%E7%BD%91%E7%BB%9C">网络</a></li><li><a href="#%E6%95%85%E9%9A%9C%E5%9F%9F">故障域</a></li></ul></li><li><a href="#%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%BB%BA%E8%AE%AE">操作系统建议</a><ul><li><a href="#ceph-%E4%BE%9D%E8%B5%96%E9%A1%B9">CEPH 依赖项</a></li><li><a href="#%E5%B9%B3%E5%8F%B0">平台</a></li></ul></li></ul></li></ul><!-- tocstop --><h1><span id="ceph-简介">CEPH 简介</span></h1><p>无论您是想为云平台提供Ceph 对象存储和&#x2F;或 Ceph 块设备服务、部署Ceph 文件系统还是将 Ceph 用于其他目的，所有 Ceph 存储集群部署都从设置每个 Ceph 节点、您的网络和 Ceph存储集群。一个 Ceph 存储集群至少需要一个 Ceph Monitor、Ceph Manager 和 Ceph OSD（Object Storage Daemon）。运行 Ceph 文件系统客户端时也需要 Ceph 元数据服务器。<br><img src="/images/Ceph/Ceph%E7%AE%80%E4%BB%8B/1.jpg"></p><ul><li>Monitors：Ceph Monitor ( ceph-mon) 维护集群状态图，包括监视器图、管理器图、OSD 图、MDS 图和 CRUSH 图。这些映射是 Ceph 守护进程相互协调所需的关键集群状态。监视器还负责管理守护进程和客户端之间的身份验证。通常至少需要三个监视器才能实现冗余和高可用性。</li><li>管理器：Ceph 管理器守护进程 ( ceph-mgr) 负责跟踪运行时指标和 Ceph 集群的当前状态，包括存储利用率、当前性能指标和系统负载。Ceph Manager 守护进程还托管基于 python 的模块来管理和公开 Ceph 集群信息，包括基于 Web 的Ceph Dashboard和 REST API。高可用性通常至少需要两个管理器。</li><li>Ceph OSDs：一个对象存储守护进程（Ceph OSD， ceph-osd）存储数据，处理数据复制，恢复，重新平衡，并通过检查其他 Ceph OSD 守护进程的心跳向 Ceph Monitors 和 Managers 提供一些监控信息。通常至少需要三个 Ceph OSD 才能实现冗余和高可用性。</li><li>MDS：Ceph 元数据服务器（MDS ceph-mds）代表Ceph 文件系统存储元数据（即 Ceph 块设备和 Ceph 对象存储不使用 MDS）。Ceph 元数据服务器允许 POSIX 文件系统用户执行基本命令（如 ls、find等），而不会给 Ceph 存储集群带来巨大负担。</li></ul><p>Ceph 将数据作为对象存储在逻辑存储池中。使用 CRUSH算法，Ceph 计算出哪个归置组 (PG) 应该包含该对象，以及哪个 OSD 应该存储该归置组。CRUSH 算法使 Ceph 存储集群能够动态扩展、重新平衡和恢复。</p><h2><span id="建议">建议</span></h2><p>要开始在生产中使用 Ceph，您应该查看我们的硬件建议和操作系统建议。</p><ul><li>硬件推荐<ul><li>中央处理器</li><li>内存</li><li>记忆</li><li>数据存储</li><li>网络</li><li>故障域</li><li>最低硬件建议</li></ul></li><li>操作系统建议<ul><li>Ceph 依赖项</li><li>平台</li></ul></li></ul><h2><span id="硬件推荐">硬件推荐</span></h2><p>Ceph 被设计为在商用硬件上运行，这使得构建和维护 PB 级数据集群在经济上是可行的。在规划集群硬件时，您需要平衡许多考虑因素，包括故障域和潜在的性能问题。硬件规划应该包括在许多主机上分布 Ceph 守护进程和其他使用 Ceph 的进程。通常，我们建议在为特定类型的守护进程配置的主机上运行特定类型的 Ceph 守护进程。我们建议将其他主机用于利用您的数据集群的进程（例如，OpenStack、CloudStack 等）。<br>也可以查看<a href="https://ceph.com/community/blog/">Ceph 博客。</a></p><h3><span id="cpu">CPU</span></h3><p>CephFS 元数据服务器 (MDS) 是 CPU 密集型的。因此，CephFS 元数据服务器 (MDS) 应具有四核（或更好）CPU 和高时钟频率 (GHz)。OSD 节点需要足够的处理能力来运行 RADOS 服务、使用 CRUSH 计算数据放置、复制数据以及维护它们自己的集群映射副本。</p><blockquote><p><strong>一个 Ceph 集群的要求与另一个集群的要求不同，但这里有一些通用准则。</strong></p></blockquote><p>在 Ceph 的早期版本中，我们会根据每个 OSD 的核心数来提出硬件建议，但这个每个 OSD 的核心数指标不再像每个 IOP 的周期数和每个 OSD 的 IOP 数一样有用。例如，对于 NVMe 驱动器，Ceph 可以轻松地在真实集群上使用五个或六个内核，并在单个 OSD 上隔离使用多达大约十四个内核。因此，每个 OSD 的核心不再像以前那样紧迫。选择硬件时，选择每个内核的 IOP。</p><p>监控节点和管理器节点对 CPU 的要求不高，只需要适度的处理器。如果您的主机除了运行 Ceph 守护进程外还将运行 CPU 密集型进程，请确保您有足够的处理能力来运行 CPU 密集型进程和 Ceph 守护进程。（OpenStack Nova 是 CPU 密集型进程的一个例子。）我们建议您在单独的主机上（即，在不是您的监视器和管理器节点的主机上）运行非 Ceph CPU 密集型进程，以避免占用资源争论。</p><h3><span id="内存">内存</span></h3><p>通常，RAM 越大越好。适度集群的监视器&#x2F;管理器节点可能使用 64GB 就可以了；对于拥有数百个 OSD 的更大集群，128GB 是一个合理的目标。BlueStore OSD 的内存目标默认为 4GB。考虑到操作系统和管理任务（如监控和指标）的谨慎余量以及恢复期间增加的消耗：建议为每个 BlueStore OSD 预配 ~8GB。</p><h4><span id="监视器和管理器ceph-mon-和-ceph-mgr">监视器和管理器（CEPH-MON 和 CEPH-MGR）</span></h4><p>监视器和管理器守护进程的内存使用量通常随集群的大小而变化。请注意，在启动时以及拓扑更改和恢复期间，这些守护程序将需要比稳态操作期间更多的 RAM，因此请计划使用高峰期。对于非常小的集群，32 GB 就足够了。对于多达 300 个 OSD 的集群，需要 64GB</p><h4><span id="元数据服务器-ceph-mds">元数据服务器 (CEPH-MDS)</span></h4><p>元数据守护程序内存利用率取决于其缓存配置为消耗的内存量。对于大多数系统，我们建议至少使用 1 GB。 看mds_cache_memory。</p><h4><span id="记忆">记忆</span></h4><p>Bluestore 使用自己的内存来缓存数据，而不是依赖操作系统的页面缓存。在 Bluestore 中，您可以通过更改配置选项来调整 OSD 尝试消耗的内存量osd_memory_target 。</p><ul><li>通常不建议设置osd_memory_target低于 2GB（Ceph 可能无法将内存消耗保持在 2GB 以下，这可能会导致性能极慢）。</li><li>将内存目标设置在 2GB 和 4GB 之间通常可行，但可能会导致性能下降，因为元数据可能会在 IO 期间从磁盘读取，除非活动数据集相对较小。</li><li>4GB 是当前的默认osd_memory_target大小。此默认值是为典型用例选择的，旨在平衡内存需求和 OSD 性能。</li><li>osd_memory_target当有很多（小）对象或处理大（256GB&#x2F;OSD 或更多）数据集时，设置高于 4GB 可以提高性能。</li></ul><blockquote><p>OSD 内存自动调整是“尽力而为”。虽然 OSD 可以取消映射内存以允许内核回收它，但不能保证内核会在特定时间范围内实际回收释放的内存。这尤其适用于旧版本的 Ceph，其中透明大页面可以防止内核回收从碎片大页面中释放的内存。现代版本的 Ceph 在应用程序级别禁用透明大页面以避免这种情况，尽管这仍然不能保证内核会立即回收未映射的内存。OSD 有时仍可能超出其内存目标。我们建议在您的系统上预算大约 20% 的额外内存，以防止 OSD 在临时峰值期间或由于内核回收已释放页面的任何延迟而出现 OOM。</p></blockquote><p>使用旧版 FileStore 后端时，页面缓存用于缓存数据，因此通常不需要调整。<br>使用旧版 FileStore 后端时，OSD 内存消耗与系统中每个守护进程的 PG 数量有关。</p><h3><span id="数据存储">数据存储</span></h3><p>仔细规划您的数据存储配置。在规划数据存储时，需要考虑显着的成本和性能权衡。同时进行的操作系统操作以及多个守护进程对单个驱动器的读写操作的同时请求会大大降低性能。</p><h4><span id="硬盘驱动器">硬盘驱动器</span></h4><p>OSD 应该有足够的硬盘驱动器空间来存储对象数据。我们建议最小硬盘驱动器大小为 1 TB。考虑更大磁盘的每 GB 成本优势。我们建议将硬盘驱动器的价格除以千兆字节数，得出每千兆字节的成本，因为更大的驱动器可能会对每千兆字节的成本产生重大影响。例如，价格为 75.00 美元的 1 TB 硬盘的成本为每 GB 0.07 美元（即 75 美元 &#x2F; 1024 &#x3D; 0.0732）。相比之下，价格为 150.00 美元的 3 TB 硬盘的成本为每 GB 0.05 美元（即 150 美元 &#x2F; 3072 &#x3D; 0.0488）。在前面的示例中，使用 1 TB 的磁盘通常会使每 GB 的成本增加 40%——使您的集群的成本效率大大降低。</p><blockquote><p>在单个 SAS &#x2F; SATA 驱动器上运行多个 OSD 不是一个好主意。但是，NVMe 驱动器可以通过拆分为两个或更多 OSD 来提高性能。<br>在单个驱动器上运行 OSD 和监视器或元数据服务器也不是一个好主意。</p></blockquote><p>存储驱动器受寻道时间、访问时间、读写时间以及总吞吐量的限制。这些物理限制会影响整体系统性能，尤其是在恢复期间。我们建议为操作系统和软件使用专用（最好是镜像）驱动器，并为主机上运行的每个 Ceph OSD 守护进程使用一个驱动器（上面的模数 NVMe）。许多不是由硬件故障引起的“慢 OSD”问题是由于在同一驱动器上运行操作系统和多个 OSD 而引起的。由于在小型集群上解决性能问题的成本可能超过额外磁盘驱动器的成本，因此您可以通过避免让 OSD 存储驱动器负担过重的诱惑来优化您的集群设计规划。</p><blockquote><p>您可以在每个 SAS &#x2F; SATA 驱动器上运行多个 Ceph OSD 守护进程，但这可能会导致资源争用并降低整体吞吐量。</p></blockquote><h4><span id="态硬盘">态硬盘</span></h4><p>性能改进的一个机会是使用固态驱动器 (SSD) 来减少随机访问时间和读取延迟，同时加快吞吐量。与硬盘驱动器相比，SSD 每 GB 的成本通常是硬盘驱动器的 10 倍以上，但 SSD 的访问时间通常至少比硬盘驱动器快 100 倍。<br>SSD 没有移动机械部件，因此它们不一定受到与硬盘驱动器相同类型的限制。SSD 确实有很大的局限性。在评估 SSD 时，重要的是要考虑顺序读写的性能。</p><blockquote><p>我们建议探索使用 SSD 来提高性能。但是，在对 SSD 进行重大投资之前，我们强烈建议查看 SSD 的性能指标并在测试配置中测试 SSD 以衡量性能。</p></blockquote><p>相对便宜的 SSD 可能会吸引您的经济意识。谨慎使用。选择用于 Ceph 的 SSD 时，可接受的 IOPS 是不够的。</p><p>SSD 在历史上一直是对象存储的成本高昂，但新兴的 QLC 驱动器正在缩小差距。通过将 WAL+DB 卸载到 SSD，HDD OSD 可能会看到显着的性能提升。</p><p>Ceph 加速 CephFS 文件系统性能的一种方法是将 CephFS 元数据的存储与 CephFS 文件内容的存储分开。Ceph 为 CephFS 元数据提供了一个默认metadata池。您永远不必为 CephFS 元数据创建一个池，但您可以为您的 CephFS 元数据池创建一个仅指向主机的 SSD 存储介质的 CRUSH 映射层次结构。有关详细信息，请参阅 <a href="https://docs.ceph.com/en/quincy/rados/operations/crush-map-edits/#crush-map-device-class">CRUSH 设备类</a>。</p><h4><span id="控制器">控制器</span></h4><p>磁盘控制器 (HBA) 会对写入吞吐量产生重大影响。仔细考虑您的选择以确保它们不会造成性能瓶颈。值得注意的是，RAID 模式 (IR) HBA 可能比更简单的“JBOD”(IT) 模式 HBA 表现出更高的延迟，并且 RAID SoC、写缓存和电池备份会显着增加硬件和维护成本。某些 RAID HBA 可以配置有 IT 模式“个性”。</p><p><a href="https://ceph.com/community/blog/">Ceph 博客</a>通常是有关 Ceph 性能问题的极佳信息来源。有关更多详细信息，请参阅<a href="https://ceph.com/community/ceph-performance-part-1-disk-controller-write-throughput/">Ceph 写入吞吐量 1</a>和<a href="https://ceph.com/community/ceph-performance-part-2-write-throughput-without-ssd-journals/">Ceph 写入吞吐量 2</a>。</p><h4><span id="对标">对标</span></h4><p>BlueStore 在 O_DIRECT 中打开块设备，并频繁使用 fsync 以确保数据安全地持久化到介质中。您可以使用 评估驱动器的低级写入性能fio。例如，4kB 随机写性能测量如下：</p><pre><code>#fio --name=/dev/sdX --ioengine=libaio --direct=1 --fsync=1 --readwrite=randwrite --blocksize=4k --runtime=300</code></pre><h4><span id="写缓存">写缓存</span></h4><p>企业 SSD 和 HDD 通常包括断电保护功能，使用多级缓存来加速直接或同步写入。这些设备可以在两种缓存模式之间切换——易失性缓存通过 fsync 刷新到持久介质，或同步写入的非易失性缓存。</p><p>通过“启用”或“禁用”写入（易失性）缓存来选择这两种模式。当启用易失性缓存时，Linux 使用“回写”模式的设备，禁用时，它使用“直写”。</p><p>默认配置（通常启用缓存）可能不是最佳配置，并且 OSD 性能可能会通过禁用写缓存在增加 IOPS 和减少 commit_latency 方面得到显着提高。</p><p>因此，鼓励用户fio如前所述对他们的设备进行基准测试，并为他们的设备保留最佳缓存配置。<br>hdparm可以使用、sdparm或 smartctl读取中的值来查询缓存配置&#x2F;sys&#x2F;class&#x2F;scsi_disk&#x2F;*&#x2F;cache_type，例如：</p><pre><code>#hdparm -W /dev/sda/dev/sda: write-caching =  1 (on)# sdparm --get WCE /dev/sda    /dev/sda: ATA       TOSHIBA MG07ACA1  0101WCE           1  [cha: y]# smartctl -g wcache /dev/sdasmartctl 7.1 2020-04-05 r5049 [x86_64-linux-4.18.0-305.19.1.el8_4.x86_64] (local build)Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.orgWrite cache is:   Enabled# cat /sys/class/scsi_disk/0\:0\:0\:0/cache_typewrite back</code></pre><p>可以使用相同的工具禁用写缓存：</p><pre><code>#hdparm -W0 /dev/sda/dev/sda: setting drive write-caching to 0 (off) write-caching =  0 (off)# sdparm --clear WCE /dev/sda    /dev/sda: ATA       TOSHIBA MG07ACA1  0101# smartctl -s wcache,off /dev/sdasmartctl 7.1 2020-04-05 r5049 [x86_64-linux-4.18.0-305.19.1.el8_4.x86_64] (local build)Copyright (C) 2002-19, Bruce Allen, Christian Franke, www.smartmontools.org=== START OF ENABLE/DISABLE COMMANDS SECTION ===Write cache disabled</code></pre><p>hdparm通常，使用、sdparm或禁用缓存smartctl 会导致 cache_type 自动更改为“write through”。如果不是这样，你可以尝试直接如下设置。（用户应注意，设置 cache_type 也会正确保留设备的缓存模式，直到下一次重启）：</p><pre><code>#echo &quot;write through&quot; &gt; /sys/class/scsi_disk/0\:0\:0\:0/cache_type# hdparm -W /dev/sda/dev/sda: write-caching =  0 (off)</code></pre><p>这个 udev 规则（在 CentOS 8 上测试过）会将所有 SATA&#x2F;SAS 设备 cache_types 设置为“write through”：</p><pre><code>#cat /etc/udev/rules.d/99-ceph-write-through.rulesACTION==&quot;add&quot;, SUBSYSTEM==&quot;scsi_disk&quot;, ATTR&#123;cache_type&#125;:=&quot;write through&quot;</code></pre><p>这个 udev 规则（在 CentOS 7 上测试过）会将所有 SATA&#x2F;SAS 设备 cache_types 设置为“write through”：</p><pre><code>#cat /etc/udev/rules.d/99-ceph-write-through-el7.rulesACTION==&quot;add&quot;, SUBSYSTEM==&quot;scsi_disk&quot;, RUN+=&quot;/bin/sh -c &#39;echo write through &gt; /sys/class/scsi_disk/$kernel/cache_type&#39;&quot;</code></pre><p>该sdparm实用程序可用于一次查看&#x2F;更改多个设备上的易失性写入缓存：</p><pre><code>#sdparm --get WCE /dev/sd*    /dev/sda: ATA       TOSHIBA MG07ACA1  0101WCE           0  [cha: y]    /dev/sdb: ATA       TOSHIBA MG07ACA1  0101WCE           0  [cha: y]# sdparm --clear WCE /dev/sd*    /dev/sda: ATA       TOSHIBA MG07ACA1  0101    /dev/sdb: ATA       TOSHIBA MG07ACA1  0101</code></pre><h4><span id="其他注意事项">其他注意事项</span></h4><p>您通常会在每个主机上运行多个 OSD，但您应该确保 OSD 驱动器的总吞吐量不超过满足客户端读取或写入数据需求所需的网络带宽。您还应该考虑集群在每个主机上存储的总数据的百分比。如果特定主机上的百分比很大并且该主机发生故障，则可能导致诸如超过 之类的问题，这会导致 Ceph 停止操作以作为防止数据丢失的安全预防措施。full ratio</p><p>当您在每个主机上运行多个 OSD 时，您还需要确保内核是最新的。请参阅<a href="https://docs.ceph.com/en/quincy/start/os-recommendations">OS Recommendations</a>了解有关注意事项glibc并 syncfs(2)确保您的硬件在每个主机运行多个 OSD 时按预期运行。</p><h3><span id="网络">网络</span></h3><p>在您的机架中提供至少 10 Gb&#x2F;s 的网络。</p><h4><span id="速度">速度</span></h4><p>在 1 Gb&#x2F;s 网络上复制 1 TB 数据需要三个小时，在 1 Gb&#x2F;s 网络上复制 10 TB 数据需要三十个小时。但是在 10 Gb&#x2F;s 网络上复制 1 TB 数据只需要 20 分钟，而在 10 Gb&#x2F;s 网络上复制 10 TB 数据只需要一个小时。</p><h4><span id="成本">成本</span></h4><p>Ceph 集群越大，OSD 故障就越常见。degraded归置组 (PG) 从一种状态恢复到另一种状态的速度越快越好。值得注意的是，快速恢复将可能导致数据暂时不可用甚至丢失的多重重叠故障的可能性降至最低。当然，在配置您的网络时，您必须在价格与性能之间取得平衡。active + clean</p><p>一些部署工具使用 VLAN 来使硬件和网络布线更易于管理。使用 802.1q 协议的 VLAN 需要支持 VLAN 的 NIC 和交换机。该硬件的额外费用可能会被网络设置和维护方面节省的运营成本所抵消。当使用 VLAN 处理集群和计算堆栈（例如 OpenStack、CloudStack 等）之间的 VM 流量时，使用 10 Gb&#x2F;s 以太网或更好的以太网具有额外的价值；截至 2022 年，40 Gb&#x2F;s 或25&#x2F;50&#x2F;100 Gb&#x2F;s 网络在生产集群中很常见。</p><blockquote><p>架顶式 (TOR) 交换机还需要快速和冗余的上行链路来旋转主干交换机&#x2F;路由器，通常至少为 40 Gb&#x2F;s。</p></blockquote><h4><span id="底板管理控制器-bmc">底板管理控制器 (BMC)</span></h4><p>您的服务器机箱应该有底板管理控制器 (BMC)。众所周知的例子是 iDRAC (Dell)、CIMC (Cisco UCS) 和 iLO (HPE)。管理和部署工具也可能广泛使用 BMC，尤其是通过 IPMI 或 Redfish，因此请考虑带外网络在安全性和管理方面的成本&#x2F;收益权衡。Hypervisor SSH 访问、VM 映像上传、OS 映像安装、管理套接字等会给网络带来巨大的负载。运行三个网络似乎有点矫枉过正，但每个流量路径都代表潜在的容量、吞吐量和&#x2F;或性能瓶颈，您在部署大规模数据集群之前应该仔细考虑。</p><h3><span id="故障域">故障域</span></h3><p>故障域是阻止访问一个或多个 OSD 的任何故障。那可能是主机上停止的守护进程；硬盘故障、操作系统崩溃、网卡故障、电源故障、网络中断、停电等等。在规划您的硬件需求时，您必须权衡通过将过多的责任置于过少的故障域中来降低成本的诱惑与隔离每个潜在故障域的额外成本之间的平衡。</p><h4><span id="最低硬件建议">最低硬件建议</span></h4><p>Ceph 可以在廉价的商用硬件上运行。小型生产集群和开发集群可以使用适度的硬件成功运行。</p><table>    <tr>        <td>过程</td>        <td>标准</td>        <td>最低推荐</td>    </tr>    <tr>        <td rowspan="5">ceph-osd</td>    </tr>        <tr>            <td>处理器</td>            <td>最少 1 个核心 <br>  每 200-500 MB/s 1 个核心 <br> 每 1000-3000 IOPS 1 个核心  <br>  结果在复制之前。 <br>  结果可能因不同的 CPU 型号和 Ceph 功能而异。（纠删码、压缩等）。<br> ARM 处理器可能特别需要额外的内核。 <br> 实际性能取决于许多因素，包括驱动器、网络和客户端吞吐量和延迟。强烈建议进行基准测试。 </td>        </tr>        <tr>            <td>内存</td>            <td>每个守护进程 4GB+（越多越好）<br> 2-4GB 经常运行（可能很慢） <br> 小于 2GB 不推荐 </td>        </tr>        <tr>            <td>卷存储</td>            <td>每个守护进程 1 个存储驱动器 </td>        </tr>        <tr>            <td>数据库/文件</td>            <td>每个守护进程 1 个 SSD 分区（可选） </td>        </tr>        <tr>            <td>网络</td>            <td>1x 1GbE+ NIC（推荐 10GbE+） </td>        </tr>    <tr>        <td rowspan="4">ceph-mon</td>    </tr>        <tr>            <td>处理器</td>            <td>最少 2 个核心 </td>        </tr>        <tr>            <td>内存</td>            <td>每个守护进程 2-4GB+ </td>        </tr>        <tr>        <td>磁盘空间</td>        <td>每个守护进程 60 GB </td>        </tr>        <tr>            <td>网络</td>            <td>1x 1GbE+ NIC</td>        </tr>    <tr>        <td rowspan="4">ceph-mds</td>    </tr>        <tr>            <td>处理器</td>            <td>最少 2 个核心 </td>        </tr>        <tr>            <td>内存</td>            <td>每个守护进程 2GB+ </td>        </tr>        <tr>            <td>磁盘空间</td>            <td>每个守护进程 1MB</td>        </tr>        <tr>            <td>网络</td>            <td>1x 1GbE+ NIC</td>        </tr></table><blockquote><p>如果您使用单个磁盘运行 OSD，请为您的卷存储创建一个分区，该分区与包含操作系统的分区分开。通常，我们建议操作系统和卷存储使用单独的磁盘。</p></blockquote><h2><span id="操作系统建议">操作系统建议</span></h2><h3><span id="ceph-依赖项">CEPH 依赖项</span></h3><p>作为一般规则，我们建议在较新版本的 Linux 上部署 Ceph。我们还建议在具有长期支持的版本上进行部署。</p><h4><span id="内核">内核</span></h4><blockquote><p>Ceph 内核客户端</p></blockquote><p>如果您使用内核客户端映射 RBD 块设备或挂载 CephFS，一般建议是使用 <a href="http://kernel.org/">http://kernel.org</a> 或您在任何客户端上的 Linux 发行版提供的“稳定”或“长期维护”内核系列主机。</p><p>对于 RBD，如果选择跟踪长期内核，我们目前推荐基于 4.x 的“长期维护”内核系列或更高版本：</p><p>对于 RBD，如果选择跟踪长期内核，我们目前推荐基于 4.x 的“长期维护”内核系列或更高版本：</p><ul><li>4.19.z</li><li>4.14.z</li><li>5.x</li></ul><p>对于 CephFS，请参阅有关使用内核驱动程序安装 CephFS 的部分 以获取内核版本指南。</p><p>较旧的内核客户端版本可能不支持您的CRUSH 可调配置文件或 Ceph 集群的其他较新功能，需要将存储集群配置为禁用这些功能。</p><h3><span id="平台">平台</span></h3><p>下面的图表显示了 Ceph 的需求如何映射到各种 Linux 平台。一般而言，对内核和系统初始化包（即 sysvinit、systemd）之外的特定发行版的依赖性非常小。</p><table>    <tr>        <td>Release Name</td>        <td>Tag</td>        <td>CentOS</td>        <td>Ubuntu</td>        <td>OpenSUSE C</td>        <td>Debian C</td>    </tr>        <tr>            <td>Quincy</td>            <td>17.2.z</td>            <td>8 A</td>            <td>20.04 A</td>            <td>15.3</td>            <td>11</td>        </tr>        <tr>            <td>Pacific</td>            <td>16.2.z</td>            <td>8 A</td>            <td>18.04 C,<br>20.04 A</td>            <td>15.2</td>            <td>10<br>11</td>        </tr>        <tr>            <td>Octopus</td>            <td>15.2.z</td>            <td>7 B <br> 8 A</td>            <td>18.04 C,<br>20.04 A</td>            <td>15.2</td>            <td>10</td>        </tr></table><ul><li>A : Ceph 提供软件包，并对其中的软件进行了全面的测试。</li><li>B : Ceph 提供了软件包，并对其中的软件做了基本的测试。</li><li>C : Ceph 只提供包。尚未对这些版本进行任何测试。</li></ul><blockquote><p>Centos 7 用户：Btrfs在 Octopus 版本中不再在 Centos 7 上进行测试。我们建议bluestore改用。</p></blockquote><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客中插入 Chart 动态图表</title>
      <link href="/2023/04/08/chatjs/"/>
      <url>/2023/04/08/chatjs/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>该文基本(全部)来自于chatjs中文文档</p><h1><span id="背景">背景</span></h1><!-- toc --><ul><li><a href="#hexo-%E4%B8%AD%E7%9A%84-chartjs">Hexo 中的 Chartjs</a></li><li><a href="#%E7%A4%BA%E4%BE%8B">示例</a><ul><li><a href="#%E6%8A%98%E7%BA%BF%E5%9B%BE">折线图</a></li><li><a href="#%E7%9B%B8%E5%85%B3%E5%B1%9E%E6%80%A7">相关属性</a></li></ul></li></ul><!-- tocstop --><span id="more"></span><p>Chartjs是一款简单优雅的数据可视化工具，对比其他图表库如echarts、highcharts、c3、flot、amchart等，它的画面效果、动态效果都更精致，它的 文档首页 就透出一股小清新，基于 HTML5 Canvas，它拥有更好的性能且响应式，基本满足了一般数据展示的需要，包括折线图，条形图，饼图，散点图，雷达图，极地图，甜甜圈图等</p><h1><span id="hexo-中的-chartjs">Hexo 中的 Chartjs</span></h1><p>为了方便在 Hexo 中使用这么漂亮的图表库，我自己写了一个 Hexo 的 Chartjs 插件。插件的安装和使用非常的简单，只需要进入博客目录，然后打开命令行，用npm安装一下：</p><pre><code>npm install hexo-tag-chart --save</code></pre><p>之后在文章内使用 chart 的 tag 就可以了</p><pre><code>&#123;% chart 90% 300 %&#125;\\TODO option goes here&#123;% endchart %&#125;</code></pre><p>其中 chart 是标签名，endchart 是结束标签，不需要更改，90% 是图表容器的相对宽度，默认是100%，300 是图表容器的高度，默认是按正常比例缩放的，你可以通过设置 options 里面的 aspectRatio 属性来调整宽高比例，另外还有许多属性可以自定义，你可以查看 官方文档。在标签之间的部分，都是需要自己填充的图表数据和属性。</p><p>我们来看一个样例，你可以把鼠标移上去看看动态效果。</p><div style="width: 90%;margin: 0 auto">    <canvas id="chart5607" style="height: 300px"></canvas></div><script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script><script type="text/javascript">    var ctx = document.getElementById('chart5607').getContext('2d');    var options =     {    type: 'line',    data: {    labels: ['January', 'February', 'March', 'April', 'May', 'June', 'July'],    datasets: [{        label: 'My First dataset',        backgroundColor: 'rgb(255, 99, 132)',        borderColor: 'rgb(255, 99, 132)',        data: [0, 10, 5, 2, 20, 30, 45]        }]    },    options: {        responsive: true,        title: {        display: true,        text: 'Chart.js Line Chart'        }    }};    new Chart(ctx, options);</script><p>上面这个样例可以通过以下代码来实现：</p><pre><code>&#123;% chart 90% 300 %&#125;    &#123;    type: 'line',    data: &#123;    labels: ['January', 'February', 'March', 'April', 'May', 'June', 'July'],    datasets: [&#123;        label: 'My First dataset',        backgroundColor: 'rgb(255, 99, 132)',        borderColor: 'rgb(255, 99, 132)',        data: [0, 10, 5, 2, 20, 30, 45]        &#125;]    &#125;,    options: &#123;        responsive: true,        title: &#123;        display: true,        text: 'Chart.js Line Chart'        &#125;    &#125;&#125;&#123;% endchart %&#125;</code></pre><h1><span id="示例">示例</span></h1><p>现在你已经基本学会了在Hexo中插入图表了，我再展示一些更炫酷的图表吧，你可以自己去尝试一下。</p><h2><span id="折线图">折线图</span></h2><p>效果</p><div style="width: 90%;margin: 0 auto">    <canvas id="chart9686" style="height: 300px"></canvas></div><script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script><script type="text/javascript">    var ctx = document.getElementById('chart9686').getContext('2d');    var options =   //其中 chart 是标签名，endchart 是结束标签，不需要更改，可以通过设置 options 里面的  aspectRatio 属性来调整图表的宽高比例，默认 width 为 100%，height 为 300px。 {    type: 'line',    data: { //  折线图数据    labels: ['8.4', '8.5', '8.6', '8.7', '8.8', '8.9', '8.10'], //  横坐标    datasets: [{ //  纵坐标变量名称        label: 'My First dataset',        lineTension: 0,        borderColor: 'rgb(255, 99, 132)', //  线条颜色        data: [0, 10, 5, 2, 20, 30, 45] //  纵坐标        }]    },    options: {        responsive: true,        title: {        display: true,        text: 'Chart.js 折线图' //  图表名称        }    }};    new Chart(ctx, options);</script><pre><code>&#123;% chart 90% 300 %&#125;  //其中 chart 是标签名，endchart 是结束标签，不需要更改，可以通过设置 options 里面的               aspectRatio 属性来调整图表的宽高比例，默认 width 为 100%，height 为 300px。 &#123;    type: 'line',    data: &#123; //  折线图数据    labels: ['8.4', '8.5', '8.6', '8.7', '8.8', '8.9', '8.10'], //  横坐标    datasets: [&#123; //  纵坐标变量名称        label: 'My First dataset',        lineTension: 0,        borderColor: 'rgb(255, 99, 132)', //  线条颜色        data: [0, 10, 5, 2, 20, 30, 45] //  纵坐标        &#125;]    &#125;,    options: &#123;        responsive: true,        title: &#123;        display: true,        text: 'Chart.js 折线图' //  图表名称        &#125;    &#125;&#125;&#123;% endchart %&#125;</code></pre><h2><span id="相关属性">相关属性</span></h2><table><thead><tr><th>名称</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>backgroundColor</td><td>Color&#x2F;Color[]</td><td>线条背景色</td></tr><tr><td></td><td></td><td></td></tr><tr><td>borderColor</td><td>Color&#x2F;Color[]</td><td>线条颜色</td></tr></tbody></table><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatjs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用存储介绍</title>
      <link href="/2023/04/08/Ceph/1.%E5%AD%98%E5%82%A8%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E5%AD%98%E5%82%A8%E4%BB%8B%E7%BB%8D/"/>
      <url>/2023/04/08/Ceph/1.%E5%AD%98%E5%82%A8%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/%E5%B8%B8%E7%94%A8%E5%AD%98%E5%82%A8%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1><span id></span></h1><span id="more"></span><h1><span id="常用存储介绍">常用存储介绍</span></h1><p><img src="/images/Ceph/%E5%B8%B8%E7%94%A8%E5%AD%98%E5%82%A8%E4%BB%8B%E7%BB%8D/1.jpg"></p><script type="text&#x2F;javascript" src="https://unpkg.com/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://unpkg.com/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://unpkg.com/hexo-simple-mindmap@0.8.0/dist/mindmap.min.css">]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
